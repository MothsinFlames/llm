---
source: "https://zhuanlan.zhihu.com/p/700588653"
---
前几天，幻方发布的 [DeepSeek-V2](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2405.04434) 引起了大家的热烈讨论。首先，最让人哗然的是1块钱100万token的价格，普遍比现有的各种竞品API便宜了两个数量级，以至于有人调侃“这个价格哪怕它输出乱码，我也会认为这个乱码是一种艺术”；其次，从模型的技术报告看，如此便宜的价格背后的关键技术之一是它新提出的MLA（ **M** ulti-head **L** atent **A** ttention），这是对GQA的改进，据说能比GQA更省更好，也引起了读者的广泛关注。
接下来，本文将跟大家一起梳理一下从MHA、MQA、GQA到MLA的演变历程，并着重介绍一下MLA的设计思路。
![](https://pica.zhimg.com/v2-3a7be6f050ec186174ddadaf95f7bfc4_1440w.jpg)
## MHA
MHA（ **M**ulti-**H**ead **A**ttention），开山之作 [《Attention is all you need》](https://link.zhihu.com/?target=https%3A//kexue.fm/archives/4765) 提出的一种Attention形式
在数学上，MHA等价于多个独立的单头注意力的拼接，假设输入的（行）向量序列为 $\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_l$ ，其中 $\boldsymbol{x}_i\in\mathbb{R}^d$ ，那么MHA可以形式地记为
$$\begin{gathered}
\boldsymbol{o}_t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\right] \\
\boldsymbol{o}_t^{(s)} = Attention\left(\boldsymbol{q}_t^{(s)}, \boldsymbol{k}_{\leq t}^{(s)} ,\boldsymbol{v}_{\leq t}^{(s)}\right)\triangleq\frac{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)\boldsymbol{v}_i^{(s)}}{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)} \\
\boldsymbol{q}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d\times d_k}\\  
\boldsymbol{k}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_k^{(s)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_k^{(s)}\in\mathbb{R}^{d\times d_k} \\
\boldsymbol{v}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d\times d_v}
\end{gathered}$$
简单起见，这里省略了Attention矩阵的缩放因子。实践上，常见的设置是 $d_k = d_v = d / h$ ，对于LLAMA2-7b有 $d=4096, h=32, d_k = d_v = 128$ ，LLAMA2-70b则是 $d=8192,h=64, d_k = d_v = 128$
由于这里只考虑了主流的自回归LLM所用的Causal Attention，因此在token by token递归生成时，新预测出来的第 $t+1$ 个token，并不会影响到已经算好的 $\boldsymbol{k}_{\leq t}^{(s)} ,\boldsymbol{v}_{\leq t}^{(s)}$ ，因此这部分结果我们可以缓存下来供后续生成调用，避免不必要的重复计算，这就是所谓的 [KV Cache](https://zhida.zhihu.com/search?content_id=243816950&content_type=Article&match_order=1&q=KV+Cache&zhida_source=entity) 。
而后面的MQA、GQA、MLA，都是围绕“如何减少KV Cache同时尽可能地保证效果”这个主题发展而来的产物。
**优点：**
- 多头并行计算，提升效率；
- 每个head可以关注不同子空间的特征信息，丰富表达能力。
**缺点：**
- 每次生成都需要计算历史的QKV矩阵，显存和计算量较大。
## 瓶颈
一个自然的问题是：为什么降低KV Cache的大小如此重要？
众所周知，一般情况下LLM的推理都是在GPU上进行，单张GPU的显存是有限的，一部分我们要用来存放模型的参数和前向计算的激活值，这部分依赖于模型的体量，选定模型后它就是个常数；另外一部分我们要用来存放模型的KV Cache，这部分不仅依赖于模型的体量，还依赖于模型的输入长度，也就是在推理过程中是动态增长的，当Context长度足够长时，它的大小就会占主导地位，可能超出一张卡甚至一台机（8张卡）的总显存量。
在GPU上部署模型的原则是：能一张卡部署的，就不要跨多张卡；能一台机部署的，就不要跨多台机。这是因为“卡内通信带宽 > 卡间通信带宽 > 机间通信带宽”，由于“木桶效应”，模型部署时跨的设备越多，受设备间通信带宽的的“拖累”就越大，事实上即便是单卡H100内SRAM与HBM的带宽已经达到了3TB/s，但对于Short Context来说这个速度依然还是推理的瓶颈，更不用说更慢的卡间、机间通信了。
所以，减少KV Cache的目的就是要实现在更少的设备上推理更长的Context，或者在相同的Context长度下让推理的batch size更大，从而实现更快的推理速度或者更大的吞吐总量。当然，最终目的都是为了实现更低的推理成本。
要想更详细地了解这个问题，读者可以进一步阅读 [《FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness》](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2205.14135) 、 [《A guide to LLM inference and performance》](https://link.zhihu.com/?target=https%3A//www.baseten.co/blog/llm-transformer-inference-guide/) 、 [《LLM inference speed of light》](https://link.zhihu.com/?target=https%3A//zeux.io/2024/03/15/llm-inference-sol/) 等文章，这里就不继续展开了（主要是笔者水平也有限，唯恐说多错多）。
## MQA
MQA，即“ **M**ulti-**Q**uery **A**ttention”，是减少KV Cache的一次非常朴素的尝试，
[《Fast Transformer Decoding: One Write-Head is All You Need》](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1911.02150) ，2019
意味着早在LLM火热之前，减少KV Cache就已经是研究人员非常关注的一个课题了。
MQA的思路很简单，直接让所有Attention Head共享同一个K、V，用公式来说，就是取消MHA所有的 $\boldsymbol{k},\boldsymbol{v}$ 的上标 ${}^{(s)}$ ：
$$\begin{gathered}
\boldsymbol{o}_t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\right] \\
\boldsymbol{o}_t^{(s)} = Attention\left(\boldsymbol{q}_t^{(s)}, \boldsymbol{k}_{\leq t}^{\color{#ccc}{\smash{\bcancel{(s)}}}} ,\boldsymbol{v}_{\leq t}^{\color{#ccc}{\smash{\bcancel{(s)}}}}\right)\triangleq\frac{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{\color{#ccc}{\smash{\bcancel{(s)}}}}{}^{\top}\right)\boldsymbol{v}_i^{\color{#ccc}{\smash{\bcancel{(s)}}}}}{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{\color{#ccc}{\smash{\bcancel{(s)}}}}{}^{\top}\right)} \\
\boldsymbol{q}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d\times d_k}\\
\boldsymbol{k}_i^{\color{#ccc}{\smash{\bcancel{(s)}}}} = \boldsymbol{x}_i\boldsymbol{W}_k^{\color{#ccc}{\smash{\bcancel{(s)}}}}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_k^{\color{#ccc}{\smash{\bcancel{(s)}}}}\in\mathbb{R}^{d\times d_k} \\
\boldsymbol{v}_i^{\color{#ccc}{\smash{\bcancel{(s)}}}} = \boldsymbol{x}_i\boldsymbol{W}_v^{\color{#ccc}{\smash{\bcancel{(s)}}}}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{\color{#ccc}{\smash{\bcancel{(s)}}}}\in\mathbb{R}^{d\times d_v}
\end{gathered}
$$
使用MQA的模型包括 [PaLM](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2204.02311) 、 [StarCoder](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2305.06161) 、 [Gemini](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2312.11805) 等。很明显，MQA直接将KV Cache减少到了原来的 $1/h$ ，这是非常可观的，单从节省显存角度看已经是天花板了。
效果方面，目前看来大部分任务的损失都比较有限，且MQA的支持者相信这部分损失可以通过进一步训练来弥补回。此外，注意到MQA由于共享了K、V，将会导致Attention的参数量减少了将近一半，而为了模型总参数量的不变，通常会相应地增大FFN/GLU的规模，这也能弥补一部分效果损失。
**优点：**
- 节省显存，KV Cache降低为原始的1/h; 减少计算和通信开销，提升推理速度。
**缺点：**
- 性能下降：KV Cache压缩过于严重，影响模型训练稳定性和模型效果。
## GQA
有人担心MQA对KV Cache的压缩太严重，以至于会影响模型的学习效率以及最终效果。
为此，MHA与MQA之间的过渡版本GQA（ **G**rouped-**Q**uery **A**ttention）应运而生， [《GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints》](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2305.13245)
GQA的思想-将所有Head分为 $g$ 个组（ $g$ 可以整除 $h$ ），每组共享同一对K、V，用数学公式表示为
$$\begin{gathered}
\boldsymbol{o}_t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\right] \\
\boldsymbol{o}_t^{(s)} = Attention\left(\boldsymbol{q}_t^{(s)}, \boldsymbol{k}_{\leq t}^{\color{red}{(\lceil sg/h\rceil)}} ,\boldsymbol{v}_{\leq t}^{\color{red}{(\lceil sg/h\rceil)}}\right)\triangleq\frac{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{\color{red}{(\lceil sg/h\rceil)}}{}^{\top}\right)\boldsymbol{v}_i^{\color{red}{(\lceil sg/h\rceil)}}}{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{\color{red}{(\lceil sg/h\rceil)}}{}^{\top}\right)} \\
\boldsymbol{q}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d\times d_k}\\  
\boldsymbol{k}_i^{\color{red}{(\lceil sg/h\rceil)}} = \boldsymbol{x}_i\boldsymbol{W}_k^{\color{red}{(\lceil sg/h\rceil)}}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_k^{\color{red}{(\lceil sg/h\rceil)}}\in\mathbb{R}^{d\times d_k} \\
\boldsymbol{v}_i^{\color{red}{(\lceil sg/h\rceil)}} = \boldsymbol{x}_i\boldsymbol{W}_v^{\color{red}{(\lceil sg/h\rceil)}}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{\color{red}{(\lceil sg/h\rceil)}}\in\mathbb{R}^{d\times d_v}
\end{gathered}$$
这里的 $⌈⋅⌉\lceil\cdot\rceil\lceil\cdot\rceil$ 是上取整符号。GQA提供了MHA到MQA的自然过渡，当 $g=h$ 时就是MHA， $g=1$ 时就是MQA，当 $1 < g < h$ 时，它只将KV Cache压缩到 $g/h$ ，压缩率不如MQA，但同时也提供了更大的自由度，效果上更有保证。
GQA代表：Meta： [LLAMA2-70B](https://link.zhihu.com/?target=https%3A//llama.meta.com/llama2/) ，及 [LLAMA3](https://link.zhihu.com/?target=https%3A//llama.meta.com/llama3/) 全系列，还有 [TigerBot](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2312.08688) 、 [DeepSeek-V1](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2401.02954) 、 [StarCoder2](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2402.19173) 、 [Yi](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2403.04652) 、 [ChatGLM2](https://link.zhihu.com/?target=https%3A//github.com/THUDM/ChatGLM2-6B) 、 [ChatGLM3](https://link.zhihu.com/?target=https%3A//github.com/THUDM/ChatGLM3) 等
相比使用MQA的模型更多（ChatGLM虽然在它的介绍中说自己是MQA，但实际是 $g=2$ 的GQA）。
在llama2/3-70B中，GQA的 $g=8$ ，其他用了GQA的同体量模型基本上也保持了这个设置，这并非偶然，而是同样出于推理效率的考虑。我们知道，70B这个体量的模型，如果不进行极端的量化，那么不可能部署到单卡（A100/H100 80G）上。单卡不行，那么就能单机了，一般情况下一台机可以装8张卡，刚才我们说了，Attention的每个Head实际上是独立运算然后拼接起来的，当 $g=8$ 时，正好可以每张卡负责计算一组K、V对应的Attention Head，这样可以在尽可能保证K、V多样性的同时最大程度上减少卡间通信。
**优点：**
- 性能和效率之间平衡：保证KV多样性同时，减少KV Cache大小；
- 稳定性：相比MQA，训练过程较为稳定；
**缺点：** 需人为合理设置组数g。
## MLA
![](https://pic4.zhimg.com/v2-7196f75caf3ba4edb698035db636d131_1440w.jpg)
MLA（ **M**ulti-head **L**atent **A**ttention）就相对容易一些了。
DeepSeek-V2的技术报告里-从低秩投影的角度引入MLA的，以至于有部分读者提出“为什么 [LoRA](https://zhida.zhihu.com/search?content_id=243816950&content_type=Article&match_order=1&q=LoRA&zhida_source=entity) 提出这么久了，直到MLA才提出对KV Cache低秩分解的做法”之类的疑问。
然而，笔者认为低秩投影这个角度并不贴近本质，因为要说低秩投影的话，事实上只要我们将GQA的所有K、V叠在一起，就会发现GQA也相当于在做低秩投影：
$$\underbrace{\left[\boldsymbol{k}_i^{(1)},\cdots,\boldsymbol{k}_i^{(g)},\boldsymbol{v}_i^{(1)},\cdots,\boldsymbol{v}_i^{(g)}\right]}_{\boldsymbol{c}_i\in\mathbb{R}^{g(d_k+d_v)}} = \boldsymbol{x}_i \underbrace{\left[\boldsymbol{W}_k^{(1)},\cdots,\boldsymbol{W}_k^{(g)},\boldsymbol{W}_v^{(1)},\cdots,\boldsymbol{W}_v^{(g)}\right]}_{\boldsymbol{W}_c\in\mathbb{R}^{d\times g(d_k+d_v)}} $$
这里我们将所有 $\boldsymbol{k}_i^{(s)},\boldsymbol{v}_i^{(s)}$ 拼在一起记为 $\boldsymbol{c}_i$ ，相应的投影矩阵也拼在一起记为 $\boldsymbol{W}_c$ ，注意到一般都有 $d_c = g(d_k+d_v) < d$ ，所以 $\boldsymbol{x}_i$ 到 $\boldsymbol{c}_i$ 的变换就是一个低秩投影。所以，MLA的本质改进不是低秩投影，而是低秩投影之后的工作。
### Part 1：MLA without RoPE
GQA在投影之后做了什么呢？首先它将向量对半分为两份分别作为K、V，然后每一份又均分为 $g$ 份，每一份复制 $h/g$ 次，以此来“凑”够 $h$ 个Attention Head所需要的K、V。我们知道分割、复制都是简单的线性变换，所以MLA的第一个想法是**将这些简单的线性变换换成一般的线性变换，以增强模型的能力**：
$$\begin{gathered}
\boldsymbol{o}_t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\right] \\
\boldsymbol{o}_t^{(s)} = Attention\left(\boldsymbol{q}_t^{(s)}, \boldsymbol{k}_{\leq t}^{(s)} ,\boldsymbol{v}_{\leq t}^{(s)}\right)\triangleq\frac{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)\boldsymbol{v}_i^{(s)}}{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)} \\
\boldsymbol{q}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d\times d_k}\\  
\boldsymbol{k}_i^{(s)} = \boldsymbol{c}_i\boldsymbol{W}_k^{(s)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_k^{(s)}\in\mathbb{R}^{d_c\times d_k} \\
\boldsymbol{v}_i^{(s)} = \boldsymbol{c}_i\boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_c\times d_v} \\[10pt]
\boldsymbol{c}_i = \boldsymbol{x}_i \boldsymbol{W}_c\in\mathbb{R}^{d_c},\quad \boldsymbol{W}_c\in\mathbb{R}^{d\times d_c}
\end{gathered}$$
#### 理解
通过一个低秩矩阵将输入 $\boldsymbol{x}_i \in \mathbb{R}^{d}$ **降维映射** 为 $\boldsymbol{c}_i \in \mathbb{R}^{d_c}$ （论文中叫latent vectore, 它没有上标 $(S)^{(S)}$ 说明与head无关，多个head之间是共享的）， **于是原始KV Cache缓存每个head的** $\boldsymbol{k}_i^{(s)},\boldsymbol{v}_i^{(s)}$ **变为缓存** $\boldsymbol{c}_i$ ；然后在通过 **升维** 矩阵 $\boldsymbol{W}_k^{(s)}$ 和$\boldsymbol{W}_v^{(s)}$ 将 $\boldsymbol{c}_i \in \mathbb{R}^{d_c}$ 映射为 $\boldsymbol{k}_i^{(s)}、\boldsymbol{v}_i^{(s)}$ ，后续就执行Attention就行了。
$2*n_{h}*d_{h}*l \Rightarrow d_{c}*l$ ，其中 $n_{h}$ 表示head的数量， $d_{h}$ 表示每个head的 $\boldsymbol{k}_i^{(s)}、\boldsymbol{v}_i^{(s)}$ 的dim， $l$ 表示layers的数量， $d_{c}(\ll n_hd_h)$ 表示的 $\boldsymbol{c}_i$ 的维度。
**矩阵吸收合并：**
这样虽然减少了缓存，但推理时每次都要经过升维矩阵运算得到K、V，没有减少计算量？实际过程中，可以使用矩阵吸收(absorbed)的方式将矩阵进行合并。  $$\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)\top} = \left( \boldsymbol{x}_t \boldsymbol{W}_q^{(s)} \right) \left( \boldsymbol{c}_i \boldsymbol{W}_k^{(s)} \right)^\top = \boldsymbol{x}_t \left( \boldsymbol{W}_q^{(s)} \boldsymbol{W}_k^{(s)\top} \right) \boldsymbol{c}_i^\top$$ 这样在Q和K计算时，权重矩阵 $\boldsymbol{W}_q^{(s)} \boldsymbol{W}_k^{(s)}$ 可以合并在一块；同时 $\boldsymbol{W}_v^{(s)}$ 可以吸收到 $\boldsymbol{o}_t^{(s)}$ 上，被Concat后的网络层权重吸收合并。这样就让 $\boldsymbol{c}_i$ 直接参与计算，不需要再额外计算出来K和V了。  
**Q矩阵降维：**
在DeepSeek-V2中，为了节约训练过程中参数量，也对Q进行了低秩投影，这个 **与KV Cache无关** 。将上述Q的计算方式换成下列方式。  
$$\begin{aligned}  \boldsymbol{c}'_i &= \boldsymbol{x}_i \boldsymbol{W}'_c \in \mathbb{R}^{d'_c},  & \boldsymbol{W}'_c &\in \mathbb{R}^{d \times d'_c} \\  \boldsymbol{q}_i^{(s)} &= \boldsymbol{c}'_i\boldsymbol{W}_q^{(s)} \in \mathbb{R}^{d_k}, & \boldsymbol{W}_q^{(s)} &\in \mathbb{R}^{d'_c \times d_k} \\   \end{aligned}$$
-----
然而，理论上这样是能增加模型能力，但别忘了GQA的主要目的是减少KV Cache，出于节省计算和通信成本的考虑，我们一般会缓存的是投影后的 $\boldsymbol{k}_i, \boldsymbol{v}_i$ 而不是投影前的 $\boldsymbol{c}_i$ 或 $\boldsymbol{x}_i$ ，而MLA的这个做法，通过不同的投影矩阵再次让所有的K、V Head都变得各不相同，那么KV Cache的大小就恢复成跟MHA一样大了，违背了GQA的初衷。
对此，MLA发现，我们可以结合Dot-Attention的具体形式，通过一个简单但不失巧妙的恒等变换来规避这个问题。首先，在训练阶段还是照常进行，此时优化空间不大；然后，在推理阶段，我们利用
$$\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top} = \left(\boldsymbol{x}_t\boldsymbol{W}_q^{(s)}\right) \left(\boldsymbol{c}_i\boldsymbol{W}_k^{(s)}\right){}^{\top} = \boldsymbol{x}_t\left(\boldsymbol{W}_q^{(s)}\boldsymbol{W}_k^{(s)}{}^{\top}\right)\boldsymbol{c}_i^{\top}$$
这意味着推理阶段，我们可以将 $\boldsymbol{W}_q^{(s)}\boldsymbol{W}_k^{(s)}{}^{\top}$ 合并起来作为Q的投影矩阵，那么 $\boldsymbol{c}_i$ 则取代了原本的 $\boldsymbol{k}_i$ ，同理，在 $\boldsymbol{o}_t$ 后面我们还有一个投影矩阵，于是 $\boldsymbol{v}_i^{(s)} = \boldsymbol{c}_i\boldsymbol{W}_v^{(s)}$ 的 $\boldsymbol{W}_v^{(s)}$ 也可以吸收到后面的投影矩阵中去，于是等效地 $\boldsymbol{v}_i$ 也可以用 $\boldsymbol{c}_i$ 代替，也就是说此时KV Cache只需要存下所有的 $\boldsymbol{c}_i$ 就行，而不至于存下所有的 $\boldsymbol{k}_i^{(s)}$ 、 $\boldsymbol{v}_i^{(s)}$ 。注意到 $\boldsymbol{c}_i$ 跟 ${}^{(s)}$ 无关，也就是说是所有头共享的，即MLA在推理阶段它可以恒等变换为一个MQA。
再次强调，**本文的主题是一直都是减少KV Cache**，那到目前为止，MLA做到了什么呢？**答案是通过不同的投影矩阵来增强了GQA的能力，并且推理时可以保持同样大小的KV Cache**。**那么反过来，如果我们只要优于GQA，那么是不是就可以再次减少KV Cache了？** 换言之， $d_c$ 没必要取 $g(d_k+d_v)$ ，而是取更小的值（DeepSeek-V2取了512），从而进一步压缩KV Cache，这就是MLA的核心思想。
（注：这里有一个细节，就是 $\boldsymbol{W}_q^{(s)}\boldsymbol{W}_k^{(s)}{}^{\top}$ 合并成一个矩阵的恒等变换，理论上只有在无限精度下才成立，实际上如果我们使用单精度尤其是BF16的话，经过变换后的精度损失往往还是挺明显的，经过多层累积后可能放大到比较可观的程度，这里可能要根据实际误差看要不要做一些后处理。）
### Part 2 MLA with RoPE
一切似乎都很完美，看上去一个又好又省的理想设计就要出炉了。不过别急，当我们再深入思考一下就会发现，到目前为止的MLA有一个难以绕开的缺陷——不兼容 [RoPE（旋转位置编码）](https://link.zhihu.com/?target=https%3A//kexue.fm/archives/8265) 。
刚才我们说了，MLA之所以能保持跟GQA一样大小的KV Cache，其关键一步是“将 $\boldsymbol{W}_q^{(s)}\boldsymbol{W}_k^{(s)}{}^{\top}$ 合并成一个（跟位置无关的）矩阵作为Q的投影矩阵”，但如果加了RoPE的话，这一步就无法实现了。这是因为RoPE是一个跟位置相关的、 $d_k\times d_k$ 的分块对角矩阵 $\boldsymbol{\mathcal{R}}_m$ ，满足 $\boldsymbol{\mathcal{R}}_m\boldsymbol{\mathcal{R}}_n^{\top}=\boldsymbol{\mathcal{R}}_{m-n}$ ，MLA加入RoPE之后会让 $\boldsymbol{W}_q^{(s)}\boldsymbol{W}_k^{(s)}{}^{\top}$ 之间多插入了一项 $\boldsymbol{\mathcal{R}}_{t-i}$ ：
$$\begin{gathered}
\boldsymbol{q}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i}\color{#000000}\quad,\quad\boldsymbol{k}_i^{(s)} = \boldsymbol{c}_i\boldsymbol{W}_k^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i} \\
\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top} = \left(\boldsymbol{x}_t\boldsymbol{W}_q^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_t}\color{#000000}\right) \left(\boldsymbol{c}_i\boldsymbol{W}_k^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i}\color{#000000}\right){}^{\top} = \boldsymbol{x}_t\left(\boldsymbol{W}_q^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_{t-i}}\color{#000000}\boldsymbol{W}_k^{(s)}{}^{\top}\right)\boldsymbol{c}_i^{\top}
\end{gathered}$$
**这里的 $\boldsymbol{W}_q^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_{t-i}}\color{#000000}\boldsymbol{W}_k^{(s)}{}^{\top}$ 就无法合并为一个固定的投影矩阵了（跟位置差 $t-i$ 相关），从而MLA的想法无法结合RoPE实现。**
前段时间，笔者也很荣幸跟DeepSeek团队讨论过这个问题，但这个问题可以说非常本质，所以当时笔者实际上也没能提出什么有效的建议。最简单的方式是放弃RoPE，换用其他基于Attention Bias的位置编码，如 [ALIBI](https://link.zhihu.com/?target=https%3A//kexue.fm/archives/9431%23ALIBI) ，但DeepSeek的实验显示它明显不如RoPE（注意，MLA不是不能加RoPE，而是加了RoPE之后无法用恒等变换技巧来减少KV Cache），笔者也提议过换 [Sandwich](https://link.zhihu.com/?target=https%3A//kexue.fm/archives/9431%23Sandwich) ，它不像ALIBI单调衰减到负无穷，估计效果会好些，但感觉是治标不治本。还有一个折中的办法是将 $\boldsymbol{q}_i$ 的输入也改为 $\boldsymbol{c}_i$ ，然后RoPE加在 $\boldsymbol{c}_i$ 之后，即
$$\boldsymbol{q}_i^{(s)} = \boldsymbol{c}_i\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i}\color{#000000}\boldsymbol{W}_q^{(s)},\quad\boldsymbol{k}_i^{(s)} = \boldsymbol{c}_i\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i}\color{#000000}\boldsymbol{W}_k^{(s)}$$
这样 $\boldsymbol{\mathcal{R}}_i$ 就可以吸收到 $\boldsymbol{c}_i$ 中去，但这样就没有 $\boldsymbol{\mathcal{R}}_m\boldsymbol{\mathcal{R}}_n^{\top}=\boldsymbol{\mathcal{R}}_{m-n}$ 的运算了，此时的RoPE不再是通过绝对位置实现相对位置，而单纯是在Q、K上加绝对位置，让模型自己想办法提炼相对位置信息。
最后发布的MLA，采取了一种混合的方法——**每个Attention Head的Q、K新增 $d_r$ 个维度用来添加RoPE**，**其中K新增的维度每个Head共享：**
$$\begin{gathered} \boldsymbol{o}_t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\right] \\
\boldsymbol{o}_t^{(s)} = Attention\left(\boldsymbol{q}_t^{(s)}, \boldsymbol{k}_{\leq t}^{(s)} ,\boldsymbol{v}_{\leq t}^{(s)}\right)\triangleq\frac{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)\boldsymbol{v}_i^{(s)}}{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)} \\
\boldsymbol{q}_i^{(s)} = \left[\boldsymbol{x}_i\boldsymbol{W}_{qc}^{(s)}, \boldsymbol{x}_i\boldsymbol{W}_{qr}^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i}\right]\in\mathbb{R}^{d_k + d_r},\quad \boldsymbol{W}_{qc}^{(s)}\in\mathbb{R}^{d\times d_k},\boldsymbol{W}_{qr}^{(s)}\in\mathbb{R}^{d\times d_r}\\
\boldsymbol{k}_i^{(s)} = \left[\boldsymbol{c}_i\boldsymbol{W}_{kc}^{(s)}, \boldsymbol{x}_i\boldsymbol{W}_{kr}^{\color{#ccc}{\smash{\bcancel{(s)}}}}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i}\right]\in\mathbb{R}^{d_k+d_r},\quad \boldsymbol{W}_{kc}^{(s)}\in\mathbb{R}^{d_c\times d_k}, \boldsymbol{W}_{kr}^{\color{#ccc}{\smash{\bcancel{(s)}}}}\in\mathbb{R}^{d\times d_r} \\
\boldsymbol{v}_i^{(s)} = \boldsymbol{c}_i\boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_c\times d_v} \\
\boldsymbol{c}_i = \boldsymbol{x}_i \boldsymbol{W}_c\in\mathbb{R}^{d_c},\quad \boldsymbol{W}_c\in\mathbb{R}^{d\times d_c} \end{gathered}$$
这样一来，没有RoPE的维度就可以重复“Part 1”的操作，在推理时KV Cache只需要存 $\boldsymbol{c}_i$ ，新增的带RoPE的维度就可以用来补充位置信息，并且由于所有Head共享，所以也就只有在K Cache这里增加了 $d_r$ 个维度，原论文取了 $d_r = d_k / 2 = 64$ ，相比原本的 $d_c=512$ ，增加的幅度不大。
### Part 3
最后有一个细节，就是MLA的最终版本，还将Q的输入也改为了低秩投影形式，这与减少KV Cache无关，主要是为了减少训练期间参数量和相应的梯度（原论文说的是激活值，个人表示不大理解）所占的显存：
$$\begin{gathered} \boldsymbol{o}_t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\right] \\
\boldsymbol{o}_t^{(s)} = Attention\left(\boldsymbol{q}_t^{(s)}, \boldsymbol{k}_{\leq t}^{(s)} ,\boldsymbol{v}_{\leq t}^{(s)}\right)\triangleq\frac{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)\boldsymbol{v}_i^{(s)}}{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)} \\
\boldsymbol{q}_i^{(s)} = \left[\boldsymbol{c}_i'\boldsymbol{W}_{qc}^{(s)}, \boldsymbol{c}_i'\boldsymbol{W}_{qr}^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i}\right]\in\mathbb{R}^{d_k + d_r},\quad \boldsymbol{W}_{qc}^{(s)}\in\mathbb{R}^{d_c'\times d_k},\boldsymbol{W}_{qr}^{(s)}\in\mathbb{R}^{d_c'\times d_r}\\
\boldsymbol{k}_i^{(s)} = \left[\boldsymbol{c}_i\boldsymbol{W}_{kc}^{(s)}, \boldsymbol{x}_i\boldsymbol{W}_{kr}^{\color{#ccc}{\smash{\bcancel{(s)}}}}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i}\right]\in\mathbb{R}^{d_k+d_r},\quad \boldsymbol{W}_{kc}^{(s)}\in\mathbb{R}^{d_c\times d_k}, \boldsymbol{W}_{kr}^{\color{#ccc}{\smash{\bcancel{(s)}}}}\in\mathbb{R}^{d\times d_r} \\
\boldsymbol{v}_i^{(s)} = \boldsymbol{c}_i\boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_c\times d_v} \\
[10pt] \boldsymbol{c}_i' = \boldsymbol{x}_i \boldsymbol{W}_c'\in\mathbb{R}^{d_c'},\quad \boldsymbol{W}_c'\in\mathbb{R}^{d\times d_c'} \\
\boldsymbol{c}_i = \boldsymbol{x}_i \boldsymbol{W}_c\in\mathbb{R}^{d_c},\quad \boldsymbol{W}_c\in\mathbb{R}^{d\times d_c} \\
\end{gathered}$$
注意 $\boldsymbol{k}_i^{(s)}$ 中的第二项，带RoPE的部分，其输入还是 $\boldsymbol{x}_i$ 而不是 $\boldsymbol{c}_i$ ，这里保持了原论文的设置，不是笔误， $d_c'$ 原论文的取值是1536，跟 $d_c=512$ 不同。同时，我们把带RoPE的MHA放在下面，方便大家对比：
$$\begin{gathered}
\boldsymbol{o}_t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\right] \\[10pt]
\boldsymbol{o}_t^{(s)} = Attention\left(\boldsymbol{q}_t^{(s)}, \boldsymbol{k}_{\leq t}^{(s)} ,\boldsymbol{v}_{\leq t}^{(s)}\right)\triangleq\frac{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)\boldsymbol{v}_i^{(s)}}{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)} \\[15pt]
\boldsymbol{q}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d\times d_k}\\  
\boldsymbol{k}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_k^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_k^{(s)}\in\mathbb{R}^{d\times d_k} \\
\boldsymbol{v}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d\times d_v}
\end{gathered}$$
可以发现，其实在训练阶段，除了多了一步低秩投影以及只在部分维度加RoPE外，MLA与Q、K的Head Size由 $d_k$ 换成 $d_k + d_r$ 的MHA基本无异。
推理阶段的MLA则改为
$$\begin{gathered} \boldsymbol{o}_t = \left[\boldsymbol{o}_t^{(1)}\boldsymbol{W}_v^{(1)}, \boldsymbol{o}_t^{(2)}\boldsymbol{W}_v^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\boldsymbol{W}_v^{(h)}\right] \\[10pt] \boldsymbol{o}_t^{(s)} = Attention\left(\boldsymbol{q}_t^{(s)}, \boldsymbol{k}_{\leq t}^{(s)} ,\boldsymbol{c}_{\leq t}\right)\triangleq\frac{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)\boldsymbol{c}_i}{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)} \\[15pt] \boldsymbol{q}_i^{(s)} = \left[\boldsymbol{c}_i'\boldsymbol{W}_{qc}^{(s)}\boldsymbol{W}_{kc}^{(s)}{}^{\top}, \boldsymbol{c}_i'\boldsymbol{W}_{qr}^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i}\right]\in\mathbb{R}^{d_c + d_r}\\  \boldsymbol{k}_i^{(s)} = \left[\boldsymbol{c}_i, \boldsymbol{x}_i\boldsymbol{W}_{kr}^{\color{#ccc}{\smash{\bcancel{(s)}}}}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i}\right]\in\mathbb{R}^{d_c+d_r}\\ \boldsymbol{W}_{qc}^{(s)}\in\mathbb{R}^{d_c'\times d_k},\boldsymbol{W}_{kc}^{(s)}\in\mathbb{R}^{d_c\times d_k},\boldsymbol{W}_{qr}^{(s)}\in\mathbb{R}^{d_c'\times d_r},\boldsymbol{W}_{kr}^{\color{#ccc}{\smash{\bcancel{(s)}}}}\in\mathbb{R}^{d\times d_r} \\[10pt] \boldsymbol{c}_i' = \boldsymbol{x}_i \boldsymbol{W}_c'\in\mathbb{R}^{d_c'},\quad \boldsymbol{W}_c'\in\mathbb{R}^{d\times d_c'} \\ \boldsymbol{c}_i = \boldsymbol{x}_i \boldsymbol{W}_c\in\mathbb{R}^{d_c},\quad \boldsymbol{W}_c\in\mathbb{R}^{d\times d_c} \\ \end{gathered}$$
此时Q、K的Head Size变成了 $d_c + d_r$ ，V的Head Size 则变成了 $d_c$ ，按照原论文的设置，这是 $d_k$ 、 $d_v$ 的4倍。所以实际上MLA在推理阶段做的这个转换，虽然能有效减少KV Cache，但其推理的计算量是增加的。
那为什么还能提高推理效率呢？这又回到“瓶颈”一节所讨论的问题了，我们可以将LLM的推理分两部分：第一个Token的生成（Prefill）和后续每个Token的生成（Generation），Prefill阶段涉及到对输入所有Token的并行计算，然后把对应的KV Cache存下来，这部分对于计算、带宽和显存都是瓶颈，MLA虽然增大了计算量，但KV Cache的减少也降低了显存和带宽的压力，大家半斤八两；但是Generation阶段由于每步只计算一个Token，实际上它更多的是带宽瓶颈和显存瓶颈，因此MLA的引入理论上能明显提高Generation的速度。
还有一个细节充分体现了这个特性。一般的LLM架构参数满足 $h \times d_k = d$ ，即num\_heads \* head\_size = hidden\_size，但DeepSeek-V2不一样，它 $d_k=128,d=5120$ ，但 $h=128$ ，是一般设置的3倍！这是因为MLA的KV Cache大小跟 $h$ 无关，增大 $h$ 只会增加计算量和提升模型能力，但不会增加KV Cache，所以不会带来速度瓶颈。
## 小结
本文简单概述了多头注意力的演变历程，特别是从MHA向MQA、GQA，最终到MLA的变化理念，最后详细展开了对MLA的介绍。在本文中，MLA被视为GQA的一般化，它用投影矩阵的方式替代了GQA的分割、重复，并引入了一个恒等变换技巧来可以进一步压缩KV Cache，同时采用了一种混合方法来兼容RoPE。总的来说，MLA称得上是一种非常实用的注意力变体。




模型地址：[https://huggingface.co/openbmb/MiniCPM3-4B](https://link.zhihu.com/?target=https%3A//huggingface.co/openbmb/MiniCPM3-4B)  
github地址：[https://github.com/OpenBMB/MiniCPM](https://link.zhihu.com/?target=https%3A//github.com/OpenBMB/MiniCPM)  
本项目地址：[应用github](https://link.zhihu.com/?target=https%3A//github.com/LDLINGLINGLING/MiniCPM_Series_Tutorial/blob/main/README_application.md)  
面向人员：对transformer已经有了基本了解，对算法底层有深入了解需求的人，  

## 三代注意力

MiniCPM基于[transformer](https://zhida.zhihu.com/search?content_id=249285961&content_type=Article&match_order=2&q=transformer&zhida_source=entity)架构，MiniCPM1.0使用MHA(muti-head-attention)，MiniCPM2.0使用GQA，MiniCPM3.0使用MLA  
## MiniCPM 1.0 MHA:  
示意图：  
1) 两个token
2) token------byte 2x4
3) byte-------muti-head m x 4x3
4) m x 2 x 3
![](https://pic4.zhimg.com/v2-47126f95cb826a55076e9d9c64ce6979_1440w.jpg)

  
图中与具体实现存在以下差别：  

1. Wq、Wk、Wv的个数不同

图中：在(3)(4)步中，x通过和第i头的Wiq，Wik，Wiv投影得到第i头的qi，ki，vi的值。  
代码中：代码中没有只有一个Wq即self.project\_q，将x与self.project\_q相乘后得到所有头的q值，再对q进行维度的切分，获得qi。  
简而言之：就是图中是先切分后投影，代码是先投影后切分  

2. 图中步骤（4）和（5）之间缺少注意力计算，即图中的Z应该是如下操作得到的：
$$
Attention = Softmax\left( \frac{Q.K^T}{\sqrt{ d_{k} }} \right)V
$$
MiniCPM1.0代码实现（节选）  

1.  首先输入有三个矩阵对[hidden\_state](https://zhida.zhihu.com/search?content_id=249285961&content_type=Article&match_order=1&q=hidden_state&zhida_source=entity) **H** 分别进行投影获得q，k，v三个值，

类似图中（4）操作，但是存在上述第一点差异

```
query = self.project_q(hidden_q) # [b,l,H] 
key = self.project_k(hidden_kv) # 
[b,l,H] value = self.project_v(hidden_kv) # [b,l,H]
```

1.  然后对q, k, v三个值在最后一维H进行切分成 $num\_heads\times h$，

维度都从\[b,l,H\]变成\[b,l,num\_head,h\]。类似图中（4）操作，但是存在上述第一点差异

```
.#都是[b,l,num_head,h]
query = query.view(batch_size, len_q, self.num_heads, self.dim_head).permute(0, 2, 1, 3)
key = key.view(batch_size, len_k, self.num_heads, self.dim_head).permute(0, 2, 1, 3)
value = value.view(batch_size, len_k, self.num_heads, self.dim_head).permute(0, 2, 1, 3)

```

3.进行以下图中的注意力计算，示意图缺乏此操作。
$$
Attention = Softmax\left( \frac{Q.K^T}{\sqrt{ d_{k} }} \right)V
$$
```
# 下面这行计算的是Q*KT，并且加上绝对位置编码
score = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(self.dim_head)
score = score + position_bias

# 下面这行代码是加上了上三角为负无穷大的attentionmask，也就是单向注意力的由来
score = torch.masked_fill(
    score,
    attention_mask.view(batch_size, 1, len_q, len_k) == torch.tensor(False),
    torch.scalar_tensor(float("-inf"), device=score.device, dtype=score.dtype),
)
# 以下是进行softmax操作
score = self.softmax(score)

# 将pad的位置注意力归零
score = torch.masked_fill(
    score,
    attention_mask.view(batch_size, 1, len_q, len_k) == torch.tensor(False),
    torch.scalar_tensor(0, device=score.device, dtype=score.dtype),
)

# 以下这行是计算softmax（q*kT）*v的结果，socre=softmax（q*kT
# (batch_size, num_heads, len_q, len_k) @ (batch_size, num_heads, len_k, dim_head) -> (batch_size, num_heads, len_q, dim_head)
score = torch.matmul(score, value)
```

4.以下实现上图中（5）操作，score的最终输出即为图中的z

```
# 以下两行是将多头聚合成单头
score = score.view(batch_size, self.num_heads, len_q, self.dim_head).permute(0, 2, 1, 3)
score = score.contiguous().view(batch_size, len_q, self.num_heads * self.dim_head)

# 和o矩阵相乘，作为attention的最终输出
score = self.attention_out(score)
```

##   MiniCPM 2.0 GQA

MiniCPM 2.0 的 GQA（**Grouped Query Attention**）：  
MHA的特点
在MHA中，Wq和Wk，Wv的个数是一致的，都是等于num\_heads,这也是MHA名字的由来。  

![](https://picx.zhimg.com/v2-91beb4900ec799e6809fb87e26ea2a0d_1440w.jpg)

#key_MHA和GQA的差异
==**MHA是Q、K、V三者数量都和注意力头数(num\_heads)相同。**==  
==**GQA是Q的数量仍然和注意力头数(num\_heads)相同。将注意力头数(num\_heads)平均分成多个group，k和v每一个group共用**==

![](https://pic1.zhimg.com/v2-1f519befa964070d27a6e2de4d5c09ba_1440w.jpg)

  
在每个头计算[注意力分数](https://zhida.zhihu.com/search?content_id=249285961&content_type=Article&match_order=1&q=%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0&zhida_source=entity)时（下图），Q每个头都是不同的，但是下图中的K和V在同一个group中是共用的。  

![](https://pic1.zhimg.com/v2-e545fe041c57479e2508aa1ee6e26ece_1440w.jpg)

###   GQA的优势：

1.1 模型参数量下降，计算量下降：

以k为例，进行描述，v的原理相同。

1.  如果每一个注意力头ki的shape为\[l,h\],多头汇聚的K=\[k1,k2,....kn\]
2.  那么MHA中多头汇聚K的shape为 \[l,num\_heads\*h\] ,GQA多头汇聚的K的shape为 \[l,num\_groups\*h\] 。
3.  num\_heads=num\_groups\*N，N是每个group有多少个head，明显num\_heads>num\_groups,
4.  因此num\_heads\*h>num\_groups\*h
5.  又因为K=x\*Wk，所以MHA中Wk的维度是\[hidden\_size,num\_heads\*h\]要高于GQA中Wk的维度\[hidden\_size,num\_groups\*h\]
6.  所以GQA可以减少模型参数，也就减少了计算量。

减少了内存：原来 3 x m x 4 x 3----> m x 4 x 3 + 2 x m/k x 4 x 3

1--->1/3+2/3k
k=2，降低1/3
k=4，降低1/2

1.2 减少kv cache：

kvcache的原理就按下不表了，由于kvcach是对之前对话的k和v值的缓存，用空间换时间的做法。  
从上面第一点我们可以知道，GQA的k和v的shape是\[hidden\_size,num\_groups\*h\]，相比MHA的shape\[l,num\_heads\*h\]减少了N倍（N是一个group有多少个head共享）。  
**==因此kvcach的缓存也就下降了N倍，这对模型推理至关重要，即提高了模型的上下文长度（kvcach减少了内存），也增加速度（降低了io\_bound）==**  

GQA的代码（节选则MiniCPM2.0）  
1.  投影获得Q、K、V值

```
query_states = self.q_proj(hidden_states)
key_states = self.k_proj(hidden_states)
value_states = self.v_proj(hidden_states)
```

2.将Q按num\_heads切分，K和V按num\_groups（代码中的num\_key\_value\_heads）切分

```
query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
```

3.为q和k加入rope的位置信息

```
cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)
```

4.如果存在kvcach（代码中的past\_key\_value），将kvcach更新到k和v中

```
if past_key_value is not None:
    cache_kwargs = {"sin": sin, "cos": cos}  # Specific to RoPE models
    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
```

5.将k和v值重复N（代码中的num\_key\_value\_groups）次，这里是为了让k，v能够和q相同的size进行后续注意力操作  

```
key_states = repeat_kv(key_states, self.num_key_value_groups)
value_states = repeat_kv(value_states, self.num_key_value_groups)
```

6.注意力计算

```
attn_output = torch.nn.functional.scaled_dot_product_attention(
    query_states,
    key_states,
    value_states,
    attn_mask=attention_mask,
    dropout_p=self.attention_dropout if self.training else 0.0,
    # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.
    is_causal=self.is_causal and attention_mask is None and q_len > 1,
)
```

7.注意力输出值的多头汇聚后与矩阵O投影，作为[注意力层](https://zhida.zhihu.com/search?content_id=249285961&content_type=Article&match_order=1&q=%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%B1%82&zhida_source=entity)的最终输出值。

```
attn_output = attn_output.transpose(1, 2).contiguous()
attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)

attn_output = self.o_proj(attn_output)
```

## MiniCPM 3.0 的 MLA：

### 示意图

1.  mla示意图：

![](https://picx.zhimg.com/v2-b1c7fc6690a7e949f73cfb2f61efaa2d_1440w.jpg)

### gqa示意图：

![](https://pica.zhimg.com/v2-329e78fe3ee27705b00b91df7e56142c_1440w.jpg)

### MLA和GQA的差异

### 不对k、v进行repeat操作，模型能力上升

GQA是一个group中所有的头共享k和v，所以在计算多头注意力时需要进行复制操作，复制后k的shape从\[l,num\_groups\*h\] 上升到\[l,num\_heads\*h\]：  

```
# gqa的多头kv获取是通过复制同组的k、v进行的
key_states = repeat_kv(key_states, self.num_key_value_groups)
value_states = repeat_kv(value_states, self.num_key_value_groups)
```

  
但是GQA在实验中由于重复的k,v确实导致了性能不如MHA，所以MLA的目标是同时获得GQA的优点（kvcach小）和MHA的优点（k、v的值不重复）。  
从图中可以看出MLA进行的是split操作，但是GQA进行的是repeat操作。  

### 2\. kvcach保存不一致

1.  ==GQA直接保存的是没有进行repeat操作之前的k、v值==
2.  ==MLA保存的是图中的c向量：==

原因是图中可以看到k、v不重复，所以无法共享，如果直接保存k、v值，那么kvcach和MHA的大小是相等的，将比GQA大N\_groups倍。  
MLA直接保存c向量，可以通过c向量和矩阵Wk和Wv计算出k，v值。而且c向量占用显存比较少，增加的计算量也比较有限。  

### 3\. MLA无法直接适应rope位置编码：

为了解决这个问题，将c向量后面一部分直接保留，没有再和Wk、Wv相乘，这一部分的c和q能够保留原始的rope特性。  
简单来说就是，最终计算q和k的注意力时，q和k都有一半的维度保留了原始的注意力做法，从而保留了rope的[位置编码](https://zhida.zhihu.com/search?content_id=249285961&content_type=Article&match_order=3&q=%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81&zhida_source=entity)特性。  

### MiniCPM3.0 的MLA代码（节选）

1.  投影、线性变换操作，获取q,k,v的值（暂未考虑位置编码）

```
# 以下两行是对获取q和k的rope位置编码
cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
# q_pe和k_pe分别为q和k的位置编码
q_pe, k_pe = apply_rotary_pos_emb(q_pe, k_pe, cos, sin, position_ids)

# 对于最终的q值，后面qk_nope_head_dim的维度采用的没有rope位置编码的q_pe
# 前面qk_nope_head_dim采用没有rope位置编码的q_nope
query_states = k_pe.new_empty(bsz, self.num_heads, q_len, self.q_head_dim)
query_states[:, :, :, : self.qk_nope_head_dim] = q_nope
query_states[:, :, :, self.qk_nope_head_dim :] = q_pe

# 对于最终的k，后面qk_nope_head_dim的维度采用的没有rope位置编码的k_pe
# 前面qk_nope_head_dim采用没有rope位置编码的k_nope
key_states = k_pe.new_empty(bsz, self.num_heads, q_len, self.q_head_dim)
key_states[:, :, :, : self.qk_nope_head_dim] = k_nope
key_states[:, :, :, self.qk_nope_head_dim :] = k_pe
```

2.对q和k进行位置编码操作

这里使用的rope位置编码，但是由于rope的外推性，具体采用那种rope的变体，后期考虑开新坑具体的操作是q和k前面的一半维度是不带位置编码的，后面的一半维度是带有位置编码的  

```
# 以下两行是对获取q和k的rope位置编码
cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
# q_pe和k_pe分别为q和k的位置编码
q_pe, k_pe = apply_rotary_pos_emb(q_pe, k_pe, cos, sin, position_ids)

# 对于最终的q值，后面qk_nope_head_dim的维度采用的没有rope位置编码的q_pe
# 前面qk_nope_head_dim采用没有rope位置编码的q_nope
query_states = k_pe.new_empty(bsz, self.num_heads, q_len, self.q_head_dim)
query_states[:, :, :, : self.qk_nope_head_dim] = q_nope
query_states[:, :, :, self.qk_nope_head_dim :] = q_pe

# 对于最终的k，后面qk_nope_head_dim的维度采用的没有rope位置编码的k_pe
# 前面qk_nope_head_dim采用没有rope位置编码的k_nope
key_states = k_pe.new_empty(bsz, self.num_heads, q_len, self.q_head_dim)
key_states[:, :, :, : self.qk_nope_head_dim] = k_nope
key_states[:, :, :, self.qk_nope_head_dim :] = k_pe
```

3.剩下的就是进行常规的softmax等操作求注意力分数了

```
# 这里求q成kT的值
attn_weights = (
    torch.matmul(query_states, key_states.transpose(2, 3)) * self.softmax_scale
)

# upcast attention to fp32
# 这里求注意力分数
attn_weights = nn.functional.softmax(
    attn_weights, dim=-1, dtype=torch.float32
).to(query_states.dtype)
attn_weights = nn.functional.dropout(
    attn_weights, p=self.attention_dropout, training=self.training
)
# 分数和v相乘，获得注意力输出
attn_output = torch.matmul(attn_weights, value_states)
attn_output = attn_output.transpose(1, 2).contiguous()
#聚合多头为单头
attn_output = attn_output.reshape(bsz, q_len, self.num_heads * self.v_head_dim)
# 和o矩阵进行投影，获得attention的最终输出
attn_output = self.o_proj(attn_output)
```