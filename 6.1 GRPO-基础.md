https://zhuanlan.zhihu.com/p/21516563677

GRPO（Group Relative Policy Optimization，分组相对策略优化）是DeepSeek团队提出的一种强化学习算法，专为优化大规模语言模型（LLM）的推理与对齐任务设计。它在PPO（Proximal Policy Optimization）框架上创新引入**组内对比机制**和**无价值函数设计**，显著提升了训练效率与稳定性。以下从核心思想、技术原理、应用场景及优势四个维度展开详解：

---
### 一、核心思想与设计动机
1. **解决PPO的瓶颈问题**  
   PPO需同时维护策略网络（Actor）和价值网络（Critic），导致显存占用高、训练易崩溃。GRPO摒弃价值网络，通过**组内奖励归一化**直接计算相对优势，显存降低40%以上，训练中断率从17%降至2.3%。
2. **组对比机制**  
   针对同一输入提示（prompt），一次性生成多条输出（称为“组”，通常4-16条），在组内进行奖励标准化：  $$A_i = \frac{r_i - \text{mean}(\{r_1, \dots, r_G\})}{\text{std}(\{r_1, \dots, r_G\})}$$
   其中  $r_i$ 为第i条输出的原始奖励， $A_i$ 为相对优势值。该设计将绝对奖励转化为组内相对差异，提升跨任务泛化性。
---
### 二、技术原理与算法细节
1. **目标函数设计**  
   GRPO融合PPO的裁剪机制与KL散度正则化，目标函数为：  
   $$
   J_{\text{GRPO}}(\theta) = \mathbb{E} \left[ \frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t} \min \left( r_t A_{i,t},  \text{clip}(r_t, 1-\epsilon, 1+\epsilon) A_{i,t} \right) - \beta D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}}) \right]
   $$  
   -  $r_t = \pi_\theta / \pi_{\text{old}}$ ：新旧策略概率比  
   -  $\text{clip}$  操作限制策略更新幅度 $\epsilon \approx 0.2$   
   -  $\beta D_{\text{KL}}$  约束策略偏离参考模型（如SFT模型）。

$$
\begin{aligned}
\mathcal{J}_{G R P O}(\theta)  =\mathbb{E}_{\left[q \sim P(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_{\theta_{o l d}}(O \mid q)\right] }
\frac{1}{G} \sum_{i=1}^G\left(\min \left(\frac{\pi_\theta\left(o_i \mid q\right)}{\pi_{\theta_{o l d}}\left(o_i \mid q\right)} A_i, \operatorname{clip}\left(\frac{\pi_\theta\left(o_i \mid q\right)}{\pi_{\theta_{\text {old }}}\left(o_i \mid q\right)}, 1-\varepsilon, 1+\varepsilon\right) A_i\right)-\beta \mathbb{D}_{K L}\left(\pi_\theta \| \pi_{r e f}\right)\right),
\end{aligned}
$$
$$
\mathbb{D}_{K L}\left(\pi_\theta \| \pi_{r e f}\right)=\frac{\pi_{r e f}\left(o_i \mid q\right)}{\pi_\theta\left(o_i \mid q\right)}-\log \frac{\pi_{r e f}\left(o_i \mid q\right)}{\pi_\theta\left(o_i \mid q\right)}-1
$$


1. **动态梯度正则化**  
   KL散度项通过无偏估计实现，避免传统KL计算的高方差问题：  
   $$
   D_{\text{KL}} = \frac{\pi_{\text{ref}}}{\pi_\theta} - \log \frac{\pi_{\text{ref}}}{\pi_\theta} - 1
   $$  
   系数  $\beta$  通常设为0.05–0.1，过高会导致奖励崩溃。
2. **单步更新优化（工程实现）**  
   实际部署中（如VLM-R1项目），GRPO常采用**单步更新策略**：  
   - 每一步训练后同步更新参考策略 $\pi_{\text{ref}} = \pi_{\text{old}}$ 
   - 重要性采样比率  $r_t \equiv 1$ ，目标函数简化为：  
     $$
     J = \mathbb{E}[A_i] - \beta D_{\text{KL}}
     $$  
   此设计提升计算效率且保持数值稳定。

### 三、训练流程与关键技术
1. **四阶段训练框架**（以DeepSeek-R1为例）  

| **阶段**             | **目标**          | **作用**                                  |
| ------------------ | --------------- | --------------------------------------- |
| 监督微调（SFT）冷启动       | 数千个长思维链（CoT）样本  | 提供初始策略  $\pi_{\text{ref}}$，为模型提供初始的推理能力 |
| 面向推理的强化学习（RL）      | 编程、数学、科学和逻辑推理任务 | 组内奖励引导多步推理对齐                            |
| 拒绝采样（RS）和监督微调（SFT） | 生成合成数据，扩展泛化能力   | 筛选高奖励样本，提升模型的非推理能力，如事实知识、对话能力           |
| 针对所有场景的强化学习        | 与人类偏好保持一致，提升安全性 | 多奖励模型混合优化                               |
   通过此流程，DeepSeek-R1在AIME数学竞赛中Pass@1准确率从15.6%提升至71.0%。
2. **程序化奖励函数设计**  
   奖励函数组合显著影响收敛速度：  
   ```python  
   reward_funcs = [  
       xmlcount_reward_func,    # XML结构奖励（权重0.3）  
       soft_format_reward_func, # 宽松格式检测（权重0.2）  
       correctness_reward_func  # 最终答案正确性（权重0.5）  
   ]  
   ```  
 实验表明，组合奖励比单一正确率奖励收敛快41%（2900步 vs 4200步），PSLE准确率提升至61.7%。
 
3. 不使用模特卡罗搜索树和过程奖励（PRM），不使用PRM的原因是
	1. 难以对过程建立较好的reward判断，容易出现reward hack。
	2. 难以控制奖励的粒度，比如句子级、token级、还是cot级。
	3. 数据标注变得复杂，也难以泛化。

4. 不使用[MCTS](https://zhida.zhihu.com/search?content_id=253325529&content_type=Article&match_order=1&q=MCTS&zhida_source=entity)（蒙特卡洛搜索树），这个主要由于句子生成的状态空间太大（比如长链条数据假设上限长度10000，中文词表数30000，状态空间将高达）$30000^{10000}$,状态空间太大，难以训练泛化性良好的Critic 模型估计Value。
5. R1使用少量（数千条）的long-cot数据进行冷启动的SFT目标在于以下几点：
	1. 由于目标策略$\pi_\theta$与base模型的策略$\pi$相差甚远，这会导致收敛速度慢的问题，使用少量long-cot数据冷启动，这类似于深度学习训练中，首先使用较大的learn rate靠近目标，再使用较小的learn rate精调。
	2. 直接使用强化学习，不进行任何的sft将导致不可读、语言混合等问题。这个问题出现的原因，个人认为还是由于目标策略$\pi_\theta$与base模型的策略$\pi$相差甚远，强化学习仅由结果奖励（ORM）粒度太粗，难以控制细粒度的生成问题。
	3. 语言一致性奖励，将混合语言的采样输出进行惩罚，将提高人类可读性。（规则->reward->SFT->reward model）
### 四、应用场景与性能优势
1. **数学推理任务**  
   - GRPO使14B参数模型在GSM8K测试集准确率提升23.5%  
   - 单GPU训练成本降低87%（Unsloth优化）。
2. **高效推理优化**  
   - **S-GRPO变体**（华为）：引入**早退机制**与**衰减奖励**，模型在中间步骤提前生成答案，推理提速60%：  
     - 正确奖励按  $e^{-\lambda t}$  衰减，鼓励尽早退出  
     - 在GSM8K和AIME任务中平均准确率提升6%。
   - **MixGRPO变体**（腾讯）：混合ODE（确定性）/SDE（随机）采样，仅优化关键时间步：  
     - 训练时间减少71%，ImageReward分数从1.088提升至1.629。
3. **联邦学习部署**  
   结合Flower框架的联邦学习方案，实现跨域数据隐私训练：  
   ```
   医院/银行 → 加密梯度 → Flower服务器 → 聚合更新 → 分发模型  
   ```  
   兼顾数据安全与模型性能。
---
### 五、与传统PPO的对比分析
| **维度** | **PPO**     | **GRPO**         |     |
| ------ | ----------- | ---------------- | --- |
| 计算复杂度  | 高（需价值网络）    | 低（无价值网络）         |     |
| 显存占用   | 高（双模型参数）    | 降低40%以上          |     |
| 训练稳定性  | 易崩溃（奖励黑客问题） | KL约束+组归一化，中断率<3% |     |
| 优势计算   | GAE依赖价值估计   | 组内奖励标准化          |     |
| 适用场景   | 通用RL任务      | 大模型推理/对齐任务       |     |
![|775](https://picx.zhimg.com/v2-bf844ae88aa8d7c8d0dc9340af8c07bb_1440w.jpg)

![|600](https://picx.zhimg.com/v2-1f83ac6e40b052ca2090b09a15e47201_1440w.jpg)

### 六、总结与技术意义
GRPO通过**组内相对优化**和**动态KL约束**，实现了大模型RL训练的轻量化与稳定化，成为DeepSeek、华为、腾讯等机构的核心优化方案。其技术演进体现了RLHF从复杂价值函数依赖向对比学习范式的转变，为数学推理、代码生成等复杂任务提供了高效训练路径。未来方向包括：
- **联邦GRPO**：解决跨域数据孤岛问题  
- **多模态扩展**：应用于视觉-语言联合任务（如VLM-R1）  
- **理论深化**：组最优规模与动态  $\beta$  的自适应策略。




https://zhuanlan.zhihu.com/p/20992071041
HuggingFace表示，将以DeepSeek-R1的技术报告为指导，分3个步骤完成这个项目：
- 第1步：用DeepSeek-R1蒸馏高质量语料库，来复制R1-Distill模型。
- 第2步：复制DeepSeek用来构建R1-Zero的纯强化学习（RL）pipeline。这可能涉及为数学、推理和代码整理新的大规模数据集。
- 第3步：通过多阶段训练，从基础模型过渡到RL版本。

![|675](https://pic4.zhimg.com/v2-bfa11970867666599ae60c51c0539657_1440w.jpg)

DeepSeek开源了6个用R1蒸馏的小模型，其中蒸馏版Qwen-1.5甚至能在部分任务上超过GPT-4o。

![|600](https://picx.zhimg.com/v2-4d246555f3e5329e5bdde14ced205ce7_1440w.jpg)

### 1.05 伯克利团队30美元成本复刻 R1-Zero

来自[UC伯克利](https://zhida.zhihu.com/search?content_id=253220837&content_type=Article&match_order=1&q=UC%E4%BC%AF%E5%85%8B%E5%88%A9&zhida_source=entity)博士生潘家怡和另两位研究人员，在CountDown游戏中复现了DeepSeek R1-Zero。
团队验证了通过强化学习RL，3B的基础语言模型也能够自我验证和搜索。
更令人兴奋的是，成本不到30美金（约217元），就可以亲眼见证「啊哈」时刻。
在[消融实验](https://zhida.zhihu.com/search?content_id=253220837&content_type=Article&match_order=1&q=%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C&zhida_source=entity)中，运行了Qwen-2.5-Base（0.5B、1.5B、3B、7B四种参数规模）。
基础模型的参数规模是决定性能的关键。

![|500](https://pic1.zhimg.com/v2-ab076f6a221a6f209f5adb45c165995e_1440w.jpg)

额外的指令微调（SFT）并非是必要的，这也印证了R1-Zero的设计决策。

![|500](https://picx.zhimg.com/v2-512ed1cb83858768b4e3c8589e6f2d8f_1440w.jpg)


这是首个验证[LLM](https://zhida.zhihu.com/search?content_id=253220837&content_type=Article&match_order=1&q=LLM&zhida_source=entity)推理能力的实现可以纯粹通过RL，无需监督微调的开源研究

基础模型和指令模型两者区别：
- 指令模型运行速度快，但最终表现与基础模型相当
- 指令输出的模型更具结构性和可读性

![|500](https://pica.zhimg.com/v2-0129370c084d111f23c4a1279a349a38_1440w.jpg)
具体的RL算法并不重要。PPO、GRPO、PRIME这些算法中，长思维链（Long CoT）都能够涌现，且带来不错的性能表现。

![|500](https://pic2.zhimg.com/v2-3a1bb5bc67e071d313b709db89872859_1440w.jpg)




### 1.09 港科大团队使用8K样本完成7B模型复刻

港科大助理教授[何俊贤](https://zhida.zhihu.com/search?content_id=253220837&content_type=Article&match_order=1&q=%E4%BD%95%E4%BF%8A%E8%B4%A4&zhida_source=entity)的团队（共同一作黄裕振、Weihao Zeng），只用了8K个样本，就在7B模型上复刻出了DeepSeek-R1-Zero和DeepSeek-R1的训练。

项目地址：[https://github.com/hkust-nlp/simpleRL-reason](https://link.zhihu.com/?target=https%3A//github.com/hkust-nlp/simpleRL-reason)

他们以Qwen2.5-Math-7B（基础模型）为起点，直接对其进行强化学习。

整个过程中，没有进行监督微调（SFT），也没有使用奖励模型。

最终，模型在[AIME](https://zhida.zhihu.com/search?content_id=253220837&content_type=Article&match_order=1&q=AIME&zhida_source=entity)基准上实现了33.3%的准确率，在[AMC](https://zhida.zhihu.com/search?content_id=253220837&content_type=Article&match_order=1&q=AMC&zhida_source=entity)上为62.5%，在MATH上为77.2%。

这一表现不仅超越了Qwen2.5-Math-7B-Instruct，并且还可以和使用超过50倍数据量和更复杂组件的PRIME和rStar-MATH相媲美！

![|700](https://pic2.zhimg.com/v2-2b59c99537da34c34cb46052db93401b_1440w.jpg)

     

![|700](https://pic3.zhimg.com/v2-513760b13db384611bd74038bd47feaa_1440w.jpg)

其中，Qwen2.5-7B-SimpleRL-Zero是在Qwen2.5-Math-7B基础模型上仅使用纯PPO方法训练的，仅采用了MATH数据集中的8K样本。

Qwen2.5-7B-SimpleRL则首先通过Long CoT监督微调（SFT）作为冷启动，然后再进行强化学习。

在这两种方法中，团队都只使用了相同的8K MATH样本，仅此而已。

大概在第44步的时候，「啊哈时刻」出现了！模型的响应中，出现了自我反思。

并且，在这个过程中，模型还显现了更长的CoT推理能力和自我反思能力。

![](https://pic4.zhimg.com/v2-2250fd0c18fc236c32dfc8b58583608d_1440w.jpg)


在博客中，研究者详细剖析了实验设置，以及在这个强化学习训练过程中所观察到的现象，例如长链式思考（CoT）和自我反思机制的自发形成。

与DeepSeek R1类似，研究者的强化学习方案极其简单，没有使用奖励模型或MCTS蒙特卡洛树搜索类技术。

他们使用的是[PPO算法](https://zhida.zhihu.com/search?content_id=253220837&content_type=Article&match_order=1&q=PPO%E7%AE%97%E6%B3%95&zhida_source=entity)，并采用基于规则的奖励函数，根据生成输出的格式和正确性分配奖励：

- 如果输出以指定格式提供最终答案且正确，获得+1的奖励
- 如果输出提供最终答案但不正确，奖励设为-0.5
- 如果输出未能提供最终答案，奖励设为-1

该实现基于[OpenRLHF](https://zhida.zhihu.com/search?content_id=253220837&content_type=Article&match_order=1&q=OpenRLHF&zhida_source=entity)。初步试验表明，这个[奖励函数](https://zhida.zhihu.com/search?content_id=253220837&content_type=Article&match_order=2&q=%E5%A5%96%E5%8A%B1%E5%87%BD%E6%95%B0&zhida_source=entity)有助于策略模型快速收敛，产生符合期望格式的输出。

#### **第一部分：SimpleRL-Zero（从头开始的强化学习）**

接下来，研究者为我们分享了训练过程动态分析和一些有趣的涌现模式。

##### **训练过程动态分析**

如下所示，所有基准测试的准确率在训练过程中都在稳步提高，而输出长度则呈现先减少后逐渐增加的趋势。

经过进一步调查，研究者发现，Qwen2.5-Math-7B基础模型在初始阶段倾向于生成大量代码，这可能源于模型原始训练数据的分布特征。

输出长度的首次下降，是因为强化学习训练逐渐消除了这种代码生成模式，转而学会使用自然语言进行推理。

随后，生成长度开始再次增加，此时出现了自我反思机制。

![](https://pica.zhimg.com/v2-f822b8afc3f01162eccf4283eca02eae_1440w.jpg)

训练奖励和输出长度

##### **自我反思机制的涌现**

在训练到第 40 步左右时，研究者观察到：模型开始形成自我反思模式，这正是DeepSeek-R1论文中所描述的「[aha moment](https://zhida.zhihu.com/search?content_id=253220837&content_type=Article&match_order=1&q=aha+moment&zhida_source=entity)」（顿悟时刻）。

![](https://pic1.zhimg.com/v2-9a7f6231c3d76367614946aff6310530_1440w.jpg)


#### **第二部分：SimpleRL（基于模仿预热的强化学习）**

如前所述，研究者在进行强化学习之前，先进行了long CoT SFT预热，使用了8,000个从QwQ-32B-Preview中提取的MATH示例响应作为SFT数据集。

这种冷启动的潜在优势在于：模型在开始强化学习时已具备long CoT思维模式和自我反思能力，从而可能在强化学习阶段实现更快更好的学习效果。

![](https://pic2.zhimg.com/v2-ff43814ff805f4515572f4bd5e1b67c7_1440w.jpg)


与RL训练前的模型（Qwen2.5-Math-7B-Base + 8K QwQ知识蒸馏版本）相比，Qwen2.5-7B-SimpleRL的平均性能显著提升了6.9个百分点。

此外，Qwen2.5-7B-SimpleRL不仅持续优于Eurus-2-7B-PRIME，还在5个基准测试中的3个上超越了Qwen2.5-7B-SimpleRL-Zero。

#### **训练过程分析**

![](https://pica.zhimg.com/v2-b6e8c69d383f17e6888b595aa1eddcb6_1440w.jpg)


![](https://pic1.zhimg.com/v2-419b656fa8494f76bc0eb1dceda0879c_1440w.jpg)


Qwen2.5-SimpleRL的训练动态表现与Qwen2.5-SimpleRL-Zero相似。

有趣的是，尽管研究者先进行了long CoT SFT，但在强化学习初期仍然观察到输出长度减少的现象。

他们推测，这可能是因为从QwQ提取的推理模式不适合小型策略模型，或超出了其能力范围。

因此，模型选择放弃这种模式，转而自主发展新的长链式推理方式。


### 1.1 [open-r1](https://link.zhihu.com/?target=https%3A//github.com/huggingface/open-r1)
https://zhuanlan.zhihu.com/p/21062322587

由huggingface组建，目前刚上线2周，发布了最新进展[open-r1/update-1](https://link.zhihu.com/?target=https%3A//huggingface.co/blog/open-r1/update-1)，在MATH-500任务上接近deepseek的指标，可以在[open-r1/open-r1-eval-leaderboard](https://link.zhihu.com/?target=https%3A//huggingface.co/spaces/open-r1/open-r1-eval-leaderboard)查看指标的排行榜。

![](https://pic3.zhimg.com/v2-d775ce991214e815e8ac6160377be6ac_1440w.jpg)

### **1.2 [mini-deepseek-r1](https://link.zhihu.com/?target=https%3A//github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/mini-deepseek-r1-aha-grpo.ipynb)**

用 GRPO 和倒计时游戏复制出一个简单版本的 R1。

在大约 50 步时，模型学会了正确的格式，即<think>...</think>\\n<answer>...</answer>;在 100 步时，[解方程](https://zhida.zhihu.com/search?content_id=253234935&content_type=Article&match_order=1&q=%E8%A7%A3%E6%96%B9%E7%A8%8B&zhida_source=entity)的成功率约为 25%，并且模型开始用文字进行 “推理”;在 200 步时，收敛变慢，成功率约为 40%。模型开始学习一种新的“格式”，它通过尝试不同的组合并检查结果来解方程，这种方式类似于编程解决问题的方式；在 450 步时，解方程的成功率为 50%，性能仍然在缓慢提升，并且模型保持了从 200 步开始的新格式。

### 1.3 **[open-r1-multimodal](https://link.zhihu.com/?target=https%3A//github.com/EvolvingLMMs-Lab/open-r1-multimodal)**

基于 huggingface/open-r1 和 [deepseek-ai](https://zhida.zhihu.com/search?content_id=253234935&content_type=Article&match_order=1&q=deepseek-ai&zhida_source=entity)/DeepSeek-R1 实现了[多模态](https://zhida.zhihu.com/search?content_id=253234935&content_type=Article&match_order=1&q=%E5%A4%9A%E6%A8%A1%E6%80%81&zhida_source=entity) R1。集成了 Qwen2-VL 系列、Aria-MoE 以及 transformers 中可用的其他[视觉语言](https://zhida.zhihu.com/search?content_id=253234935&content_type=Article&match_order=1&q=%E8%A7%86%E8%A7%89%E8%AF%AD%E8%A8%80&zhida_source=entity)模型（[VLMs](https://zhida.zhihu.com/search?content_id=253234935&content_type=Article&match_order=1&q=VLMs&zhida_source=entity)）。

![](https://pica.zhimg.com/v2-0b4c1d36c4bd08ac7589cb30a8a2e852_1440w.jpg)

open-r1-multimodel

### 1.4 [open-thoughts](https://link.zhihu.com/?target=https%3A//github.com/open-thoughts/open-thoughts)

目标是整理一个推理数据集，用于训练最先进的小型推理模型，使其在数学和代码推理[基准测试](https://zhida.zhihu.com/search?content_id=253234935&content_type=Article&match_order=1&q=%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95&zhida_source=entity)中超越 DeepSeek-R1-Distill-Qwen-32B 和 DeepSeek-R1-Distill-Qwen-7B。

目前已在以下领域生成数据：代码（Code）领域；数学（Math）领域；科学（Science）领域；谜题（Puzzle）领域。

![](https://picx.zhimg.com/v2-ef92a6df1033958cff46cd6e9609a06d_1440w.jpg)

### 1.5 [TinyZero](https://link.zhihu.com/?target=https%3A//github.com/Jiayi-Pan/TinyZero)

TinyZero 是在倒计时和乘法任务中对 DeepSeek R1 Zero进行复制。基于 veRL 构建，通过强化学习（RL），3B 基础语言模型（LM）自行发展出自我验证和搜索能力。可以低于 30 美元的价格体验到"Ahah moment"。

![|500](https://pic4.zhimg.com/v2-53f52782b2aabccd301285f4cd8ca311_1440w.jpg)

### 1.6 [simpleRL-reason](https://link.zhihu.com/?target=https%3A//github.com/hkust-nlp/simpleRL-reason)

DeepSeek-R1 和 Kimi-k1.5 使用简单的强化学习算法来学习新兴的长思维链（CoT）和自我反思模式，并取得了良好的结果，其中没有使用 MCTS 和奖励模型。然而，他们的实验是基于大规模强化学习环境中的大型模型。目前尚不清楚小型模型是否能表现出类似的行为，需要多少数据，以及定量结果与其他方法相比如何。[simpleRL-reason](https://link.zhihu.com/?target=https%3A//github.com/hkust-nlp/simpleRL-reason)重现了 DeepSeek-R1-Zero 和 DeepSeek-R1 用于复杂数学推理的训练，从 Qwen-2.5-Math-7B（基础模型）开始，并且仅使用来自原始数学数据集的 8K（查询、最终答案）示例，平均获得了近 20 个绝对百分点的提升。

![](https://pic4.zhimg.com/v2-085396d0c5bc54eaa4f79544b32ebe2d_1440w.jpg)

### [1.7 RAGEN](https://link.zhihu.com/?target=https%3A//github.com/ZihanWang314/RAGEN)

RAGEN 是用于训练智能体模型的 DeepSeek-R1 (-Zero) 方法的首次复现，主要在gym-sokoban（传统的推箱子游戏）任务上进行训练。

![](https://pic2.zhimg.com/v2-9d484045298b004ef6d7d3ec1c642c37_1440w.jpg)

## 2.数据集

- [OpenThoughts-114k](https://link.zhihu.com/?target=https%3A//huggingface.co/datasets/open-thoughts/OpenThoughts-114k)：拥有 114,000 个高质量示例，涵盖数学、科学、代码和谜题等。
- [bespokelabs/Bespoke-Stratos-17k](https://link.zhihu.com/?target=https%3A//huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k)：对伯克利 Sky-T1 数据的复制，使用 DeepSeek-R1 创建了一个包含问题、推理过程和答案的数据集。
- [R1-Distill-SFT](https://link.zhihu.com/?target=https%3A//huggingface.co/datasets/ServiceNow-AI/R1-Distill-SFT)：目前有 17000 个样本，目的是创建数据以支持 Open-R1 项目。
- [multimodal-open-r1-8k-verified](https://link.zhihu.com/?target=https%3A//huggingface.co/datasets/lmms-lab/multimodal-open-r1-8k-verified)：r1多模态[数据验证](https://zhida.zhihu.com/search?content_id=253234935&content_type=Article&match_order=1&q=%E6%95%B0%E6%8D%AE%E9%AA%8C%E8%AF%81&zhida_source=entity)集。
- [cognitivecomputations/dolphin-r1](https://link.zhihu.com/?target=https%3A//huggingface.co/datasets/cognitivecomputations/dolphin-r1)：包含 80 万个样本的数据集，其中的数据来自 DeepSeek-R1 和 Gemini flash 的生成结果，同时还有来自 Dolphin chat 的 20 万个样本。

## 3.参考链接

- [awesome-deep-reasoning](https://link.zhihu.com/?target=https%3A//github.com/modelscope/awesome-deep-reasoning)
- [https://huggingface.co/open-r1](https://link.zhihu.com/?target=https%3A//huggingface.co/open-r1)
- [open-r1/mini-r1-contdown-game](https://link.zhihu.com/?target=https%3A//huggingface.co/blog/open-r1/mini-r1-contdown-game)
- [open-r1-multimodal](https://link.zhihu.com/?target=https%3A//github.com/EvolvingLMMs-Lab/open-r1-multimodal)
- [simpleRL-reason](https://link.zhihu.com/?target=https%3A//github.com/hkust-nlp/simpleRL-reason)
- [RAGEN](https://link.zhihu.com/?target=https%3A//github.com/ZihanWang314/RAGEN)
- [open-thoughts](https://link.zhihu.com/?target=https%3A//github.com/open-thoughts/open-thoughts)
- [TinyZero](https://link.zhihu.com/?target=https%3A//github.com/Jiayi-Pan/TinyZero)
- [https://github.com/AlpacaACE/o1\-imitator](https://link.zhihu.com/?target=https%3A//github.com/AlpacaACE/o1-imitator)