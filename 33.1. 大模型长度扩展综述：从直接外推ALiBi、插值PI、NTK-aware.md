---
created: 2025-09-17T12:23:11 (UTC +08:00)
tags: [ntk-aware]
source: https://blog.csdn.net/v_JULY_v/article/details/135072211
author: 成就一亿技术人!
---
## 前言
-   \[6\] bloc97. _NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation_, 2023.
    
    其对应的URL为：_https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware\_ scaled\_rope\_allows\_llama\_models\_to\_have/_
    
    提出了**NTK-Aware插值**
    
     
-   \[7\] bloc97. **_Add NTK-Aware interpolation "by parts" correction_** , 2023.
    
    其URL为： https://github.com/jquesnelle/scaled-rope/pull/1.
    
    提出了 **“NTK-by-parts”插值**
    
-   \[9\] S. Chen, S. Wong, L. Chen, and Y. Tian. **_Extending context window of large language models via positional interpolation_** , 2023. arXiv: 2306.15595.
    
    该研究团队来自Meta，该篇论文提出了 **位置内插PI**
    

## 第一部分 背景知识：从进制表示谈到直接外推、线性内插、进制转换

### 1.1 从进制表示到直接外推

注，本部分内容援引自苏剑林博客中的部分内容，为更易懂，我在其基础上做了一定的修改、解读

#### 1.1.1 进制表示

假设我们有一个1000以内（不包含1000）的整数![n](https://latex.csdn.net/eq?n)要作为条件输入到模型中，那么要以哪种方式比较好呢？

1.  最朴素的想法是直接作为一维浮点向量输入，然而0～999这涉及到近千的跨度，对基于梯度的优化器来说并不容易优化得动。那缩放到0～1之间呢？也不大好，因为此时相邻的差距从1变成了0.001，模型和优化器都不容易分辨相邻的数字
2.  进一步，对于一个整数，比如759，这是一个10进制的三位数，每位数字是0～9。既然我们自己都是用10进制来表示数字的，为什么不直接将10进制表示直接输入模型呢？也就是说，我们将整数n以一个三维向量\[a,b,c\]来输入，a,b,c分别是n的百位、十位、个位
    
    至于如果想要进一步缩小数字的跨度，我们还可以进一步缩小进制的基数，如使用8进制、6进制甚至2进制，代价是进一步增加输入的维度

#### 1.1.2 直接外推

苏剑林说，假设我们还是用三维10进制表示训练了模型，模型效果还不错。然后突然来了个新需求，将n上限增加到2000以内，那么该如何处理呢？

如果还是用10进制表示的向量输入到模型，那么此时的输入就是一个四维向量了。然而，原本的模型是针对三维向量设计和训练的，所以新增一个维度后，模型就无法处理了。可能有读者想说，为什么不能提前预留好足够多的维度呢？

没错，是可以提前预留多几维，训练阶段设为0，推理阶段直接改为其他数字，这就是外推（Extrapolation）

[![直接外推](https://i-blog.csdnimg.cn/blog_migrate/ab0a2d27c81515d613723ef9cbd18fd2.png)](https://kexue.fm/usr/uploads/2023/07/4094653858.png)

然而，训练阶段预留的维度一直是0，如果推理阶段改为其他数字，效果不见得会好，因为模型对没被训练过的情况不一定具有适应能力。也就是说，由于某些维度的训练数据不充分，所以直接进行外推通常会导致模型的性能严重下降。

### 1.2 从线性内插到进制转换

#### 1.2.1 线性内插

于是，有人想到了将外推改为内插(Interpolation)，简单来说就是将2000以内压缩到1000以内

[![线性内插](https://i-blog.csdnimg.cn/blog_migrate/785cee3513218ada3cabf72888af5b14.png)](https://kexue.fm/usr/uploads/2023/07/4113541717.png)

1.  比如通过除以2，1749就变成了874.5，然后转为三维向量\[8,7,4.5\]输入到原来的模型中
    
    从绝对数值来看，新的\[7,4,9\]实际上对应的是1498，是原本对应的2倍，映射方式不一致；
    
    从相对数值来看，原本相邻数字的差距为1，现在是0.5，最后一个维度更加“拥挤”
    
2.  所以，做了内插修改后，通常都需要微调训练，以便模型重新适应拥挤的映射关系

当然，有读者会说外推方案也可以微调。是的，但内插方案微调所需要的步数要少得多

-   因为很多场景(比如位置编码)下，相对大小(或许说序信息)更加重要，换句话说模型只需要知道874.5比874大就行了，不需要知道它实际代表什么多大的数字。而原本模型已经学会了875比874大，加之模型本身有一定的泛化能力，所以再多学一个874.5比874大不会太难
-   不过，内插方案也不尽完美，当处理范围进一步增大时，相邻差异则更小，并且这个相邻差异变小集中在个位数，剩下的百位、十位，还是保留了相邻差异为1
    
    换句话说，内插方法使得不同维度的分布情况不一样，每个维度变得不对等起来，模型进一步学习难度也更大

#### 1.2.2 进制转换

有没有不用新增维度，又能保持相邻差距的方案呢？有，那就是进制转换

刚才说到，我们关心的场景主要利用序信息

1.  原来训练好的模型已经学会了![875>874](https://latex.csdn.net/eq?875%3E874)，而在16进制下同样有![875>874](https://latex.csdn.net/eq?875%3E874)，比较规则是一模一样的
2.  唯一担心的是每个维度超过9之后(10～15)模型还能不能正常比较，但事实上一般模型也有一定的泛化能力，所以每个维度稍微往外推一些是没问题的。所以，这个转换进制的思路，甚至可能不微调原来模型也有效

另外，为了进一步缩窄外推范围，我们还可以换用更小的![\lceil\sqrt[3]{2000} \mid=13](https://latex.csdn.net/eq?%5Clceil%5Csqrt%5B3%5D%7B2000%7D%20%5Cmid%3D13)即13进制而不是16进制

## 第二部分 从RoPE、直接外推到位置内插Position Interpolation

基于transformer的大型语言模型已经成为许多NLP任务的首选模型，其远程能力(如上下文学习(ICL))至关重要

在执行NLP任务时，其上下文窗口的最大长度一直是预训练LLM的主要限制之一。故，是否能够通过少量的微调(或不进行微调)来动态扩展上下文窗口已经变得越来越受关注。为此，transformer的位置编码是经常讨论的核心焦点问题

1.  最初的Transformer架构使用了绝对正弦位置编码，后来被改进为可学习的绝对位置编码\[_Convolutional sequence to sequence learning_\]
    
    此后，相对位置编码方案\[_Self-attention with relative position representations_\]进一步提升了transformer的性能
    
    目前，最流行的相对位置编码是T5 relative Bias\[_Exploring the limits of transfer learning with a unified text-to-text transformer_\]、**RoPE**\[34\]、XPos\[35\]和ALiBi\[**_Attention with linear biases enables input length extrapolation_**\]
    
2.  位置编码的一个反复出现的限制是无法对「训练期间看到的上下文窗口之外的情况」进行泛化
    
    _One reoccurring limitation with positional encodings is the inability to generalize past the context window seen during training_
    
    虽然**ALiBi**等一些方法能够进行有限的泛化，但没有一种方法能够泛化到明显长于预训练长度的序列
    
3.  好在已经有一些工作正在尝试克服这种限制
    
    比如**位置插值**(Position Interpolation, PI)\[_Extending context window of large language models via positional interpolation_\]
    
    通过对RoPE进行轻微修改，并对少量数据进行微调，从而扩展上下文长度

> 很快，在下文的第三部分，你还将看到
> 
> ___
> 
> 作为一种替代方案，Reddit一网友bloc97通过[该帖子](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/ "该帖子")，提出了“**NTK-aware**”插值方法\[_NTK-Aware Scaled RoPE allows LLaMA models to have extended(8k+) context size without any fine-tuning and minimal perplexity degradation_\]，该方法考虑到高频信号的损失
> 
> 此后，对“NTK感知”插值提出了两项改进
> 
> 1.  无需微调的预训练模型的“**动态NTK**”插值方法\[14\]
> 2.  在对少量较长的上下文数据进行微调时表现最佳的“**NTK-by-parts**”插值方法\[7\]
> 
> “NTK感知”插值和“Dynamic NTK”插值已经在开源模型中出现，如Code Llama\[31\](使用“NTK感知”插值)和Qwen 7B\[2\](使用“动态NTK”)

### 2.1 旋转位置嵌入

#### 2.1.1 RoPE的快速回顾

YaRN的基础是\[_RoFormer: Enhanced transformer with rotary position embedding_\]中介绍的旋转位置嵌入(RoPE)

> RoPE是理解本文的重要基础，但考虑到本博客内已有另一篇文章详细阐述了位置编码与RoPE，所以如果你对本节有任何疑问，可进一步参考此文《[一文通透位置编码：从标准位置编码、旋转位置编码RoPE到ALiBi、LLaMA 2 Long(含NTK-aware简介)](https://blog.csdn.net/v_JULY_v/article/details/134085503 "一文通透位置编码：从标准位置编码、旋转位置编码RoPE到ALiBi、LLaMA 2 Long(含NTK-aware简介)")》

所以下面只参照YaRN论文做个最简单的回顾

1.  首先，我们在一个隐藏层上工作，隐藏神经元的集合用![D](https://latex.csdn.net/eq?D)表示
    
    给定向量序列![\mathbf{x}_{1}, \cdots, \mathbf{x}_{L} \in \mathbb{R}^{|D|}](https://latex.csdn.net/eq?%5Cmathbf%7Bx%7D_%7B1%7D%2C%20%5Ccdots%2C%20%5Cmathbf%7Bx%7D_%7BL%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7B%7CD%7C%7D)，遵循RoPE的表示法，注意力层首先将向量转换为查询向量和关键向量：![\mathbf{q}_{m}=f_{q}\left(\mathbf{x}_{m}, m\right) \in \mathbb{R}^{|D|}, \mathbf{k}_{n}=f_{k}\left(\mathbf{x}_{n}, n\right) \in \mathbb{R}^{|D|}](https://latex.csdn.net/eq?%5Cmathbf%7Bq%7D_%7Bm%7D%3Df_%7Bq%7D%5Cleft%28%5Cmathbf%7Bx%7D_%7Bm%7D%2C%20m%5Cright%29%20%5Cin%20%5Cmathbb%7BR%7D%5E%7B%7CD%7C%7D%2C%20%5Cmathbf%7Bk%7D_%7Bn%7D%3Df_%7Bk%7D%5Cleft%28%5Cmathbf%7Bx%7D_%7Bn%7D%2C%20n%5Cright%29%20%5Cin%20%5Cmathbb%7BR%7D%5E%7B%7CD%7C%7D)
2.  接下来，注意力权重被计算为
    
    ![\operatorname{softmax}\left(\frac{\mathbf{q}_{m}^{T} \mathbf{k}_{n}}{\sqrt{|D|}}\right)](https://latex.csdn.net/eq?%5Coperatorname%7Bsoftmax%7D%5Cleft%28%5Cfrac%7B%5Cmathbf%7Bq%7D_%7Bm%7D%5E%7BT%7D%20%5Cmathbf%7Bk%7D_%7Bn%7D%7D%7B%5Csqrt%7B%7CD%7C%7D%7D%5Cright%29)
    
    其中![q_m](https://latex.csdn.net/eq?q_m)、![k_n](https://latex.csdn.net/eq?k_n)被认为是列向量，因此![\mathbf{q}_{m}^{T} \mathbf{k}_{n}](https://latex.csdn.net/eq?%5Cmathbf%7Bq%7D_%7Bm%7D%5E%7BT%7D%20%5Cmathbf%7Bk%7D_%7Bn%7D)就是简单的欧氏内积。在RoPE中，我们首先假设![|D|](https://latex.csdn.net/eq?%7CD%7C)是偶数，并将嵌入空间和隐藏状态识别为complex vector spaces，相当于下文所说的![d_{model}](https://latex.csdn.net/eq?d_%7Bmodel%7D)
    
    ![\mathbb{R}^{|D|} \cong \mathbb{C}^{|D| / 2}](https://latex.csdn.net/eq?%5Cmathbb%7BR%7D%5E%7B%7CD%7C%7D%20%5Ccong%20%5Cmathbb%7BC%7D%5E%7B%7CD%7C%20/%202%7D)
    
    其中内积![\mathbf{q}^{T} \mathbf{k}](https://latex.csdn.net/eq?%5Cmathbf%7Bq%7D%5E%7BT%7D%20%5Cmathbf%7Bk%7D)转化为![\operatorname{Re}\left(\mathbf{q}^{*} \mathbf{k}\right)](https://latex.csdn.net/eq?%5Coperatorname%7BRe%7D%5Cleft%28%5Cmathbf%7Bq%7D%5E%7B*%7D%20%5Cmathbf%7Bk%7D%5Cright%29)的实部「_where the inner product **q** T **k** becomes the real part of the standard Hermitian inner product Re(**q** ∗**k**)，如对该点有疑问的，请参见___[此文](https://blog.csdn.net/v_JULY_v/article/details/134085503 "此文")___的3.2.1节_」，更具体地说，同构将实数部分和复数部分交织在一起(_the isomorphisms interleave the real part and the complex part_)
    
    ![\begin{aligned} \left(\left(\mathbf{x}_{m}\right)_{1}, \cdots,\left(\mathbf{x}_{m}\right)_{|D|}\right) & \mapsto\left(\left(\mathbf{x}_{m}\right)_{1}+i\left(\mathbf{x}_{m}\right)_{2}, \cdots,\left(\left(\mathbf{x}_{m}\right)_{|D|-1}+i\left(\mathbf{x}_{m}\right)_{|D|}\right)\right) \\ \left(\left(\mathbf{q}_{m}\right)_{1}, \cdots,\left(\mathbf{q}_{m}\right)_{|D|}\right) & \mapsto\left(\left(\mathbf{q}_{m}\right)_{1}+i\left(\mathbf{q}_{m}\right)_{2}, \cdots,\left(\left(\mathbf{q}_{m}\right)_{|D|-1}+i\left(\mathbf{q}_{m}\right)_{|D|}\right)\right) \end{aligned}](https://latex.csdn.net/eq?%5Cbegin%7Baligned%7D%20%5Cleft%28%5Cleft%28%5Cmathbf%7Bx%7D_%7Bm%7D%5Cright%29_%7B1%7D%2C%20%5Ccdots%2C%5Cleft%28%5Cmathbf%7Bx%7D_%7Bm%7D%5Cright%29_%7B%7CD%7C%7D%5Cright%29%20%26%20%5Cmapsto%5Cleft%28%5Cleft%28%5Cmathbf%7Bx%7D_%7Bm%7D%5Cright%29_%7B1%7D&plus;i%5Cleft%28%5Cmathbf%7Bx%7D_%7Bm%7D%5Cright%29_%7B2%7D%2C%20%5Ccdots%2C%5Cleft%28%5Cleft%28%5Cmathbf%7Bx%7D_%7Bm%7D%5Cright%29_%7B%7CD%7C-1%7D&plus;i%5Cleft%28%5Cmathbf%7Bx%7D_%7Bm%7D%5Cright%29_%7B%7CD%7C%7D%5Cright%29%5Cright%29%20%5C%5C%20%5Cleft%28%5Cleft%28%5Cmathbf%7Bq%7D_%7Bm%7D%5Cright%29_%7B1%7D%2C%20%5Ccdots%2C%5Cleft%28%5Cmathbf%7Bq%7D_%7Bm%7D%5Cright%29_%7B%7CD%7C%7D%5Cright%29%20%26%20%5Cmapsto%5Cleft%28%5Cleft%28%5Cmathbf%7Bq%7D_%7Bm%7D%5Cright%29_%7B1%7D&plus;i%5Cleft%28%5Cmathbf%7Bq%7D_%7Bm%7D%5Cright%29_%7B2%7D%2C%20%5Ccdots%2C%5Cleft%28%5Cleft%28%5Cmathbf%7Bq%7D_%7Bm%7D%5Cright%29_%7B%7CD%7C-1%7D&plus;i%5Cleft%28%5Cmathbf%7Bq%7D_%7Bm%7D%5Cright%29_%7B%7CD%7C%7D%5Cright%29%5Cright%29%20%5Cend%7Baligned%7D)
3.  为了将嵌入![x_m](https://latex.csdn.net/eq?x_m)、![x_n](https://latex.csdn.net/eq?x_n)转换为查询向量和键向量，我们首先给出了R\-linear算子
    
    ![\mathbf{W}_{q}, \mathbf{W}_{k}: \mathbb{R}^{|D|} \rightarrow \mathbb{R}^{|D|}](https://latex.csdn.net/eq?%5Cmathbf%7BW%7D_%7Bq%7D%2C%20%5Cmathbf%7BW%7D_%7Bk%7D%3A%20%5Cmathbb%7BR%7D%5E%7B%7CD%7C%7D%20%5Crightarrow%20%5Cmathbb%7BR%7D%5E%7B%7CD%7C%7D)
    
    在复坐标中，函数![f_q](https://latex.csdn.net/eq?f_q)，![f_k](https://latex.csdn.net/eq?f_k)分别由以下的式子计算得到
    
    ![f_{q}\left(\mathbf{x}_{m}, m\right)=e^{i m \theta} \mathbf{W}_{q} \mathbf{x}_{m}](https://latex.csdn.net/eq?f_%7Bq%7D%5Cleft%28%5Cmathbf%7Bx%7D_%7Bm%7D%2C%20m%5Cright%29%3De%5E%7Bi%20m%20%5Ctheta%7D%20%5Cmathbf%7BW%7D_%7Bq%7D%20%5Cmathbf%7Bx%7D_%7Bm%7D)
    
    ![f_{k}\left(\mathbf{x}_{n}, n\right)=e^{i n \theta} \mathbf{W}_{k} \mathbf{x}_{n}](https://latex.csdn.net/eq?f_%7Bk%7D%5Cleft%28%5Cmathbf%7Bx%7D_%7Bn%7D%2C%20n%5Cright%29%3De%5E%7Bi%20n%20%5Ctheta%7D%20%5Cmathbf%7BW%7D_%7Bk%7D%20%5Cmathbf%7Bx%7D_%7Bn%7D)
    
    其中![\theta=\operatorname{diag}\left(\theta_{1}, \cdots, \theta_{|D| / 2}\right)](https://latex.csdn.net/eq?%5Ctheta%3D%5Coperatorname%7Bdiag%7D%5Cleft%28%5Ctheta_%7B1%7D%2C%20%5Ccdots%2C%20%5Ctheta_%7B%7CD%7C%20/%202%7D%5Cright%29)是对角矩阵，![\theta_{d}=b^{-2 d /|D|}](https://latex.csdn.net/eq?%5Ctheta_%7Bd%7D%3Db%5E%7B-2%20d%20/%7CD%7C%7D)，其中的![d](https://latex.csdn.net/eq?d)就是transformer里的维度索引变换![i](https://latex.csdn.net/eq?i)，且 b= 10000
    
    **相当于下文将提到的
    
    “ 基本情况下第 ![i](https://latex.csdn.net/eq?i)维的 ![\theta_{i}](https://latex.csdn.net/eq?%5Ctheta_%7Bi%7D)：
    
    ![\theta_{i}=\text { base }^{-2 i /d_{model}}](https://latex.csdn.net/eq?%5Ctheta_%7Bi%7D%3D%5Ctext%20%7B%20base%20%7D%5E%7B-2%20i%20/d_%7Bmodel%7D%7D)
    
    相当于
    
    ![PE_{(pos,2i+1)} = cos\left ( \frac{pos}{10000^{\frac{2i}{d_{model}}}} \right )](https://latex.csdn.net/eq?PE_%7B%28pos%2C2i&plus;1%29%7D%20%3D%20cos%5Cleft%20%28%20%5Cfrac%7Bpos%7D%7B10000%5E%7B%5Cfrac%7B2i%7D%7Bd_%7Bmodel%7D%7D%7D%7D%20%5Cright%20%29) ”**
4.  通过上述这种方式，RoPE将每个(复值)隐藏神经元与一个单独的频率![\theta_{d}](https://latex.csdn.net/eq?%5Ctheta_%7Bd%7D)关联起来
    
    这样做的好处是，查询向量和关键向量之间的点积只取决于如下所示的相对距离![m-n](https://latex.csdn.net/eq?m-n)
    
    ![\begin{aligned} & \left\langle f_{q}\left(\mathbf{x}_{m}, m\right), f_{k}\left(\mathbf{x}_{n}, n\right)\right\rangle_{\mathbb{R}} \\ = & \operatorname{Re}\left(\left\langle f_{q}\left(\mathbf{x}_{m}, m\right), f_{k}\left(\mathbf{x}_{n}, n\right)\right\rangle_{\mathbb{C}}\right) \\ = & \operatorname{Re}\left(\mathbf{x}_{m}^{*} \mathbf{W}_{q}^{*} \mathbf{W}_{k} \mathbf{x}_{n} e^{i \theta(m-n)}\right) \\ = & g\left(\mathbf{x}_{m}, \mathbf{x}_{n}, m-n\right) . \end{aligned}](https://latex.csdn.net/eq?%5Cbegin%7Baligned%7D%20%26%20%5Cleft%5Clangle%20f_%7Bq%7D%5Cleft%28%5Cmathbf%7Bx%7D_%7Bm%7D%2C%20m%5Cright%29%2C%20f_%7Bk%7D%5Cleft%28%5Cmathbf%7Bx%7D_%7Bn%7D%2C%20n%5Cright%29%5Cright%5Crangle_%7B%5Cmathbb%7BR%7D%7D%20%5C%5C%20%3D%20%26%20%5Coperatorname%7BRe%7D%5Cleft%28%5Cleft%5Clangle%20f_%7Bq%7D%5Cleft%28%5Cmathbf%7Bx%7D_%7Bm%7D%2C%20m%5Cright%29%2C%20f_%7Bk%7D%5Cleft%28%5Cmathbf%7Bx%7D_%7Bn%7D%2C%20n%5Cright%29%5Cright%5Crangle_%7B%5Cmathbb%7BC%7D%7D%5Cright%29%20%5C%5C%20%3D%20%26%20%5Coperatorname%7BRe%7D%5Cleft%28%5Cmathbf%7Bx%7D_%7Bm%7D%5E%7B*%7D%20%5Cmathbf%7BW%7D_%7Bq%7D%5E%7B*%7D%20%5Cmathbf%7BW%7D_%7Bk%7D%20%5Cmathbf%7Bx%7D_%7Bn%7D%20e%5E%7Bi%20%5Ctheta%28m-n%29%7D%5Cright%29%20%5C%5C%20%3D%20%26%20g%5Cleft%28%5Cmathbf%7Bx%7D_%7Bm%7D%2C%20%5Cmathbf%7Bx%7D_%7Bn%7D%2C%20m-n%5Cright%29%20.%20%5Cend%7Baligned%7D)
    
    在实坐标中，RoPE可以用下面的函数来表示
    
    ![f_{\mathbf{W}}\left(\mathbf{x}_{m}, m, \theta_{d}\right)=\left(\begin{array}{ccccccc} \cos m \theta_{1} & -\sin m \theta_{1} & 0 & 0 & \cdots & 0 & 0 \\ \sin m \theta_{1} & \cos m \theta_{1} & 0 & 0 & \cdots & 0 & 0 \\ 0 & 0 & \cos m \theta_{2} & -\sin m \theta_{2} & \cdots & 0 & 0 \\ 0 & 0 & \sin m \theta_{2} & \cos m \theta_{2} & \cdots & 0 & 0 \\ 0 & 0 & 0 & 0 & \cdots & \cos m \theta_{l} & -\sin m \theta_{l} \\ 0 & 0 & 0 & 0 & \cdots & \sin m \theta_{l} & \cos m \theta_{l} \end{array}\right) \mathbf{W} \mathbf{x}_{m}](https://latex.csdn.net/eq?f_%7B%5Cmathbf%7BW%7D%7D%5Cleft%28%5Cmathbf%7Bx%7D_%7Bm%7D%2C%20m%2C%20%5Ctheta_%7Bd%7D%5Cright%29%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bccccccc%7D%20%5Ccos%20m%20%5Ctheta_%7B1%7D%20%26%20-%5Csin%20m%20%5Ctheta_%7B1%7D%20%26%200%20%26%200%20%26%20%5Ccdots%20%26%200%20%26%200%20%5C%5C%20%5Csin%20m%20%5Ctheta_%7B1%7D%20%26%20%5Ccos%20m%20%5Ctheta_%7B1%7D%20%26%200%20%26%200%20%26%20%5Ccdots%20%26%200%20%26%200%20%5C%5C%200%20%26%200%20%26%20%5Ccos%20m%20%5Ctheta_%7B2%7D%20%26%20-%5Csin%20m%20%5Ctheta_%7B2%7D%20%26%20%5Ccdots%20%26%200%20%26%200%20%5C%5C%200%20%26%200%20%26%20%5Csin%20m%20%5Ctheta_%7B2%7D%20%26%20%5Ccos%20m%20%5Ctheta_%7B2%7D%20%26%20%5Ccdots%20%26%200%20%26%200%20%5C%5C%200%20%26%200%20%26%200%20%26%200%20%26%20%5Ccdots%20%26%20%5Ccos%20m%20%5Ctheta_%7Bl%7D%20%26%20-%5Csin%20m%20%5Ctheta_%7Bl%7D%20%5C%5C%200%20%26%200%20%26%200%20%26%200%20%26%20%5Ccdots%20%26%20%5Csin%20m%20%5Ctheta_%7Bl%7D%20%26%20%5Ccos%20m%20%5Ctheta_%7Bl%7D%20%5Cend%7Barray%7D%5Cright%29%20%5Cmathbf%7BW%7D%20%5Cmathbf%7Bx%7D_%7Bm%7D)
    
    如此，便有
    
    ![f_{q}=f_{\mathbf{W}_{q}}, f_{k}=f_{\mathbf{W}_{k}}](https://latex.csdn.net/eq?f_%7Bq%7D%3Df_%7B%5Cmathbf%7BW%7D_%7Bq%7D%7D%2C%20f_%7Bk%7D%3Df_%7B%5Cmathbf%7BW%7D_%7Bk%7D%7D)

#### 2.1.2 **位置![n](https://latex.csdn.net/eq?n)的旋转位置编码(RoPE)，本质上就是数字![n](https://latex.csdn.net/eq?n)的![\beta](https://latex.csdn.net/eq?%5Cbeta)进制编码**

首先，如苏剑林所说，位置![n](https://latex.csdn.net/eq?n)的旋转位置编码(RoPE)，本质上就是数字![n](https://latex.csdn.net/eq?n)的![\beta](https://latex.csdn.net/eq?%5Cbeta)进制编码

![\left\lfloor\frac{n}{\beta^{m-1}}\right\rfloor \bmod \beta](https://latex.csdn.net/eq?%5Cleft%5Clfloor%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7Bm-1%7D%7D%5Cright%5Crfloor%20%5Cbmod%20%5Cbeta)

也就是先除以![\beta^{k-1}](https://latex.csdn.net/eq?%5Cbeta%5E%7Bk-1%7D)次方，然后求模(余数)

> 以上咋推导得来的呢？为方便大家一目了然，我再解释下
> 
> ___
> 
> 例如，让我们找到十进制数12345中从右边数的第三位的数字，相当于![n=12345](https://latex.csdn.net/eq?n%3D12345)，![\beta =10](https://latex.csdn.net/eq?%5Cbeta%20%3D10)(因为是十进制)，![m=3](https://latex.csdn.net/eq?m%3D3)(因为要找的是第三位)
> 
> 1.  按照公式，我们首先计算![\beta^{m-1}=10^{3-1}=10^{2}=100](https://latex.csdn.net/eq?%5Cbeta%5E%7Bm-1%7D%3D10%5E%7B3-1%7D%3D10%5E%7B2%7D%3D100)
> 2.  然后，我们计算 ![n](https://latex.csdn.net/eq?n) 除以 ![\beta^{m-1}](https://latex.csdn.net/eq?%5Cbeta%5E%7Bm-1%7D)，即![12345 \div 100=123.45](https://latex.csdn.net/eq?12345%20%5Cdiv%20100%3D123.45)
> 3.  接下来，我们取这个结果的向下取整值，也就是去掉小数部分，得到![\lfloor 123.45\rfloor=123](https://latex.csdn.net/eq?%5Clfloor%20123.45%5Crfloor%3D123)
> 4.  最后，我们对![\beta](https://latex.csdn.net/eq?%5Cbeta) 取模，得到![123 \bmod 10=3](https://latex.csdn.net/eq?123%20%5Cbmod%2010%3D3)
> 
> 所以，12345的从右边数的第三位数字是3

其次，苏剑林在其博客中再说道

-   RoPE的构造基础是Sinusoidal位置编码，可以改写为下面的公式(_记为公式2_)

![\left[\cos \left(\frac{n}{\beta^{0}}\right), \sin \left(\frac{n}{\beta^{0}}\right), \cos \left(\frac{n}{\beta^{1}}\right), \sin \left(\frac{n}{\beta^{1}}\right), \cdots, \cos \left(\frac{n}{\beta^{d/2 -1}}\right), \sin \left(\frac{n}{\beta^{d/2 -1}}\right)\right]](https://latex.csdn.net/eq?%5Cleft%5B%5Ccos%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7B0%7D%7D%5Cright%29%2C%20%5Csin%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7B0%7D%7D%5Cright%29%2C%20%5Ccos%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7B1%7D%7D%5Cright%29%2C%20%5Csin%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7B1%7D%7D%5Cright%29%2C%20%5Ccdots%2C%20%5Ccos%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7Bd/2%20-1%7D%7D%5Cright%29%2C%20%5Csin%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7Bd/2%20-1%7D%7D%5Cright%29%5Cright%5D)

其中，![\beta=10000^{\frac{2}{d}}](https://latex.csdn.net/eq?%5Cbeta%3D10000%5E%7B%5Cfrac%7B2%7D%7Bd%7D%7D)

> 可能有的读者还是有点问题，可能还是得再解释下
> 
> ___
> 
> 首先，我们通过上文已多次提到的此文《[一文通透位置编码：从标准位置编码、旋转位置编码RoPE到ALiBi、LLaMA 2 Long(含NTK-aware简介)](https://blog.csdn.net/v_JULY_v/article/details/134085503 "一文通透位置编码：从标准位置编码、旋转位置编码RoPE到ALiBi、LLaMA 2 Long(含NTK-aware简介)")》，来回顾下transformer原始论文中的Sinusoidal位置编码
> 
> ![PE_{(pos,2i+1)} = cos\left ( \frac{pos}{10000^{\frac{2i}{d_{model}}}} \right )](https://latex.csdn.net/eq?PE_%7B%28pos%2C2i&plus;1%29%7D%20%3D%20cos%5Cleft%20%28%20%5Cfrac%7Bpos%7D%7B10000%5E%7B%5Cfrac%7B2i%7D%7Bd_%7Bmodel%7D%7D%7D%7D%20%5Cright%20%29)
> 
> ![PE_{(pos,2i)} = sin\left ( \frac{pos}{10000^{\frac{2i}{d_{model}}}} \right )](https://latex.csdn.net/eq?PE_%7B%28pos%2C2i%29%7D%20%3D%20sin%5Cleft%20%28%20%5Cfrac%7Bpos%7D%7B10000%5E%7B%5Cfrac%7B2i%7D%7Bd_%7Bmodel%7D%7D%7D%7D%20%5Cright%20%29)
> 
> 如阿荀所说，可知
> 
> ![\cos \left(\frac{n}{10000^{2 i / d}}\right)=\cos \left(\frac{n}{10000^{(2 / d) * i}}\right)=\cos \left(\frac{n}{\left(10000^{(2 / d)}\right)^{i}}\right)=\cos \left(\frac{n}{\beta^{i}}\right)](https://latex.csdn.net/eq?%5Ccos%20%5Cleft%28%5Cfrac%7Bn%7D%7B10000%5E%7B2%20i%20/%20d%7D%7D%5Cright%29%3D%5Ccos%20%5Cleft%28%5Cfrac%7Bn%7D%7B10000%5E%7B%282%20/%20d%29%20*%20i%7D%7D%5Cright%29%3D%5Ccos%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cleft%2810000%5E%7B%282%20/%20d%29%7D%5Cright%29%5E%7Bi%7D%7D%5Cright%29%3D%5Ccos%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7Bi%7D%7D%5Cright%29)
> 
> 其中
> 
> 1.  ![\beta=10000^{\frac{2}{d}}](https://latex.csdn.net/eq?%5Cbeta%3D10000%5E%7B%5Cfrac%7B2%7D%7Bd%7D%7D)，![n](https://latex.csdn.net/eq?n)即指的索引![pos](https://latex.csdn.net/eq?pos)
> 2.  ![i](https://latex.csdn.net/eq?i) 的取值范围为![[0,...,\frac{d_{model}}{2}]](https://latex.csdn.net/eq?%5B0%2C...%2C%5Cfrac%7Bd_%7Bmodel%7D%7D%7B2%7D%5D)，啥意思？意味着
> 
> |**pos**
> |---|
> (0 2 4等偶数维用sin函数计算)|****![i](https://latex.csdn.net/eq?i)****|
> |0|![i = 0 // 2 = 0](https://latex.csdn.net/eq?i%20%3D%200%20//%202%20%3D%200)|
> |1|![i = 1 //2 =0](https://latex.csdn.net/eq?i%20%3D%201%20//2%20%3D0)|
> |2|![i = 2 // 2 = 1](https://latex.csdn.net/eq?i%20%3D%202%20//%202%20%3D%201)|
> |3|![i = 3 // 2 = 1](https://latex.csdn.net/eq?i%20%3D%203%20//%202%20%3D%201)|
> |4|![i = 4 // 2 = 2](https://latex.csdn.net/eq?i%20%3D%204%20//%202%20%3D%202)|
> |5|![i = 5//2 = 2](https://latex.csdn.net/eq?i%20%3D%205//2%20%3D%202)|
> |6|
> |....|
> |510|
> |511|
> 
> 所以，也就有了上面的公式2
> 
> ![\left[\cos \left(\frac{n}{\beta^{0}}\right), \sin \left(\frac{n}{\beta^{0}}\right), \cos \left(\frac{n}{\beta^{1}}\right), \sin \left(\frac{n}{\beta^{1}}\right), \cdots, \cos \left(\frac{n}{\beta^{d/2 -1}}\right), \sin \left(\frac{n}{\beta^{d/2 -1}}\right)\right]](https://latex.csdn.net/eq?%5Cleft%5B%5Ccos%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7B0%7D%7D%5Cright%29%2C%20%5Csin%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7B0%7D%7D%5Cright%29%2C%20%5Ccos%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7B1%7D%7D%5Cright%29%2C%20%5Csin%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7B1%7D%7D%5Cright%29%2C%20%5Ccdots%2C%20%5Ccos%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7Bd/2%20-1%7D%7D%5Cright%29%2C%20%5Csin%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7Bd/2%20-1%7D%7D%5Cright%29%5Cright%5D)

现在，对比公式1、公式2，是不是也有一模一样的![\frac{n}{\beta^{m-1}}](https://latex.csdn.net/eq?%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7Bm-1%7D%7D)

至于模运算，它的最重要特性是周期性，而公式2的cos、sin是不是刚好也是周期函数？所以，除掉取整函数这个无关紧要的差异外，RoPE(或者说Sinusoidal位置编码)其实就是数字![n](https://latex.csdn.net/eq?n)的![\beta](https://latex.csdn.net/eq?%5Cbeta)进制编码

且提前说两点

1.  有了这个联系之后，直接外推方案就是啥也不改，**内插方案就是将n换成n/k，其中k是要扩大的倍数**，这就是下文很快要介绍的Meta提出的Positional Interpolation
2.  至于进制转换，就是要扩大k倍表示范围，那么原本的β进制至少要扩大成![\beta\left(k^{2 / d}\right)](https://latex.csdn.net/eq?%5Cbeta%5Cleft%28k%5E%7B2%20/%20d%7D%5Cright%29)进制
    
    且虽然是d维向量，但cos,sin是成对出现的，所以相当于d/2位β进制表示，因此要开d/2次方而不是d次方），或者等价地原来的底数10000换成10000k，这基本上就是下文马上要介绍的NTK-aware Scaled RoPE

### 2.2 直接外推之线性偏差注意力ALiBi

|模型名称|隐藏层维度|层数|注意力头数|词表大小|训练数据（tokens）|位置编码|最大长度|
|---|---|---|---|---|---|---|---|
|Baichuan-7B|4,096|32|32|64,000|1.2 万亿|RoPE|4,096|
|Baichuan-13B|5,120|40|40|64,000|1.4 万亿|ALiBi|4,096|
|Baichuan 2-7B|4096|32|32|125,696|2.6万亿|RoPE|4096|
|Baichuan 2-13B|5120|40|40|125,696|2.6万亿|ALiBi|4096|

注意看上表的位置编码那一列，baichuan 7B无论第一代还是第二代，位置编码均用的RoPE，而baichuan 13B则无论是第一代还是第二代，均用的ALiBi

下面便详细介绍下该ALiBi

简言之，ALiBi是对Transformers进行长度外推，即在短上下文窗口上进行训练，并在较长的上下文窗口上进行推理

-   好处是虽然一开始不用对模型结构做任何更改
-   但坏处是直接把位置外推到没有见到的地方会导致模型灾难性的崩坏(例如体现在PPL陡增)，为了弥补，需要再做一些微调

ALiBi全称是Attention with Linear Biases，通过论文《[Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409 "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation")》提出，其不像标准transformer那样，在embedding层添加位置编码，而是在softmax的结果后添加一个静态的不可学习的偏置项(说白了，就是数值固定)

具体而言，如下图所示

![](https://i-blog.csdnimg.cn/blog_migrate/b800739c2baecd4b0b2067f5dd586f1d.png)

1.  当计算每个头的注意力分数时，线性偏差注意力方法ALiBi会向每个注意力分数(![q_i\cdot k_j](https://latex.csdn.net/eq?q_i%5Ccdot%20k_j)，左)添加一个常数偏差(右)
    
    _When computing attention scores for each head, our linearly biased attention method, ALiBi, adds a constant bias (right) to each attention score (qi· kj , left)._  
    
    左边是自注意力得分，关于q和k的内积
    
    右边是一个相对距离的矩阵，
    
    q1  q2 q3 q4 q5
    
    k1  k2  k3 k4 k5
    
    所以才有
    
    ![\rightarrow](https://latex.csdn.net/eq?%5Crightarrow)  q1和k1之间的距离是0，所以对应位置就是0
    
    ![\rightarrow](https://latex.csdn.net/eq?%5Crightarrow)  q2和k1之间的距离是「相对位置偏移为“**k的索引**”1」 - 「q的索引2」，得到1-2 = -1，就对应到了中间矩阵的取值为-1了
    
    以此类推，相对距离矩阵的中间对角线上都是0，然后左下角的取值都是对应的「k的索引」-「q的索引」了
    
2.  那m具体怎么取值呢，按论文中的说法是
    
    当8个heads的时候，m的取值为：
    
    ![\frac{1}{2^{1}}, \frac{1}{2^{2}}, \ldots, \frac{1}{2^{8}}](https://latex.csdn.net/eq?%5Cfrac%7B1%7D%7B2%5E%7B1%7D%7D%2C%20%5Cfrac%7B1%7D%7B2%5E%7B2%7D%7D%2C%20%5Cldots%2C%20%5Cfrac%7B1%7D%7B2%5E%7B8%7D%7D)![1, 2, ..., i](https://latex.csdn.net/eq?1%2C%202%2C%20...%2C%20i)
    
    如果是16个heads，则m的取值为：
    
    ![\frac{1}{2^{0.5}}, \frac{1}{2^{1}}, \frac{1}{2^{1.5}}, \ldots, \frac{1}{2^{8}}](https://latex.csdn.net/eq?%5Cfrac%7B1%7D%7B2%5E%7B0.5%7D%7D%2C%20%5Cfrac%7B1%7D%7B2%5E%7B1%7D%7D%2C%20%5Cfrac%7B1%7D%7B2%5E%7B1.5%7D%7D%2C%20%5Cldots%2C%20%5Cfrac%7B1%7D%7B2%5E%7B8%7D%7D)
    
    相当于追加了一半的1/sqrt(2)到原来的8个head的每个m的取值
    
    扩展到一般情况就是：对于n个head的话，m的取值就是![2^{\frac{-8}{n}}](https://latex.csdn.net/eq?2%5E%7B%5Cfrac%7B-8%7D%7Bn%7D%7D)，即如下
    
    ![2^{\frac{-8}{1}}, 2^{\frac{-8}{2}}, 2^{\frac{-8}{3}}, \ldots, 2^{\frac{-8}{n}}](https://latex.csdn.net/eq?2%5E%7B%5Cfrac%7B-8%7D%7B1%7D%7D%2C%202%5E%7B%5Cfrac%7B-8%7D%7B2%7D%7D%2C%202%5E%7B%5Cfrac%7B-8%7D%7B3%7D%7D%2C%20%5Cldots%2C%202%5E%7B%5Cfrac%7B-8%7D%7Bn%7D%7D)， 这样的m个坡度了
    
    最终整体的公式便是
    
    ![\operatorname{softmax}\left(\mathbf{q}_{i} \mathbf{K}^{\top}+m \cdot[-(i-1), \ldots,-2,-1,0]\right)](https://latex.csdn.net/eq?%5Coperatorname%7Bsoftmax%7D%5Cleft%28%5Cmathbf%7Bq%7D_%7Bi%7D%20%5Cmathbf%7BK%7D%5E%7B%5Ctop%7D&plus;m%20%5Ccdot%5B-%28i-1%29%2C%20%5Cldots%2C-2%2C-1%2C0%5D%5Cright%29)
    
    对于第i个query来说，他们之间的相对距离就是：k的索引 - q的索引
    
    具体而言，k的索引 遍历![1, 2, ..., i](https://latex.csdn.net/eq?1%2C%202%2C%20...%2C%20i)，而q的索引 取值为![i](https://latex.csdn.net/eq?i)
    

// 待更

### 2.3 位置内插：基于Positional Interpolation扩大模型的上下文窗口

#### 2.3.1 RoPE的问题：直接外推会出现比较大的Attention Score

再次回顾一下RoPE

给定位置索引![m \in[0, c)](https://latex.csdn.net/eq?m%20%5Cin%5B0%2C%20c%29)和嵌入向量![\mathbf{x}:=\left[x_{0}, x_{1}, \ldots, x_{d-1}\right]^{\top}](https://latex.csdn.net/eq?%5Cmathbf%7Bx%7D%3A%3D%5Cleft%5Bx_%7B0%7D%2C%20x_%7B1%7D%2C%20%5Cldots%2C%20x_%7Bd-1%7D%5Cright%5D%5E%7B%5Ctop%7D)，其中![d](https://latex.csdn.net/eq?d)是注意力头的维度，RoPE定义了一个向量值复杂函数![\mathbf{f}(\mathbf{x}, m)](https://latex.csdn.net/eq?%5Cmathbf%7Bf%7D%28%5Cmathbf%7Bx%7D%2C%20m%29)，如下所示

![\mathbf{f}(\mathbf{x}, m)=\left[\left(x_{0}+\mathrm{i} x_{1}\right) e^{\mathrm{i} m \theta_{0}},\left(x_{2}+\mathrm{i} x_{3}\right) e^{\mathrm{i} m \theta_{1}}, \ldots,\left(x_{d-2}+\mathrm{i} x_{d-1}\right) e^{\mathrm{i} m \theta_{d / 2-1}}\right]^{\top}](https://latex.csdn.net/eq?%5Cmathbf%7Bf%7D%28%5Cmathbf%7Bx%7D%2C%20m%29%3D%5Cleft%5B%5Cleft%28x_%7B0%7D&plus;%5Cmathrm%7Bi%7D%20x_%7B1%7D%5Cright%29%20e%5E%7B%5Cmathrm%7Bi%7D%20m%20%5Ctheta_%7B0%7D%7D%2C%5Cleft%28x_%7B2%7D&plus;%5Cmathrm%7Bi%7D%20x_%7B3%7D%5Cright%29%20e%5E%7B%5Cmathrm%7Bi%7D%20m%20%5Ctheta_%7B1%7D%7D%2C%20%5Cldots%2C%5Cleft%28x_%7Bd-2%7D&plus;%5Cmathrm%7Bi%7D%20x_%7Bd-1%7D%5Cright%29%20e%5E%7B%5Cmathrm%7Bi%7D%20m%20%5Ctheta_%7Bd%20/%202-1%7D%7D%5Cright%5D%5E%7B%5Ctop%7D)

其中![i:=\sqrt{-1}](https://latex.csdn.net/eq?i%3A%3D%5Csqrt%7B-1%7D)是虚数单位，而![\theta_{j}=10000^{-2 j / d}](https://latex.csdn.net/eq?%5Ctheta_%7Bj%7D%3D10000%5E%7B-2%20j%20/%20d%7D)，从而使用 RoPE之后，其自注意力得分为：![a(m, n)=\operatorname{Re}\langle\mathbf{f}(\mathbf{q}, m), \mathbf{f}(\mathbf{k}, n)\rangle](https://latex.csdn.net/eq?a%28m%2C%20n%29%3D%5Coperatorname%7BRe%7D%5Clangle%5Cmathbf%7Bf%7D%28%5Cmathbf%7Bq%7D%2C%20m%29%2C%20%5Cmathbf%7Bf%7D%28%5Cmathbf%7Bk%7D%2C%20n%29%5Crangle)

把这个自注意得分的公式![a(m, n)](https://latex.csdn.net/eq?a%28m%2C%20n%29)推导下

![\begin{aligned} a(m, n) & =\operatorname{Re}\langle\mathbf{f}(\mathbf{q}, m), \mathbf{f}(\mathbf{k}, n)\rangle \\ & =\operatorname{Re}\left[\sum_{j=0}^{d / 2-1}\left(q_{2 j}+\mathrm{i} q_{2 j+1}\right)\left(k_{2 j}-\mathrm{i} k_{2 j+1}\right) e^{\mathrm{i}(m-n) \theta_{j}}\right] \\ & =\sum_{j=0}^{d / 2-1}\left(q_{2 j} k_{2 j}+q_{2 j+1} k_{2 j+1}\right) \cos \left((m-n) \theta_{j}\right)+\left(q_{2 j} k_{2 j+1}-q_{2 j+1} k_{2 j}\right) \sin \left((m-n) \theta_{j}\right) \\ & =: \quad a(m-n) \end{aligned}](https://latex.csdn.net/eq?%5Cbegin%7Baligned%7D%20a%28m%2C%20n%29%20%26%20%3D%5Coperatorname%7BRe%7D%5Clangle%5Cmathbf%7Bf%7D%28%5Cmathbf%7Bq%7D%2C%20m%29%2C%20%5Cmathbf%7Bf%7D%28%5Cmathbf%7Bk%7D%2C%20n%29%5Crangle%20%5C%5C%20%26%20%3D%5Coperatorname%7BRe%7D%5Cleft%5B%5Csum_%7Bj%3D0%7D%5E%7Bd%20/%202-1%7D%5Cleft%28q_%7B2%20j%7D&plus;%5Cmathrm%7Bi%7D%20q_%7B2%20j&plus;1%7D%5Cright%29%5Cleft%28k_%7B2%20j%7D-%5Cmathrm%7Bi%7D%20k_%7B2%20j&plus;1%7D%5Cright%29%20e%5E%7B%5Cmathrm%7Bi%7D%28m-n%29%20%5Ctheta_%7Bj%7D%7D%5Cright%5D%20%5C%5C%20%26%20%3D%5Csum_%7Bj%3D0%7D%5E%7Bd%20/%202-1%7D%5Cleft%28q_%7B2%20j%7D%20k_%7B2%20j%7D&plus;q_%7B2%20j&plus;1%7D%20k_%7B2%20j&plus;1%7D%5Cright%29%20%5Ccos%20%5Cleft%28%28m-n%29%20%5Ctheta_%7Bj%7D%5Cright%29&plus;%5Cleft%28q_%7B2%20j%7D%20k_%7B2%20j&plus;1%7D-q_%7B2%20j&plus;1%7D%20k_%7B2%20j%7D%5Cright%29%20%5Csin%20%5Cleft%28%28m-n%29%20%5Ctheta_%7Bj%7D%5Cright%29%20%5C%5C%20%26%20%3D%3A%20%5Cquad%20a%28m-n%29%20%5Cend%7Baligned%7D)

可知，这个自注意力得分![a(m, n)](https://latex.csdn.net/eq?a%28m%2C%20n%29)仅仅依赖于相对位置 ![m-n](https://latex.csdn.net/eq?m-n)(通过三角函数)。 这里 q和 k是特定注意力头的查询和键向量，在每一层，RoPE被应用于查询和键嵌入以计算注意力分数(_is only dependent on relative position m − n through trigonometric functions. Here q and k are the query and key vector for a specific attention head. At each layer, RoPE is applied on both query and key embeddings for computing attention scores_)

虽然RoPE的上界确实随着 |m − n|的减小而衰减，但上界仍然可能相当大(即上界可能严重依赖于![v_{j}](https://latex.csdn.net/eq?v_%7Bj%7D)的大小)，因此是无效的

实际上，如果我们将所有三角函数视为基函数(即![\phi_{j}(s):=e^{i s \theta_{j}}](https://latex.csdn.net/eq?%5Cphi_%7Bj%7D%28s%29%3A%3De%5E%7Bi%20s%20%5Ctheta_%7Bj%7D%7D))，并将上述方程视为以下基函数展开：

![a(s)=\operatorname{Re}\left[\sum_{j=0}^{d / 2-1} h_{j} e^{\mathrm{i} s \theta_{j}}\right]](https://latex.csdn.net/eq?a%28s%29%3D%5Coperatorname%7BRe%7D%5Cleft%5B%5Csum_%7Bj%3D0%7D%5E%7Bd%20/%202-1%7D%20h_%7Bj%7D%20e%5E%7B%5Cmathrm%7Bi%7D%20s%20%5Ctheta_%7Bj%7D%7D%5Cright%5D)

其中 ![s](https://latex.csdn.net/eq?s)是查询和键之间的位置跨度(![m-n](https://latex.csdn.net/eq?m-n)嘛)，![h_{j}:=\left(q_{2 j}+\mathrm{i} q_{2 j+1}\right)\left(k_{2 j}-\mathrm{i} k_{2 j+1}\right)](https://latex.csdn.net/eq?h_%7Bj%7D%3A%3D%5Cleft%28q_%7B2%20j%7D&plus;%5Cmathrm%7Bi%7D%20q_%7B2%20j&plus;1%7D%5Cright%29%5Cleft%28k_%7B2%20j%7D-%5Cmathrm%7Bi%7D%20k_%7B2%20j&plus;1%7D%5Cright%29)是依赖于 ![q](https://latex.csdn.net/eq?q)和 ![k](https://latex.csdn.net/eq?k)的复系数，如下图所示，在范围 \[0, 2048\]内， ![a(s)](https://latex.csdn.net/eq?a%28s%29)的幅度可能很小，但在该区域之外却会产生巨大的值

![](https://i-blog.csdnimg.cn/blog_migrate/305ff8750372d9033a30b930b5a76cd6.png)

根本原因在于三角函数族![\left\{\phi_{j}\right\}](https://latex.csdn.net/eq?%5Cleft%5C%7B%5Cphi_%7Bj%7D%5Cright%5C%7D)(具有足够大的 ![d](https://latex.csdn.net/eq?d))是一个通用逼近器，可以拟合任意的函数

因此，对于![a(s)](https://latex.csdn.net/eq?a%28s%29)，总是存在对应于范围\[0, 2048\]内较小函数值的系数![\left\{h_{j}\right\}](https://latex.csdn.net/eq?%5Cleft%5C%7Bh_%7Bj%7D%5Cright%5C%7D)(即键和查询)，但在其他区域却较大

#### 2.3.2 什么是位置内插Positional Interpolation——Meta于23年6月底提出

由于语言模型通常是用固定的上下文长度进行预训练的，自然会问如何通过在相对较少的数据量上进行微调来扩展上下文长度

对于使用RoPE作为位置嵌入的语言模型，**Chen等人\[9\]**和**_kaiokendev_\[21\]**同时提出了位置插值(position Interpolation, PI)，将上下文长度扩展到预训练极限之外

对于后者Super-HOT _kaiokendev_(2023)的工作，它在RoPE中插入了位置编码，将上下文窗口从2K扩展到8K

对于前者Chen等人的工作「_他们是Meta的这4位研究者：Shouyuan Chen、Sherman Wong、Liangjian Chen、Yuandong Tian_」，其对应的论文为《[Extending context window of large language models via positional interpolation](https://arxiv.org/abs/2306.15595 "Extending context window of large language models via positional interpolation")》，可知

1.  关键思想是，不是进行外推，而是直接将位置索引缩小(_不是插值位置嵌入，而是**插值位置索引**，这对于RoPE等位置编码更合适，并且可能需要较少的训练，因为没有添加可训练参数，即Instead of interpolating position embeddings, our method **interpolates position indices**, which is more suitable for RoPE like position encodings and may require less training since no trainable parameters are added_)
    
    使得最大位置索引与**目标长度大小**，即预训练阶段的先前上下文窗口限制相匹配(_we directly down-scale the position indices so that the maximum position index matches the previous context window limit in the pre-training stage)_
    
    至于理论依据就是可以在相邻的整数位置上插值位置编码，毕竟位置编码可以应用在非整数的位置上(而非在训练位置之外 进行外推)
    
    _we interpolate the position encodings at neighboring integer positions,utilizing the fact that position encodings can be applied on non-integer positions_
    
    如下图所示，下图左上角为预训练阶段的位置向量范围\[0,2048\]，右上角为长度外推的部分(2048,4096\]
    
    ![](https://i-blog.csdnimg.cn/blog_migrate/4da4b85e2362cc65ab0efcf1707c553e.png)
    
    如果直接使用位置(2048,4096\]进行推理，那么因为模型没有见过这一部分的位置，效果会出现灾难性的下降。那么，就<u>把[0,4096]这个区间”压缩“到[0,2048]</u>不就可以了嘛
    
    于是，原先的1就变成了0.5，4096就变成了2048，这就是**位置内插法**，即把没见过的位置映射到见过的位置
2.  相当于对于绝对位置 ![m](https://latex.csdn.net/eq?m) ，我们把它”**缩放**“一下，变成![\frac{m L^{\prime}}{L}](https://latex.csdn.net/eq?%5Cfrac%7Bm%20L%5E%7B%5Cprime%7D%7D%7BL%7D) 。其中， ![L](https://latex.csdn.net/eq?L)为原先支持的长度(如2048)，![L'](https://latex.csdn.net/eq?L%27)为需要扩展的长度(如4096)。这样，在计算query和key的时候，就有
    
    ![](https://i-blog.csdnimg.cn/blog_migrate/b5a16be6fb4505d1dc6a9a329a7f75ce.png)
    
    其中![L ' > L](https://latex.csdn.net/eq?L%20%27%20%3E%20L)是超出预训练限制的新上下文窗口
3.  考虑到扩展的上下文长度与原始上下文长度之间的比例一直特别重要，我们以此定义
    
    ![s=\frac{L^{\prime}}{L}](https://latex.csdn.net/eq?s%3D%5Cfrac%7BL%5E%7B%5Cprime%7D%7D%7BL%7D)
    
    有了![s](https://latex.csdn.net/eq?s)这个定义(_这个![s](https://latex.csdn.net/eq?s)其实本质上即指位置内插需要扩大的倍数_)，我们便可以将公式![f_{\mathbf{W}}^{\prime}\left(\mathbf{x}_{m}, m, \theta_{d}\right)=f_{\mathbf{W}}\left(\mathbf{x}_{m}, \frac{m L}{L^{\prime}}, \theta_{d}\right)](https://latex.csdn.net/eq?f_%7B%5Cmathbf%7BW%7D%7D%5E%7B%5Cprime%7D%5Cleft%28%5Cmathbf%7Bx%7D_%7Bm%7D%2C%20m%2C%20%5Ctheta_%7Bd%7D%5Cright%29%3Df_%7B%5Cmathbf%7BW%7D%7D%5Cleft%28%5Cmathbf%7Bx%7D_%7Bm%7D%2C%20%5Cfrac%7Bm%20L%7D%7BL%5E%7B%5Cprime%7D%7D%2C%20%5Ctheta_%7Bd%7D%5Cright%29)重写并简化为以下一般形式(_其中![g(m)=m / s](https://latex.csdn.net/eq?g%28m%29%3Dm%20/%20s)，![h\left(\theta_{d}\right)=\theta_{d}](https://latex.csdn.net/eq?h%5Cleft%28%5Ctheta_%7Bd%7D%5Cright%29%3D%5Ctheta_%7Bd%7D)_)：
    
                                              ![f_{\mathbf{W}}^{\prime}\left(\mathbf{x}_{m}, m, \theta_{d}\right)=f_{\mathbf{W}}\left(\mathbf{x}_{m}, g(m), h\left(\theta_{d}\right)\right)](https://latex.csdn.net/eq?f_%7B%5Cmathbf%7BW%7D%7D%5E%7B%5Cprime%7D%5Cleft%28%5Cmathbf%7Bx%7D_%7Bm%7D%2C%20m%2C%20%5Ctheta_%7Bd%7D%5Cright%29%3Df_%7B%5Cmathbf%7BW%7D%7D%5Cleft%28%5Cmathbf%7Bx%7D_%7Bm%7D%2C%20g%28m%29%2C%20h%5Cleft%28%5Ctheta_%7Bd%7D%5Cright%29%5Cright%29)
4.  最终，通过位置插值方法，将预训练的7B、13B、33B和65B LLaMA模型(Touvron等人，2023)扩展到大小为32768的各种上下文窗口，只需要在Pile(_是个书籍语料库_)等数据集上上进行1000步的微调即可获得良好的质量(_Position Interpolation can easily enable very long context windows (e.g. 32768), requiring only fine-tuning for 1000 steps on the Pile (Gao et al., 2020) to achieve a good quality_)，这与预训练成本相比，微调的成本可以忽略不计
    
    ![\rightarrow](https://latex.csdn.net/eq?%5Crightarrow)  且微调过程只需要数万到数十万个示例，微调的结果对示例的选择不敏感。 原因在于模型在微调阶段仅适应新的上下文窗口，从良好的初始化开始，而不是获取新的知识
    
    (_We also find that the result of the fine-tuning is not sensitive to the choice of examples. The reason may be that_ _the model is only adapting to the new context window during the fine-tuning phase__, starting from a good initialization, as opposed to acquiring new knowledge_)
    
    ![\rightarrow](https://latex.csdn.net/eq?%5Crightarrow)  总之，PI除了重新缩放使用位置插值扩展的模型的位置索引外，没有以任何方式修改LLaMA模型架构(_包括其中的自注意力机制，从而减轻了上下文窗口扩展对注意力分数计算的影响_)
    

那PI之后，是否一定要微调呢？也不一定，只是效果有所区别而已，具体而言

![](https://i-blog.csdnimg.cn/blog_migrate/c27d9f21da85343c46ad4308b62ca5b0.png)

-   PI之后，在没有微调的情况下(在步骤0)，模型可以展示出一定的语言建模能力，如扩展到8192上下文窗口的困惑度<20所示(相比之下，直接外推方法导致困惑度>![10^3](https://latex.csdn.net/eq?10%5E3))
-   PI之后，经过微调，困惑度迅速改善。 在200步时，模型超过了2048上下文窗口大小的原始模型困惑度，表明模型能够有效地使用比预训练设置更长的序列进行语言建模。 在1000步时，我们可以看到模型稳步改善，并取得了显著更好的困惑

此外，PI也比直接微调的效果更好，相比之下，仅通过直接微调扩展的LLaMA模型仅将有效上下文窗口大小 kmax从2048增加到2560，即使经过10000多步的微调也没有明显加速窗口大小的增长迹

> 以下是我司在通过PI微调llama 3时(_更多详见：[七月论文审稿GPT第5版：拿我司七月的早期paper-7方面review数据集微调LLama 3](https://blog.csdn.net/v_JULY_v/article/details/139931799 "七月论文审稿GPT第5版：拿我司七月的早期paper-7方面review数据集微调LLama 3")_)
> 
> -   直接用的longlora的代码「_至于什么是longlora，请参见此文：《[从LongLoRA到LongQLoRA(含源码剖析)：大模型上下文长度的超强扩展](https://blog.csdn.net/v_JULY_v/article/details/135375799 "从LongLoRA到LongQLoRA(含源码剖析)：大模型上下文长度的超强扩展")》_」
> -   因为longlora的代码实现了PI + S2-attn，故把S2-attn相关的部分注释掉后，即相当于通过PI微调llama 3了
> 
> ![](https://i-blog.csdnimg.cn/blog_migrate/be49519c8623e00235a099496ae78c96.png)

#### 2.3.3 位置内插的问题

话说，位置插值法有什么问题呢？

1.  我们先看下三角函数![\sin (w x)](https://latex.csdn.net/eq?%5Csin%20%28w%20x%29)，它的周期是![T=2 \pi / \omega](https://latex.csdn.net/eq?T%3D2%20%5Cpi%20/%20%5Comega)
    
    对应到RoPE里的每个维度![\sin m \theta_{j}, \cos m \theta_{j}](https://latex.csdn.net/eq?%5Csin%20m%20%5Ctheta_%7Bj%7D%2C%20%5Ccos%20m%20%5Ctheta_%7Bj%7D)，其中![\theta_{j}=10000^{-2(j-1) / d}, j \in[1,2, \ldots, d / 2]](https://latex.csdn.net/eq?%5Ctheta_%7Bj%7D%3D10000%5E%7B-2%28j-1%29%20/%20d%7D%2C%20j%20%5Cin%5B1%2C2%2C%20%5Cldots%2C%20d%20/%202%5D) (其中， ![m](https://latex.csdn.net/eq?m)是指位置， ![j](https://latex.csdn.net/eq?j)是指维度）
2.  计算得到周期为：![\frac{2 \pi}{m} b^{\frac{2(j-1)}{d}}](https://latex.csdn.net/eq?%5Cfrac%7B2%20%5Cpi%7D%7Bm%7D%20b%5E%7B%5Cfrac%7B2%28j-1%29%7D%7Bd%7D%7D)，其中，用 ![b](https://latex.csdn.net/eq?b) 表示base，即10000
    
    从周期计算的公式我们可以知道，针对不同的维度编码 ![j](https://latex.csdn.net/eq?j) ，每个维度对应的三角函数周期是越来越大的(即对应到低频、高频)
    
    如果插值是针对绝对位置![m](https://latex.csdn.net/eq?m)，那么对每个维度 ![j](https://latex.csdn.net/eq?j) 都同等地生效；但是周期小(高频)维度，插值之后会变得很密集(本来一个周期包含10个值，但是内插之后能包含20个值)，这样高频的维度就变的很拥挤
    

## 第三部分 从NTK-aware/**RoPE ABF、**NTK-by-parts到Dynamic NTK插值

### 3.1 “NTK-aware”插值：**高频外推，低频内插**

#### 3.1.1 非如PI针对所有维度平均缩放，而是高频不缩放、低频才缩放

为了解决RoPE嵌入插值时丢失高频信息(_losing high frequency information when interpolating the RoPE embeddings_)的问题，Reddit一网友bloc97通过\[_[NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/?rdt=46058 "NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation")_\]开发了“NTK-aware”插值(_据ChatGPT的推断，该篇帖子发表于23年7月2日_)

其核心思想是：**高频外推，低频内插**

1.  即不是将RoPE的每个维度平均缩放一个因子![s](https://latex.csdn.net/eq?s)，而是通过**减少对高频区域的缩放和增加对低频区域的缩放**(_即<u>高频不缩放，低频才缩放</u>_)，从而将插值压力分散到多个维度(_Instead of scaling every dimension of RoPE equally by a factor s, we spread out the interpolation pressure across multiple dimensions by **scaling** **high frequencies less and low frequencies more**_)
    
    如下图所示(_该图来自llama long论文，，如果没太看明白，不要紧，下一节会做更详细的解释说明_)，可以看到索引靠前的高频不缩放——即不插值，索引靠后的低频才缩放——即才插值
    
    ![](https://i-blog.csdnimg.cn/direct/fb9efa6c16c44f9291b36cd77af54bca.png)
    
2.  虽然人们可以通过许多方法获得这样的变换，但最简单的方法是对θ的值进行基础更改(_One can obtain such a transformation in many ways, but the simplest would be to perform a base change on the value of θ_)

啥意思呢，其实我们是要把2.1.2节中的公式2

![\left[\cos \left(\frac{n}{\beta^{0}}\right), \sin \left(\frac{n}{\beta^{0}}\right), \cos \left(\frac{n}{\beta^{1}}\right), \sin \left(\frac{n}{\beta^{1}}\right), \cdots, \cos \left(\frac{n}{\beta^{d/2 -1}}\right), \sin \left(\frac{n}{\beta^{d/2 -1}}\right)\right]](https://latex.csdn.net/eq?%5Cleft%5B%5Ccos%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7B0%7D%7D%5Cright%29%2C%20%5Csin%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7B0%7D%7D%5Cright%29%2C%20%5Ccos%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7B1%7D%7D%5Cright%29%2C%20%5Csin%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7B1%7D%7D%5Cright%29%2C%20%5Ccdots%2C%20%5Ccos%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7Bd/2%20-1%7D%7D%5Cright%29%2C%20%5Csin%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7Bd/2%20-1%7D%7D%5Cright%29%5Cright%5D)

![\frac{n}{(\beta \lambda)^{d / 2-1}}=\frac{n / k}{\beta^{d / 2-1}}](https://latex.csdn.net/eq?%5Cfrac%7Bn%7D%7B%28%5Cbeta%20%5Clambda%29%5E%7Bd%20/%202-1%7D%7D%3D%5Cfrac%7Bn%20/%20k%7D%7B%5Cbeta%5E%7Bd%20/%202-1%7D%7D)

        从而解得

![\lambda=\mathrm{k}^{2 /(\mathrm{d}-2)}](https://latex.csdn.net/eq?%5Clambda%3D%5Cmathrm%7Bk%7D%5E%7B2%20/%28%5Cmathrm%7Bd%7D-2%29%7D)

> 值得注意的是，由于d比较大(BERT是64，LLAMA是128)，![k^{2 /(d-2)}](https://latex.csdn.net/eq?k%5E%7B2%20/%28d-2%29%7D)跟![k^{2 / d}](https://latex.csdn.net/eq?k%5E%7B2%20/%20d%7D)差别不大，所以它跟上面《2.1.2 **位置![n](https://latex.csdn.net/eq?n)的旋转位置编码(RoPE)，本质上就是数字![n](https://latex.csdn.net/eq?n)的![\beta](https://latex.csdn.net/eq?%5Cbeta)进制编码**》基于进制思想提出的![k^{2 / d}](https://latex.csdn.net/eq?k%5E%7B2%20/%20d%7D)解是基本一致的

从而，NTK-aware便这样把外推和内插给结合起来了

#### 3.1.2 关于NTK-aware原Reddit帖的更多细节

YaRN论文中对“NTK-aware”的内插方案是如下表述的(_虽和上面的表示一个本质，但符号和表述上有不同_)

> “NTK-aware”插值是对RoPE的修改，使用
> 
> ![f_{\mathbf{W}}^{\prime}\left(\mathbf{x}_{m}, m, \theta_{d}\right)=f_{\mathbf{W}}\left(\mathbf{x}_{m}, g(m), h\left(\theta_{d}\right)\right)](https://latex.csdn.net/eq?f_%7B%5Cmathbf%7BW%7D%7D%5E%7B%5Cprime%7D%5Cleft%28%5Cmathbf%7Bx%7D_%7Bm%7D%2C%20m%2C%20%5Ctheta_%7Bd%7D%5Cright%29%3Df_%7B%5Cmathbf%7BW%7D%7D%5Cleft%28%5Cmathbf%7Bx%7D_%7Bm%7D%2C%20g%28m%29%2C%20h%5Cleft%28%5Ctheta_%7Bd%7D%5Cright%29%5Cright%29)
> 
> 和以下函数
> 
> ![g(m)=m](https://latex.csdn.net/eq?g%28m%29%3Dm)
> 
> ![h\left(\theta_{i}\right)=b^{\prime-2 i /d_{model}}](https://latex.csdn.net/eq?h%5Cleft%28%5Ctheta_%7Bi%7D%5Cright%29%3Db%5E%7B%5Cprime-2%20i%20/d_%7Bmodel%7D%7D)
> 
> 这里的![i](https://latex.csdn.net/eq?i)就是transformer里的![i](https://latex.csdn.net/eq?i)
> 
> ![PE_{(pos,2i+1)} = cos\left ( \frac{pos}{10000^{\frac{2i}{d_{model}}}} \right )](https://latex.csdn.net/eq?PE_%7B%28pos%2C2i&plus;1%29%7D%20%3D%20cos%5Cleft%20%28%20%5Cfrac%7Bpos%7D%7B10000%5E%7B%5Cfrac%7B2i%7D%7Bd_%7Bmodel%7D%7D%7D%7D%20%5Cright%20%29)
> 
> 且
> 
> ![b^{\prime}=b \cdot s^{\frac{d_{model}}{d_{model}-2}}](https://latex.csdn.net/eq?b%5E%7B%5Cprime%7D%3Db%20%5Ccdot%20s%5E%7B%5Cfrac%7Bd_%7Bmodel%7D%7D%7Bd_%7Bmodel%7D-2%7D%7D)
> 
> 其中![b](https://latex.csdn.net/eq?b)相当于上面的![\beta=10000^{\frac{2}{d_{model}}}](https://latex.csdn.net/eq?%5Cbeta%3D10000%5E%7B%5Cfrac%7B2%7D%7Bd_%7Bmodel%7D%7D%7D)，而![\beta](https://latex.csdn.net/eq?%5Cbeta)来源于此![](https://i-blog.csdnimg.cn/direct/5d8edac66bb740edb5293c94dcd469f1.png)
> 
> 且![s=\frac{L^{\prime}}{L}](https://latex.csdn.net/eq?s%3D%5Cfrac%7BL%5E%7B%5Cprime%7D%7D%7BL%7D) 类似于上面的![k](https://latex.csdn.net/eq?k)，![m](https://latex.csdn.net/eq?m)相当于上面的![n](https://latex.csdn.net/eq?n)——索引![pos](https://latex.csdn.net/eq?pos)
> 
> ___
> 
> 注意，原始YaRN论中用![|D|](https://latex.csdn.net/eq?%7CD%7C)表示![d_{model}](https://latex.csdn.net/eq?d_%7Bmodel%7D)，而
> 
> 1.  YaRN论文)的![|D|](https://latex.csdn.net/eq?%7CD%7C)和维度大小![d_{model}](https://latex.csdn.net/eq?d_%7Bmodel%7D)是一码事
> 2.  且上面(进制角度)的计数范围是从0到d/2-1的，而这里(YaRN论文)的计数范围是从1到|D|/2的

总之，位置插值PI中定义的缩放因子为![s](https://latex.csdn.net/eq?s)「_这个![s](https://latex.csdn.net/eq?s)其实本质上即指位置内插需要扩大的倍数_![s=\frac{L^{\prime}}{L}](https://latex.csdn.net/eq?s%3D%5Cfrac%7BL%5E%7B%5Cprime%7D%7D%7BL%7D)_，也相当于把内插就是将![n](https://latex.csdn.net/eq?n)换成![n/k](https://latex.csdn.net/eq?n/k)，即索引变成之前的![1/k](https://latex.csdn.net/eq?1/k)，相当于![k](https://latex.csdn.net/eq?k)就是要扩大的倍数，从而有![\frac{n}{n/k} = k](https://latex.csdn.net/eq?%5Cfrac%7Bn%7D%7Bn/k%7D%20%3D%20k)，比如![L'](https://latex.csdn.net/eq?L%27)为4096，![L](https://latex.csdn.net/eq?L)为2048，![k](https://latex.csdn.net/eq?k)即为2_」

而提出NTK-aware的Reddit网友bloc97则强调

1.  与简单的线性插值方案不同，我尝试设计了一种非线性插值方案，利用了NTK文献中的工具
    
    基本上，这种插值方案改变了RoPE的基数(即基频参数![b](https://latex.csdn.net/eq?b))，而不是其比例，直观地改变了每个RoPE维度向量相对于下一个维度向量的“旋转”速度
    
    且由于它不直接缩放Fourier特征，所有位置在取极端值时仍然可以完全区分开来
    
    _Instead of the simple linear interpolation scheme, I've tried to design a nonlinear interpolation scheme using tools from NTK literature.
    
    Basically this interpolation scheme changes the base of the RoPE instead of the scale, which intuitively changes the "spinning" speed which each of the RoPE's dimension vectors compared to the next.
    
    Because it does not scale the fourier features directly, all the positions are perfectly distinguishable from eachother, even when taken to the extreme_
2.  "下图(_图源：[https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware\_scaled\_rope\_allows\_llama\_models\_to\_have/?rdt=46058](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/?rdt=46058 "https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/?rdt=46058")_)显示了 LLaMA 7b 在一组 40 个非常长的提示(12k+ 上下文大小)上的平均困惑度。与更改比例(从 SuperHOT 设置为 4)相比，我们更改了一个因子 alpha，当该因子等于 8 时，上下文大小增加相同，但困惑度下降幅度要小得多。所有这些都无需任何微调"
    
    _Here's a graph showing the average perplexity of LLaMA 7b on a set of 40 very long prompts (12k+ context size). Compared to changing the scale (from SuperHOT, which was set to 4), we change a factor alpha, which when equal to 8 provides the same context size increase but with much less perplexity degradation. All without any finetuning!_
    
    ![](https://i-blog.csdnimg.cn/direct/5ef02bf90fb1425b9aae44db23324a85.png)
    
    至于这个因子alpha如何理解呢？我司七月大模型项目组同事阿荀特地推导了下(_且为方便大家理解，我把所有的符号表示在阿荀的基础上修改成和上面的统一的了_)
    
    基本情况下第 ![i](https://latex.csdn.net/eq?i)维的 ![\theta_{i}](https://latex.csdn.net/eq?%5Ctheta_%7Bi%7D)：
    
    ![\theta_{i}=\text { base }^{-2 i /d_{model}}](https://latex.csdn.net/eq?%5Ctheta_%7Bi%7D%3D%5Ctext%20%7B%20base%20%7D%5E%7B-2%20i%20/d_%7Bmodel%7D%7D)
    
    相当于
    
    ![PE_{(pos,2i+1)} = cos\left ( \frac{pos}{10000^{\frac{2i}{d_{model}}}} \right )](https://latex.csdn.net/eq?PE_%7B%28pos%2C2i&plus;1%29%7D%20%3D%20cos%5Cleft%20%28%20%5Cfrac%7Bpos%7D%7B10000%5E%7B%5Cfrac%7B2i%7D%7Bd_%7Bmodel%7D%7D%7D%7D%20%5Cright%20%29)
    
    维度索引变换![i](https://latex.csdn.net/eq?i)越小，![\theta_{i}](https://latex.csdn.net/eq?%5Ctheta_%7Bi%7D)越小，旋转频率就越高，反之越大。有理论说旋转频率高的维度使得位置信息更加有区分度，所以做缩放时应该考虑保留旋转频率高的维度尽可能不变（不调整），否则会影响原模型的能力，而旋转频率低的部分才应该被降幅（调低）
    
    引入了![\alpha](https://latex.csdn.net/eq?%5Calpha)的第![i](https://latex.csdn.net/eq?i) 维的 ![\theta_{i}](https://latex.csdn.net/eq?%5Ctheta_%7Bi%7D)：
    
    ![\theta_{i}^{\prime}=(\text { base } \cdot \alpha)^{-2 i /d_{model}}](https://latex.csdn.net/eq?%5Ctheta_%7Bi%7D%5E%7B%5Cprime%7D%3D%28%5Ctext%20%7B%20base%20%7D%20%5Ccdot%20%5Calpha%29%5E%7B-2%20i%20/d_%7Bmodel%7D%7D)
    
    ![\alpha](https://latex.csdn.net/eq?%5Calpha)直接作用于![base](https://latex.csdn.net/eq?base) ，间接影响了不同维度![i](https://latex.csdn.net/eq?i) 的 ![\theta_{i}](https://latex.csdn.net/eq?%5Ctheta_%7Bi%7D)
    
    引入 ![\alpha](https://latex.csdn.net/eq?%5Calpha) 后，相较于基本情况，旋转弧度式子在同个维度![i](https://latex.csdn.net/eq?i)的调节比率 ![r_{i}](https://latex.csdn.net/eq?r_%7Bi%7D) 为
    
    ![\begin{aligned} r_{i} & =\frac{\theta_{i}^{\prime}}{\theta_{i}} \\ & =\frac{(\text { base } \cdot \alpha)^{-2 i /d_{model}}}{\text { base }^{-2 i /d_{model}}} \\ & =\frac{1}{\alpha^{2 i /d_{model}}} \end{aligned}](https://latex.csdn.net/eq?%5Cbegin%7Baligned%7D%20r_%7Bi%7D%20%26%20%3D%5Cfrac%7B%5Ctheta_%7Bi%7D%5E%7B%5Cprime%7D%7D%7B%5Ctheta_%7Bi%7D%7D%20%5C%5C%20%26%20%3D%5Cfrac%7B%28%5Ctext%20%7B%20base%20%7D%20%5Ccdot%20%5Calpha%29%5E%7B-2%20i%20/d_%7Bmodel%7D%7D%7D%7B%5Ctext%20%7B%20base%20%7D%5E%7B-2%20i%20/d_%7Bmodel%7D%7D%7D%20%5C%5C%20%26%20%3D%5Cfrac%7B1%7D%7B%5Calpha%5E%7B2%20i%20/d_%7Bmodel%7D%7D%7D%20%5Cend%7Baligned%7D)
    
    当维度索引![i](https://latex.csdn.net/eq?i)较小时(属于高频区域)，对基本情况的调节比率![r_{i}](https://latex.csdn.net/eq?r_%7Bi%7D)接近1，也就是趋近于不调节
    
    当维度索引![i](https://latex.csdn.net/eq?i)较大时(属于低频区域)，调节比率![r_{i}](https://latex.csdn.net/eq?r_%7Bi%7D)偏小，也就是进行缩小——也即得到了出发点“高频不变、低频缩小”
    
    所以 就是影响高低频调节比率的系数，![\alpha](https://latex.csdn.net/eq?%5Calpha)越大，对**索引靠后的维度——低频维度**的缩小力度就越大，也就越能尝试越长的外推
    
    ___
    
    _还可以对比下之前上一节介绍过的这个推导(本质是一样的)_
    
    _![\left[\cos \left(\frac{n}{\beta^{0}}\right), \sin \left(\frac{n}{\beta^{0}}\right), \cos \left(\frac{n}{\beta^{1}}\right), \sin \left(\frac{n}{\beta^{1}}\right), \cdots, \cos \left(\frac{n}{\beta^{d/2 -1}}\right), \sin \left(\frac{n}{\beta^{d/2 -1}}\right)\right]](https://latex.csdn.net/eq?%5Cleft%5B%5Ccos%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7B0%7D%7D%5Cright%29%2C%20%5Csin%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7B0%7D%7D%5Cright%29%2C%20%5Ccos%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7B1%7D%7D%5Cright%29%2C%20%5Csin%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7B1%7D%7D%5Cright%29%2C%20%5Ccdots%2C%20%5Ccos%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7Bd/2%20-1%7D%7D%5Cright%29%2C%20%5Csin%20%5Cleft%28%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7Bd/2%20-1%7D%7D%5Cright%29%5Cright%5D)，其中![\beta=10000^{\frac{2}{d}}](https://latex.csdn.net/eq?%5Cbeta%3D10000%5E%7B%5Cfrac%7B2%7D%7Bd%7D%7D)_
    
    _**<u>该公式中最前面的最高频是<img alt="\frac{n}{\beta }" src="https://latex.csdn.net/eq?%5Cfrac%7Bn%7D%7B%5Cbeta%20%7D">项</u>**，引入参数__![\lambda](https://latex.csdn.net/eq?%5Clambda)后变为![\frac{n}{\beta \lambda }](https://latex.csdn.net/eq?%5Cfrac%7Bn%7D%7B%5Cbeta%20%5Clambda%20%7D)，由于![d](https://latex.csdn.net/eq?d)——![d_{model}](https://latex.csdn.net/eq?d_%7Bmodel%7D)通常很大，故而![\lambda](https://latex.csdn.net/eq?%5Clambda)很接近1，所以它还是接近于![\frac{n}{\beta }](https://latex.csdn.net/eq?%5Cfrac%7Bn%7D%7B%5Cbeta%20%7D)——不做缩放，即等价于外推_
    
    _**<u>该公式中最后面的最低频项<img alt="\frac{n}{\beta^{\mathrm{d} / 2-1}}" src="https://latex.csdn.net/eq?%5Cfrac%7Bn%7D%7B%5Cbeta%5E%7B%5Cmathrm%7Bd%7D%20/%202-1%7D%7D"></u>**，引入![\lambda](https://latex.csdn.net/eq?%5Clambda)，从而变为![\frac{n}{(\beta \lambda)^{d / 2-1}}](https://latex.csdn.net/eq?%5Cfrac%7Bn%7D%7B%28%5Cbeta%20%5Clambda%29%5E%7Bd%20/%202-1%7D%7D)，让它跟内插一致(内插就是将![n](https://latex.csdn.net/eq?n)换成![n/k](https://latex.csdn.net/eq?n/k)，其中![k](https://latex.csdn.net/eq?k)是要扩大的倍数)——做缩放，即_
    
    _![\frac{n}{(\beta \lambda)^{d / 2-1}}=\frac{n / k}{\beta^{d / 2-1}}](https://latex.csdn.net/eq?%5Cfrac%7Bn%7D%7B%28%5Cbeta%20%5Clambda%29%5E%7Bd%20/%202-1%7D%7D%3D%5Cfrac%7Bn%20/%20k%7D%7B%5Cbeta%5E%7Bd%20/%202-1%7D%7D)_
    
    _从而解得_
    
    _![\lambda=\mathrm{k}^{2 /(\mathrm{d}-2)}](https://latex.csdn.net/eq?%5Clambda%3D%5Cmathrm%7Bk%7D%5E%7B2%20/%28%5Cmathrm%7Bd%7D-2%29%7D)_
    
    相当于在bloc97的设计之下，可以通过修改这个基频参数，而把llama的长度从2048扩大4倍到8192，没任何问题
    
    这也刚好印证了bloc97这篇帖子的标题：**NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation**

与位置插值PI相比，该NTK-Aware方法在扩展非微调模型的上下文大小方面表现得更好

1.  然而，这种方法的一个主要缺点是，由于它不仅仅是一种插值方案，一些维度被轻微外推到“超出边界”的值，因此使用“NTK-aware”插值\[6\]进行微调的结果不如PI\[9\]
2.  此外，由于存在“越界”值，理论尺度因子![s](https://latex.csdn.net/eq?s)并不能准确描述真实的上下文扩展尺度。在实践中，对于给定的上下文长度扩展，尺度值![s](https://latex.csdn.net/eq?s)必须设置得高于预期尺度

值得一提的是，在本文发布前不久，Code Llama\[31\]发布了，并通过手动将基数b扩展到1M(使用“NTK-aware”扩展)，详见此文《[代码生成的原理解析：从Codex、GitHub Copliot到CodeLlama(用了NTK-aware)、CodeGeex](https://blog.csdn.net/v_JULY_v/article/details/134765564 "代码生成的原理解析：从Codex、GitHub Copliot到CodeLlama(用了NTK-aware)、CodeGeex")》的第三部分

> 最后，梳理一下各自提出的在时间顺序，可得
> 
> -   23年6月底，Meta发布位置插值PI
> -   23年7月初，bloc97于Reddit提出NTK-Aware插值
> -   23年8月底，Meta发布code llama
> -   23年9月底，Meta发布llama2 long

### 3.2 LLaMA2 Long对长度的拉长：基于本质是NTK的**RoPE ABF**

#### 3.2.1 LLaMA2 Long相比LLaMA 2的变化：修改位置编码 长度达到32K

23年9月底\[_Submitted on 27 Sep 2023 (v1), last revised 14 Nov 2023 (this version, v3)_\]，GenAI, Meta正式发布LLaMA 2 Long(_这是其论文《_[Effective Long-Context Scaling of Foundation Models](https://arxiv.org/abs/2309.16039 "Effective Long-Context Scaling of Foundation Models")_》_)，与LLaMA 2相比，LLaMA 2 Long的变化主要体现在以下两点

1.  一是训练参数上，采用了**高达4000亿token**的数据源(_We build our models by continually pretraining from LLAMA 2 checkpoints with additional 400 billion tokens formed as long training sequences_)
    
    ——相反，原始LLaMA 2包含多个变体，但最多的版本也只有700亿
2.  二是架构上，与LLaMA 2保持不变，但对位置编码进行了一个非常小的必要修改，以此完成高达3.2万token的上下文窗口支持

在LLaMA 2中，它的位置编码采用的是旋转编码RoPE方法，其通过旋转矩阵来实现位置编码的外推

1.  本质上来说，RoPE就是将表示单词、数字等信息的token embeddings映射到3D图表上，给出它们相对于其他token的位置——即使在旋转时也如此
2.  这就能够使模型产生准确且有效的响应，并且比其他方法需要的信息更少，因此占用的计算存储也更小

然，Meta的研究人员通过对70亿规模的LLaMA 2进行实验，确定了LLaMA 2中的RoPE方法的一个**局限性**，即，阻止注意力模块聚集远处token的信息

为此，Meta想出了一个非常简单的破解办法，即

> **减少每个维度的旋转角度**_(which essentially reduces the rotation angles of each dimension_)
> 
> ___
> 
> 具体而言就是将超参数“基频(base frequency)b”从10000增加到500000(_increasing the “base frequency b” of ROPE from 10, 000 to 500, 000)_

嗯，我知道大家要说什么了，没错，如你所见，llama long原论文提到，对于这个基频参数的修改，除了llama long之外，社区**Reddit**和**code llama**也同时运用了——嗯，上面都提到了，^\_^

> 即：“_We propose a simple modification to the default RoPE encoding to reduce the decaying effect – increasing the “base frequency b” of ROPE from 10, 000 to 500, 000, which essentially reduces the rotation angles of each dimension. The idea is also concurrently suggested in_
> 
> -   _the Reddit r/LocalLLaMa community
>     
>     如上文所说的，_**社区Reddit的发现来源于这里**：
> -   _and Rozière et al. (2023)_.”
>     
>     **Rozière et al. (2023)对应的是Code llama**: Open foundation models for code, 2023「_其Submitted on **24 Aug 2023** (v1), last revised 31 Jan 2024 (this version, v3)，原论文中提到：we increase the base period θ from 10,000 to 1,000,000 for fine-tuning_」
>     
>     至于对code llama的介绍详见此文《[代码生成的原理解析：从Codex、GitHub Copliot到CodeLlama、CodeGeex](https://blog.csdn.net/v_JULY_v/article/details/134765564 "代码生成的原理解析：从Codex、GitHub Copliot到CodeLlama、CodeGeex")》的第三部分
>     

此外，在附录中，Meta还通过可视化为螺旋图这一非常有趣的方式，**将**

**RoPE ABF**『_如苏剑林从此文《_[Transformer升级之路：16、“复盘”长度外推技术](https://www.spaces.ac.cn/archives/9948 "Transformer升级之路：16、“复盘”长度外推技术")_》中所讲的，Meta在LLaMA 2 Long这篇论文《Effective Long-Context Scaling of Foundation Models》中，_**<u><em>将NTK-RoPE称为RoPE-ABF（Adjusted Base Frequency</em></u>**_）__，相比神秘的NTK，ABF的名称可能更直观体现出它的含义_』

**与**

**RoPE PI的差异进行了理论分析**

![](https://i-blog.csdnimg.cn/blog_migrate/c38b5ad0ffaa5750dcba37a6e89db456.png)

-   图b所代表的PI 旨在展示位置插值对映射向量相对位置的影响。与上图a相比，相邻点之间的距离显著减小
    
    _Figure 8b aims to illustrate the impact of Position Interpolation on the relative position of the mapped vectors. The distance between the consecutive points got reduced considerably compered to Figure8a._
    
    ————
    
    **即螺旋线上的原有红点“挤压”到更小的范围，导致相邻红点之间的间距变小，显得拥挤、挤压**
    
    好比办公室面积大小不变，但每个工位的间距从2米变成1米，办公室可以摆放的工位虽然更多了，但工位间距变小使得办公舒适度降低
    
    ![](https://i-blog.csdnimg.cn/blog_migrate/4da4b85e2362cc65ab0efcf1707c553e.png)
-   图c说明了调整基频对结果的影响
    
    _The impact of_ _**Adjusted Base Frequency**(所以meta的这篇llama2 long论文里把这项类似NTK的技术称之为ABF)_ _is illustrated on Figure 8c_
    
    ![](https://i-blog.csdnimg.cn/blog_migrate/c38b5ad0ffaa5750dcba37a6e89db456.png)
    
    尽管螺旋的频率增加导致点之间的最小距离显著减小——_注意，这里的最小距离指的是立体空间中<u><strong>非相邻点</strong></u>之间的距离_(_although the minimal distance between points got considerably reduced due to the increased frequency of the helix_)
    
    但**相邻点**之间的距离几乎与图a 保持一致(_The distance between the **consecutive points** remained almost the same as on Figure 8a_)
    
    且在高维设置下，螺旋频率增加的这种影响会有所减弱(_This effect of increasedfrequency of the helix would be reduced in the high_)
    
    ——————
    
    **说白了，就是相邻红点与红点之间的距离没变——相邻之间不拥挤或挤压，但螺旋频率增加，螺旋更密集**
    
    好比办公室面积不变，每个工位的间距也不变(2米)，但原来一层楼的房子被改造成复式二层，总工位数可翻倍至20个
    
    虽然楼层变多了(高频螺旋)，但相邻工位的舒适度(2米间距)未受影响
    

总之，与RoPE PI相比，**RoPE ABF的优势主要体现在**它能以更大的粒度分配嵌入向量(the embedded vectors)，从而使模型更容易区分位置

此外，他们还观察到，嵌入向量之间的相对距离既对RoPE PI的关键参数有线性依赖性，也对RoPE ABF的关键参数也有对数依赖性。

这也就是为什么可以很容易地对基频这一超参数“下手”

#### 3.2.2 改动之后的效果

这一改动立刻奏效，缩小了RoPE对远端token的衰减效应，并且在扩展LLAMA的上下文长度上优于一项类似的名为“位置插值”的方法RoPE PI(_如下图所示，__RoPE表示基线方法__，__RoPE ABF为Meta此次发明的新方法__，__xPos是另一种应用了该方法的旋转编码变体_)

![](https://i-blog.csdnimg.cn/blog_migrate/51a3d9075781997fd09d060edf27b48f.png)

然，一个问题是，通过上面这个可视化结果，Meta观察到RoPE在长程区域出现了较大的“振荡”，这对于语言建模来说可能不是个好消息

不过，通过报告几种方法在长序列困惑度和FIRST-SENTENCE-RETRIEVAL两个任务上的表现来看，问题不大

![](https://i-blog.csdnimg.cn/blog_migrate/4af250366551950b23300e94107ef35d.png)

而且，尤其在后者任务上，他们提出的RoPE ABF是唯一一个可以始终保持性能的变体

![](https://i-blog.csdnimg.cn/blog_migrate/3ca5b62a346eff0f6ad530a4560f6773.png)

最终，LLaMA 2 Long凭借着这一改动，达成了3.2万的上下文token，并通过长下文连续预训练的共同作用，全面超越LLaMA 2、在特定任务上超越Claude 2和ChatGPT

//待更

### 3.3 相对局部距离的损失-“NTK-by-parts”插值

在本节伊始，得先介绍一个概念，即波长，所谓波长，其描述的在维![d](https://latex.csdn.net/eq?d)上嵌入的RoPE，执行完整旋转(2π)所需的标记长度

> 一般而言，把![\lambda_{d}](https://latex.csdn.net/eq?%5Clambda_%7Bd%7D)定义为RoPE嵌入在第![d](https://latex.csdn.net/eq?d)隐维处的波长：
> 
> ![\lambda_{d}=\frac{2 \pi}{\theta_{d}}=2 \pi b^{\frac{2 d}{|D|}}](https://latex.csdn.net/eq?%5Clambda_%7Bd%7D%3D%5Cfrac%7B2%20%5Cpi%7D%7B%5Ctheta_%7Bd%7D%7D%3D2%20%5Cpi%20b%5E%7B%5Cfrac%7B2%20d%7D%7B%7CD%7C%7D%7D)

有一些插值方法(例如位置插值PI)不关心波长的维数，我们将这些方法称为“盲”插值方法(blind interpolation)，比如像PI和“NTK-aware”插值这样的blind interpolation方法中，我们面对所有RoPE隐藏维度的没有做任何针对性的处理(因为它们对网络有相同的影响)，而其他方法(如YaRN)，我们将其归类为“有针对性的”插值方法

进一步，关于RoPE嵌入的一个有趣的观察是

为了解决上述问题，选择不插值更高频率的维度，而总是插值更低频率的维度(_we choose not to interpolate the higher frequency dimensions at all while always interpolating the lower frequency dimensions_)。特别是

因此，在原始上下文大小![L](https://latex.csdn.net/eq?L)和波长![\lambda](https://latex.csdn.net/eq?%5Clambda)之间引入比率![r=\frac{L}{\lambda}](https://latex.csdn.net/eq?r%3D%5Cfrac%7BL%7D%7B%5Clambda%7D)，且在第![d](https://latex.csdn.net/eq?d)个隐状态下，比率![r](https://latex.csdn.net/eq?r)以如下方式依赖于![d](https://latex.csdn.net/eq?d)

![r(d)=\frac{L}{\lambda_{d}}=\frac{L}{2 \pi b^{\prime \frac{2 d}{|D|} \mid}}](https://latex.csdn.net/eq?r%28d%29%3D%5Cfrac%7BL%7D%7B%5Clambda_%7Bd%7D%7D%3D%5Cfrac%7BL%7D%7B2%20%5Cpi%20b%5E%7B%5Cprime%20%5Cfrac%7B2%20d%7D%7B%7CD%7C%7D%20%5Cmid%7D%7D)

为了确定上述不同插值策略的边界，引入两个额外参数![\alpha](https://latex.csdn.net/eq?%5Calpha)和![\beta](https://latex.csdn.net/eq?%5Cbeta)，且针对所有隐藏维度![d](https://latex.csdn.net/eq?d)

接下来，定义斜坡函数![\gamma](https://latex.csdn.net/eq?%5Cgamma)为

![\gamma(r)=\left\{\begin{array}{ll} 0, & \text { if } r<\alpha \\ 1, & \text { if } r>\beta \\ \frac{r-\alpha}{\beta-\alpha}, & \text { otherwise } \end{array}\right.](https://latex.csdn.net/eq?%5Cgamma%28r%29%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7D%200%2C%20%26%20%5Ctext%20%7B%20if%20%7D%20r%3C%5Calpha%20%5C%5C%201%2C%20%26%20%5Ctext%20%7B%20if%20%7D%20r%3E%5Cbeta%20%5C%5C%20%5Cfrac%7Br-%5Calpha%7D%7B%5Cbeta-%5Calpha%7D%2C%20%26%20%5Ctext%20%7B%20otherwise%20%7D%20%5Cend%7Barray%7D%5Cright.)

从而借助该函数，“NTK-by-parts”方法可以定义如下

> “NTK-by-parts”插值是对RoPE的一种修改，基于以下函数
> 
> ![\begin{array}{l} g(m)=m \\ h\left(\theta_{d}\right)=(1-\gamma(r(d))) \frac{\theta_{d}}{s}+\gamma(r(d)) \theta_{d} \end{array}](https://latex.csdn.net/eq?%5Cbegin%7Barray%7D%7Bl%7D%20g%28m%29%3Dm%20%5C%5C%20h%5Cleft%28%5Ctheta_%7Bd%7D%5Cright%29%3D%281-%5Cgamma%28r%28d%29%29%29%20%5Cfrac%7B%5Ctheta_%7Bd%7D%7D%7Bs%7D&plus;%5Cgamma%28r%28d%29%29%20%5Ctheta_%7Bd%7D%20%5Cend%7Barray%7D)
> 
> ![\alpha](https://latex.csdn.net/eq?%5Calpha)和![\beta](https://latex.csdn.net/eq?%5Cbeta)的值根据具体情况进行调整。当然，通过实验发现，对于Llama家族的模型而言，其中![\alpha](https://latex.csdn.net/eq?%5Calpha)和![\beta](https://latex.csdn.net/eq?%5Cbeta)的比较好的取值是![\alpha=1](https://latex.csdn.net/eq?%5Calpha%3D1)和![\beta = 32](https://latex.csdn.net/eq?%5Cbeta%20%3D%2032)

// 待更

### 3.4 **"Dynamic NTK"插值**

在很多用例中，以从1到最大上下文大小不等的序列长度进行多次前向传递。一个典型的例子是自回归生成，其中序列长度在每一步之后递增1

有两种方法可以应用使用比例因子![s](https://latex.csdn.net/eq?s)的插值方法(包括PI、"NTK-aware" and "NTK-by-parts"):

1.  方法1：在整个推理周期中，嵌入层是固定的，包括缩放因子![s=L^{\prime} / L](https://latex.csdn.net/eq?s%3DL%5E%7B%5Cprime%7D%20/%20L)，其中![L'](https://latex.csdn.net/eq?L%27)是固定数量的扩展上下文大小
2.  方法2：在每次前向传递中，位置嵌入更新缩放因子(_the position embedding updates the scale factor_)：![s=\max \left(1, l^{\prime} / L\right)](https://latex.csdn.net/eq?s%3D%5Cmax%20%5Cleft%281%2C%20l%5E%7B%5Cprime%7D%20/%20L%5Cright%29)，其中![l'](https://latex.csdn.net/eq?l%27)是当前序列的序列长度

上述方法中，方法1的问题在于模型在长度小于![L](https://latex.csdn.net/eq?L)时可能出现性能折扣，当序列长度大于![L'](https://latex.csdn.net/eq?L%27)时可能出现突然退化

对此，故提出了方法2，我们称这种推理时间方法为动态缩放方法，当再与“NTK-aware”插值相结合时，我们称之为“动态NTK”插值

一个值得注意的事实是，“动态NTK”插值在![L](https://latex.csdn.net/eq?L)上预训练的模型上工作得非常好，而不需要任何微调(![L' =L](https://latex.csdn.net/eq?L%27%20%3DL))

## 第四部分 YaRN全面解析

介绍完“NTK-aware”插值、“NTK-by-parts”插值、"Dynamic NTK"插值之后，接下来即将介绍YaRN(另一种RoPE扩展方法)，这是一种改进的方法，可以有效地扩展使用旋转位置嵌入(RoPE)训练的模型的上下文窗口，包括LLaMA\[38\]、GPT-NeoX\[5\]和PaLM\[10\]家族的模型

### 4.1 YaRN怎么来的：基于“NTK-by-parts”插值修改注意力

除了前述的插值技术，我们还观察到，在对logits进行softmax操作之前引入温度t可以统一地影响困惑度，无论数据样本和扩展上下文窗口上的token位置如何，更准确地说，我们将注意力权重的计算修改为

![\operatorname{softmax}\left(\frac{\mathbf{q}_{m}^{T} \mathbf{k}_{n}}{t \sqrt{|D|}}\right)](https://latex.csdn.net/eq?%5Coperatorname%7Bsoftmax%7D%5Cleft%28%5Cfrac%7B%5Cmathbf%7Bq%7D_%7Bm%7D%5E%7BT%7D%20%5Cmathbf%7Bk%7D_%7Bn%7D%7D%7Bt%20%5Csqrt%7B%7CD%7C%7D%7D%5Cright%29)

通过将RoPE重新参数化为一组2D矩阵对，给实现注意力缩放带来了明显的好处(_The reparametrization of RoPE as a set of 2D matrices has a clear benefit on the implementation of this attention scaling_)

1.  我们可以利用“长度缩放”技巧，简单地将复杂的RoPE嵌入按相同比例进行缩放，使得qm和kn都以常数因子![\sqrt{1 / t}](https://latex.csdn.net/eq?%5Csqrt%7B1%20/%20t%7D)进行缩放
    
    这样一来，在不修改代码的情况下，YaRN能够有效地改变注意力机制
    
    _we can instead use a "length scaling" trick which scales both **q**m and **k**n by a constant factor p 1/t by simply scaling the complex RoPE embeddings by the same amount.
    
    With this, YaRN can effectively alter the attention mechanism without modifying its code._
2.  此外，在推理和训练期间，它没有额外开销，因为RoPE嵌入是提前生成并在所有向前传递中被重复使用的。结合“NTK-by-parts”插值方法，我们就得到了YaRN方法
    
    _Furthermore, it has zero overhead during both inference and training, as RoPE embeddings are generated in advance and are reused for all forward passes. Combining it with the "NTK-by-parts" interpolation, we have the YaRN method_

对于LLaMA和LLaMA 2模型，我们推荐以下值:

![\sqrt{\frac{1}{t}}=0.1 \ln (s)+1](https://latex.csdn.net/eq?%5Csqrt%7B%5Cfrac%7B1%7D%7Bt%7D%7D%3D0.1%20%5Cln%20%28s%29&plus;1)

上式是在未进行微调的LLaMA 7b、13b、33b和65b模型上，使用“NTK-by-parts”方法对各种因素的尺度扩展进行最小困惑度![\sqrt{1 / t}](https://latex.csdn.net/eq?%5Csqrt%7B1%20/%20t%7D)拟合得到的(_The equation above is found by fitting p 1/t at the lowest perplexity against the scale extension by various factors s using the "NTK-by-parts" method_)

且相同的t值也适用于Llama 2模型(7b、13b和70b)，这表明熵增加和温度常数t的性质可能具有一定的“普遍性”，并且可以推广到某些模型和训练数据中(_It suggests that the property of increased entropy and the temperature constant t may have certain degree of "universality" and may be generalizable across some models and training data_)

1.  YaRN方法在微调和非微调场景中均超过以前所有方法，由于其占用空间较小，YaRN与修改注意力机制库(如Flash Attention 2\[13\])直接兼容
2.  且在对不到0.1%的原始预训练数据进行微调后，YaRN在上下文窗口扩展中达到了最先进的性能
    
    同时，如果YaRN与动态缩放的推理技术相结合而得到的Dynamic-yarn，其允许在超过2倍的上下文窗口扩展，而无需任何微调

### 4.2 实际应用效果

// 待更

## 第五部分 LongLora所用的Shifted Sparse Attention(S2-Attn)

此部分独立成文为：[从LongLoRA到LongQLoRA(含源码剖析)：超长上下文大模型的高效微调方法](https://blog.csdn.net/v_JULY_v/article/details/135375799 "从LongLoRA到LongQLoRA(含源码剖析)：超长上下文大模型的高效微调方法")

## 参考文献与推荐阅读

1.  了解几种外推方案做了什么
    
    [https://zhuanlan.zhihu.com/p/647145964](https://zhuanlan.zhihu.com/p/647145964 "https://zhuanlan.zhihu.com/p/647145964")
    
    [https://zhuanlan.zhihu.com/p/642398400](https://zhuanlan.zhihu.com/p/642398400 "https://zhuanlan.zhihu.com/p/642398400")
2.  [Transformer升级之路：7、长度外推性与局部注意力](https://kexue.fm/archives/9431 "Transformer升级之路：7、长度外推性与局部注意力")
3.  [Transformer升级之路：10、RoPE是一种β进制编码](https://kexue.fm/archives/9675 "Transformer升级之路：10、RoPE是一种β进制编码")
4.  [大语言模型结构之：RoPE位置外推](https://zhuanlan.zhihu.com/p/670244026 "大语言模型结构之：RoPE位置外推")
5.  大模型上下文长度扩展的一篇综述文献
    
    [The What, Why, and How of Context Length Extension Techniques in Large Language Models -- A Detailed Survey](https://arxiv.org/abs/2401.07872 "The What, Why, and How of Context Length Extension Techniques in Large Language Models -- A Detailed Survey")
6.  [RoPE外推的缩放法则 —— 尝试外推RoPE至1M上下文](https://zhuanlan.zhihu.com/p/660073229 "RoPE外推的缩放法则 —— 尝试外推RoPE至1M上下文")

## 创作、修订、完善记录

1.  23年12.19，开始写本文的前两部分
2.  12.21，修订第二部分..
3.  12.22，新增一节：“2.1.2 位置![n](https://latex.csdn.net/eq?n)的旋转位置编码(RoPE)，本质上就是数字![n](https://latex.csdn.net/eq?n)的![\beta](https://latex.csdn.net/eq?%5Cbeta)进制编码”
    
    且结合苏剑林的博文，补充说明：2.4.1 “NTK-aware”插值：高频外推，低频内插
    
    另，开始写：2.4.2 相对局部距离的损失-“NTK-by-parts”插值，和第三部分 YaRN的全面解析
4.  ___
    
    24年4.28，根据PI的论文多次优化本文2.3节的内容
5.  4.29，新增第五部分的标题
    
    第五部分 LongLora所用的Shifted Sparse Attention(S2-Attn)
6.  5.8，把原来属于「第二部分 从RoPE、直接外推、位置内插到NTK-aware/NTK-by-parts/Dynamic NTK插值」的内容拆成两部分，成为如今的
    
    第二部分 从RoPE、直接外推到位置内插Position Interpolation
    
    第三部分 从NTK-aware、NTK-by-parts到Dynamic NTK插值
7.  7.31，受修订此文《一文通透位置编码：从标准位置编码、旋转位置编码RoPE到ALiBi、LLaMA 2 Long(含NTK-aware简介)》给带来的启发，完善本文相关细节
8.  ___
    
    **25**年8.17，为了让扩大模型上下文长度相关的技术 更好的统一在本文之中
    
    一方面，**把「3.2 LLaMA 2 Long对长度的拉长：基于本质是NTK的RoPE ABF」部分的内容**，从《一文通透位置编码：从标准位置编码、旋转位置编码RoPE到ALiBi、LLaMA 2 Long(含NTK-aware简介)》中取出来，**放到了本文中**
    
    且反复修改，毕竟有榜样可依，即李杜诗篇往年传，乃千锤百炼之果
    
    二方面，把ALiBi部分的内容，也放到了本文中
    
9.  ..
