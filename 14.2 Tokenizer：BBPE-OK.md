---
---


## 摘要

[[14.1 Tokenizer：BPE-OK]]经详细地剖析了 BPE算法了，那么这篇文章剖析的就是 BPE 算法的进阶版本：BBPE 算法

BPE 是把字符对（如英文字符对（'a','b'））不断循环地对字符对合并组成新的字符，这种方法的好处是当遇到合并的字符，可以用合并后的字符对来表示，而遇到不是合并后的字符对时，用初始字符来表示（如果是英文的话就是 26 个英文字符）。

举个例子：现在有一个合并后的字符对（a,b），当对字符串为"abc"进行编码时，就可以编码为（a,b）c，这样就可以大大地对字符串进行压缩，并且就表义时，一个token可以表示的范围更大了。

但是，如果训练的语料不是英文，或者为特殊符号如（emoji表情），那么该怎么办呢？？？

为了解决上面的问题，BBPE 算法诞生了。

## BBPE 的思想是什么？

BBPE **用byte 构建最基础词表，将 BPE 的从字符级别扩展到子节（Byte）级别**。在 byte 序列上使用 BPE 算法进行相邻合并，其核心思想是从字节开始，不断找词频最高、且连续的两个字节组合合并，直到达到目标词数。这样的方式可以更好地处理多语言文本和特殊字符。例如，在处理包含多种语言的文本时，传统的 BPE 可能会因为不同语言的字符编码差异而遇到问题，但 BBPE 以一个字节为一种 “字符”，不管实际字符集用了几个字节来表示一个字符。

## BBPE 算法实践

### bbpe训练文本

### 统计字节对频率和合并

首先输入一段文本（这段文本我乱输入的，没有任何含义）进行统计词频，但BBPE 在统计词频的时候会把字符都改为字节

```
original word: 你好啊
word to byte: \xe4 \xbd \xa0 \xe5 \xa5 \xbd \xe5 \x95 \x8a
original word: 你好
word to byte: \xe4 \xbd \xa0 \xe5 \xa5 \xbd
original word: 你好啊
word to byte: \xe4 \xbd \xa0 \xe5 \xa5 \xbd \xe5 \x95 \x8a
original word: 你好
word to byte: \xe4 \xbd \xa0 \xe5 \xa5 \xbd
original word: 我
word to byte: \xe6 \x88 \x91
original word: 啊
word to byte: \xe5 \x95 \x8a
original word: 走
word to byte: \xe8 \xb5 \xb0

Initial vocabulary frequency: 
defaultdict(<class 'int'>, 
{'\\xe4 \\xbd \\xa0 \\xe5 \\xa5 \\xbd \\xe5 \\x95 \\x8a': 2, 
'\\xe4 \\xbd \\xa0 \\xe5 \\xa5 \\xbd': 2, '\\xe6 \\x88 \\x91': 1, 
'\\xe5 \\x95 \\x8a': 1, '\\xe8 \\xb5 \\xb0': 1})
```

然后不断地统计字节对的出现的频率，把频率最多的字节对合并在一起,然后在Initial vocabulary frequency中替代原本的字节。

在这里我设置的合并循环次数为 3

```
num_merge: 0
best_pair: ('\\xe4', '\\xbd')
合并前的vocab: {'\\xe4  \\xbd  \\xa0  \\xe5 \\xa5  \\xbd  \\xe5  \\x95  \\x8a': 2, '\\xe4 \\xbd \\xa0 \\xe5 \\xa5 \\xbd': 2, '\\xe6 \\x88 \\x91': 1, '\\xe5 \\x95 \\x8a': 1, '\\xe8 \\xb5 \\xb0': 1})
合并后的vocab: {'\\xe4\\xbd  \\xa0  \\xe5  \\xa5  \\xbd  \\xe5  \\x95 \\x8a': 2, '\\xe4\\xbd \\xa0 \\xe5 \\xa5 \\xbd': 1}
num_merge: 1
best_pair: ('\\xe4\\xbd', '\\xa0')
合并前的vocab: {'\\xe4\\xbd  \\xa0  \\xe5  \\xa5 \\xbd \\xe5  \\x95  \\x8a': 2, '\\xe4\\xbd \\xa0 \\xe5 \\xa5 \\xbd': 1}
合并后的vocab: {'\\xe4\\xbd\\xa0  \\xe5  \\xa5  \\xbd  \\xe5  \\x95  \\x8a': 2, '\\xe4\\xbd\\xa0 \\xe5 \\xa5 \\xbd': 1}
num_merge: 2
best_pair: ('\\xe4\\xbd\\xa0', '\\xe5')
合并前的vocab: {'\\xe4\\xbd\\xa0  \\xe5  \\xa5  \\xbd  \\xe5  \\x95  \\x8a': 2, '\\xe4\\xbd\\xa0 \\xe5 \\xa5 \\xbd': 1}
合并后的vocab: {'\\xe4\\xbd\\xa0\\xe5  \\xa5  \\xbd  \\xe5  \\x95  \\x8a': 2, '\\xe4\\xbd\\xa0\\xe5 \\xa5 \\xbd': 1}
```

从num\_merge: 0 中可以看到，合并前的vocab：\\\\xe4 \\\\xbd（字节对中有空格），合并后的vocab:\\\\xe4\\\\xbd(没有空格，再后续的操作中可以把这个字节对看作为一个整体)

### **合并字节对的过程解析：**

```
merge_rules: {('\\xe4', '\\xbd'): 0, ('\\xe4\\xbd', '\\xa0'): 1, ('\\xe4\\xbd\\xa0', '\\xe5'): 2}
```

merge\_rules的键为合并的字节对，值为合并的第几轮进行的合并

从上面的规则可以看出，其合并的过程为

第一步('\\\\xe4', '\\\\xbd')合并为'\\\\xe4\\\\xbd'

第二步('\\\\xe4\\\\xbd', '\\\\xa0')合并为\\\\xe4\\\\xbd\\\\xa0'

第三步(‘\\\\xe4\\\\xbd\\\\xa0’, '\\\\xe5')合并为‘\\\\xe4\\\\xbd\\\\xa0\\\\xe5'

### bbpe算法的编码

编码很简单，就是对要进行编码的字节串在merge\_rules进行匹配，如果匹配的到就用merge\_rules的值来替代原来的字节串中的值，一直循环，merge\_rules不能匹配到字节串中的任何一个值。

从上面可以看出，后面合并的字节对合并可能是基于上一轮合并的字节对进行的合并，所以我们遍历merge\_rules的顺序应该从后往前开始遍历来匹配字节，既从\\\\xe4\\\\xbd\\\\xa0\\\\xe5往\\\\xe4\\\\xbd进行遍历。

为什么要这样顺序遍历，我这里可以给出一个例子：

现在对\['\\\\xe4 \\\\xbd \\\\xa0 \\\\xe5 \\\\xa5 \\\\xbd \\\\xe5 \\\\x95 \\\\x8a'\]这个字节串进行编码，如果从前往后进行匹配。

那么第一轮匹配后的结果为\[‘\\\\xe4\\\\xbd ','\\\\xa0 ', '\\\\xe5 ',' \\\\xa5',' \\\\xbd ', ' \\\\xe5',' \\\\x95 ',' \\ \\x8a'\]’

但是这样的话，’\\\\xe4\\\\xbd\\\\xa0\\\\xe5‘就不能在下一轮进行匹配了，很明显这是不合理的，因为\\\\xe4\\\\xbd\\\\xa0\\\\xe5是在\\\\xe4\\\\xbd 基础上进行的规则合并，它的优先级是要高于‘\\\\xe4\\\\xbd’的

所以要从后往前进行匹配。

![](https://picx.zhimg.com/v2-aa0333cc1213bb534777babd29295f09_1440w.jpg)

编码后的字节串

### 编码后转化为 token\_id

因为任何一个合并的字节对和字节都有一个id。先看在不在merge\_rules中，如果在就用对应的合并的字节对的id来表示。如果不在merge\_rules中，那么就用最原始的字节来表示其id。

![](https://pic3.zhimg.com/v2-b3b39bf11427a4d2acc45ba252fb28f8_1440w.jpg)

字节串转化为id

### bbpe的解码

解码就十分简单了，只需要把字节都拼在一起，然后decode就好了

![](https://pic1.zhimg.com/v2-4ecc6207ab15c3ed454c6c91814cab3c_1440w.jpg)

解码

## 代码

这个代码是临时写的，可能写的很乱，请多见谅

```python
import regex as re
from collections import OrderedDict
import collections

def merge_and_convert_to_bytes(str_list):
    # 方法1: 直接合并后解码
    merged_str = ''.join(str_list)
    # 移除多余的反斜杠
    merged_str = merged_str.replace('\\\\', '\\')
    # 使用 unicode_escape 解码，然后用 latin1 编码为字节
    result = merged_str.encode('latin1').decode('unicode_escape').encode('latin1')
    return result


def iterate_bytearray():
    byte_array = bytearray(range(256))
    return byte_array
    # for byte in byte_array:
    #     print(f"十进制: {byte}, 十六进制: {hex(byte)}, 字节: {bytes([byte])}")

def generate_letter_dict():
    letter_dict = {}
    bytearray=iterate_bytearray()
    for i ,byte in zip(range(256),bytearray):
        if i==32 or i==98:
            letter_dict['空格'] = i
            continue
        letter_dict[str(bytes([byte])).replace("b'", "'").replace("'", "").replace(" ", "")] = i

    print("letter_dict:", letter_dict)

    return letter_dict

class BPE:
    def __init__(self):
        self.vocab = collections.defaultdict(int)
        self.merge_rules = {}
        self.max_merge_times = 0
        self.tokens_dict = {}
        self.id_to_token_dict = {}
    def get_vocab(self, corpus):
        """获取语料库中每个单词的频率"""
        for word in corpus.split():
            print("original word:", word)
            #print((''.join(list(word)) + ' </w>'))
            word=str((''.join(list(word))).encode("utf-8")).replace("b'", "'").replace("'", "").replace(" ", "")
            print("word to byte:", word)
            new_word = ''
            for i,char in enumerate(word):
                if i==0:
                    new_word+=char
                    continue
                if char=='\\':
                    new_word+=" "+char
                else:
                    new_word+=char
            
            self.vocab[new_word] += 1
            #print("self.vocab:", self.vocab)
        
    def get_stats(self):
        """获取词汇表中相邻字节对的频率"""
        pairs = collections.defaultdict(int)
        for word, freq in self.vocab.items():
            #print("get_stats word:", word)
            symbols = word.split()
            for i in range(len(symbols) - 1):
                pairs[symbols[i], symbols[i+1]] += freq
        
        return pairs

    def merge_vocab(self, pair):
        #print("pair:", pair)
        print("合并前的vocab:", self.vocab)
        """合并最频繁的相邻字节对,更新词汇表"""
        bigram = re.escape(' '.join(pair))
        p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
        new_vocab = {}
        for word in self.vocab:
            if ' '.join(pair) in word:
                new_word = word.replace(' '.join(pair), ''.join(pair))
   
            new_vocab[new_word] = self.vocab[word]
        #print("new_vocab:", new_vocab)
        self.vocab = new_vocab
        print("合并后的vocab:", self.vocab)
    def merge(self, num_merges):
        """执行指定次数的合并操作"""
        self.max_merge_times = num_merges
        for i in range(num_merges):
            print("num_merge:", i)
            pairs = self.get_stats()
            if not pairs:
                break
            best_pair = max(pairs, key=pairs.get)
            print("best_pair:", best_pair)
            self.merge_vocab(best_pair)
            self.merge_rules[best_pair] = i
            #print("self.merge_rules:", self.merge_rules)
        tokens_dict = generate_letter_dict()
        #tokens_dict['</w>']=max(tokens_dict.values())+1
        
        print("merge_rules:", self.merge_rules)
        for key in self.merge_rules.keys():
            tokens_dict[''.join(key)]=max(tokens_dict.values())+1
        with open('merge_result.txt', 'w', encoding='utf-8') as f:
            for key, value in tokens_dict.items():
                key=key.replace("b'", "'").replace("'", "").replace(" ", "")
                f.write(f"{key} {value}\n")
        sorted_items = sorted(self.merge_rules.items(), key=lambda x: x[1],reverse=True)
        self.merge_rules=OrderedDict(sorted_items)
    def get_tokens_dict(self,tokens_dict_file):
        tokens_dict = {}
        with open(tokens_dict_file, 'r', encoding='utf-8') as f:
            for line in f:
        
                key, value = line.split()
                tokens_dict[key] = int(value)
        self.tokens_dict = tokens_dict
        for key, value in tokens_dict.items():
            self.id_to_token_dict[value] = key

        return tokens_dict
    

    def token_to_id(self, tokens):
        # # print("tokens_dict:", self.tokens_dict)
        # # print("tokens:", tokens)
        # # tokens = [token.replace("b'", "'").replace("'", "").replace(" ", "") for token in tokens]
        # print("tokens:", tokens)
        #print("self.tokens_dict:", self.tokens_dict)
        return [self.tokens_dict[token] for token in tokens]

    def id_to_token(self, ids):
        return [self.id_to_token_dict[id] for id in ids]
    def encode(self, text):
        """对文本进行BPE编码"""
        encoded_tokens = []
        tokens = []
        for word in text.split():
            print("in word:", word)
            word=str((' '.join(list(word))).encode("utf-8")).replace("b'", "'").replace("'", "").replace(" ", "")
            new_word = ''
            for i,char in enumerate(word):
                if i==0:
                    new_word+=char
                    continue
                if char=='\\':
                    new_word+=" "+char
                else:
                    new_word+=char
            print("init byte:", new_word)
            print("self.merge_rules:", self.merge_rules)
            while True:
                change = False
                for key, value in self.merge_rules.items():
                    bigram = ''.join(key).replace("b'", "'").replace("'", "")
                    # print("bigram:", bigram)
                    # print("new_word:", new_word)
                    # print("key:", key)
                    new_bigram=""
                    for i,char in enumerate(bigram):
                        if i==0:
                            new_bigram+=char
                            continue
                        if char=='\\':
                            new_bigram+=" "+char
                        else:
                            new_bigram+=char
                    # print("bigram:", type(bigram))
                    # print("word:", type(word))
                    #print("new_word:", "|"+new_word+"|")
                    #print("new_bigram:", "|"+new_bigram+"|")
                    if new_bigram in new_word:
                        #print("naaa")
                        new_word = new_word.replace(new_bigram, bigram)
                       #     print("change word:", word)
                        #print("replace new_word:", new_word)
                        change = True
                #print("replace new_word:", new_word)
                #break
                if not change:
                    tokens.append(new_word)
                    break
            #encoded_tokens.extend(tokens)
        return tokens
    
    def decode(self, ids_list):
        return ' '.join([self.id_to_token(ids) for ids in ids_list])
        




# 模拟merge过程
corpus = "你好啊 你好 你好啊 你好 我 啊 走"
print(f"Original text: {corpus}")

bpe = BPE()
bpe.get_vocab(corpus)
print(f"\nInitial vocabulary: {bpe.vocab}")

num_merges = 3
bpe.merge(num_merges)
print(f"\nMerge rules: {bpe.merge_rules}")
bpe.get_tokens_dict('merge_result.txt')
#print(f"\nTokens dictionary: {bpe.tokens_dict}")
# 对文本进行编码
print("输入text:", corpus)
encoded_text = bpe.encode(corpus)
print(f"\nEncoded text: {encoded_text}")

ids_list = []
for encoded_token in encoded_text:
    print("encoded_token:", encoded_token, "token_to_id:", bpe.token_to_id(encoded_token.split()))
    ids_list.append(bpe.token_to_id(encoded_token.split()))


for ids in ids_list:
    print("ids:", ids, "id_to_token:", bpe.id_to_token(ids))
    try:
        result1 = merge_and_convert_to_bytes(bpe.id_to_token(ids))
        decoded_text = result1.decode('utf-8')
        print("解码后的文本:", decoded_text)
    except UnicodeDecodeError:
        print("无法解码为UTF-8文本")
    
```

## 在大语言模型中的应用

现在很多大语言模型采用 BBPE 分词，如 **GPT、qwen2** 等，这主要是因为 BBPE 具有诸多优势。
- 首先，BBPE 可以有效地平衡词汇表大小和步数（编码句子所需的 token 数量）。在大语言模型中，词汇表大小直接影响模型的存储需求和计算复杂度。如果词汇表过大，会增加模型的训练和推理时间；如果词汇表过小，又会导致信息丢失，影响模型的性能。BBPE 通过在字节级别进行操作，能够将高频词依旧切分成完整的整词，低频词被切分成有意义的子词，从而有效地平衡了词汇表大小和步数。
- 其次，BBPE 能够处理未登录词，提高模型的泛化能力。在自然语言处理中，未登录词是一个常见的问题。如果模型不能有效地处理未登录词，就会导致信息丢失，影响模型的性能。BBPE 通过将单词拆分成有意义的子词，可以更好地处理未登录词，提高模型的泛化能力。
- 最后，BBPE 可以跨语言共用词表，显著压缩词表的大小。在大语言模型中，通常需要处理多种语言的文本。如果每种语言都使用独立的词表，会增加模型的存储需求和计算复杂度。BBPE 以字节为基础进行操作，可以跨语言共用词表，显著压缩词表的大小，提高模型的处理效率和性能。