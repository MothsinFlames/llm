---
created: 2025-08-20T14:17:25 (UTC +08:00)
tags: [大模型,多模态大模型,ChatGPT]
source: https://zhuanlan.zhihu.com/p/689868547
author: 北京大学 计算机科学与技术硕士
---

# 五光十色的多模态大模型——浅探视觉-语言大模型的关键模块设计

> ## Excerpt
> 引言 多模态大模型的视觉编码器用哪种预训练ViT？两阶段训练是否有必要？ViT的参数应该冻结还是打开？大语言模型应该用Base还是Chat版本？是否要加入纯语言的安全对齐数据？训几个epoch合适？......随着大语言模型…

---
### 引言

**多模态大模型的[视觉编码器](https://zhida.zhihu.com/search?content_id=241434856&content_type=Article&match_order=1&q=%E8%A7%86%E8%A7%89%E7%BC%96%E7%A0%81%E5%99%A8&zhida_source=entity)用哪种预训练ViT？两阶段训练是否有必要？ViT的参数应该冻结还是打开？大语言模型应该用Base还是Chat版本？是否要加入纯语言的安全对齐数据？训几个epoch合适？......**

随着大语言模型进入多模态时代，LLaVa、[MiniGPT-4](https://zhida.zhihu.com/search?content_id=241434856&content_type=Article&match_order=1&q=MiniGPT-4&zhida_source=entity)、[BLIP-2](https://zhida.zhihu.com/search?content_id=241434856&content_type=Article&match_order=1&q=BLIP-2&zhida_source=entity)、[InstructBLIP](https://zhida.zhihu.com/search?content_id=241434856&content_type=Article&match_order=1&q=InstructBLIP&zhida_source=entity)等开源视觉-语言大模型接踵而至，它们在视觉编码器、大语言模型、训练方案等关键方面各不相同，颇有“乱花渐欲迷人眼”之感。近日，Standford的Percy Liang团队在LLaVa-v1.5的基础上对各种视觉-语言模型的关键组件进行了系统的消融实验分析，总结成了[Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2402.07865)这篇文章，今天我们来进行解读，以挖掘视觉-语言模型设计中的关键要素，借鉴此文推荐的best practice。

![](https://pica.zhimg.com/v2-1a75c2fc604851cca6b5725d11a0ff64_1440w.jpg)

### 设计空间：多模态大模型的设计需要关注哪些关键要素？

目前开源多模态大模型的架构设计和训练方案逐渐趋同，典型方案如下：

-   **模型架构**上，以CLIP或者其他方式预训练的ViT编码图像特征，通过MLP/[Q-Former](https://zhida.zhihu.com/search?content_id=241434856&content_type=Article&match_order=1&q=Q-Former&zhida_source=entity)等桥接模块后得到了和语言空间对齐的特征，作为图像表征输入LLaMa等主干大语言模型（下图给出了LLaVa-v1.5的架构）；
-   **训练方案**上，先冻结住ViT和语言大模型，在大规模图像-文本对上训练桥接模块进行特征的对齐，第二阶段则打开桥接模块和大语言模型的参数，在高质量的视觉[SFT数据](https://zhida.zhihu.com/search?content_id=241434856&content_type=Article&match_order=1&q=SFT%E6%95%B0%E6%8D%AE&zhida_source=entity)上进行Instruction Tuning。

![](https://pica.zhimg.com/v2-c90cc3a38d06c18e98e4eef7bec10b1c_1440w.jpg)

LLaVa-v1.5的模型架构哦

在这套框架之下，存在诸多可以变换的关键要素，本文将它们归纳为以下四个部分:

1.  **优化策略**：视觉编码器的参数应该冻结还是打开？两阶段训练是否有必要？
2.  **视觉表征和图片处理**：使用哪一类预训练ViT最好？图像如何进行预处理效果最好？
3.  **大语言模型的选择**：应该使用base模型还是chat模型？和纯语言的对齐数据联合训练是否有用？
4.  **Scaling性质**：最佳训练epochs和数据量。

![](https://pic1.zhimg.com/v2-1dedbc135769f79052679ff680c2e64a_1440w.jpg)

多模态大模型的设计空间

### 评测数据集与训练设定

为了评价各关键组件的不同选择的优劣，本文构建了一套全面的视觉-语言下游任务评测数据集，以采用不同关键组件选择的模型在它们上面的zero-shot测试性能评价不同组件选择的好坏。这套benchmark包含以下数据集：

-   **Open-Ended VQA**：VizWiz、VQAv2、GQA和TextVQA；
-   **Localization**: RefCOCO、RefCOCO+、RefCOCOg、OCID-Ref；
-   **Challeng Sets**: [VSR](https://zhida.zhihu.com/search?content_id=241434856&content_type=Article&match_order=1&q=VSR&zhida_source=entity)（空间关系推理）、[TallyQA](https://zhida.zhihu.com/search?content_id=241434856&content_type=Article&match_order=1&q=TallyQA&zhida_source=entity)（物体计数）和[POPE](https://zhida.zhihu.com/search?content_id=241434856&content_type=Article&match_order=1&q=POPE&zhida_source=entity)（幻觉程度）。

![](https://pica.zhimg.com/v2-612267b161360c2ba9546d5c436dc24c_1440w.jpg)

本文使用的评测数据集组合

训练数据方面，本文则遵从LLaVa-v1.5使用的训练数据，第一阶段对齐训练数据为从CC3M和LAION等大规模图文预训练数据集中选取的558K图文对，第二阶段SFT数据由LLaVa Synthetic Data、Standard VQA Data、Multiple Choice VQA Data、Captioning Data、Referring Expression Data、ShareGPT (Language-Only)等混合而成，总数据量为665K。模型方面，本文在LLaVa-v1.5 7B和13B两个规模的模型设定下对各设计要素进行了充分的消融实验分析，下文详细解读。

### 设计要素1：优化策略

首先回顾LLaVa-v1.5的优化策略：如下图所示，第一阶段只打开桥接MLP（Projection）的参数，进行图生文训练，对齐视觉编码器和LLM的表示；第二阶段则打开桥接MLP和LLM的参数，进行多模态SFT。作者提了两个问题：

-   **Q1：两阶段训练是否必要（第一阶段是否可省）？**
-   **Q2: 视觉编码器的参数（ViT）应该打开还是冻结？**

![](https://pic1.zhimg.com/v2-77f9bb570a277e7a490086bc536bdea6_1440w.jpg)

**A1：第一阶段图文对齐训练是冗余的，可以省去以提升训练效率。**

如下图所示，在7B和13B两种规格下，只有第二阶段SFT训出的模型（橙色）在各测试集上的性能都与两阶段训出的模型（绿色）相当或略好，说明**第一阶段图文对齐训练是冗余的，去掉它不会导致性能下降，还能省略20%-25%的训练时间**。

![](https://pic3.zhimg.com/v2-1b9b67361483b9d9b479e3bd73695852_1440w.jpg)

SFT单阶段训练 v.s. 模态对齐+SFT 两阶段训练

**A2：打开视觉编码器（ViT）参数有害，尤其是导致定位任务掉点严重**

如下图所示，采用单阶段训练、冻结住视觉编码ViT参数的模型（橙色）在大多数任务上性能最好，**打开所有ViT参数训练会导致RefCOCO等视觉定位任务严重掉点**。_笔者猜想：可能是SFT训练数据中包含的需要定位能力的部分较少，全量训练破坏了定位所需的视觉特征，如果SFT数据中有定位类型的数据，可能结论会有变化。_

![](https://pica.zhimg.com/v2-4f7017effce11c35a635c50cfb156332_1440w.jpg)

ViT参数是否打开的影响

### 设计要素2：视觉表征和图片处理

首先回顾LLaVa-v1.5的视觉表征和图片处理策略：使用图文对比学习预训练的CLIP ViT抽取视觉表征，图片输入时使用letterbox padding（把非正方形的图像padding成方形）。作者提了两个问题：

-   **Q1: 视觉编码器用哪种预训练策略得到的ViT模型最好？**
-   **Q2：图片预处理策略该用lettebox padding, resize & crop，还是简单的naive resize？**

**A1: CLIP和SigLIP明显优于DINO-v2和ImgaNet-1k ViT；SigLIP+DINO-v2组合最好**

作者首先做了了只有单个预训练ViT模型做视觉编码器时的对比实验，测试了图文对比学习预训练的CLIP、SigLIP（使用Sigmoid Loss优化的CLIP）、纯视觉自监督预训练的DINO-v2和ImageNet-1k上纯视觉有监督预训练的ViT，模型规模都是ViT-Large。结果如下图所示，**使用大规模图文对对比学习预训练的CLIP和SigLIP作为LLaVa-v1.5的视觉编码器时，对应的下游任务性能显著好于纯视觉预训练的DINO-v2和ImageNet-1k ViT。**

![](https://pic4.zhimg.com/v2-f38eecac3d6cad291952d58e06b252ed_1440w.jpg)

作者进一步探究了集成两个预训练模型作为视觉编码器时哪种组合效果较好（集成的方式是在channel，也就是hidden dim维度直接拼接特征），发现**DINO-v2和SigLIP组合的效果最好**。作者猜测该现象是由于DINO-v2编码low-leve特征和SigLIP编码的high-level语义特征可以较好地互补。

![](https://pica.zhimg.com/v2-f475c16a77bfade320b884bb827c8e72_1440w.jpg)

DINO-v2+CLIP和DINO-v2+SigLIP的效果对比

**A2: 对CLIP而言Naive Resize最好，对SIigLIP而言Naive Resize与Letterbox Padding相当**

作者在使用CLIP、SigLIP作为视觉编码器的模型上分别比较了各种图像预处理策略的好坏，发现如下图左所示，**对CLIP而言，简单的Naive Resize最好，而对SigLIP而言Naive Resize与Letterbox Padding的效果相当，对两种视觉编码器而言resize & crop的性能是最差的，因为crop部分区域会导致信息损失**。

![](https://pic2.zhimg.com/v2-6132f6ca78336ccb4587fd10032fb233_1440w.jpg)

另外，作者还探究了图像分辨率对下游任务效果的影响。如上图右所示，将图片分辨率从224开大到384对许多任务都能带来显著提升，当然相应的视觉token数目和计算代价也会显著上升。

### 设计要素3：基座大语言模型的选择

首先回顾LLaVa-v1.5的基座大模型选择和训练数据选取策略：使用从LLaMa-2 SFT而来的Vicuna-v1.5作为基座大模型，训练时加入纯语言的安全对齐数据。作者对此提了两个问题：

-   **Q1：使用base模型好还是经过SFT的instruct-tuned模型好？**
-   **Q2：纯语言的安全对齐数据是否能增强模型在图文任务上的安全性？**

**A1：base模型和instruct-tune模型在下游任务上效果接近，但base模型幻觉更少**

如下面的雷达图所示，LLaMa-2 base充当基座的模型在各下游任务上和Vicuna-v1.5这个instruct-tuned版本充当基座的模型效果**基本相当**，而且在VSR（Visual Spatial Reasoning）上LlaMa-2 base对应的性能明显更好。在可量化的下游任务指标之外，作者还定性评估了两类模型的幻觉程度，发现**base模型产生的幻觉更少**：

> Instruct-tuned LMs lead to VLMs that are more verbose, prone to hallucination, and generally less specific in their response.

因此作者**推荐在构建多模态大模型时使用base版本的LLM作为文本侧的基座**。

![](https://picx.zhimg.com/v2-931d6e7a61078774b5d34a2a8f3d423d_1440w.jpg)

LLaMa-2 base充当基座的模型在各下游任务上和Vicuna-v1.5这个instruct-tuned版本充当基座的模型效果基本相当

![](https://picx.zhimg.com/v2-50b025ddea244e1a67d18edad8dbb5cb_1440w.jpg)

LLaMa-2 base充当基座的模型比Vicuna-v1.5这个instruct-tuned版本充当基座的模型幻觉更少

**A2: 在纯语言的安全数据上联合训练有助于提升多模态任务的安全性**

LLaVa-v1.5使用的训练数据包括纯文本数据集ShareGPT中的40K样本，含有大量恶意prompt和对应的ChatGPT产生的较安全的回复。作者对这部分训练数据进行了消融实验，发现**纯语言的安全数据几乎不影响各种多模态下游任务的量化指标（下图左），但能增强对话的安全性，降低种族主义等有害倾向（下图右）。**

![](https://pic4.zhimg.com/v2-8a471183f5a8bcb44196eea298dfc4dd_1440w.jpg)

纯语言的安全数据几乎不影响各种多模态下游任务的量化指标（左），但能增强对话的安全性，降低种族主义等有害倾向（右）。

### 设计要素4：Scaling性质

作者在最后探究了训练epoch数、训练数据量等scaling相关的性质，关键结论有：

-   **训练时间：两个epoch最合适，再增加训练时间收益不显著；**
-   **训练数据：在LLaVa-v1.5的数据基础上添加图像多样性高的LRV-Instruct数据集带来的提升显著。**

![](https://pic3.zhimg.com/v2-10998a080e19d34d5f94010742066cb0_1440w.jpg)

### 小结

总结一下本文归纳的一套多模态大模型训练最佳实践：

-   **优化策略：打开桥接模块和语言模型的参数进行单阶段训练，冻结视觉编码器的参数；**
-   **视觉表征：使用SigLIP或者SigLIP+DINO-v2当视觉编码器，图像预处理用naive resize；**
-   **语言模型：base模型比instrcut-tuned模型好，加入纯语言的安全对齐数据可以增强在多模态任务上的安全性；**
-   **Scaling性质：训练2个epoch比较合适，添加图像多样的多模态SFT能带来显著收益。**

作者采用以上的best practice训练了7B和13B版本的Prism模型，从下图可以看出性能显著优于同等参数量的LLaVa-v1.5和InstructBLIP，代码和模型开源在[GitHub - TRI-ML/prismatic-vlms: A flexible and efficient codebase for training visually-conditioned language models (VLMs)](https://link.zhihu.com/?target=https%3A//github.com/TRI-ML/prismatic-vlms)。

![](https://pic1.zhimg.com/v2-89bea71e044d806bfe398b9fd47ab2f4_1440w.jpg)
