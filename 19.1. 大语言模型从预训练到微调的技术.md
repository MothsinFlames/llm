---
title: "[PPT]浅析大语言模型从预训练到微调的技术原理"
source: https://zhuanlan.zhihu.com/p/647843722
author: []
published: 
created: 2025-03-06
description: 0. 大纲LLaMA、ChatGLM、Falcon等大语言模型的比较tokenizer、位置编码、Layer Normalization、激活函数等大语言模型的分布式训练技术数据并行、张量模型并行、流水线并行、3D并行零冗余优化器ZeRO、CPU卸载技术Ze…
tags:
---
## 0\. 大纲
- **LLaMA、ChatGLM、[Falcon](https://zhida.zhihu.com/search?content_id=232095641&content_type=Article&match_order=1&q=Falcon&zhida_source=entity)等大语言模型的比较**
	- tokenizer、[位置编码](https://zhida.zhihu.com/search?content_id=232095641&content_type=Article&match_order=1&q=%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81&zhida_source=entity)、[Layer Normalization](https://zhida.zhihu.com/search?content_id=232095641&content_type=Article&match_order=1&q=Layer+Normalization&zhida_source=entity)、[激活函数](https://zhida.zhihu.com/search?content_id=232095641&content_type=Article&match_order=1&q=%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0&zhida_source=entity)等
- **大语言模型的[分布式训练技术](https://zhida.zhihu.com/search?content_id=232095641&content_type=Article&match_order=1&q=%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF&zhida_source=entity)**
	- [数据并行](https://zhida.zhihu.com/search?content_id=232095641&content_type=Article&match_order=1&q=%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C&zhida_source=entity)、[张量模型并行](https://zhida.zhihu.com/search?content_id=232095641&content_type=Article&match_order=1&q=%E5%BC%A0%E9%87%8F%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C&zhida_source=entity)、[流水线并行](https://zhida.zhihu.com/search?content_id=232095641&content_type=Article&match_order=1&q=%E6%B5%81%E6%B0%B4%E7%BA%BF%E5%B9%B6%E8%A1%8C&zhida_source=entity)、3D并行
	- 零冗余优化器ZeRO、CPU卸载技术ZeRo-offload
	- 混合精度训练、激活重计算技术
	- Flash Attention、Paged Attention
- #PEFT
	- prompt tuning、prefix tuning、adapter、LLaMA-adapter、 LoRA
![](https://pic2.zhimg.com/v2-49840d0d1956821bc42968d94ee0174d_1440w.jpg)

## 1\. 大语言模型的细节


### 1.2 训练目标

![](https://picx.zhimg.com/v2-a81c6976ad10fa184e8d06d253e2fb85_1440w.jpg)

### 1.3 tokenizer

![](https://pic2.zhimg.com/v2-f89b3658226267349ca78ec7d24ffebf_1440w.jpg)

### 1.4 位置编码

![](https://pic2.zhimg.com/v2-96cd3f95a7353f567fcc20fa1d0494b3_1440w.jpg)

### 1.5 层归一化

![](https://picx.zhimg.com/v2-a8f6d1b2c1733ce021087465cabcc4c7_1440w.jpg)

### 1.6 激活函数

![](https://pic1.zhimg.com/v2-8559c280267519dee9ecdd9347a1d8b0_1440w.jpg)

### 1.7 Multi-query Attention与Grouped-query Attention

![](https://pic3.zhimg.com/v2-8f59eb9fec3c480719947ce0a21d3766_1440w.jpg)

### 1.8 并行transformer block

![](https://picx.zhimg.com/v2-7eb329d0c97d687e0d8587956ca9be55_1440w.jpg)

### 1.9 总结-训练稳定性

![](https://pic2.zhimg.com/v2-0747dc3f99dab8449754591cf421ea2d_1440w.jpg)

## 2\. LLM的分布式预训练

### 2.0 点对点通信与集体通信

![](https://pic1.zhimg.com/v2-6afec010baec4423ca99f6536aeacc92_1440w.jpg)

![](https://pic4.zhimg.com/v2-128a9ca5f94d419583a9b55e45ca3fbf_1440w.jpg)

### 2.1 数据并行

![](https://pic4.zhimg.com/v2-68c25e51c449391bb34cecf70c12e137_1440w.jpg)

### 2.2 张量并行

![](https://pic3.zhimg.com/v2-b07d479a9e7378f3a4c0832b67af913a_1440w.jpg)

![](https://pic3.zhimg.com/v2-650a56eb0862434851ff512219dab11c_1440w.jpg)

### 2.3 流水线并行

![](https://picx.zhimg.com/v2-fd0d181427806b6f80305dd65ce02519_1440w.jpg)

### 2.4 3D并行

![](https://picx.zhimg.com/v2-76e6883e7810955c0f7e902bd4379d63_1440w.jpg)

### 2.5 混合精度训练

![](https://pic3.zhimg.com/v2-17c4a3719b7a65a34b93198521efe876_1440w.jpg)

### 2.6 激活重计算

![](https://pic1.zhimg.com/v2-96ea3abb7648b07b369794188ae34ed2_1440w.jpg)

### 2.7 ZeRO，零冗余优化器

![](https://picx.zhimg.com/v2-d721953ac25eab11035cd805adf2454b_1440w.jpg)

### 2.8 CPU-offload，ZeRO-offload

![](https://pica.zhimg.com/v2-da9ee996daabce739a7e3c8aab8f472c_1440w.jpg)

### 2.9 Flash Attention

![](https://pic3.zhimg.com/v2-4b2de839febd4a7b894c29af9c593ea2_1440w.jpg)

### 2.10 vLLM: Paged Attention

![](https://pic3.zhimg.com/v2-a9d3cb64f139aee4275833773f312140_1440w.jpg)

## 3\. LLM的参数高效微调

### 3.0 为什么进行参数高效微调？

![](https://pic4.zhimg.com/v2-066f9645ba023050e57de025c2d6e81d_1440w.jpg)

### 3.1 prompt tuning

![](https://pica.zhimg.com/v2-2e40643ec0b9997f6fad6830248f78b8_1440w.jpg)

### 3.2 prefix tuning

![](https://pic2.zhimg.com/v2-5b0baf3c2c1b525d27f41a0a3ad9a8c7_1440w.jpg)

### 3.3 adapter

![](https://pic2.zhimg.com/v2-8e613772407d7c2d4342e9e6aae15145_1440w.jpg)

### 3.4 LLaMA adapter

![](https://pic1.zhimg.com/v2-6c364d3fb4c10f1a0be7bde7953ce8a4_1440w.jpg)

### 3.5 LoRA

![](https://picx.zhimg.com/v2-f653d4b28df1a7f27105254d57fecd0b_1440w.jpg)

### 3.6 实验比较

![](https://pic3.zhimg.com/v2-34f02bd6b4f59bf68e033d861810c086_1440w.jpg)

## 4\. 参考文献

![](https://pica.zhimg.com/v2-89186827ab554b17503c2aad0ac98f32_1440w.jpg)

1. [分析transformer模型的参数量、计算量、中间激活、KV cache](https://zhuanlan.zhihu.com/p/624740065)
2. [【万字长文】LLaMA, ChatGLM, BLOOM的参数高效微调实践](https://zhuanlan.zhihu.com/p/635710004)
3. [FlashAttention:加速计算,节省显存, IO感知的精确注意力](https://zhuanlan.zhihu.com/p/639228219)