---
created: 2025-09-15T16:02:03 (UTC +08:00)
tags: [LLM,Agent,上下文]
source: https://zhuanlan.zhihu.com/p/1930993912346046842
author: 关于作者北方的郎专注模型与代码，公众号：AI方法与实践吕阿华、郭达森、浮生梦晓也关注了他回答1,201文章1,596关注者14,100关注他发私信
---
# 上下文腐蚀 
在大模型时代，越来越多的厂商纷纷推出拥有百万甚至千万级 token 上下文窗口的语言模型。从 Claude、GPT‑4 到 Gemini、Qwen3，似乎“上下文越大，能力越强”成为一种共识。但这种直觉，真的靠谱吗？Chroma 团队通过一系列系统实验揭示出令人惊讶的现象：大上下文并不一定带来更好表现，反而可能造成一种悄无声息的退化——他们称之为 **Context Rot（上下文腐蚀）**。

这种腐蚀并不是模型“看不到”输入，而是它的注意机制、信息整合能力，在长输入下逐渐失效、漂移，甚至出现幻觉和拒答。在保持任务难度不变的前提下，仅仅拉长输入，模型就会失控。这不仅挑战了我们对 LLM 能力的普遍假设，也对长上下文系统设计提出了警告。

论文地址：[Context Rot: How Increasing Input Tokens Impacts LLM Performance](https://link.zhihu.com/?target=https%3A//research.trychroma.com/context-rot)

试验记录：[https://github.com/chroma-core/context-rot](https://link.zhihu.com/?target=https%3A//github.com/chroma-core/context-rot)

## 一、引言

研究伊始，作者指出一个重要问题：当前的长上下文能力评估，大多采用“干草堆里的针”（[Needle in a Haystack](https://zhida.zhihu.com/search?content_id=260675607&content_type=Article&match_order=1&q=Needle+in+a+Haystack&zhida_source=entity), NIAH）类任务——即在大段文本中定位特定内容。这类任务虽然易于自动化评估，却远不能覆盖真实应用中 LLM 所面临的复杂推理、对话理解、摘要抽取等挑战。

![](https://pic4.zhimg.com/v2-fd1aece1a57a05f4de483ba4da4848fd_1440w.jpg)

更重要的是，NIAH 一类 benchmark 通常通过词面匹配判断是否命中 needle，对模型的语义理解、推理能力几乎无要求。于是，本文提出核心研究问题：

**“在保持任务难度一致的情况下，单纯增加上下文长度，会对模型性能造成怎样的影响？”**

为此，研究团队设计了精密的控制实验，隔离“输入长度”这一唯一变量，剖析它如何潜移默化地腐蚀模型的表现。

![](https://pica.zhimg.com/v2-4d7b3c3069a007288325024ee4d92b96_1440w.jpg)

## 二、研究贡献

本文主要贡献包括：

1.  **首次系统研究 18 个主流模型**（包括闭源模型和开源模型），在各种上下文长度下的表现变化，覆盖多任务类型。
2.  量化了不同类型干扰信息、问答相似度对模型性能的具体影响，揭示“腐蚀”并非单调线性变化。
3.  公开数据与代码，构建可复现、可拓展的研究平台，支持未来社区进一步探索上下文处理机制。

这些成果填补了以往长上下文研究中的重要空白。

## 三、相关工作回顾

现有评估长上下文能力的主流方式是检索类任务，如原始 NIAH、其变体 NoLiMa，以及偏向摘要检测的 AbsenceBench。这些任务多围绕“是否能找出特定句子”进行评价，忽视了真正语义理解所需的复杂背景知识整合与推理链条。

更复杂的 benchmark 如 Graphwalks、Latent List、LongBench 等，虽然引入推理与结构化输入，但难以控制任务本身的复杂度随长度变化而上升，因此无法准确量化“长度”本身的影响。

本文在前人工作基础上提出创新设计：固定任务难度，仅调控输入长度和干扰信息，从而独立研究 context rot 的形成机制。

## 四、扩展版 Needle in a Haystack 任务设计

![](https://pic3.zhimg.com/v2-cfe8fe8edceeb793408b6ee3fea7b50c_1440w.jpg)

### 4.1 语义匹配问题的引入

传统 NIAH 任任务中，needle（目标信息）和问题之间通常是词面一致。但在真实应用中，如对话和问答系统，这种直接匹配非常少见。大多数情况下，模型需要理解某些隐含的背景知识，例如地理位置、时间因果、角色关系等。

![](https://pic4.zhimg.com/v2-263608e103772188236dfabe785e0781_1440w.jpg)

为此，本文设计了多个不同相似度级别的问题–needle 对，包括词面一致、语义一致、有隐含知识关联和弱语义相似等。通过引入这些“语义问答对”，研究能更真实地评估模型理解能力如何受到上下文长度的影响。

### 4.2 干扰信息与无关内容的区分

研究者进一步细化了上下文中的“非目标内容”：

![](https://pic4.zhimg.com/v2-d1751425bceda628152181bd487e42d9_1440w.jpg)

-   **干扰项（distractor）**：与 needle 有关联，但不能作为答案。比如，提到类似地点或人物的内容。
-   **无关项（irrelevant）**：与问题完全无关的文本。

实验显示：干扰项比无关项对模型干扰更大，且随着数量增多，其负面影响呈指数级增长。尤其在语义问答对中，干扰项会显著误导模型注意力，造成定位错误、幻觉式输出甚至完全拒答。

## 五、实验设计与测试模型

为了验证上下文腐蚀的普遍性与机制，作者构建了以下三种任务：

### 5.1 扩展版 NIAH

控制 needle 问答对的相似度、干扰类型与上下文长度，系统评估模型在不同组合下的命中表现。

![](https://pic1.zhimg.com/v2-4d47bfc34d49055254f6afde4adfe4e4_1440w.jpg)

### 5.2 长对话问答任务（[LongMemEval](https://zhida.zhihu.com/search?content_id=260675607&content_type=Article&match_order=1&q=LongMemEval&zhida_source=entity)）

模拟真实多轮对话，目标是评估模型在较早一轮中获取信息并在后续正确引用的能力。它测试模型是否能够维持长期记忆和正确检索机制。

![](https://pic1.zhimg.com/v2-a564ffad1b621914b864804e0e352842_1440w.jpg)

![](https://pic4.zhimg.com/v2-bdf4f0a6dbd8acdeb99734154acaba6b_1440w.jpg)

### 5.3 复制任务（Repeated Words）

给定几百至上千个重复单词（如“猫猫猫猫...”），插入一个不同词（如“狗”），让模型复制并识别“狗”所在的位置。这种设计揭示模型在“超长注意范围”内是否具备精确定位与复制能力。

测试模型包括：

-   Claude 系列（Haiku、Sonnet、Opus）
-   OpenAI 系列（GPT-4.1 各版本、GPT-4o）
-   Gemini 系列（2.0 Flash、2.5 Pro）
-   Qwen3 系列（8B、32B、235B）
-   以及部分嵌入模型用于相似度计算
    

测试维度丰富，充分覆盖了主流大模型架构。

![](https://pic1.zhimg.com/v2-f2708fdae3f2a3d909c0caae09dccb82_1440w.jpg)

Impact of Distractors: Performance by Number of Distractors - arXiv haystack/PG essay needles

![](https://pic2.zhimg.com/v2-7e7be438da8fad0a41054da01eac0085_1440w.jpg)

Impact of Distractors: Performance by Individual Distractors - arXiv haystack/PG essay needles

## 六、实验发现一：问答相似度对性能的影响

研究发现：当问答对为词面一致时，大多数模型即使在长上下文下也能稳定命中 needle。但只要转为语义匹配，性能立即开始波动，并随输入长度增长迅速下降。

![](https://pic3.zhimg.com/v2-6606dc537257f6da4331b1f372da5df4_1440w.jpg)

Needle-Haystack Similarity: Experimental Setup

尤其在包含隐性背景知识的问答中，表现尤为不稳定。模型有时答非所问，有时给出模糊答案，有时干脆说“无法找到相关信息”。

Claude 与 GPT‑4 系列在 20K~50K token 时仍保持较高准确率，但当长度达到 100K 或更长时，准确率显著下滑。Gemini 与 Qwen 系列呈现波动曲线，不同输入顺序对其表现影响极大。

![](https://pic4.zhimg.com/v2-03a101b5b37551d156646dfd03c0f789_1440w.jpg)

Needle-Haystack Similarity Results

这说明：语义复杂度与上下文长度之间存在非线性交互关系，不能简单靠参数量堆砌解决。

## 七、实验发现二：干扰项的致命性影响

研究进一步比较了在加入多个干扰项的情况下，各模型的抗干扰能力：

![](https://pic1.zhimg.com/v2-00379f4370722400a4739d15baef63c0_1440w.jpg)

Haystack Structure: Sample Experimental Setup

![](https://pic1.zhimg.com/v2-5cf02ec59f74d2144eff8998a07eb6aa_1440w.jpg)

-   当上下文中仅含无关信息时，大部分模型仍能正确命中 needle。
-   一旦引入相似句式或语义关联的干扰项，即便只有一两个，命中率就急剧下降。
-   部分模型（如 Gemini）表现为频繁“踩雷”，在干扰项上误判为答案。

![](https://pic4.zhimg.com/v2-3fe9a424f17c8a256e52a90e62fb194b_1440w.jpg)

![](https://pic4.zhimg.com/v2-8c4cdd5f7eb7daa6c42f294195a4cda3_1440w.jpg)

![](https://pic2.zhimg.com/v2-56f6a0934d700e34510ce98a7e400613_1440w.jpg)

Claude 更倾向于谨慎策略——当有多个可能选项时，会输出“无法确定”或“内容缺失”类保守回答。GPT‑4 系列则在不确定时更可能产生幻觉，给出错误但语义通顺的答案。

这些行为差异表明：不同模型架构对注意力干扰的容忍度不同，但普遍存在“信噪比一旦降低就性能崩溃”的脆弱性。

___

## 八、实验发现三：复制任务揭示注意力边界

复制任务设计简单却揭示了模型最底层的问题——在长上下文下，它是否还能记住某个精确位置的信息。

![](https://pica.zhimg.com/v2-b3580523c3b40cd1b8ae2ca0e80bbeb8_1440w.jpg)

![](https://pic4.zhimg.com/v2-c88b985a76ed26f57e321e250d1e9763_1440w.jpg)

![](https://pic4.zhimg.com/v2-3da3845d5be5a0ebb16385805716b7ff_1440w.jpg)

![](https://pic3.zhimg.com/v2-8577ef376b854a4bf99e53be260a50da_1440w.jpg)

![](https://pic3.zhimg.com/v2-15dbdd2c3999cacdffed807849c49da8_1440w.jpg)

![](https://pic3.zhimg.com/v2-bcc772272591bcd624ed95bf49012cb6_1440w.jpg)

当文本长度超过 20K token，几乎所有模型都会出现以下现象：

-   **偏移错误**：复制位置前后错几格
-   **错词**：复制成相似或上下文高频词
-   **脱指令**：生成无关文本
-   **拒答**：表示无法完成任务或保持沉默

GPT‑4o 与 Claude Opus 在 10K 以内表现最佳，但超过 50K 后同样开始出现波动。Gemini 及 Qwen 在不同版本中表现不一，但无一例外都受输入长度严重影响。

这一任务表明，context rot 不只是认知或推理层的问题，更直接作用于底层注意机制——模型在长文本中丢失了“位置精度”，就像人在千页长卷中寻找某个字一样，渐渐迷失方向。

## 九、结论与实践建议

综上，作者总结如下：

1.  **Context Rot 是真实存在的通病**，尤其在上下文超过几十万 token 时表现明显。
2.  问题不在于模型“能不能看到”所有 token，而是“能不能正确处理”。
3.  输入中若包含语义相似的干扰项，Context Rot 会迅速放大，造成误判、幻觉或拒答。
4.  不同模型在腐蚀表现上有差异，但无一幸免；即使是最强模型也无法完全规避腐蚀。
5.  盲目扩大上下文窗口无法解决问题，反而可能加重腐蚀风险。

作者建议未来开发者应：

-   使用 **RAG（检索增强生成）** 策略，将上下文裁剪至必要片段；
-   引入 **[Chunking](https://zhida.zhihu.com/search?content_id=260675607&content_type=Article&match_order=1&q=Chunking&zhida_source=entity)** 技术，把长任务拆解为多个小任务；
-   通过语义相似度聚类、注意力机制再设计，提升模型抗干扰能力。

## 总结

[《Context Rot》](https://zhida.zhihu.com/search?content_id=260675607&content_type=Article&match_order=1&q=%E3%80%8AContext+Rot%E3%80%8B&zhida_source=entity)是一篇极具现实意义的研究论文。它击碎了“大上下文必然强大”的迷思，以丰富实验和细致分析展示出隐藏在百万 token 背后的真相：长文本不等于长记忆，更多 token 不等于更高性能。

真正强大的模型，不是能记住更多，而是能准确、专注地记住重要部分。上下文设计能力，将成为未来 AI 应用工程师最核心的竞争力之一。

——完——

[@北方的郎](https://www.zhihu.com/people/7af62e4119791a452e88718cb5ccc0be) · 专注模型与代码

喜欢的朋友，欢迎赞同、关注、分享三连 ^O^
