Trust region policy optimization（TPRO）算法[[6]](#ref_6)是现代强化学习的基础，它以自然策略梯度优化为基础，迅速获得普及，成为主流强化学习算法，因为它在经验上比自然策略梯度算法表现得更好、更稳定。尽管此后它已被近端策略优化 (PPO) 超越，但它的仍然具有重要的意义。
我们将讨论TRPO背后的单调改进定理（关注直觉）以及将其与自然策略梯度区分开的三个变化。
### 3.1 自然策略梯度算法的缺陷
-   近似值可能会**违反KL约束**，从而导致分析得出的步长过大，超出限制要求
-   矩阵 $F^{-1}$ 的**计算时间太长**，是 $O(N^3)$ 复杂度的运算
-   我们没有检查更新是否真的改进了策略。由于存在大量的近似过程，**策略可能并没有优化**
### 3.2 算法理论
针对自然策略梯度算法的问题，我们希望可以对策略的优化进行量化，从而保证每次的更新一定是优化作用的。为此，我们需要计算两种策略之间预期回报的差异。这里采用的是**原策略预期回报添加新策略预期优势**的方式[[7]](#ref_7)。该表达式在原策略下计算优势函数，无需重新采样：
$$J(\pi_{\theta+\Delta\theta})=J(\pi_{\theta})+\mathbb{E}_{\tau\sim\pi_{\theta+}\Delta\theta}\sum_{t=0}^{\infty}\gamma^t A^{\pi_{\theta}}(s_t,a_t)$$
其中优势函数的定义为：
$$A^{\pi_\theta}(s,a) = \mathbb{E}\left(Q^{\pi_\theta}{(s,a)-V^{\pi_\theta}}(s)\right)$$
在给定的策略和状态下，计算特定动作$a$的期望累积奖励与总体期望值的差值，描述了该动作的相对吸引力。
由于时间范围是无限的，引入**状态的折扣分布**
$$\rho_{\pi}(s)=(P(s_0=s)+\gamma P(s_1=s)+\gamma^2P(s_2=s)+\ldots)$$
原差异表达式可重新表示为：
$$J(\pi_{\theta+\Delta\theta})=J(\pi_\theta)+\sum\limits_{s\in\mathcal{S}}\rho_{\pi_{\theta+\Delta\theta}}(s)\sum\limits_{a\in\mathcal{A}}\pi_{\theta+\triangle\theta}(a\mid s)A^{\pi_\theta}(s,a)$$
我们无法在不对更新策略进行采样的情况下知道对应于更新策略的状态分布，故引入近似误差，使用**当前策略近似**：
$$J(\pi_{\theta+\Delta\theta})\approx J(\pi_\theta)+\sum\limits_{s\in\mathcal{S}}\rho_{\pi_{\theta}}(s)\sum\limits_{a\in\mathcal{A}}\pi_{\theta+\triangle\theta}(a\mid s)A^{\pi_\theta}(s,a)$$
接下来，我们将状态分布求和替换为期望，方便实际计算时使用蒙特卡洛模拟进行采样，同时将动作求和替换为**重要性采样**。通过重要性采样，可以有效利用当前策略的行动期望，并针对新策略下的概率进行了修正：
$$J(\pi_{\theta+\Delta\theta})\approx J(\pi_\theta)+\mathbb{E}_{s\sim\rho_{\pi\theta}}\dfrac{\pi_{\theta+\Delta\theta}(a\mid s)}{\pi_\theta(a\mid s)}A^{\pi_\theta}(s,a)$$
描述更新策略相对于原策略的预期优势称为**替代优势（surrogate advantage）**：
$$J(\pi_{\theta+\Delta\theta})-J(\pi_{\theta})\approx\mathbb{E}_{s\sim\rho_{\pi\theta}}\dfrac{\pi_{\theta+\Delta\theta}(a\mid s)}{\pi_{\theta}(a\mid s)}A^{\pi_{\theta}}(s,a)=\mathcal{L}_{\pi_{\theta}}(\pi_{\theta+{\Delta\theta}})$$
之前产生的近似误差可以用两种策略之间最坏情况的KL散度表示：$$J(\pi_{\theta+\Delta\theta})-J(\pi_{\theta})\geq\mathcal{L}_{\pi\theta}(\pi_{\theta+{\Delta}\theta})-C\mathcal{D}_{K L}^{\mathrm{max}}(\pi_{\theta}||\pi_{\theta+{}\Delta\theta})$$论文中推导出 C 的值以及目标函数改进的下限。如果我们改进右侧，可以保证左侧也得到改进。本质上，如果替代优势 $\mathcal{L}_{\pi_{\theta}}(\pi_{\theta+{\Delta\theta}})$ 超过最坏情况下的近似误差 $C\mathcal{D}_{K L}^{\mathrm{max}}(\pi_{\theta}||\pi_{\theta+{}\Delta\theta})$ ，我们一定会改进目标。
这就是**单调改进定理**。相应的过程是**最小化最大化算法（MM）**。即如果我们改进下限，我们也会将目标改进至少相同的量。
### 3.3 算法实现
在实际的算法实现方面，TRPO和自然策略梯度算法没有太大的区别。TRPO主要有三个改进，每个改进都解决了原始算法中的一个问题。TRPO的核心是利用单调改进定理，验证更新是否真正改进了我们的策略。
**3.2.1 共轭梯度法（conjugate gradient method）**
在自然策略梯度算法中，计算逆Fisher矩阵是一个耗时且数值不稳定的过程，特别是对于神经网络，参数矩阵可以变得非常大， $O(\theta^3)$ 的时间复杂度将无法计算。
好消息是，我们对逆矩阵本身并不感兴趣。观察自然策略梯度的方程式，如果我们可以直接得到乘积 $F^{-1}\nabla log{\pi_\theta}(x)$ ，就不再需要逆。
引入共轭梯度法，这是一个近似上式乘积的数值过程，这样我们就可以避免计算逆矩阵。共轭梯度通常在 $|\theta|$ 步内收敛，从而可以处理大矩阵。
共轭梯度法并不是TRPO中独有的，例如在Truncated Natural Policy Gradient[^9]中也部署了相同的方法，但它仍然是算法中的一个重要组成部分。
**3.2.2 线搜索（line search）**
虽然自然梯度策略中提供了给定KL散度约束的最佳步长，但由于存在较多的近似值，实际上可能不满足该约束。
TRPO 通过执行线搜索来解决此问题，通过不断地迭代减小更新的大小，直到第一个不违反约束的更新。这个过程可以看作是不断缩小信任区域，即我们相信更新可以实际改进目标的区域。
![](https://pic1.zhimg.com/v2-6757b09082490dc71709351bb80ce3d6_1440w.jpg)
图6. 线搜索算法
更新迭代减小的方式是使用指数衰减率 $\alpha^j$ ， $j$ 随迭代的进行而增加。同时，在约束条件的判断中，我们看到KL散度约束不是唯一满足的约束，它还保证了替代优势 $\mathcal{L}(\theta)$ 非负，这就是接下来介绍的改进检查。
**3.2.3 改进检查**
在TRPO中，我们并没有假设更新会提高替代优势 $\mathcal{L}(\theta)$ ，而是真正检查了它。尽管实际计算时需要根据旧策略计算优势，以及使用重要性抽样来调整概率，会花费一些时间，但验证更新是否真正改进了策略是有必要的。
**3.2.4 算法完整框架**
![|725](https://pic4.zhimg.com/v2-84f5ef04c524d5ce023550954b4ef92b_1440w.jpg)
图7. TRPO算法
### 3.4 详细解释
**核心理解：TRPO 是什么？为什么需要它？**
1.  **问题根源 (策略梯度更新的灾难性偏移)：**
   *   在强化学习中，策略梯度方法通过沿着预期回报梯度方向更新策略参数 $θ$ 来优化策略 $π_θ$。
   *   然而，标准的策略梯度更新（如 $θ ← θ + α * ∇_θ J(θ)$）可能导致**策略更新步长过大**。
   *   策略 $π_θ$ 的微小参数变化 $Δθ$，可能导致策略本身（动作概率分布 $π_θ(a|s)$）发生**巨大变化**。
   *   这种剧烈变化会使基于**旧策略 $π_{θ\_{old}}$** 收集的数据（用于计算梯度）计算的梯度估计**失效**（因为新策略下的状态分布和动作选择概率已截然不同），这种现象称为**分布偏移（Distribution Shift）**。
   *   后果：策略性能可能**急剧下降（崩溃）** 且难以恢复，训练变得极其不稳定。样本效率低下（因为数据很快失效）。
2.  **TRPO 的核心思想与“信任域”（Trust Region）：**
   *   **核心洞见：** 限制策略更新的幅度，确保新策略 $π_θ$ 不会偏离旧策略 $π_{θ\_{old}}$ 太远，使得基于旧策略数据计算的梯度估计在新策略下**仍然近似有效**。
   *   **“信任域”概念：** 在参数空间 $θ$ 或策略空间 $π$ 中，围绕当前策略 $π_{θ\_{old}}$ 定义一个**小区域（信任域）**。在这个区域内，我们“信任”基于旧数据计算的梯度方向是相对准确的，可以安全地进行策略改进优化。
   *   **数学形式化（优化问题）：** TRPO 将策略优化问题转化为一个带约束的优化问题：
       最大化   $$E_{s \sim ρ_{θ\_{old}}, a \sim π_{θ\_{old}}} [ (π_θ(a|s) / π_{θ\_{old}}(a|s)) * A_{θ\_{old}}(s, a) ]$$  （即期望优势函数）
       约束条件： $$E_{s \sim ρ_{θ\_{old}}} [ D_KL( π_{θ\_{old}}(·|s) || π_θ(·|s) ) ] ≤ δ$$
       *   $ρ_{θ\_{old}}(s)$：旧策略 $π_{θ\_{old}}$ 下的状态访问分布（折扣加权）。
       *   $A_{θ\_{old}}(s, a)$：在状态 $s$ 下执行动作 $a$ 相对于旧策略 $π_{θ\_{old}}$ 的优势函数（通常用 $V_φ$ 估计）。
       *   **目标函数：** 使用**重要性采样（Importance Sampling）** 来估计新策略 $π_θ$ 下的期望优势，但数据来自旧策略 $π_{θ\_{old}}$。这称为**替代优势函数（Surrogate Advantage）** $L(θ)$。
       *   **约束条件：** 要求新策略 $π_θ$ 与旧策略 $π_{θ\_{old}}$ 在所有状态 $s$（按 $ρ_{θ\_{old}}$ 加权）上的**平均KL散度（Kullback-Leibler Divergence）** 不超过一个预设的小阈值 $δ$。KL散度 $D_KL(p || q)$ 衡量两个概率分布 $p$ 和 $q$ 的差异（非对称）。$D_KL(π\_{old} || π_new) ≤ δ$ 约束了策略更新的幅度。
3.  **TRPO 的优势：**
   *   **理论保证：** 在一定的近似条件下，TRPO 可以**单调地改进策略性能**（或至少不降低性能）。这是其最核心的理论贡献。
   *   **显著提高稳定性：** 有效避免了策略性能崩溃，使得训练复杂策略（如神经网络参数化的策略）成为可能。
   *   **较高的样本效率：** 允许在同一个批次的数据上进行多次策略更新（利用约束保护）。
4.  **TRPO 的挑战：**
   *   **计算复杂：** 处理 KL 散度约束（尤其是对于高维参数空间如神经网络）非常复杂，需要用到二阶优化方法（共轭梯度法 + 线搜索），计算开销巨大。
   *   **实现难度高：** 相比一阶方法（如标准SGD/Adam），二阶方法的实现和调试更复杂。
   *   **超参数 $δ$ 敏感：** 约束阈值 $δ$ 的选择对性能影响很大，且难以调整。
   *   **近似误差：** 目标函数和约束中的期望都是基于采样估计的，且使用了重要性采样和函数近似（如 $V_φ$），存在近似误差。
**TRPO 架构详解：流程、模型与 Loss（优化目标）**
**核心模型：**
TRPO 也是一个 **Actor-Critic** 方法，通常涉及两个主要模型（有时参考模型是隐式的）：
1.  **策略模型 (Actor / Policy Network - $π_θ$)：** 待优化的主模型（在LLM中即语言模型本身）。输入状态 $s$，输出动作概率分布 $π_θ(a|s)$。
2.  **价值模型 (Critic / Value Network - $V_φ$)：** 评估状态值 $V_φ(s)$。用于估计优势函数 $A_{θ\_{old}}(s, a) ≈ Q(s, a) - V(s)$。通常通过最小化 $E[(V_φ(s) - V^{targ}(s))^2]$ 来训练，其中 $V^{targ}$ 可以是基于回报或 $TD(λ)$ 的目标值。
3.  **(隐式) 旧策略模型 ($π_{θ\_{old}}$)：** TRPO 的更新基于从 $π_{θ\_{old}}$ 收集的数据。在每次数据收集前，$π_{θ\_{old}}$ 就是当前的 $π_θ$。数据收集期间和后续优化过程中，$π_{θ\_{old}}$ 固定。
**核心流程 (TRPO 算法)：**
```
```mermaid
graph LR
   A[初始化策略模型 π_θ, 价值模型 V_φ] --> B[使用 π_θ 收集一批数据]
   B --> C[估计优势值 A_θ_old]
   C --> D[构建替代目标 L(θ) 和 KL 约束]
   D --> E[求解带约束优化问题]
   E --> F[应用更新 θ_k+1 = θ_k + α_j * Δθ]
   F --> G[更新价值函数 V_φ]
   G --> B
```
1.  **初始化：** 初始化策略参数 $θ$ 和价值函数参数 $φ$。
2.  **数据收集 (Rollout)：** 使用**当前策略 $π_θ$（此时它也是 $π_{θ\_{old}}$）** 与环境交互，收集一组轨迹数据 ${τ_i}$（包含状态 $s$, 动作 $a$, 奖励 $r$, 下一个状态 $s'$, 终止标志等）。
3.  **估计优势函数：** 基于收集到的数据，使用**当前价值函数 $V_φ$** 估计每个状态-动作对 $(s, a)$ 的优势值 $A_{θ\_{old}}(s, a)$。常用方法：
   *   **广义优势估计 (GAE(λ))：** 与PPO中完全相同，是TRPO/PPO的标准做法。公式：
       $$
       δ_t = r_t + γ * V_φ(s_{t+1}) - V_φ(s_t)
       A_t^{GAE(λ)} = Σ_{l=0}^{∞} (γλ)^l δ_{t+l}
       $$
   *   **其他方法：** 如 $n$-步回报、$TD(λ)$ 等。
4.  **构建优化问题：** 定义 TRPO 的核心优化问题：
   *   **替代目标函数 (Surrogate Objective) $L(θ)$：**
       $$
       L(θ) = E_{s \sim ρ_{θ\_{old}}, a \sim π_{θ\_{old}}} [ r(θ) * A_{θ\_{old}}(s, a) ]
       = E_{s \sim ρ_{θ\_{old}}, a \sim π_{θ\_{old}}} [ (π_θ(a|s) / π_{θ\_{old}}(a|s)) * A_{θ\_{old}}(s, a) ]
       $$
     r(θ) 是重要性采样比率
       这个目标函数是在**旧策略的数据分布**下，估计**新策略**的期望优势。最大化 $L(θ)$ 旨在提升策略性能。
   *   **约束函数：**
       $$
       C(θ) = E_{s \sim ρ_{θ\_{old}}} [ D_KL( π_{θ\_{old}}(·|s) || π_θ(·|s) ) ] ≤ δ
       $$
       $δ$ 是预设的KL散度阈值（如 0.01, 0.05）。这个约束强制新策略 $π_θ$ 在旧策略访问过的状态 $s$ 上，其动作分布不能与旧策略 $π_{θ\_{old}}$ 相差太远（平均KL散度不超过 $δ$）。
5.  **求解带约束优化问题 (TRPO的核心难点与创新)：**
   TRPO 采用以下步骤近似求解这个复杂的约束优化问题（涉及大量近似和数值技巧）：
   *   **a. 目标函数的一阶近似 (线性化)：** 在 $θ = {θ\_{old}}$ 处对 $L(θ)$ 进行一阶泰勒展开：
       $$
       L(θ) ≈ L({θ\_{old}}) + g^T (θ - {θ\_{old}})
       $$
       其中 $g = ∇_θ L(θ) |_{θ={θ\_{old}}}$ 是 $L(θ)$ 在 ${θ\_{old}}$ 处的梯度。由于 $L({θ\_{old}}) = E[... * A] = 0$（在 ${θ\_{old}}$ 处比率 $r=1$，且优势函数的期望为0），所以目标简化为：
       $$
       最大化 g^T (θ - {θ\_{old}})
       $$
   *   **b. 约束函数的二阶近似 (曲率信息)：** 在 $θ = {θ\_{old}}$ 处对约束 $C(θ)$ 进行二阶泰勒展开：
       $$
       C(θ) ≈ C({θ\_{old}}) + ∇_θ C(θ)|_{θ={θ\_{old}}}^T (θ - {θ\_{old}}) + 1/2 (θ - {θ\_{old}})^T H (θ - {θ\_{old}})
       $$
       *   $C({θ\_{old}}) = E[D_KL(π\_{old}||π\_{old})] = 0$
       *   $∇_θ C(θ)|_{θ={θ\_{old}}}$：KL散度约束在 ${θ\_{old}}$ 处的梯度。**关键点：在 ${θ\_{old}}$ 处，KL散度在其最小值0处，所以其一阶导数为0！** 即 $∇_θ C(θ)|_{θ={θ\_{old}}} = 0$。
       *   $H$：KL散度约束 $C(θ)$ 在 ${θ\_{old}}$ 处的 **Hessian 矩阵**（二阶导数矩阵）。实际上，$H$ 是 $C(θ)$ 的 Hessian 的期望，即 **Fisher 信息矩阵（FIM）**：
           $$
           H = E_{s \sim ρ_{θ\_{old}}, a \sim π_{θ\_{old}}} [ ∇_θ log π_{θ\_{old}}(a|s) (∇_θ log π_{θ\_{old}}(a|s))^T ] |_{θ={θ\_{old}}}
           $$
           这称为 **Fisher Information Matrix (FIM)**。它编码了策略 $π_θ$ 在 ${θ\_{old}}$ 处关于其参数变化的**曲率信息**。
       因此，约束简化为：
       $$
       1/2 (θ - {θ\_{old}})^T H (θ - {θ\_{old}}) ≤ δ
       $$
       这是一个**二次型约束**。
   *   **c. 优化问题近似：** 结合 a 和 b 的近似，原问题近似为：
       $$
       最大化 g^T (θ - {θ\_{old}})
       约束条件： 1/2 (θ - {θ\_{old}})^T H (θ - {θ\_{old}}) ≤ δ
       $$
       这是一个**带二次约束的线性优化问题**。
   *   **d. 求解更新方向 (Natural Gradient)：** 这个近似问题的解析解（在满足约束下最大化线性目标的方向）由 **Natural Policy Gradient** 给出：
       $$
       Δθ ∝ H^{-1} g
       $$
       这个方向 $H^{-1}g$ 称为 **Natural Gradient**。与普通梯度 $g$ 相比，Natural Gradient 考虑了参数空间（由FIM $H$ 度量）的曲率，在分布空间（策略空间）中指向最陡上升方向，而不是在欧几里得参数空间中。这有助于处理参数冗余和病态条件问题。
   *   **e. 计算更新方向 (共轭梯度法 - Conjugate Gradient)：** 直接计算 $H^{-1}g$ 在高维空间（神经网络参数）中**极其昂贵**（需要存储和求逆巨大的 $n x n$ 矩阵，$n$ 是参数数量）。TRPO 采用**共轭梯度法（Conjugate Gradient, CG）** 来**近似求解**线性方程组 $Hx = g$ 的解 $x ≈ H^{-1}g$。CG 只需计算 $H$ 与向量 $v$ 的乘积 $Hv$，而无需显式存储 $H$。计算 $Hv$ 可以通过以下公式高效实现：
       $$
       Hv = ∇_θ ( (∇_θ log π_{θ\_{old}}(a|s))^T v ) )   // 先计算 D_θ = ∇_θ log π_{θ\_{old}}(a|s)
       $$
       即先计算 $D_θ = ∇_θ log π_{θ\_{old}}(a|s)$（标量对参数的梯度），然后计算 $(D_θ^T v)$（标量），再对这个标量关于 $θ$ 求梯度 $∇_θ (D_θ^T v)$。这个计算可以通过两次反向传播高效完成，无需构造 $H$。
   *   **f. 确定更新步长 (线搜索 - Line Search)：** 得到更新方向 $Δθ_{ng} ≈ H^{-1}g$ 后，最终的参数更新为：
       $$
       θ_{new} = θ_{old} + α * Δθ_{ng}
       $$
       但直接使用 $Δθ_{ng}$ 可能违反 KL 约束（因为之前的近似是局部的）。TRPO 采用**回溯线搜索（Backtracking Line Search）** 来确定一个合适的步长缩放因子 $α$：
       *   从 $α = 1$ (或 $α = √(2δ / (Δθ_{ng}^T H Δθ_{ng}))$ 的理论最大步长) 开始。
       *   尝试 $θ_{proposed} = θ_{old} + α * Δθ_{ng}$。
       *   计算实际的平均 KL 散度 $E_s[D_KL(π\_{old}(·|s) || π_new(·|s))]$（基于样本估计）。
       *   如果 $KL > δ * 1.5$ (或其他容忍系数，如 1.5-2.0)，则减小 $α$ (如 $α ← α / 2$)。
       *   重复直到满足 $KL ≤ δ * tolerance$ **并且** $L(θ_{proposed}) ≥ L(θ_{old})$ (确保目标不下降)。如果找不到满足条件的 $α$，则可能跳过更新或使用很小的 $α$。
6.  **应用参数更新：** 将找到的满足 KL 约束和性能改进条件的 $θ_{new}$ 设置为当前策略参数 $θ$。
7.  **更新价值函数 (可选但推荐)：** 使用新收集的数据（或结合旧数据）更新价值函数 $V_φ$，通常通过最小化 $E[(V_φ(s) - V^{targ}(s))^2]$（如用 $TD(λ)$ 或 Monte Carlo 回报计算 $V^{targ}$）。
8.  **循环：** 回到步骤2，使用更新后的策略收集新的数据，重复步骤3-7。
**关键公式总结与解释 (TRPO)：**

| 组件          | 公式                                                                                                                                                                   | 解释                                                                                                                                                                                |
| :---------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **替代目标函数**  | $L(θ) = E_{s \sim ρ_{θ\_{old}}, a \sim π_{θ\_{old}}} [ (π_θ(a\|s) / π_{θ\_{old}}(a\|s)) * A_{θ\_{old}}(s, a) ]$                                                                        | 基于旧策略数据估计的新策略期望优势。最大化它旨在提升策略性能。重要性采样允许使用旧数据。                                                                                                                                      |
| **KL 散度约束** | $C(θ) = E_{s \sim ρ_{θ\_{old}}} [ D_KL( π_{θ\_{old}}(·\|s) \|\| π_θ(·\|s) ) ] ≤ δ$                                                                                            | **核心约束。** 限制新策略 $π_θ$ 在旧策略访问的状态上的动作分布变化幅度。$δ$ 是预设阈值。$D_KL(p \|\| q) = Σ p(x) log(p(x)/q(x))$，非对称，这里 $p=π\_{old}$, $q=π_new$，衡量用 $π_new$ 近似 $π\_{old}$ 的信息损失。约束平均 KL ≤ δ 定义了“信任域”。 |
| **目标线性近似**  | $L(θ) ≈ g^T (θ - {θ\_{old}})$ <br> $g = ∇_θ L(θ) \|_{θ={θ\_{old}}}$                                                                                                      | 在 ${θ\_{old}}$ 处的一阶泰勒展开。$L({θ\_{old}})=0$。                                                                                                                                            |
| **约束二阶近似**  | $C(θ) ≈ 1/2 (θ - {θ\_{old}})^T H (θ - {θ\_{old}}) ≤ δ$ <br> $H = FIM = E_{s,a \sim π\_{old}} [ ∇_θ log π_{θ\_{old}}(a\|s) (∇_θ log π_{θ\_{old}}(a\|s))^T ]$                       | **关键近似。** 在 ${θ\_{old}}$ 处的二阶泰勒展开。KL散度在 ${θ\_{old}}$ 处的一阶导为0。$H$ 是 Fisher 信息矩阵 (FIM)，是 KL 散度在 ${θ\_{old}}$ 处的 Hessian 的期望。FIM 定义了策略空间的局部度量。                                             |
| **自然梯度方向**  | $Δθ_{ng} ∝ H^{-1} g$                                                                                                                                                 | 带二次约束的线性优化问题的解析解方向。$H^{-1}g$ 是 Natural Gradient，在由 FIM $H$ 定义的黎曼空间中指向最速上升方向。克服了参数空间的病态问题，更新更符合分布变化。                                                                               |
| **共轭梯度求解**  | $Solve Hx = g for x ≈ H^{-1}g using Conjugate Gradient$ <br> $Hv product computed efficiently$                                                                       | 避免显式计算和求逆 $H$。CG 迭代求解 $Hx=g$。核心是高效计算 $Hv$：$Hv = ∇_θ ( (∇_θ log π_{θ\_{old}}(a\|s))^T v )$。通过两次自动微分实现。                                                                               |
| **线搜索**     | $θ_{new} = θ_{old} + α * Δθ_{ng}$ <br> $Find largest α s.t.:$ <br> $1. E_s[D_KL(π\_{old}\|π_new)] ≤ δ * tolerance (e.g., 1.5-2.0)$ <br> $2. L(θ_{new}) ≥ L(θ_{old})$ | 确保实际更新满足 **1. 放宽的KL约束**（考虑近似误差）和 **2. 目标函数不下降**（单调改进）。回溯搜索（从理论最大步长开始不断折半 $α$ 直到满足条件）。                                                                                             |
**TRPO vs. PPO：核心区别**

| 特性                 | TRPO (Trust Region Policy Optimization)                 | PPO (Proximal Policy Optimization)                        |
| :------------------- | :------------------------------------------------------ | :-------------------------------------------------------- |
| **核心机制**         | **显式约束** (KL Divergence ≤ δ)                         | **隐式/近似约束** (通过目标函数中的 Clipping 机制)         |
| **优化方法**         | **二阶优化** (Natural Gradient + Conjugate Gradient)     | **一阶优化** (Standard Gradient Descent, e.g., Adam)      |
| **约束处理**         | 严格求解带约束优化问题 (线搜索保证)                     | 通过 Clipped Surrogate Objective *近似* 实现约束          |
| **计算复杂度**       | **非常高** (需计算 FIM-vector products, CG 迭代, 线搜索) | **相对较低** (标准一阶优化，无 CG/FIM)                    |
| **实现难度**         | **高** (复杂数值算法)                                   | **较低** (易于集成到现有 DL 框架)                         |
| **样本效率**         | 高 (可多次更新)                                         | 高 (可多次更新，K epochs & minibatches)                   |
| **稳定性**           | **理论保证单调改进** (近似条件下)                       | 实践中非常稳定 (Clipping 有效)，但无严格单调性保证        |
| **超参数敏感性**     | 对 **δ** (KL阈值) 敏感                                  | 对 **ε** (Clip range) 敏感                                |
| **主要优势**         | 理论保障强                                             | 实现简单，计算高效，广泛应用                              |
| **主要劣势**         | 计算开销巨大，实现复杂                                 | 无严格理论保证，Clip 机制有时保守                         |
| **在 LLM RLHF 中应用** | 极少 (计算成本过高)                                    | **主导地位** (效率与效果的平衡)                           |
**TRPO 在理论上的重要性：**
1.  **自然梯度：** TRPO 将 Natural Gradient 引入了深度强化学习，为解决高维参数空间中的策略优化问题提供了理论基础。
2.  **信任域方法：** 它严格形式化了“信任域”的概念，并通过约束优化框架实现。
3.  **单调改进理论：** 在近似条件下，TRPO 保证了策略性能的单调改进（或至少不降低），这是策略优化算法的重要里程碑。
4.  **PPO 的基石：** PPO 可以看作是 TRPO 的一种**一阶近似**和**工程简化**。PPO 的 Clipped Surrogate Objective 的设计动机直接来源于 TRPO 的优化目标 $L(θ)$ 和约束条件。PPO 试图用更简单高效的方式达到 TRPO 的稳定性效果。
**可能深入的问题:**
1.  **为什么 KL 散度约束比参数空间约束更好？** KL 散度直接在**策略（分布）空间**度量差异，这更直接地与策略性能相关。参数空间中的欧几里得距离（如 $||θ - {θ\_{old}}||^2 ≤ δ$）不能可靠地反映策略分布的变化程度（由于参数冗余、函数近似器的非线性等）。
2.  **Fisher 信息矩阵 (FIM) 在 TRPO 中的作用是什么？** FIM ($H$) 是 KL 散度约束在 ${θ\_{old}}$ 处的 Hessian 矩阵的期望。它定义了策略分布空间的局部曲率度量。$H^{-1}g$ (Natural Gradient) 考虑了这种曲率，在分布空间中给出最速上升方向，比普通梯度 $g$ 更有效、更稳定。
3.  **为什么使用共轭梯度法 (CG)？** 直接计算 $H^{-1}g$ 对于高维神经网络参数不可行（$O(n^3)$ 复杂度，$n$ 是参数数）。CG 是一种迭代方法，只需计算 $Hv$（矩阵-向量积），而 $Hv$ 可以通过两次自动微分高效计算（$O(n)$ 复杂度），无需显式存储 $H$ ($O(n^2)$ 内存）。CG 通常能在远少于 $n$ 次迭代内得到一个好的近似解。
4.  **线搜索 (Line Search) 为什么必要？** 因为目标函数 $L(θ)$ 和约束 $C(θ)$ 的二阶近似只在 ${θ\_{old}}$ 附近有效。使用完整的 Natural Gradient 方向 $Δθ_{ng} = H^{-1}g$ 更新，实际 KL 散度可能远超 $δ$（近似失效）。线搜索通过回溯减小步长 $α$，确保实际 KL 散度满足放宽的约束 ($≤ δ * tolerance$) 且目标函数 $L(θ)$ 不下降。
5.  **TRPO 的替代目标 $L(θ)$ 和策略梯度有什么关系？** $∇_θ L(θ) |_{θ={θ\_{old}}} = g = E[...] = ∇_θ J(θ) |_{θ={θ\_{old}}}$。即，在 ${θ\_{old}}$ 点，$L(θ)$ 的梯度等于标准策略梯度目标 $J(θ)$（期望累积奖励）的梯度。$L(θ)$ 是 $J(θ)$ 在旧策略分布下的一阶局部近似，但通过约束，TRPO 允许在更大的范围内安全地优化 $L(θ)$。
6.  **TRPO 对超参数 $δ$ 有多敏感？如何调整？** $δ$ 非常敏感。太小会导致更新过于保守，学习缓慢；太大会导致约束失效，失去稳定性。通常需要通过实验调整（如尝试 0.01, 0.05, 0.1）。监控平均 KL 散度是必须的。有时会使用自适应 $δ$（如根据历史 KL 调整）。
7.  **为什么 TRPO 在 LLM RLHF 中很少用？** 主要原因就是**巨大的计算开销**。LLM 的参数量巨大（B 到 100B+），计算精确的 Natural Gradient (即使用 CG 和高效的 $Hv$) 所需的时间和内存成本在工程上难以承受。PPO 提供了一种计算效率高得多的替代方案，在实践中被证明足够稳定和有效。
**总结：**
TRPO 是强化学习策略优化领域的一个里程碑式算法。它通过**显式的平均KL散度约束** ($E_s[D_KL(π\_{old} || π_new)] ≤ δ$) 定义了策略更新的**信任域**，从根本上解决了策略梯度训练不稳定的问题，并提供了**单调改进的理论保证**。其核心在于将问题转化为带约束优化，并利用**自然梯度** ($H^{-1}g$) 作为更新方向。高效实现依赖于**共轭梯度法** (CG) 近似求解 $H^{-1}g$ 和**回溯线搜索**确保约束满足与性能提升。
尽管 TRPO 因其**极高的计算复杂度**和**实现难度**在 LLM RLHF 等大规模应用中让位于其简化后代 PPO，但其理论贡献（信任域、自然梯度、约束优化框架）深刻影响了后续算法的发展（包括 PPO）。深入理解 TRPO 的原理、公式推导（尤其是一阶/二阶近似、FIM、Natural Gradient）以及求解过程（CG + Line Search），是掌握现代策略优化算法精髓的关键，也彰显了对基础理论的深刻把握。
### 3.4 TRPO算法的缺陷
TRPO算法解决了许多与自然策略梯度相关的问题，并获得了RL社区的广泛采用。但是，TRPO仍然存在一些缺点，特别是：
-   **无法处理大参数矩阵：** 尽管使用了共轭梯度法，TRPO仍然难以处理大的 Fisher矩阵，即使它们不需要求逆
-   **二阶优化很慢：** TRPO的实际实现是基于约束的，需要计算上述Fisher矩阵，这大大减慢了更新过程。此外，我们不能利用一阶随机梯度优化器，例如ADAM
-   **TRPO 很复杂：** TRPO很难解释、实现和调试。当训练没有产生预期的结果时，确定如何提高性能可能会很麻烦
在TRPO的基础上，Schulman等人引入了近端策略优化算法PPO（Proximal Policy Optimization）[[8]](#ref_8)。有两种主要的PPO变体需要讨论（均在17年的论文中介绍）：**PPO Penalty**和**PPO Clip**。我们首先从PPO Penalty入手，它在概念上最接近TRPO。