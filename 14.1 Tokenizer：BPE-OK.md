---
---
BPE，全称为 Byte Pair Encoding，是一种[数据压缩算法](https://zhida.zhihu.com/search?content_id=249136160&content_type=Article&match_order=1&q=%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E7%AE%97%E6%B3%95&zhida_source=entity)，后被广泛应用于自然语言处理中的词汇表构建和分词任务。它通过迭代地合并文本语料库中最频繁的相邻字符对，从而生成[子词单元](https://zhida.zhihu.com/search?content_id=249136160&content_type=Article&match_order=1&q=%E5%AD%90%E8%AF%8D%E5%8D%95%E5%85%83&zhida_source=entity)。

## BPE 算法的原理

1\. 初始化词汇表及合并过程

BPE 算法首先把文本中的所有单个字符当作初始词汇表，例如对于一段语料库中的文本，会包含各类字符，像“a”“b”“c”等。接着，统计每一个连续字节对的出现频率，比如在一些英文文本里，可能“th”这个字节对出现的频率比较高。挑出最高频的进行合并形成新的子词，即将“t”和“h”合并为“th”，并把这个新子词加入词汇表。随后重复此过程，持续寻找最高频的字节对进行合并，直至满足预设条件，这个预设条件可以是达到特定的词汇表大小或者下一个最高频的字节对出现频率为 1，接着随着合并过程的继续，可能会出现更多复杂的子词。

2\. 在每个切分出来的单词后添加后缀 “</w>”的含义

通常在每个单词末尾添加后缀 “</w>”，这个后缀有着重要的意义。它表明子词是词后缀，而且不同位置的相同字符对子词的意义有影响。举例来说，“st” 不加 “</w>” 可以出现在词首，如 “st ar”；加了 “</w>” 表明该子词位于词尾，如 “we st</w>”，二者意义截然不同。在统计单词出现频率时，会将每个单词改写为包含后缀的形式，例如，“low” 的频率为 5，那么我们将其改写为 “l o w </w>”：5。通过这样的方式，BPE 算法能够更准确地识别和处理不同位置的子词，提高分词的准确性和效率。在实际应用中，这种方式对于处理各种形态丰富的语言非常有效，尤其是在自然语言处理任务中，可以更好地表示文本的语义和语法结构。

## 三、BPE 算法的优缺点

**优点：**
1\. **缓解 OOV 问题**：BPE 算法通过动态地创建新的词汇单元，能够处理未知词（OOV）。例如，当遇到一个不在初始词汇表中的单词时，BPE 可以将其拆分为已知的子词单元，从而使模型能够对其进行处理。这在自然语言处理任务中非常重要，因为语言中总是会出现新的词汇或者罕见词。
2\. **减少单词表大小**：通过合并高频字符对，BPE 能够以更紧凑和高效的方式表示原始文本，从而减少单词表的大小。这有助于提高模型的训练效率和存储效率。例如，在处理大规模文本数据时，较小的单词表可以减少模型的参数数量，降低计算成本。
3\. **平衡词汇表大小和 token 数量**：BPE 可以有效地平衡词汇表大小和编码句子所需的 token 数量。通过选择最高频的字节对进行合并，BPE 能够在不增加过多 token 数量的情况下，减小词汇表的大小。这对于自然语言处理任务中的模型性能至关重要，因为过多的 token 数量可能会导致模型过拟合，而过小的词汇表可能无法准确表示文本的语义。
**缺点：**
贪婪和确定的符号替换：**BPE 算法是基于贪婪和确定的符号替换**，不能提供带概率的多个分片结果。这意味着在合并字节对时，BPE 总是选择最高频的字节对进行合并，而不考虑其他可能的组合。这种确定性的方法可能会**导致一些不合理的分词结果**，尤其是在处理复杂语言结构或者多义词时。例如，在处理 “bank” 这个单词时，BPE 可能会根据语料中的频率将其分为 “b”“a”“n”“k” 或者 “ban”“k” 等子词单元，但无法根据上下文提供不同的概率分片结果。

## 四、BPE代码实现和解析

### 代码实现

```python
import re
import collections

def generate_letter_dict():
    letter_dict = {}
    for i in range(26):
        letter_dict[chr(65 + i)] = i + 1  # 大写字母
        letter_dict[chr(97 + i)] = i + 27  # 小写字母
    return letter_dict

class BPE:
    def __init__(self):
        self.vocab = collections.defaultdict(int)
        self.merge_rules = {}
        self.max_merge_times = 0
        self.tokens_dict = {}
        self.id_to_token_dict = {}
    def get_vocab(self, corpus):
        """获取语料库中每个单词的频率"""
        for word in corpus.split():
            self.vocab[' '.join(list(word)) + ' </w>'] += 1
        
    def get_stats(self):
        """获取词汇表中相邻字节对的频率"""
        pairs = collections.defaultdict(int)
        for word, freq in self.vocab.items():
            #print("get_stats word:", word)
            symbols = word.split()
            for i in range(len(symbols) - 1):
                pairs[symbols[i], symbols[i+1]] += freq
        print("pairs:", pairs)
        return pairs

    def merge_vocab(self, pair):
        """合并最频繁的相邻字节对,更新词汇表"""
        bigram = re.escape(' '.join(pair))
        p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
        new_vocab = {}
        for word in self.vocab:
            #print("word:", word)
            new_word = p.sub(''.join(pair), word)
            #print("new_word:", new_word)
            #print("self.vocab[word]:", self.vocab[word])
            new_vocab[new_word] = self.vocab[word]
        #print("new_vocab:", new_vocab)
        self.vocab = new_vocab

    def merge(self, num_merges):
        """执行指定次数的合并操作"""
        self.max_merge_times = num_merges
        for i in range(num_merges):
            pairs = self.get_stats()
            if not pairs:
                break
            best_pair = max(pairs, key=pairs.get)
            self.merge_vocab(best_pair)
            self.merge_rules[best_pair] = i
            #print("self.merge_rules:", self.merge_rules)
        tokens_dict = generate_letter_dict()
        tokens_dict['</w>']=max(tokens_dict.values())+1
        
        for key in self.merge_rules.keys():
            tokens_dict[''.join(key)]=max(tokens_dict.values())+1
            # print("key:", key)
            # print("".join(key))
            #f.write(f"{''.join(key)} -> {value}\n")
        with open('merge_result.txt', 'w', encoding='utf-8') as f:
            for key, value in tokens_dict.items():
                f.write(f"{key} {value}\n")
    def get_tokens_dict(self,tokens_dict_file):
        tokens_dict = {}
        with open(tokens_dict_file, 'r', encoding='utf-8') as f:
            for line in f:
                key, value = line.strip().split()
                tokens_dict[key] = int(value)
        self.tokens_dict = tokens_dict
        for key, value in tokens_dict.items():
            self.id_to_token_dict[value] = key

        return tokens_dict
    

    def token_to_id(self, tokens):
        return [self.tokens_dict[token] for token in tokens]

    def id_to_token(self, ids):
        return [self.id_to_token_dict[id] for id in ids]
    def encode(self, text):
        """对文本进行BPE编码"""
        encoded_tokens = []
        tokens = []
        for word in text.split():
            word = ' '.join(list(word)) + ' </w>'
            while True:
                change = False
                for key, value in self.merge_rules.items():
                    bigram = ' '.join(key)
                    #print("bigram:", bigram)
                    #print("word:", word)
                    if bigram in word:
                        word = word.replace(bigram, ''.join(key))
                        #print("change word:", word)
                        change = True
            
                if not change:
                    tokens.append(word)
                    break
            #encoded_tokens.extend(tokens)
        return tokens
    
    def decode(self, ids_list):
        return ' '.join([self.id_to_token(ids) for ids in ids_list])
        

# 模拟merge过程
corpus = "low lower newest widest"
print(f"Original text: {corpus}")

bpe = BPE()
bpe.get_vocab(corpus)
print(f"\nInitial vocabulary: {bpe.vocab}")

num_merges = 3
bpe.merge(num_merges)
print(f"\nMerge rules: {bpe.merge_rules}")
bpe.get_tokens_dict('merge_result.txt')
print(f"\nTokens dictionary: {bpe.tokens_dict}")
# 对文本进行编码
encoded_text = bpe.encode(corpus)
print(f"\nEncoded text: {encoded_text}")

ids_list = []
for encoded_token in encoded_text:
    print("encoded_token:", encoded_token, "token_to_id:", bpe.token_to_id(encoded_token.split()))
    ids_list.append(bpe.token_to_id(encoded_token.split()))


for ids in ids_list:
    print("ids:", ids, "id_to_token:", bpe.id_to_token(ids))
```

**结果：**

![](https://pic3.zhimg.com/v2-5162f0d01c7d682b7dac4a5ddc7f6848_1440w.jpg)

### get\_vocab方法代码解析

这个方法主要是用来统计语料库中每个单词的频率。  

```
    for word in corpus.split():
        self.vocab[' '.join(list(word)) + ' </w>'] += 1
```

  
先按空格对句子进行切分，切出所有的单词，然后对每个单词按字母级别进行切分放入到list中，然后list再加上后缀 “</w>”，最后将单词和词频写入到self.vocab这个字典中。

```
输入：

corpus = "low lower newest widest"

输出结果：
Original text: low lower newest widest

Initial vocabulary: defaultdict(<class 'int'>, {'l o w </w>': 1, 'l o w e r </w>': 1, 'n e w e s t </w>': 1, 'w i d e s t </w>': 1})
```

### get\_stats方法代码解析

这个方法主要是用来统计词汇表中相邻字符对的频率。

```

    pairs = collections.defaultdict(int)
    for word, freq in self.vocab.items():
        symbols = word.split()
        for i in range(len(symbols) - 1):
            pairs[symbols[i], symbols[i+1]] += freq
```

  
首先先定义一个pairs 字典来记录词汇表中相邻字符对的频率，然后遍历self.vocab字典，对每个单词按空格进行切分，然后遍历切分后的每个字符，将相邻字符对作为key，频率作为value写入到pairs字典中。  

```
debug输出:

pairs: defaultdict(<class 'int'>, {('l', 'o'): 2, ('o', 'w'): 2, ('w', '</w>'): 1, ('w', 'e'): 2, ('e', 'r'): 1, ('r', '</w>'): 1, ('n', 'e'): 1, ('e', 'w'): 1, ('e', 's'): 2, ('s', 't'): 2, ('t', '</w>'): 2, ('w', 'i'): 1, ('i', 'd'): 1, ('d', 'e'): 1})
```

### merge方法代码解析

```
    self.max_merge_times = num_merges
    for i in range(num_merges):
        pairs = self.get_stats()
        if not pairs:
            break
        best_pair = max(pairs, key=pairs.get)
        self.merge_vocab(best_pair)
        self.merge_rules[best_pair] = i
        #print("self.merge_rules:", self.merge_rules)
```

首先将num\_merges赋值给self.max\_merge\_times，然后遍历num\_merges次，每次迭代中，先调用get\_stats方法获取词汇表中所有相邻字符对的频率，如果pairs为空，则说明没有能合并的字符对了，合并完成跳出循环，否则调用merge\_vocab方法把最高频的字符对合并为一个新的字符，并将合并的字符对和当前迭代次数写入到self.merge\_rules字典中。

```
tokens_dict = generate_letter_dict()
tokens_dict['</w>']=max(tokens_dict.values())+1

for key in self.merge_rules.keys():
    tokens_dict[''.join(key)]=max(tokens_dict.values())+1
with open('merge_result.txt', 'w', encoding='utf-8') as f:
    for key, value in tokens_dict.items():
        f.write(f"{key} {value}\n")
```

把大小写字母和后缀 “</w>” 以及合并后的字符赋值一个id，然后写入到merge\_result.txt文件中。

```
debug输出：
合并的字符：
Merge rules: {('l', 'o'): 0, ('lo', 'w'): 1, ('e', 's'): 2}

大小写字母和后缀 “</w>” 以及合并后的字符的id字典：
Tokens dictionary: {'A': 1, 'a': 27, 'B': 2, 'b': 28, 'C': 3, 'c': 29, 'D': 4, 'd': 30, 'E': 5, 'e': 31, 'F': 6, 'f': 32, 'G': 7, 'g': 33, 'H': 8, 'h': 34, 'I': 9, 'i': 35, 'J': 10, 'j': 36, 'K': 11, 'k': 37, 'L': 12, 'l': 38, 'M': 13, 'm': 39, 'N': 14, 'n': 40, 'O': 15, 'o': 41, 'P': 16, 'p': 42, 'Q': 17, 'q': 43, 'R': 18, 'r': 44, 'S': 19, 's': 45, 'T': 20, 't': 46, 'U': 21, 'u': 47, 'V': 22, 'v': 48, 'W': 23, 'w': 49, 'X': 24, 'x': 50, 'Y': 25, 'y': 51, 'Z': 26, 'z': 52, '</w>': 53, 'lo': 54, 'low': 55, 'es': 56}
```

### merge\_vocab方法代码解析

```
        bigram = re.escape(' '.join(pair))
        p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
        new_vocab = {}
        for word in self.vocab:
            #print("word:", word)
            new_word = p.sub(''.join(pair), word)
            #print("new_word:", new_word)
            #print("self.vocab[word]:", self.vocab[word])
            new_vocab[new_word] = self.vocab[word]
        #print("new_vocab:", new_vocab)
        self.vocab = new_vocab
```

re.escape(' '.join(pair)) 将pair中的空格转义，然后生成一个[正则表达式](https://zhida.zhihu.com/search?content_id=249136160&content_type=Article&match_order=1&q=%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F&zhida_source=entity)，用于匹配单词中的相邻字符对。  
p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') 表示匹配单词中以空格分隔的相邻字符对，(?<!\\S)表示匹配前面不是单词边界，(?!\\S)表示匹配后面不是单词边界。  
new\_vocab = {} 定义一个新字典，用于存储更新后的词汇表。  
for word in self.vocab: 遍历原始词汇表中的每个单词。  
new\_word = p.sub(''.join(pair), word) 将单词中的相邻字符对替换为合并后的字符。  
new\_vocab\[new\_word\] = self.vocab\[word\] 将更新后的单词及其频率添加到新词汇表中。  
self.vocab = new\_vocab 更新词汇表。

```
输入：
pair: ('l', 'o')

输出结果：
合并前的vocab: {'l o w </w>': 1, 'l o w e r </w>': 1, 'n e w e s t </w>': 1, 'w i d e s t </w>': 1}
合并后的vocab: {'lo w </w>': 1, 'lo w e r </w>': 1, 'n e w e s t </w>': 1, 'w i d e s t </w>': 1}
```

### encode方法代码解析

```
for word in text.split():
    word = ' '.join(list(word)) + ' </w>'
```

先按空格对句子进行切分，然后对每个单词按字母级别进行切分放入到list中，然后list再加上后缀 “</w>”。

```
    while True:
        change = False
        for key, value in self.merge_rules.items():
            bigram = ' '.join(key)
            if bigram in word:
                word = word.replace(bigram, ''.join(key))
                change = True
    
        if not change:
            tokens.append(word)
            break
```

遍历self.merge\_rules字典，将每个相邻字符对bigram和value分别赋值给key和value，如果bigram在word中，则将word中的对应字符替换为合并的bigram，并将change设置为True。如果没有合并，说明当前word已经不能再合并了，则change为False，并将处理好的word添加到tokens中，跳出合并循环。

```
debug输出：

bigram: l o
word: l o w </w>
change word: lo w </w>
```

### token\_to\_id方法代码解析

```
return [self.tokens_dict[token] for token in tokens]
```

遍历tokens，将每个token作为key从self.tokens\_dict中取出对应的id，然后返回所有token对应的id的list。

```
输入：
Encoded text: ['low </w>', 'low e r </w>', 'n e w es t </w>', 'w i d es t </w>']

输出：
encoded_token: low </w> token_to_id: [55, 53]
encoded_token: low e r </w> token_to_id: [55, 31, 44, 53]
encoded_token: n e w es t </w> token_to_id: [40, 31, 49, 56, 46, 53]
encoded_token: w i d es t </w> token_to_id: [49, 35, 30, 56, 46, 53]
```

### id\_to\_token方法代码解析

```
return [self.id_to_token_dict[id] for id in ids]
```

遍历ids，将每个id作为key从self.id\_to\_token\_dict中取出对应的token，然后返回所有id对应的token的list。

```
debug输出：
ids: [55, 53] id_to_token: ['low', '</w>']
ids: [55, 31, 44, 53] id_to_token: ['low', 'e', 'r', '</w>']
ids: [40, 31, 49, 56, 46, 53] id_to_token: ['n', 'e', 'w', 'es', 't', '</w>']
ids: [49, 35, 30, 56, 46, 53] id_to_token: ['w', 'i', 'd', 'es', 't', '</w>']
```