---
created: 2025-09-02T16:12:02 (UTC +08:00)
tags: [多模态大模型]
source: https://zhuanlan.zhihu.com/p/16815782175
author: 关于作者YiFan-Zhang学术农工吕阿华、郭达森、浮生梦晓也关注了他回答18文章139关注者12,903关注他发私信
---


![|425](https://pica.zhimg.com/v2-004e735824e2bec9c07f29e7e63a708c_1440w.jpg)

随着[MLLMs](https://zhida.zhihu.com/search?content_id=252385175&content_type=Article&match_order=1&q=MLLMs&zhida_source=entity)在人工智能领域的重要性不断提升，如何有效地评估这些模型的能力成为一个关键问题。传统的评估方法多集中于单一任务，而MLLMs的多样性和复杂性要求更全面的评估框架。多模态大模型的多个团队[MME-Team](https://zhida.zhihu.com/search?content_id=252385175&content_type=Article&match_order=1&q=MME-Team&zhida_source=entity)（代表作MME， Video-MME， 以及[MME-RealWorld](https://zhida.zhihu.com/search?content_id=252385175&content_type=Article&match_order=1&q=MME-RealWorld&zhida_source=entity)）MMBench Team（代表作MMBench，MMbench-Video等），以及LLaVA team（代表作LLaVA-Next， LLaVA-OV等）在近期提出了全面的评估综述，旨在填补这一空白，为研究人员提供一个系统的指南。

![|1025](https://pic2.zhimg.com/v2-611948c5550c875fbb9203b0c9ffb7bb_1440w.jpg)

由于近期多模态大模型评估标准发展非常快，这篇文章尝试着从benchmark种类，构建流程，评估方式，评估工具等各个方面对现有的benchmark进行整理，本文的主要贡献包括：

1.  **分类现有基准**：文章提出了一种分层分类法，将现有的评估基准分为基础能力、模型行为和扩展应用三大类。这种分类有助于研究人员快速识别适合的评估工具。同时作者也对特定领域MLLM的发展情况，与专家模型的相关比较进行了总结。
    
2.  **基准构建流程**：详细总结了基准构建的典型流程，包括样本收集、问答对的标注等，并讨论了评估过程中需要注意的事项，如数据污染、基准多样性和样本规模。
    
3.  **性能测量方法**：介绍了三种主要的评估方法：基于人类的评估、基于LLM/MLLM的评估以及基于脚本的评估。此外，还介绍了两大类评估指标和四种评估工具包。
    
4.  **未来基准的方向**：从能力定义、能力导向评估、任务导向评估和多模态整合等角度探讨未来基准的发展方向。
    

通过这篇综述，作者希望帮助研究人员更容易地找到合适的评估基准，并激发对更能反映模型优缺点的评估方法的探索。

![|825](https://pic1.zhimg.com/v2-2b159bb36478858325c3c8c348e6422a_1440w.jpg)

## benchmark主要分类

## 基准测试分类

本文介绍了多种用于评估多模态大模型（MLLMs）的代表性基准测试。作者将其分为三大类， 基础能力，模型分析相关的benchmark，以及在一些子领域具体应用的评估。以下是对一些常用分类的总结和部分实验发现

## 基础能力

### 综合评估

综合评估的目标是开发能够全面回答人类感知和推理问题的智能聊天机器人。以下是一些代表性基准测试及其特点：

-   **VQA v2**：包含453K手动注释的问答对，问题开放式，答案通常简短。
-   **[LVLM-eHub](https://zhida.zhihu.com/search?content_id=252385175&content_type=Article&match_order=1&q=LVLM-eHub&zhida_source=entity)**：汇集了47个标准文本相关视觉基准，评估结果显示MLLM在常识任务上表现优异，但在图像分类、OCR和VQA任务上表现不佳。
-   **MME**、**MMBench**、**Seed-Bench**、**SEED-Bench-2**、**MMT-Bench**, **MME-RealWorld**：这些基准测试涵盖多种感知和认知任务，显示出随着模型规模增加，性能显著提升，但在细粒度感知任务和理解图表与视觉数学上仍有挑战。

### 光学字符识别（OCR）

OCR任务评估模型在文档理解和交通等领域的性能：

-   **TextVQA** 和 **OCR-VQA**：聚焦标准文本识别任务。
-   **InfoVQA** 和 **WebSRC**：引入复杂的结构推理任务。
-   **SEED-Bench-2-Plus** 和 **OCRBench**：包括多种数据类型，展示了模型在识别常规文本、不规则文本、遮挡文本和艺术文本方面的能力。
-   **VCR**：处理嵌入图像且部分被遮挡的文本，要求模型从图像中恢复具体内容。

### 图表和文档

这些数据类型在实际应用中非常重要，要求模型理解布局和元素之间的关系：

-   **ChartQA**：针对图表的问答任务，包括条形图、折线图和饼图。
-   **DocVQA**：针对从工业文档中抓取的文档图像的问答任务。
-   **InfoVQA**：理解信息图像，问题需要基本推理和算术技能。
-   **DocGenome** 和 **CharXiv**：分析科学论文和复杂图表。
-   **MMLongBench-Doc**：关注长文档理解，文档平均长达47.5页。

### 数学推理

视觉数学问题解决能力是MLLM评估的关键方面：

-   **MathVista**：收集现有数据集和新创建的数据集，图像包括几何图形、条形图等。
-   **We-Math**：将问题分解为子问题，评估基本知识概念的掌握。
-   **MathVerse**：将每个问题转换为包含不同视觉和文本内容的6个版本。

### 多学科

多学科知识的掌握是模型专业水平的重要指标：

-   **ScienceQA**：涵盖各个领域的科学问题。
-   **MMMU**：更具挑战性，涵盖广泛学科和大学水平问题。
-   **CMMU** 和 **CMMMU**：在中文环境中进行的领域特定评估。

### 多语言

多语言能力的发展使更多社区受益：

-   **CMMMU**：收集中文多学科基准。
-   **ViOCRVQA**、**Urdu-VQA** 和 **Swahili-STR**：评估其他语言的OCR和VQA能力。
-   **Video-MME**：包括世界主要语言的多语言评估。
-   **MTVQA** 和 **M3Exam**：跨9种语言的多语言基准。

### 指令跟随

指令跟随能力直接影响响应质量和用户体验：

-   **MIA-Bench**：评估模型遵循复杂指令的能力，包含400个图像提示对。

### 多轮问答

多轮问答基准模拟人机交互场景：

-   **ConvBench**：逐步评估感知、推理和创造能力。
-   **MMDU**：多轮多图像对话，样本最多包含20张图片和27轮对话。

### 多图像理解

随着MLLMs的发展，研究人员开始探索从单图像到多图像的视觉能力：

-   **NLVR2**：每个样本包含一对类似图像和一个自然语言描述。
-   **SparklesEval** 和 **MMDU**：挑战模型在多图像和多轮对话中的能力。
-   **Mementos**、**MIRB**、**ReMI** 和 **MuirBench**：评估多图像推理能力。

### 交织图像和文本

交织图像和文本是信息传递的自然形式：

-   **MMMU** 和 **SparklesEval**：采用交织文本和图像的格式。
-   **VEGA**：评估交织图像-文本理解能力。

### 高分辨率

处理高分辨率图像是MLLMs的重要能力：

-   **V\*Bench**：评估高分辨率图像处理性能，包含191张高分辨率图像。
-   **MME-RealWorld**：包含13,366张高分辨率图像，涵盖视频监控、自动驾驶等任务。

### 视觉定位

视觉定位任务旨在根据自然语言查询定位最相关的对象/区域：

-   **RefCOCO** 系列：MLLMs在这些传统基准上表现优异。
-   **Ref-L4**：新提出的基准，具有更广泛的类别覆盖和更长的参考表达。

### 细粒度感知

细粒度感知聚焦于更细致的对象识别：

-   **FOCI**：使用ImageNet-21k的4个子集和5个附加分类数据集。
-   **MMVP**：识别CLIP模型在方向、颜色等方面的表现。
-   **LLVisionQA**：评估低级属性的感知能力。

### 视频理解

传统的视频问答基准通常是领域和任务特定的：

-   **MSVD-QA** 和 **ActivityNet-QA**：分别覆盖动作和对象识别、人类活动的视频。
-   **Video-MME**、**MVBench** 和 **MMBench-Video**：评估MLLMs在视频理解中的表现。
-   **MLVU**、**LVBench** 和 **Event-Bench**：关注长视频理解能力。
-   **EgoSchema** 和 **TempCompass**：评估特定场景和细粒度时间感知能力。

总体来说，当前的MLLMs在处理长视频和时间感知任务时表现不佳，未来需要在这些方面进行改进。

## 模型行为分析

为了更好地理解多模态大模型（MLLM），研究人员开发了多种基准来研究模型的行为或特性，包括幻觉、模型偏见、安全性和因果分析。以下是模型分析的典型方面。

### 幻觉

“多模态幻觉”指的是生成的响应内容与视觉内容不一致的现象。幻觉是影响模型可靠性和实际应用的重要问题。

根据评估结果，主要有两个因素导致幻觉：1）当前MLLMs在视觉能力上不足，例如容易被简单的图像操控或引导性问题误导。此外，即使是先进的[GPT-4V](https://zhida.zhihu.com/search?content_id=252385175&content_type=Article&match_order=1&q=GPT-4V&zhida_source=entity)在处理多张图像时，也难以分辨细微差异或推理时间关系，表明处理图像序列的能力不足。2）模型中的偏见。MLLMs在不同类型的视觉问题上表现各异，通常与区域、文化和语言相关，这可能是由于模型中记住的训练数据不平衡所致。

### 偏见

模型偏见是阻碍MLLMs可用性的重要问题。当前的基准探索了模型偏见的不同方面，并揭示了可能的原因。

VLBiasBench识别出与人类价值观不一致的响应偏见。具体来说，该基准涵盖了9种社会偏见类别，如年龄、性别和外貌。对开源和闭源模型的评估显示，像LLaVA和Shikra这样的开源模型通常表现出不同程度的偏见，而先进的闭源模型Gemini则表现出较弱的偏见。这表明开源和闭源模型在社会偏见控制方面存在巨大差距。Bingo识别出MLLMs在不同区域/文化背景的视觉问题上表现差异显著的区域偏见。考虑了三个偏见类别，包括区域、OCR和事实偏见。MM-SpuBench探讨了虚假偏见，即利用虚假相关性进行预测的倾向。作者将其归因于模型的学习过程，其中视觉标记和文本描述之间的粗粒度对齐可能导致错误的相关性。这些错误的先验嵌入在参数记忆中，会在反直觉的场景中干扰预测。例如，高共现的两个对象/属性可能导致错误预测，如将微波炉识别为厨房。评估结果表明，闭源模型通常优于开源模型。此外，模态对齐在抑制虚假偏见中起关键作用，更好的对齐技术可以提高对虚假偏见的鲁棒性。

### 安全性

模型安全性是模型实际部署中的核心问题。这类基准主要考虑鲁棒性，包括分布外（OOD）鲁棒性和对抗鲁棒性，以及越狱。

**分布外鲁棒性**：主要考虑MLLMs对未见领域的泛化能力，例如在训练语料中未遇到的不同风格的图像。例如，OODCV-VQA和Sketchy-VQA分别包含在现实生活中很少见的图像和简单的草图图像。此外，还包括从原始问题改编的OOD文本指令。[MultiTrust](https://zhida.zhihu.com/search?content_id=252385175&content_type=Article&match_order=1&q=MultiTrust&zhida_source=entity)进一步考虑来自其他领域的图像，例如MRI和红外图像。评估结果显示，MLLMs在理解OOD视觉内容上优于遵循OOD文本指令，这可能表明其在泛化新指令方面的能力不足。

**对抗鲁棒性**：对MLLMs的对抗攻击旨在欺骗模型做出错误响应。相应地，对抗鲁棒性是评估的关键方面，衡量模型对恶意攻击的鲁棒性。AttackVLM开发了一个框架来合成对抗样本并评估开源MLLMs的对抗鲁棒性。评估结果揭示了像LLaVA和MiniGPT-4这样的开源模型的对抗脆弱性。AdvDiffVLM旨在提高对抗样本生成的效率和可转移性。实验结果表明，与开源模型相比，闭源模型表现出更好的对抗鲁棒性，表明还有很大的改进空间。

**越狱**：主要关注模型拒绝非法响应请求的能力。VLLM安全基准设计了两种越狱策略，分别针对LLM和ViT，以评估模型的弹性。MultiTrust包含三个任务来测试模型对越狱的鲁棒性，包括1）在图像中插入详细的越狱提示，2）将正常的文本提示与插入图像中的越狱提示结合，3）与正相关或负相关图像配对的越狱提示。这些研究表明，与需要精心设计提示进行越狱的现代LLM相比，MLLMs在简单但有害的指令嵌入图像时更脆弱，并且当前MLLMs的调优损害了嵌入在LLM中的安全协议。

此外，MOSSBench评估了MLLMs对某些视觉刺激的过度敏感性，不顾良性上下文拒绝无害查询。基准样本中包括三种类型的刺激，包括夸大风险、否定伤害和反直觉解释。对20个MLLMs的评估强调了过度敏感性在当前MLLMs中普遍存在，尤其是在那些更安全的模型中，这可能表明模型响应的安全性和保守性之间存在权衡。

### 因果关系

因果关系指的是一个变量的变化导致另一个变量变化的因果关系。理解这种关系的能力，即因果推理，是理解和分析世界的重要能力。最近，一些研究探索了MLLMs在因果推理能力方面的评估。

CELLO引入了一个涉及人类和/或物体的因果关系的统一定义，并构建了一个包含12个因果任务的基准。评估显示，当前的MLLMs，如BLIP-2和Claude3 Sonnet，在因果推理能力上表现较弱，有些甚至不如随机猜测。

## 扩展应用

随着多模态大模型（MLLMs）的快速发展，研究人员积极探索其在下游任务中的应用，并在医学和情感等领域开发了相应的基准测试。相比于一般评估，这些基准测试更注重领域知识和技能的掌握。

### 医学图像

医学图像直接反映人体状态，是临床决策的重要组成部分。许多基准测试已被开发出来评估MLLMs在分析这类图像时的表现。

-   **SLAKE**：一个双语（中文和英文）基准测试，提供了更多的注释，包括分割掩码和边界框。
-   **PMC-VQA**：涵盖更多图像领域，包括放射学、病理学、显微镜图像和信号等。
-   **GMAI-MMBench**：包含39种医学图像模态、18个临床相关任务、18个科室和4种感知粒度的VQA格式。
-   **OmniMedVQA**：涵盖超过20个解剖区域和12种不同模态，如MRI、CT和X光，图像来源于真实医疗场景。

评估结果显示，当前的MLLMs在OmniMedVQA上的表现不佳，大多数MLLMs仅略优于随机猜测。即使表现最好的医学领域MLLM（MedVInT）也不如通用模型（如BLIP-2），这可能是由于缺乏大规模、高质量的医学图像-文本对训练所致。这些结果表明，开发专门用于医学的MLLMs还有很长的路要走。

### 情感分析

情感分析旨在从各种模态的数据（如视觉、文本和音频）中提取人类情感。与主要是客观的常见任务不同，情感分析需要解释高度主观和情感化的多模态内容，因此带来了新的挑战。MLLMs凭借其强大的泛化和推理能力，有望在这一任务中取得突破。

-   **EmoBench**：包含从一般情感和意图理解（多类分类）到社交媒体情感检测（是/否二分类）的任务，数据来源于现有数据集。
-   **FABA-Bench**：专注于面部情感分析，包含情感识别和动作单元识别两个任务。

评估结果表明，经过情感相关数据微调的MLLMs比零样本MLLMs表现更好，包括先进的闭源模型（如GPT-4V）。这表明注入情感领域知识对于情感分析的下游任务至关重要。

### 遥感

遥感是一门多学科领域，涉及从远处（通常使用卫星或航空传感器）获取和分析地球表面和大气信息。遥感在环境监测、城市规划、农业和灾害管理等众多应用中发挥着关键作用。多个基准测试已被开发出来以推进对遥感图像的理解。

-   **[RSVQA](https://zhida.zhihu.com/search?content_id=252385175&content_type=Article&match_order=1&q=RSVQA&zhida_source=entity)**：早期工作，构建了传统VQA形式的评估集，涵盖分类、物体计数和检测等任务。问题和答案简短且基于预定义的管道，涉及元素（如道路和水域）及其相关属性（如形状和大小）或位置关系。
-   **RSIEval**：手动注释了标题和视觉问题，除了常见的物体相关问题（如存在、数量或颜色），还包括需要推理/外部知识的问题，如“这张图像是在什么季节拍摄的？”。
-   **VRSBench**：综合基准测试，包含图像描述、视觉定位和VQA任务，注释了用于高级定位能力评估的边界框。
-   **RSVG**、**RSVGD**和**RRSIS-D**：专注于遥感图像中的视觉定位，尝试使用边界框或分割掩码定位对象，给定自然语言查询。

-   **MME—RealWorld** 专门为MLLM设计的遥感场景下的高分辨率VQA任务。

评估结果显示，即使是GPT-4V在处理VQA和定位任务时也存在困难，这表明需要将领域知识注入MLLMs。经过专门微调的MLLMs可以在遥感任务中达到或超过专业模型的表现，表明MLLMs在解决遥感任务中的潜力。

### 智能代理

智能代理能够感知环境并采取行动完成目标任务。最近，开发能够处理和推理多模态信息（如视觉、音频和文本）的多模态代理引起了广泛关注，MLLMs在其中发挥着关键作用。为此，多个基准测试被建立来评估MLLMs作为代理的表现。

-   **AppAgent**：主要评估代理在10个智能手机应用程序（如Google Maps）上执行50项任务的能力，使用的指标包括成功率、奖励和平均步骤。
-   **Mobile-Eval**：类似的基准测试，评估移动代理，包含每个10个主流应用程序的3个指令。
-   **GPT4Tools**：专注于工具使用能力，设计了不同方面的指标，包括整体成功率和应用特定工具（如思维、工具名称和工具参数）的成功率。

评估结果显示，即使是先进的GPT-4在零样本方式下也难以规划和执行智能手机应用程序查询，部分原因是准确预测坐标的挑战或对特定应用程序知识的不足，这需要更多探索来解决。

### 代码生成

代码生成是MLLMs的一项重要能力，在现实生活中有广泛的应用，如辅助编写代码或为复杂问题提供自动解决方案。

-   **ChartMimic**：涉及两个图表到代码生成任务，即直接模仿和定制模仿，后者指生成具有类似样式/美学和定制数据的新图表。基准测试涵盖各种类型的图形，包含1000个人工策划的三元组，即图形、Python代码和指令。
-   **WCGB**：围绕网页到代码生成，旨在评估将网页截图转换为HTML代码的能力。

据评估结果显示，LLM主干在多模态代码生成中起着重要作用。开源MLLMs仍然远远落后于闭源模型，表现最好的Phi-3-Vision仅达到GPT-4V一半的表现。此外，开源模型在生成可执行代码方面存在明显缺陷，大多数模型的成功率低于60%。

### 图形用户界面（GUI）

当前的多模态基准测试正在扩展到GUI领域，以评估MLLMs在感知和推理GUI元素方面的表现。

-   **RefExp**：早期基准测试，专注于UI屏幕内的对象定位。
-   **Widget Captioning**：增加了难度，要求模型生成UI元素的描述性语言，测试其感知能力。
-   **Screen2Words**：进一步推动了界限，要求模型生成UI节点的内容和功能描述，测试其对页面布局和功能的理解。
-   **ScreenQA**：简化了评估过程，仅使用图像和文本输入，专注于基于文本提示定位和识别UI元素的基本QA任务。
-   **Rico-semantics**：注释了50万个UI元素属性和关系，增强了评估维度，以评估模型对UI元素形状和语义关联的理解。

在这些基准测试中，MLLMs表现出一些显著的局限性。首先，在任务层面，当前模型在理解小图标和特定领域的UI组件设计方面存在困难，并且在细粒度空间理解方面表现不足。特别是在UI元素的感知和定位方面，现有的MLLMs面临重大挑战。其次，在模型层面，开源模型在这些任务中的表现普遍较差，而闭源模型如GPT-4V表现相对优越。通过对GUI数据进行进一步的监督微调，这些模型的表现接近GPT-4V。最后，在性能层面，当前模型的有效性与训练数据高度相关。对于大多数开源MLLMs来说，GUI数据属于分布外数据，限制了模型在这些任务上的表现。

### 迁移能力

MLLMs展示了强大的泛化能力，但在测试数据与训练数据的图像风格差异显著时仍面临挑战。近期研究开始关注这一问题。

-   **VLAA**：引入了两个基准测试，评估MLLMs在分布外泛化和对抗鲁棒性方面的表现，揭示了即使是GPT-4V也难以理解素描图像。
-   **BenchLMM**：深入探讨图像风格对模型性能的影响，评估MLLMs在三种不同风格（包括艺术图像风格、成像传感器风格和应用风格）中的鲁棒性，每种风格包含五个子风格。
-   **MMCBench**：集中检查模型在常见扰动下输出的一致性，反映了对各种类型噪声的鲁棒性。

在任务层面，MLLMs在处理简单对象外观查询时表现良好，特别是在是/否问题上表现出色。然而，在识别分布外视觉场景中的对象数量时，其性能显著下降。在模型层面，闭源模型在不同艺术风格之间展示了更强的迁移能力，但仍面临性能下降。此外，虽然大模型在处理噪声输入时表现出色，但模型大小并不直接与鲁棒性相关，有些较小的模型在某些场景中甚至表现更好。

### 具身AI

尽管经过几十年的探索，实现具身AI中的人类水平智能仍是一个重大挑战。这需要为代理提供学习、感知、推理、决策和控制的能力，以在开放、非结构化和动态环境中执行通用任务。MLLMs的出现提供了一个有前景的途径，利用其先进的理解和推理能力来应对具身AI中的挑战。因此，开发了许多具身AI基准测试来评估具身智能相关领域的表现。

-   \*\*Embodied Question Answering (EQA)\*\*：最早的具身问答，专注于在3D环境中从第一人称视角进行导航和信息收集。
-   **EPIC-KITCHENS**和**Ego4D**：扩展了任务范围，包括行为理解、手-物体交互、社交互动，并提供了从回顾性记忆到未来行为预测的全面能力。
-   **EMQA**和**SQA3D**：评估涉及更复杂的空间、时间和推理理解。
-   **MoTIF**和**EgoTaskQA**：进一步引入了GUI环境中的任务执行和因果分析，提供了场景、时间、空间和因果关系的诊断见解。
-   **EmbodiedScan**和**RH20T-P**：展示了数据规模和任务复杂性的显著增加，专注于机器人中的3D检测、定位和基本任务。

在模型表现方面，MLLMs在处理这些复杂任务时展示了显著的潜力，但也暴露出在精确定位、空间感知和外部知识整合方面的局限性。闭源模型在某些视觉感知任务中展示了强大的能力，但仍依赖于额外的模块或后处理步骤来解决复杂空间感知中的不足。例如，使用GPT-4V作为规划器时，由于幻觉问题，通常需要补充模块如符号规划器。此外，当使用MLLMs作为规划器时，提示设计起着关键作用，结构良好的CoT可以有效减少感知错误。相比之下，经过微调的模型在感知任务中的表现优于未经调优的闭源模型，而开源模型如LLaVA的结果相对较弱。

### 自动驾驶

MLLMs的特性使其在自动驾驶场景中具有天然的适应性。开发了若干基准测试来评估特定水平的能力。这些基准测试从早期的简单任务发展到涵盖复杂场景理解和多步推理的综合任务。

-   **BDD-X**和**HAD**：早期基准测试，专注于通过文本描述和人类建议预测车辆行为，同时理解其背后的原因。
-   **Talk2Car**和**Rank2Tell**：将重点转移到驾驶场景中的物体识别，要求模型根据文本描述识别最相关的物体并排列其重要性。
-   **DRAMA**：开始解决驾驶场景中的安全问题，提供了风险定位和解释的基准测试。
-   **NuScenes-QA**：强调3D点云数据的重要性，提出了更具挑战性的任务，如物体计数、属性识别和比较任务。
-   **NuPrompt**和**LingoQA**：进一步扩展了任务复杂性，特别是在3D跟踪和驾驶行为推理方面。
-   **SUP-AD**：引入了场景级理解的需求，不仅提供了物体级任务，还提供了详细的场景级注释以评估模型能力。
-   **DriveLM**和**Reason2Drive**：增强了模型在多步推理和解释中的评估，特别是通过图结构推理链模拟人类决策过程。
-   **MME—RealWorld** 专门为MLLM设计的自动驾驶场景下的高分辨率VQA任务。

MLLMs在处理简单感知任务（如图像和点云数据识别）时展示了良好的泛化和解释能力，特别是在常规驾驶场景中。然而，当前模型在复杂任务（如方向识别、对特殊光照/天气条件的鲁棒性、视觉定位和空间推理）方面仍然不足。这些任务通常需要传统模型的增强。闭源模型在某些方面优于开源模型，但经过微调的开源模型可以超过传统自动驾驶管道，甚至在某些任务中超越直接使用的闭源模型。要实现高级自动驾驶能力，需要涵盖广泛交通和驾驶场景的大量训练数据，MLLMs在理解来自专用传感器（如雷达）的输入方面还需要进一步改进。

## Benchmark Construction

![|950](https://pic3.zhimg.com/v2-7c32fecec7299cb583027edf228c8c9c_1440w.jpg)

本节介绍了构建基准测试的两个主要过程：数据收集和标注，如图6所示。基准测试的分类主要基于在收集或标注过程中是否需要人工或模型的参与。具体细节在此不再赘述，但是文章总结了目前benchmark的一些常见问题或者挑战

在构建 MLLM 的基准测试时，必须考虑几个重要因素以确保评估真正反映模型的能力。如果忽视这些问题，可能会导致对模型性能的误导性结论。

**多项选择题泄漏：** 许多基准测试使用多项选择题（MCQs）评估模型，这在自动化评估和统计分析方面更容易。然而，这种形式可能导致模型在不真正理解问题或图像的情况下猜对答案。为了应对这一问题，一些基准测试要求模型在提供答案的同时提供推理步骤，确保模型展示对问题及其组成部分的真正理解。此外，一些基准测试设计了更具挑战性或迷惑性的选项，避免简单答案，使模型在没有稳健推理的情况下难以成功。因此，问题的格式应根据评估场景仔细选择，确保模型的推理能力而不仅仅是猜测能力得到正确评估。

**数据泄漏：** 数据泄漏发生在模型使用已经在训练中遇到的数据进行评估时。当基准测试建立在现有学术数据集上时，训练和评估集可能会重叠，问题会更加严重。因此，在构建基准测试时，必须尽量减少与公共数据集的重叠，特别是那些用于训练的数据集。使用新的或仔细选择的数据可以降低泄漏风险。此外，去重和彻底的数据检查等技术可以帮助发现并删除训练和测试中使用的相同或非常相似的例子。

**以视觉为中心的评估：** 许多现有基准测试的一个显著问题是，模型可以在不处理视觉输入的情况下正确回答问题，仅依赖于伴随的文本。这削弱了在多模态任务中评估 MLLM 的目的。例如，在 MMMU 基准测试中，发现许多问题可以在不理解或查看图像的情况下回答，减少了对视觉模态的依赖。为了应对这一问题，新的基准测试如 VideoMME、MMEvalPro 和 CV-Bench 设计了以视觉为中心的任务，要求视觉内容对回答问题至关重要。这些基准测试确保模型不能仅基于文本回答问题，必须整合视觉信息以提供准确的回答。例如，MMEvalPro 使用三联设计（图像、问题和刻意迷惑的选项组合）确保 MLLM 彻底处理视觉输入。在构建新基准测试时，必须仔细策划问题，使图像对答案有显著影响，确保模型真正理解并利用视觉内容。

**基准测试的多样性和样本规模：** 随着 MLLM 的能力不断增强，评估它们的任务的复杂性和多样性也需要相应地发展。简单的任务已不足以揭示这些模型的局限性。此外，样本数量少的基准测试可能会产生不稳健的评估结果，高方差可能导致对模型性能的不可靠结论。因此，具有代表性的 MME-RealWorld 提出了面向真实世界场景的挑战性任务，并将问答对的数量增加到29K，成为迄今为止最大的纯手动标注基准测试。

## 各种评估方式的比较

### 评估策略

本文介绍了三种评估策略：人工评估、基于LLM/MLLM的评估和基于脚本的评估。尽管这些策略的成本依次递减，每种方法都有其优缺点。

1.  **人工评估**被认为是最有效的方法，因为多模态大模型（MLLMs）的最终目的是供人类使用。然而，人工评估无疑增加了时间和劳动成本。此外，如果评估者数量较少，个体偏好可能会影响评分。
    
2.  **基于LLM/MLLM的评估方法可以分为两类**：1. **浅层模型参与**：在这种情况下，LLMs/MLLMs只部分参与评估过程，例如找到最匹配的答案。对那些不擅长遵循指令的MLLMs提供了一种灵活的替代方案，以匹配预测和正确答案。 2. **全模型参与**：这种方法主要用于开放式任务，答案格式和内容不固定。LLMs/MLLMs可以比较参考答案和生成答案，或直接评分。尽管这种方法减少了人力劳动，但其评估结果受到LLMs/MLLMs自身能力的限制，有时不同的LLMs会产生完全不同的评估结果。
    
3.  **基于脚本的评估**方法较为简单，通常用于基于多项选择或“是或否”问题的基准测试。这些评估根据预定义规则进行结果比较。这种方法评估速度快，但也最终准确性很大程度上依赖于正则表达式匹配的有效性。如果模型的指令遵循能力差，输出混乱，这种评估方法可能会失败。因此，在使用这种评估方法时，构建适当的提示以使MLLMs输出规范化是至关重要的。
    

## 现有可用的评估toolkit

![|825](https://pic3.zhimg.com/v2-7099bf6c61ac83bb5ea7e36d1ba81cda_1440w.jpg)

评估数据集的分散分布、繁琐的准备工作以及由于环境要求不同而导致的潜在冲突和结果不匹配，对多模态模型在众多基准上的严格评估和结果整合带来了挑战。工具包的存在简化了多数据集和多模型的集成，支持同时在多个数据集上进行评估，同时简化了环境配置。

**VLMEvalKit** 代码库支持超过 70 种不同的 MLLMs，包括专有 API 和开源模型，以及超过 20 个多模态基准，涵盖广泛的任务和场景。对于来自不同来源和格式的大量数据集，VLMEvalKit 标准化了数据的预处理，并通过路径或 base64 编码存储多模态内容。每个评估基准与一个相应的 TSV 文件关联，可以一键下载并通过 MD5 校验和验证完整性。TSV 文件中的每一行代表一个评估样本，包括索引、问题、答案、图像或图像路径以及多选问题的选项。每个数据集类根据其问题类型继承自相应的基类，如 VQA、MCQ、Y/N，并支持 .build\_prompt() 接口。评估脚本使用此功能从评估样本构建多模态消息，创建各种模态内容的交错序列。

**LMMs-Eval**支持超过 50 个任务和十几个模型。通过预处理数据集和记录模型输出，LMMs-Eval 实现了多任务的一键评估，从而减少了与数据收集和分散评估结果相关的开销。

传统的评估基准使用固定的问题和答案进行静态评估，这与现实世界的使用场景不符。虽然像 VibeEval 和 LLaVA-Bench (Wilder) 这样的基准利用真实世界的数据来测试模型能力，但训练数据的持续更新使得数据污染不可避免。LiveBench 通过定期从互联网上收集真实新闻，并利用强大的商业多模态模型构建 QA 对，形成每月评估数据集。这种方法确保数据保持真实，并最大限度地减少污染，同时通过控制问题数量来管理数据集构建和更新的成本。

## 挑战与未来方向

尽管学术界和工业界已经引入了数百个基准来评估这些模型，但当前的评估环境仍然面临若干挑战。

首先，缺乏一个被广泛接受的、标准化的能力分类法，现有的评估集通常定义了各自不同的能力维度。其次，当前的评估基准在覆盖关键能力方面存在缺口，尤其是在指令跟随、复杂多模态推理、多轮对话体验和创造力评估等领域。第三，针对特定任务的评估尤其在商业相关领域如发票识别、多模态知识库理解和用户界面操作等方面仍然匮乏。最后，虽然现有的多模态评估集主要集中在图像和视频模态上，但在音频和三维表示能力的评估方面仍显不足。解决这些挑战对于未来发展更为健壮和全面的多模态模型评估方法至关重要。

### 明确的能力分类法

多模态基准的快速增长极大地扩展了多模态评估的广度和深度。这些基准通常定义了从几个到几十个的能力维度。然而，不同基准之间的这些维度存在显著的重叠。例如，光学字符识别（OCR）、名人识别和场景理解等能力在诸如 MME 和 MMBench 等基准中都很常见。这种冗余凸显了在多模态领域内开发一个被广泛接受的细致入微的能力分类法的紧迫性，以促进连贯的进展。这样的分类法设计提供了多种探索途径。由于与人类认知的对齐是 MLLMs 的一个关键特征，分类法应建立在强有力的理论基础上，与心理学和认知科学中的既定研究相一致。

### 基于能力的评估

尽管发展迅速，目前的 MLLM 评估仍不够全面，主要集中在通过客观问题评估感知和推理能力。这在评估方法与实际应用场景之间造成了明显的差距。此外，基于客观评估结果优化模型往往导致开发者在指令调优过程中引入大量客观问题语料，可能损害实际对话体验的质量。尽管像 WildVision 和 OpenCompass MultiModal Arena3 这样的主观多模态评估平台已经出现，但仍需要更多研究来开发更贴近实际使用场景的评估方法。当前的评估策略主要依赖于收集或制作特定问题来评估特定能力。然而，复杂的多模态问题通常需要整合多种技能。例如，与图表相关的问题可能涉及 OCR、空间关系识别、推理和计算。这些独立能力缺乏解耦评估是当前评估框架中的一个重大限制。此外，现有的多模态评估没有充分覆盖关键能力，如指令跟随，只有少数基准如 VisIT-Bench 和 MIA-Bench 涉及这一方面。多轮对话作为人类与多模态模型交互的主要模式，仍然是大多数当前模型的弱点，相应的评估仍处于初级阶段，如 ConvBench 和 MMDU。在复杂多模态推理领域，当前的评估主要集中在解决数学和考试问题上，需要在难度和与日常使用案例的对齐方面进行改进。值得注意的是，多模态创造性任务的评估，作为这些模型的一个关键应用领域，例如基于图像和文本提示的文本生成，仍然基本未被探索，突显了当前评估环境中的一个关键缺口。

### 基于任务的评估

由于 MLLM 仍处于发展的早期阶段，其商业应用仍然有限。因此，当前的评估主要集中在评估基本能力，而不是在现实世界应用中的表现。展望未来，开发评估框架以评估 MLLM 在特定任务上的表现，尤其是那些具有商业价值的任务，是至关重要的。这些任务可能包括大规模文档处理、多模态知识库理解、异常检测和工业视觉检查。在为 MLLMs 构建任务特定的评估时，除了性能指标，还必须考虑计算成本和推理速度，并与传统的计算机视觉方法如 OCR、目标检测、动作识别等进行比较，以评估其实际适用性。此外，MLLMs 的一个重要功能在于其在规划、作为代理与环境交互以解决复杂问题方面的潜力。开发多样化的虚拟环境，以允许 MLLMs 作为代理进行交互和展示其解决问题的能力，可能成为未来评估的关键组成部分。目前，这一领域的评估工作仍处于早期阶段，表明这是多模态 AI 评估领域未来研究和发展的一个有前景的领域。

### 包含更多模态

当前对 MLLMs 的评估主要集中在图像和视频模态上，而对其他模态的关注较少。在音频领域，像 Qwen-Audio 这样的模型评估音频相关能力主要集中在传统音频任务上，如自动语音识别、语音到文本翻译、声学场景分类和声乐声音分类。然而，在评估与语音元信息识别相关的能力方面，仍然存在显著的差距，如口音识别、情感检测等。三维模态评估领域也处于起步阶段，像 ScanRefer 和 ScanReason 这样的工作代表了早期的尝试。此外，随着像 GPT-4o、Gemini 和 VITA 这样的全能 MLLMs 的出现，迫切需要开发评估框架，以评估模型在多个模态上的同时感知和跨模态推理能力。这一不断发展的环境强调了扩展和多样化评估方法的重要性，以跟上 MLLMs 快速发展的能力，确保在更广泛的模态及其交互方面进行全面评估。

-   最后，欢迎大家关注github，聚合了Multimodal Large Language Models, Large Language Models, and Diffusion Models以及一些前沿研究方向的一些阅读笔记，非常欢迎大家补充完善
