---
created: 2025-09-02T16:08:55 (UTC +08:00)
tags: [数学模型]
source: https://zhuanlan.zhihu.com/p/688215018
author: 关于作者YiFan-Zhang学术农工吕阿华、郭达森、浮生梦晓也关注了他回答18文章139关注者12,903关注他发私信
---

# 万字长文总结多模态大模型最新进展（Modality

> ## Excerpt
> 系列笔记 yearn：万字长文总结多模态大模型评估最新进展 yearn：万字长文梳理RL最新进展：从policy gradient到REINFORCE++ yearn：万字长文总结多模态大模型最新进展（Video篇） yearn：万字长文总结多模态大模型…

---
系列笔记

[yearn：万字长文总结多模态大模型评估最新进展](https://zhuanlan.zhihu.com/p/16815782175)

[yearn：万字长文梳理RL最新进展：从policy gradient到REINFORCE++](https://zhuanlan.zhihu.com/p/24421624957)

[yearn：万字长文总结多模态大模型最新进展（Video篇）](https://zhuanlan.zhihu.com/p/704246896)

[yearn：万字长文总结多模态大模型后训练：从幻觉到o1-reasoning](https://zhuanlan.zhihu.com/p/31278114666)

[多模态大型语言模型](https://zhida.zhihu.com/search?content_id=241067039&content_type=Article&match_order=1&q=%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B&zhida_source=entity)（MLLM）最近已成为一个新兴的研究热点，它将强大的大型语言模型（LLMs）作为大脑来执行多模态任务。 MLLM的惊人新能力，如基于图像撰写故事和无OCR的数学推理，在传统方法中很少见，这表明了通向通用人工智能的潜在路径。

通常人们会在pair数据上进行大规模（相对于instruction tuning）的预训练，以促进不同模态之间的对齐。对齐数据集通常是图像文本对或自动语音识别（ASR）数据集，它们都包含文本。更具体地说，图像文本对以自然语言句子的形式描述图像，而ASR数据集包含语音的转录。对齐预训练的常见方法是保持预训练模块（例如视觉编码器和LLMs）冻结，并训练一个可学习的接口,本文调研了到近期位置不同的接口设计以及学习方法相关的文章。

-   [Flamingo: a Visual Language Model for Few-Shot Learning](https://zhuanlan.zhihu.com/write)
-   [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://zhuanlan.zhihu.com/write)
-   [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://zhuanlan.zhihu.com/write)
-   [Visual Instruction Tuning](https://zhuanlan.zhihu.com/write)
-   [Improved Baselines with Visual Instruction Tuning](https://zhuanlan.zhihu.com/write)
-   [LLaVA-NeXT: Improved reasoning, OCR, and world knowledge](https://zhuanlan.zhihu.com/write)
-   [Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models](https://zhuanlan.zhihu.com/write)
-   [MIMIC-IT: Multi-Modal In-Context Instruction Tuning](https://zhuanlan.zhihu.com/write)
-   [LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding](https://zhuanlan.zhihu.com/write)
-   [SVIT: Scaling up Visual Instruction Tuning](https://zhuanlan.zhihu.com/write)
-   [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://zhuanlan.zhihu.com/write)
-   [NExT-GPT: Any-to-Any Multimodal LLM](https://zhuanlan.zhihu.com/write)
-   [InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition](https://zhuanlan.zhihu.com/write)
-   [CogVLM: Visual Expert for Pretrained Language Models](https://zhuanlan.zhihu.com/write)
-   [OtterHD: A High-Resolution Multi-modality Model](https://zhuanlan.zhihu.com/write)
-   [Monkey : Image Resolution and Text Label Are Important Things for Large Multi-modal Models](https://zhuanlan.zhihu.com/write)
-   [LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models](https://zhuanlan.zhihu.com/write)
-   [MoE-LLaVA: Mixture of Experts for Large Vision-Language Models](https://zhuanlan.zhihu.com/write)
-   [LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images](https://zhuanlan.zhihu.com/write)
-   [Yi-VL](https://zhuanlan.zhihu.com/write)

## Flamingo: a Visual Language Model for Few-Shot Learning

![](https://pic4.zhimg.com/v2-d9dda02dfcef8135293e92d80e08e239_1440w.jpg)

总的来说，首先，Perceiver Resampler接收来自视觉编码器的时空特征（从图像或视频获取），并输出固定数量的视觉标记。其次，这些视觉标记用于通过新初始化的交叉注意力层对冻结的语言模型进行条件化，这些层被插入到预训练的语言模型层之间。这些新层为语言模型提供了一种表达方式，以便将视觉信息纳入到下一个标记预测任务中

### Visual processing and the Perceiver Resampler

**视觉编码器**：是一个预训练并冻结的Normalizer-Free ResNet (NFNet)，使用Radford等人提出的 two-term contrastive loss，在图像和文本对数据集上对视觉编码器进行对比目标的预训练。使用最终阶段的输出，即一个二维空间网格的特征，将其压平为一个一维序列。对于视频输入，帧以1 FPS进行采样并独立编码，以获得一个三维时空特征网格，然后将学习到的时间嵌入添加到其中。特征然后被压平为一维，然后输入到Perceiver Resampler中。

![](https://picx.zhimg.com/v2-ae76ce59b9f74cdf8b12004bf27b10ad_1440w.jpg)

Perceiver Resampler模块将由Vision Encoder输出的可变大小的时空视觉特征网格映射到固定数量的输出标记（图中为五个），与输入图像分辨率或输入视频帧数无关。这个transformer具有一组学习到的潜在向量作为查询，而键和值则是由时空视觉特征与学习到的潜在向量的连接组成。

**Perceiver Resampler**：从不同大小的大型特征图到少量视觉标记。这个模块将视觉编码器连接到冻结的语言模型，如上图所示。它以视觉编码器中的图像或视频特征的可变数量作为输入，并产生固定数量的视觉输出（64个），从而降低了视觉-文本交叉注意力的计算复杂度。类似于Perceiver和DETR，本文学习了预定义数量的潜在输入查询，这些查询被输入到一个Transformer中，并对视觉特征进行交叉关注。消融研究中展示了使用这样一个视觉-语言重采样模块优于一个普通的Transformer和一个MLP。

### [GATED XATTN-DENSE](https://zhida.zhihu.com/search?content_id=241067039&content_type=Article&match_order=1&q=GATED+XATTN-DENSE&zhida_source=entity) details

![](https://pic4.zhimg.com/v2-e81aac64a8a40565ffff65e7a2bbc157_1440w.jpg)

上图提供了一个GATED XATTN-DENSE块的示意图，以及它与一个冻结的LM块的连接方式，同时附上了伪代码。下图绘制了Flamingo-3B模型的24个LM层在训练过程中（从0％到100％）不同层中tanh门控值的绝对值的演变。冻结的LM堆栈的所有层似乎都利用了视觉信息，因为tanh门控的绝对值从其0初始化中迅速增长。我们还注意到，绝对值似乎随着深度增加而增加。然而，从这个观察中很难得出强有力的结论：门控之前的激活的规模也可能随着深度变化。未来的工作需要更好地理解这些添加层对优化动态和模型本身的影响。

![](https://picx.zhimg.com/v2-5c811461ae73b35f0561f062e585be71_1440w.jpg)

### Multi-visual input support

![](https://pic2.zhimg.com/v2-70d604d87d88a0485bccc35c7a5a63ed_1440w.jpg)

首先通过在文本中的视觉数据位置插入image标签以及特殊标记BOS表示“序列开始”或EOC表示“块结束”）来处理文本。 图像由Vision Encoder和Perceiver Resampler独立处理，以提取视觉标记。 在给定的文本标记处，模型仅与最后一个前导图像/视频对应的视觉标记进行交叉关注。 指示文本标记可以关注的图像/视频，或者在没有前导图像/视频时为0

上图说明了本文使用的mask方法，以限制某个文本标记看到的视觉标记数量。我们还对图像/视频和文本的交错序列的符号化进行了规范化。 交错的视觉数据和文本序列。我们考虑交错的图像/视频和文本示例：每个示例包含一系列文本 ，一系列图像/视频 ，以及图像在文本中的位置序列。基于视觉数据的位置，我们定义一个函数 : \[1, \] ↦ → \[0, \]，它为每个文本位置分配最后一个出现在该位置之前的图像/视频的索引（或者如果该位置之前没有视觉数据，则为0）。函数 定义了我们考虑用于预测的标记 $l$ 的可用视觉输入：前面的标记 $y_{< l}=(y_1,\dots,y_{l-1})$.

### 训练细节

1.  **训练数据集**由不同格式的训练数据集混合而成。去除交错的图像文本数据集M3W导致性能下降超过17％，而去除传统的配对图像文本对也会导致性能下降（下降9.8％），这表明需要不同类型的数据集。

![](https://pic4.zhimg.com/v2-75e7cf6153f4dc0230f3ef86e37ae1b9_1440w.jpg)

1.  **冻结LM组件可以防止灾难性遗忘**。如果从头开始训练，我们观察到性能大幅下降了-12.9％。有趣的是，微调我们预训练的LM也导致了性能下降了-8.0％。
2.  **数据集加权**。M3W、ALIGN、LTIP和VTP，其权重分别为1.0、0.2、0.2和0.03。这些权重是在小模型规模下经验性地获得的，并且在之后保持不变。

## BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models

![](https://picx.zhimg.com/v2-65240d0fb56f2efd0e6eb6704216e603_1440w.jpg)

BLIP-2框架概述。我们通过一个两阶段策略预训练轻量级的查询Transformer，以弥合模态差距。第一阶段从冻结的图像编码器中引导视觉-语言表示学习。第二阶段从冻结的LLM中引导视觉到语言的生成学习，这使得零样本指导的图像到文本生成成为可能。&lt;br&gt;

LLM本质上是个语言模型，自然无法直接接受其他模态的信息。所以如何把各个模态的信息，统一到LLM能理解的特征空间，就是第一步要解决的问题。为此，作者提出了Q-Former。

![](https://picx.zhimg.com/v2-93f0dc063e4575bf5586d14ae3807e35_1440w.jpg)

（左）Q-Former和BLIP-2的第一阶段视觉-语言表示学习目标的模型架构。我们共同优化三个目标，这些目标强制查询（一组可学习的嵌入）提取与文本最相关的视觉表示。（右）每个目标的自注意力屏蔽策略，以控制查询-文本交互。

Learned Query的引入在这里至关重要。可以看到这些Query通过Cross-Attention与图像的特征交互，通过Self-Attention与文本的特征交互。这样做的好处有两个：（1）这些Query是基于两种模态信息得到的；（2）无论多大的视觉Backbone，最后都是Query长度的特征输出，大大降低了计算量。比如在实际实验中，ViT-L/14的模型的输出的特征是257x1024的大小，最后也是32x768的Query特征。针对Q-Former的三个训练任务分别是 Image-Text Contrastive Learning (ITC)，Image-grounded Text Generation (ITG)，Image-Text Matching (ITM)。第一阶段，对于模型的训练，就是由以上三个任务组成，通过这几个任务，实现了对于特征的提取与融合。但现在模型还没见过LLM。我们现在用传感器完成了数据的提取与融合，下一步，我们得把数据转换成处理器能识别的格式。

![](https://pica.zhimg.com/v2-5e59f4f4a156ae2a86f8f85c580596c2_1440w.jpg)

BLIP-2的第二阶段视觉到语言生成预训练，从冻结的大型语言模型（LLM）中引导。 （顶部）引导基于解码器的LLM（例如OPT）。 （底部）引导基于编码器-解码器的LLM（例如FlanT5）。全连接层从Q-Former的输出维度调整到所选LLM的输入维度。

通过第一阶段的训练，Query已经浓缩了图片的精华，现在要做的，就是把Query变成LLM认识的样子。这里作者针对两类不同LLM设计了不同的任务：

Decoder类型的LLM（如OPT）：以Query做输入，文本做目标； Encoder-Decoder类型的LLM（如FlanT5）：以Query和一句话的前半段做输入，以后半段做目标；

为了适合各模型不同的Embedding维度，作者引入了一个FC层做维度变换。

### 训练细节

作为图文预训练的工作，工程问题往往是关键。BLIP2的训练过程主要由以下几个值得关注的点：

1.  训练数据方面：包含常见的 COCO，VG，SBU，CC3M，CC12M 以及 115M的LAION400M中的图片。采用了BLIP中的CapFilt方法来Bootstrapping训练数据。
2.  CV模型：选择了CLIP的ViT-L/14和ViT-G/14，特别的是，作者采用倒数第二层的特征作为输出。
3.  训练时，CV模型和LLM都是冻结的状态，并且参数都转为了FP16。这使得模型的计算量大幅度降低。主要训练的基于BERT-base初始化的Q-Former只有188M的参数量。

## InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning

![](https://pic1.zhimg.com/v2-2611061e42ef32d1ce9f1e40392717ae_1440w.jpg)

InstructBLIP的模型架构。Q-Former从冻结的图像编码器的输出嵌入中提取了指示感知的视觉特征，并将这些视觉特征作为软提示输入馈送给冻结的LLM。我们使用语言建模损失对模型进行指令调整，以生成响应。

视觉编码器提取输入图片的特征，并喂入 Q-Former 中。此外，Q-Former 的输入还包括可学习的 Queries (BLIP-2 的做法) 和 Instruction。Q-Former 的内部结构黄色部分所示，其中可学习的 Queries 通过 Self-Attention 和 Instruction 交互，可学习的 Queries 通过 Cross-Attention 和输入图片的特征交互，鼓励提取与任务相关的图像特征。

Q-Former 的输出通过一个 FC 层送入 LLM，Q-Former 的预训练过程遵循 BLIP-2 的两步：1) 不用 LLM，固定视觉编码器的参数预训练 Q-Former 的参数，训练目标是视觉语言建模。2) 固定 LLM 的参数，训练 Q-Former 的参数，训练目标是文本生成。

在推理的时候，对于大部分数据集，如 image captioning，open-ended VQA 等，InstructBLIP 可以直接使用 LLM 生成的文本作为输出；对于 classification 和 multi-choice VQA 这样的任务，InstructBLIP 遵循 ALBEF 的做法生成固定的几种答案，根据概率选择最后的结果作为输出。这种做法的数据集包括有 ScienceQA、IconQA、A-OKVQA (多项选择)、HatefulMemes、Visual Dialog、MSVD 和 MSRVTT 数据集。

### Tricks

1.  **数据重采样**由于训练数据集数量太大，而且每个数据集的大小存在显着差异，均匀混合它们可能会导致模型过拟合较小的数据集，并欠拟合更大的数据集。因此，作者改了一下采样数据的概率，从某个数据集里面采样数据的概率是$p_d=\frac{\sqrt{S_d}}{\sum_{i=1}^D\sqrt{S_i}}$，其中$S_i$是单个数据集的大小。

## Visual Instruction Tuning

### 数据构造

结合GPT-4优异的文字能力，将原始数据构造成结构化的文本信息作为Context，同时通过prompt template请求GPT-4得到一些结果，来生成原始的instruction data。在训练时，则可加入visual token，以得到align后的instruction-tuned model

![](https://pica.zhimg.com/v2-6349286b44a2973fab52e78e695366d0_1440w.jpg)

训练分两步，第一步做对齐，只训projection layer；第二步e2e finetune，vision encoder（clip vit-L）是freeze的。可以看到instruction tuning对任务效果影响巨大，另外每个任务本身的指令数据也对各个任务都有互补作用

![](https://pic3.zhimg.com/v2-f4271c2a888517a1abaf84611b5bb880_1440w.jpg)

使用不同训练数据在LLaVA-Bench（COCO）上的消融实验。我们报告相对分数，相对于一个仅使用地面真实图像标题和边界框作为视觉输入的文本GPT-4模型。我们使用我们模型输出的答案和GPT-4（仅文本）的答案来提示GPT-4，并让它在两者之间进行比较并给出一个带有解释的评分。

## Improved Baselines with Visual Instruction Tuning

**Response formatting prompts**。我们发现，像InstructBLIP这样的方法无法很好地平衡短形式和长形式VQA的原因主要有以下几点。首先，是响应格式上的模糊提示。例如，Q: {问题} A: {答案}。这样的提示并不清楚地指示了期望的输出格式，甚至在自然的视觉对话中，**也可能使LLM在行为上过度拟合为短形式答案**。其次，没有对LLM进行微调。第一个问题由于InstructBLIP只对Qformer进行了指导调整而进一步恶化。它需要Qformer的视觉输出令牌来控制LLM的输出长度，使其为长形式或短形式，就像前缀调整一样，但是Qformer可能缺乏正确执行此操作的能力，因为与LLMa等LLM相比，其容量有限。为了解决这个问题，我们建议**使用一个单一的响应格式提示，清楚地指示输出格式**，在促进短答案时附加到VQA问题的末尾：用一个词或短语回答问题。我们经验证明，当LLM使用这样的提示进行微调时，LLaVA能够根据用户的指示正确调整输出格式，并且不需要对VQA数据进行额外处理，这进一步实现了对各种数据源的扩展。

**Academic task oriented data** 我们进一步包括了额外的学术任务导向的VQA数据集，用于VQA、OCR和区域级感知，以各种方式增强模型的能力，如表1所示。我们首先包括了InstructBLIP中使用的四个额外数据集：开放知识VQA（OKVQA ，A-OKVQA ）和OCR（OCRVQA ，TextCaps）。A-OKVQA被转换为多项选择问题，并使用特定的响应格式提示：直接用给定选项的字母回答。仅使用InstructBLIP使用的数据集子集，LLaVA就在表1中的所有三个任务上都超过了它，表明LLaVA的有效设计。此外，我们发现进一步添加区域级VQA数据集（Visual Genome，RefCOCO）可以提高模型对细粒度视觉细节的定位能力。

**Additional scaling.** 进一步增加了输入图像的分辨率，以使LLM能够清晰地“看到”图像的细节，并将GQA数据集作为额外的视觉知识源。我们还加入了ShareGPT 数据，并将LLM扩展到13B，在MM-Vet上的结果显示了将LLM扩展到13B时的最显著的改进，表明了基础LLM能力对视觉对话的重要性。

**Limitations.。尽管LLaVA-1.5展示了令人期待的结果，但必须承认存在一些限制。首先，LLaVA利用完整的图像补丁，可能会延长每个训练迭代的时间。虽然视觉重采样器可以减少LLM中的视觉补丁数量，但它们目前不能像LLaVA那样有效地收敛，可能是由于重采样器中的可训练参数更多**。一个高效的样本重采样器的开发可以为未来扩展指导跟随多模态模型铺平道路。第二，由于缺乏这种指导跟随数据和上下文长度的限制，**LLaVA-1.5目前还不能处理多个图像**。第三，尽管LLaVA-1.5在遵循复杂指令方面表现出了熟练，但其问题解决能力在某些领域仍然可能受到限制，这可以通过更有能力的语言模型和高质量、针对性的视觉指导调整数据来改善。最后，尽管LLaVA的产生幻觉的倾向显著降低，但它**仍然可能产生幻觉并偶尔传播错误信息**，在关键应用（例如医学）中应谨慎使用。

## LLaVA-NeXT: Improved reasoning, OCR, and world knowledge

LLaVA-NeXT，它在推理、OCR和世界知识方面有所改进。LLaVA-NeXT甚至在几个基准测试中超越了Gemini Pro。

与LLaVA-1.5相比，LLaVA-NeXT有几个改进：

1.  将输入图像分辨率提高了4倍像素。这使得它能够捕捉更多的视觉细节。它支持三种宽高比，分辨率可达672x672、336x1344、1344x336。
2.  通过改进的视觉指导调整数据混合，提供更好的视觉推理和OCR能力。针对更多场景提供更好的视觉对话，涵盖不同的应用。具有更好的世界知识和逻辑推理能力。
3.  除了性能提升外，LLaVA-NeXT还保持了LLaVA-1.5的简约设计和数据效率。它重用了LLaVA-1.5的预训练连接器，并且仍然使用不到100万个视觉指导调整样本。最大的34B变种在约1天内使用32个A100完成训练。

### Detailed Technical Improvement

![](https://picx.zhimg.com/v2-71f9cb077319722e3a51cbcd1ace63db_1440w.jpg)

通过将图像分割成网格并独立对其进行编码，将LLaVA-1.5扩展到更高分辨率。这使得模型能够适应任何分辨率，而无需为ViTs执行位置嵌入插值。我们还将下采样图像的特征连接起来，以为LLM提供全局上下文。

**Scaling to Higher Resolutions** 我们通过将图像分成原始训练视觉编码器的分辨率的较小图像块，并独立对其进行编码来克服这一问题。在获取单个块的特征图后，我们将它们合并成目标分辨率的单个大特征图，并将其馈送到LLM中。为了为LLM提供全局上下文并减少分割-编码-合并操作的人为因素，我们还将一个降采样图像的特征连接到合并后的特征图中。这使我们能够将输入扩展到任意分辨率并保持LLaVA-1.5的数据效率。我们将这个结果模型称为LLaVA-1.5-HD。

**高质量的用户指导数据**。我们对高质量的视觉指导跟随数据的定义主要有两个标准：首先，**任务指令的多样性**，确保充分代表了在真实世界场景中可能遇到的广泛用户意图，特别是在模型部署阶段。其次，响应的优越性至关重要，目标是获得良好的用户反馈。为实现这一目标，我们考虑了两个数据来源：(1) 现有的GPT-V数据，包括LAION-GPT-V和ShareGPT-4V。(2) 为了进一步促进更多场景下更好的视觉对话，我们收集了一个包含不同应用的小型15K视觉指导调整数据集。这些指令和图像来自LLaVA演示，是真实用户的请求。我们仔细过滤可能涉及隐私问题或潜在有害的样本，并使用GPT-4V生成响应。

**多模态文档/图表数据**。 (1) 我们从训练数据中删除了TextCaps，因为我们意识到TextCaps使用与TextVQA相同的训练图像集。这使我们能够更好地了解在开发过程中评估TextVQA时我们模型的零-shot OCR能力。为了维持和进一步提高我们模型的OCR能力，我们用DocVQA和SynDog-EN替换了TextCaps。(2) 受到Qwen-VL-7B-Chat的启发，我们进一步添加了ChartQA、DVQA和AI2D，以便更好地理解图表和图表的内容。

### Open Problems in LMMs

**数据效率** 在本节中，我们进行了进一步提高数据效率的实验，通过随机子采样LLaVA-1.5的训练数据混合，采样比例范围从0.1到0.5不等。我们在图4中可视化了不同采样变体的相对性能。首先，完整的数据混合提供了最佳的知识覆盖，并允许模型实现最佳的整体性能。令我们惊讶的是，**仅使用50%的样本，模型仍然保持了超过98%的完整数据集性能**。这表明在数据效率方面还有进一步改进的空间。其次，当将数据集缩减到50%时，模型在MMBench、ScienceQA和POPE上的性能完全不降低，甚至在MMBench上略有改善。同样，当进一步将数据从50%降至30%时，模型的性能保持稳定。这些结果显示了多模态模型也具有“少即是多”的潜在好处。

**重新思考LMM中的幻觉** 将模型的输入分辨率提高到448时，这种幻觉显著减少。这一发现很有意思，因为它表明LMMs可能对训练数据中的一些错误具有鲁棒性。然而，当输入分辨率不足以使模型辨别训练数据中的所有细节，并且超出模型能力的数据量足够大时，模型会学会产生幻觉。这进一步表明，需要在提高数据注释的同时保持良好的模型处理信息的能力之间取得平衡。不平衡的扩展可能导致模型产生更多的幻觉或对视觉细节的理解能力降低。

## Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models

![](https://pica.zhimg.com/v2-0437705810c0b3004d98d48610586c7a_1440w.jpg)

Mixture-of-Modality Adaptation (MMA)概述及LaVIN的架构。在LaVIN中，采用了新颖的混合模态适配器来处理不同模态的指令。在指导调优过程中，LaVIN通过端到端的模态混合训练（Mixture of Modality Training，MMT）进行优化。

本文提出了混合模态适应（Mixture-of-Modality Adaptation，MMA）：一种端到端的优化方案，通过轻量级适配器连接图像编码器和LLM。与此同时，我们还提出了MMA中的一种新颖路由算法，可以帮助模型自动调整单模态和多模态指令的推理路径。基于MMA，我们开发了一个名为LaVIN的大型视觉语言指导模型，它在各种遵循指令的任务中展现出了比现有多模态LLM更优异的训练效率和更好的推理能力。

LaVIN在效率上具有优越性，并且与现有的多模态LLM相比具有竞争力的性能，同时也确认了它作为通用聊天机器人的巨大潜力。实验结果显示，LaVIN可以达到与先进的多模态LLM（如LLaVA）相当的性能，同时减少了高达71.4%的训练时间和99.9%的存储成本。值得注意的是，将LaVIN在ScienceQA上进行微调仅需1.4小时，使用8个A100 GPU，更新的参数仅为3.8M。

## MIMIC-IT: Multi-Modal In-Context Instruction Tuning

![](https://pic4.zhimg.com/v2-6f59245d58ce6b447d6ffdf48ab9a62f_1440w.jpg)

MIMIC-IT数据集包括280万个多模态指令-回复对，涵盖了基本能力：感知、推理和规划。每个指令都伴随着多模态的对话背景，使得在MIMIC-IT上训练的VLM能够展现出在交互式指令遵循方面的强大熟练度，实现零-shot泛化。

数据格式比较：LLaVA-Instruct-150K vs. MIMIC-IT。 (a) LLaVA-Instruct-150K由一张图片及其对应的仅包含语言的上下文信息（黄色框）组成。 (b) MIMIC-IT包含**多个图片或视频的输入数据**，并支持**多模态上下文信息**，即考虑图片/视频和语言输入作为上下文信息。

![](https://pic4.zhimg.com/v2-ccde8c99b09ee4344dc6c57b71df6069_1440w.jpg)

## LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding

本工作通过文本丰富的图像（例如电影海报、书籍封面等）增强了当前的视觉指令调整流程。具体而言，我们首先**使用公开可用的OCR工具在LAION数据集的422K个文本丰富的图像上收集结果**。此外，我们使用识别出的文本和图像标题提示纯文本GPT-4生成16K个对话，每个对话包含针对文本丰富图像的问答对。通过将我们收集的数据与先前的多模态指令遵循数据相结合，我们的模型LLaVAR大大提高了LLaVA模型在基于文本的VQA数据集上的能力（最多提高20%的准确率）。

![](https://pic3.zhimg.com/v2-d25705a7feacf4b3359d3327defe89f4_1440w.jpg)

## SVIT: Scaling up Visual Instruction Tuning

为了推动多模态能力的边界，我们提出了规模化视觉指导调整（SVIT）方法。

SVIT涉及构建一个包含420万个视觉指导调整数据点的数据集，包括160万个对话问答（QA）对，160万个复杂推理QA对，100万个引用QA对和10.6万个详细的图像描述。除了数量之外，所提出的数据集还具有高质量和丰富多样性。它是通过提示GPT-4与丰富的图像手动注释一起生成的。

此外，我们提出了一种新的数据处理方法，选择具有更好多样性和平衡性的子集，从而激发模型的优越能力。

### 数据集选择算法

![](https://pic3.zhimg.com/v2-e8ff97f47a8af096beeaa988ca51eddc_1440w.jpg)

流行的基准测试评估多模态大型语言模型（MLLM）的不同能力，这需要特定的训练数据配方来激发预训练模型。因此，我们设计了一种新的数据配方，即核心集选择算法，以更好地适应这些基准测试，并在性能和训练效率之间取得平衡。

**多样性**。我们构建了一组与流行基准测试相匹配的关键概念，即MME和MMBench。具体来说，我们设计了几个高级概念，然后使用GPT-4生成每个概念的数十个关键词。然后，我们过滤掉在SVIT数据集中频率较低的那些关键词。概念集在上表中。我们通过与概念集的重叠来衡量每个训练样本的信息量，并选择最具信息量的样本。

**平衡**。在MME基准测试中，使用“是”或“否”问题来评估模型。然而，在由GPT-4生成的数据中，这两个选择的比例极不平衡，这使得调整后的模型有倾向性地回答“是”。我们通过重新采样来调整比例。

通过以上两个操作，我们获得了157,712个样本的核心集SVIT-core-150K，其大小与LLaVA-Instruct-150K相同。我们还用SVIT-core-150K替换了LLaVA-v1.5-mix-665K中的LLaVA-Instruct-150K，从而生成了SVIT-mix-665K。

## Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond

![](https://pic3.zhimg.com/v2-8657c104fc246033ae6b776bc5700b6c_1440w.jpg)

**预训练的第一阶段**，我们主要利用**大规模的、弱标记的、网络爬取**的图文对数据集。我们的预训练数据集由几个公开可访问的来源和一些内部数据组成。我们努力清理了数据集中的某些模式。原始数据集包含总共50亿个图文对，在清理后，仅剩14亿数据，其中77.3%是英文（文本）数据，22.7%是中文（文本）数据。**我们在这个阶段冻结了大型语言模型**，只优化了视觉编码器和VL适配器。输入图像被调整为224×224。训练目标是最小化文本标记的交叉熵。最大学习率为2e−4，训练过程使用了30720的图文对批量大小，整个预训练的第一阶段持续了50000个步骤，消耗了大约15亿个图文样本。更多的超参数详见附录C，该阶段的收敛曲线如图所示。

**在多任务预训练的第二阶段**，我们引入了**高质量、细粒度的VL标注数据**，并使用**更大的输入分辨率和交替的图文数据**。同时训练了Qwen-VL的7个任务。对于文本生成，我们使用内部收集的语料库来维持LLM的能力。我们将视觉编码器的输入**分辨率从224×224增加到448×448**，减少了图像降采样造成的信息损失。我们解锁了大型语言模型并训练了整个模型。训练目标与预训练阶段相同。

**在监督微调阶段**，我们通过指令微调来对Qwen-VL预训练模型进行微调，以增强其**指令跟随和对话能力**，从而得到交互式Qwen-VL-Chat模型。多模态指令调整数据主要来自通过LLM自我指导生成的字幕数据或对话数据，这些数据通常只涉及单图对话和推理，并且仅限于图像内容理解。我们通过手动注释、模型生成和策略串联构建了一个额外的对话数据集，以将定位和多图理解能力引入Qwen-VL模型。我们确认模型有效地将这些能力转移到更广泛的语言和问题类型上。此外，我们在训练过程中混合了多模态和纯文本对话数据，以确保模型在对话能力上的普遍性。指令调整数据量为35万。在这个阶段，**我们冻结了视觉编码器，并优化了语言模型和适配器模块**。我们在下面展示了该阶段的数据格式。

## NExT-GPT: Any-to-Any Multimodal LLM

![](https://pic4.zhimg.com/v2-b7125d59f78d301182e6d5ece140e35b_1440w.jpg)

作者提出了一个端到端通用的任意对任意MM-LLM（Multimodal-Large Language Model）系统。NExT-GPT将LLM与多模态适配器和不同的扩散解码器连接起来，使NExT-GPT能够感知输入并以文本、图像、视频和音频的任意组合生成输出。

NExT-GPT基本思想是利用编码器对各种模态的输入进行编码，将其投影为LLM可理解的类语言表示。ExT-GPT利用现有的开源LLM作为核心，处理输入信息，进行语义理解和推理。LLM不仅直接生成文本标记，而且还**产生独特的“模态信号”标记，这些标记作为指令来指示解码层是否要相应地输出什么模态内容**。然后，生成带有特定指令的多模态信号，经过投影后传输到不同的编码器，最终生成相应模态的内容。

**Multimodal Encoding Stage**

首先，NExT-GPT利用现有的完善模型对各种模式的输入进行编码。 对于不同的模态，有一组替代编码器，例如 Q-Former、ViT、CLIP。 在本文中，NExT-GPT采用了ImageBind，它是跨六种模式的统一高性能编码器。 然后，通过线性投影层，不同的输入表示被映射为LLM可以理解的类似语言的表示。

**LLM Understanding and Reasoning Stage**

在LLM方面，NExT-GPT采用的是 Vicuna2，它是一种基于开源文本的 LLM，广泛用于现有的 MM-LLM中。 LLM 将不同模态的表示作为输入，并对输入进行语义理解和推理。 它输出两项内容： 1) 直接文本响应； 2) 每种模态的信号标记，用作指示解码层是否生成多模态内容以及如果生成则生成什么内容的指令。

**Multimodal Generation Stage**

从 LLM接收到多模态信号之后，基于 Transformer 的输出投影层会将信号标记表示映射为后续多模态解码器可以理解的信号表示。 具体来说，NExT-GPT采用当前现成的潜在条件扩散模型（conditioned diffusion models）用于生成不同模态结果，包括用于图像合成的Stable Diffusion模型、用于视频合成的 Zeroscope4模型和用于音频合成的 AudioLDM5 模型。

**Lightweight Multimodal Alignment Learning（轻量级多模态对齐学习）**

![](https://pic4.zhimg.com/v2-6dac669948e35a4ffcf8f916d29f61cb_1440w.jpg)

为了完成编码器对齐，作者从现有语料库和基准中准备了“X-caption”对（“X”代表图像、音频或视频，caption代表文字）数据，然后强制 LLM 根据标注caption生成每个输入模态的caption，学习过程如上图所示。

![](https://pic4.zhimg.com/v2-deb265cc609ea29cb9babefae0960e61_1440w.jpg)

在解码端，NExT-GPT集成了来自外部资源的预训练条件扩散模型，对齐的主要目的是将**扩散模型与LLM的输出指令保持一致**。 然而，在每个扩散模型和LLM之间执行全面的对齐过程将带来巨大的计算负担。 因此，我们在这里探索一种更有效的方法，即解码端指令跟随对齐，如上图所示。 具体来说，由于各种模态的扩散模型仅以文本标记输入为条件， 这种调节与NExT-GPT系统中 LLM 的模态信号标记不同，这导致扩散模型对 LLM 指令的准确解释存在差距。 因此，作者考虑**最小化 LLM 的模态信号标记表示与扩散模型的条件文本表示之间的距离**。 由于仅使用文本条件编码器（扩散模型的Text Encoder冻结），因此学习仅基于纯粹的字幕文本，即没有任何视觉或音频资源，这也确保了高度轻量级的训练。

**2.3 Modality-switching Instruction Tuning（模态转化指令调优）**

尽管编码和解码端能够与 LLM 保持一致，但距离使整个系统能够忠实地遵循和理解用户的指令并生成所需的多模态输出的目标仍然存在差距。 为了增强LLM的能力和可控性，进一步的指令调整（Instruction Tuning，IT）被认为有必要的。 IT 使用“（输入，输出）”对整体 MM-LLM 进行额外训练，其中“输入”代表用户的指令，“输出”表示符合给定指令的所需模型输出。

![](https://pic3.zhimg.com/v2-3fd3f7b2ef3dfe498b2f75d693d36c32_1440w.jpg)

具体来说，作者利用 LoRA使 NExT-GPT中的一小部分参数能够在 IT 阶段与两层投影同时更新。 如上图所示，当 IT 对话样本输入系统时，LLM 会重建并生成输入的文本内容（并使用多模态信号标记表示多模态内容），优化的目标是根据金标注和LLM的输出进行的。 除了LLM调优之外，作者还对NExT-GPT的解码端进行了微调，将输出投影编码的模态信号标记表示与扩散条件编码器编码的金多模态caption标注表示对齐。 至此，全面的调优过程更加接近与用户忠实有效交互的目标。

为了更好地进行指令调优，作者还收集了几组数据集，其中的“X”可以是图像、视频、音频或其他模态的数据：

1.  Text+X →Text Data：此类成熟的数据包括 LLaVA、miniGPT-4、VideoChat等；
2.  Text →Text+X Data：基于现有语料库中丰富的“X-caption”对，通过一些模板，作者借用 GPT-4 来生成各种文本指令来产生数据。
3.  modality-switching instruction tuning（MosIT） Data：作者设计了一些“人”角色和“机器”角色之间的模板对话示例，在此基础上促使 GPT-4 在各种场景下生成更多具有 100 多个主题或关键词的对话。

## InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition

![](https://pica.zhimg.com/v2-6d80c18cd502e8ae7ca92e7ea270578e_1440w.jpg)

InternLM-XComposer的架构和训练方案。预训练阶段对齐了视觉和语言知识，SFT阶段激发了不同的模型能力。

模型由三个组件构成

1.  视觉编码器：EVA-CLIP （CLIP的一个改进变种，通过掩码图像建模能力增强，以有效捕捉输入图像的视觉细微差异）。输入 224x224，以 stride 14 分为小 patch 后输入 transformer
2.  感知采样器（Perceive Sampler）：InternLM-XComposer 中的感知采样器作为一种专注的池化机制，旨在将初始的 257个 图像嵌入压缩为 64 个经过优化的嵌入。这些优化的嵌入随后会与大型语言模型理解的知识结构相匹配。与 BLIP2 类似，使用带有交叉注意力层的 BERTbase 作为感知采样器。
3.  LLM：InternLM-XComposer 以 InternLM 作为其基础的大型语言模型。值得注意的是，InternLM 是一款强大的语言模型，具备多语言能力，在英语和中文方面表现出色。使用公开可用的 InternLM-Chat-7B 作为大型语言模型。

## CogVLM: Visual Expert for Pretrained Language Models

tl;nr: 使用已经训练好的LLM，然后给它添加图像的功能。方法上，引入vit 做图像的encoder和MLP adapter, 来将图像编码到和text一样的embedding空间中，然后是在LLM的各层添加visual expert，它具有独立的QKV和FFN相关的参数， 并使用LLM中的层来做初始化，训练的时候冻结已经训练好的LLM部分，训练图像相关的部分。这就是作者探讨的deep fusion方法。最后的效果提升很大。 除了很少的任务没有超过Pali-x之外，其他全部sota。

浅层对齐的方法： blip-2中，把已经训练好的image encoder冻结，然后加一个Q-former或者linear layer，把image feature映射到语言模型的input embedding space中, BLIP-2 NoCaps CIDEr 121.6。收敛很快，但是结果没有联合训练的模型效果好，e.g., PaLI-X. 用浅层对齐的方法训练的chat-style模型, e.g., MiniGPT-4, LLAVA, and VisualGLM , 视觉理解能力弱表现为幻觉。

作者认为核心问题是，浅层对齐缺少不同模态信息的deep fusion，这个灵感来自p-tuning和LoRA的对比，p-tuning learns a task prefix embedding in the input while **LoRA adapts the model weights in each layer via a low-rank matrix. LoRA效果更好且更稳定**。in the shallow alignment methods, the image features act like the prefix embedding in p-tuning. 其他细节： - 语言模型权重冻结，这些权重是为文本训练的，文本的输入空间，图像的embedding在这个空间里没有很好的对应关系，每一层的输入的分布也是不断变化的，当经过几层变换之后，图像的特征分布已经和比较深的层的权重所需要的输入特征的分布不再匹配了。 - 在预训练过程中，图像字幕任务的先验，例如文字风格和字幕长度，只能在浅对齐方法中编码到视觉特征中。它削弱了视觉特征与内容之间的一致性。

### CogVLM-17B 包含：

1.  LLM：Frozen Vicuna-7B-v1.5，此模型在所有的注意力操作中都应用了因果掩码(causal mask)，包括图像特征之间的注意力。
2.  ViT encoder：EVA2-CLIP-E ，负责将图像转化为特征表示。在CogVLM-17B中，**移除了ViT编码器的最后一层**，因为该层专注于整合 \[CLS\] 特征以用于对比学习。
3.  MLP adapter：a two-layer SwiGLU MLP，用于将ViT的输出映射到与文本特征相同的空间。所有的图像特征在语言模型中共享相同的「位置编码id」。
4.  Visual expert module：在 LLM 的每一层中引入可训练的 visual expert，其包含专门处理 image feature 的「QKV矩阵」和「MLP层」，以实现深度的视觉-语言特征对齐。QKV矩阵和MLP的形状与预训练语言模型中的相同，并从中进行初始化。trainable visual expert 专门用于转换图像特征，功能上和 LLM QKV/MLP 一致，但是只针对 image feature，从而实现模态间的深度融合。

![](https://pic4.zhimg.com/v2-9ec9b63ccff457fb1ded52fd4b45ba85_1440w.jpg)

CogVLM的架构。(a) 关于输入的说明，其中图像由预训练的ViT处理，并映射到与文本特征相同的空间中。(b) 语言模型中的Transformer块。图像特征具有不同的QKV矩阵和FFN。只有紫色部分是可训练的。&lt;br&gt;

**PRETRAINING**: 用了公开可用的图像文本对进行训练，为 LAION-2B 和 COYO-700M

**The first stage**：Image captioning loss, next token prediction task on 1.5B image-text pairs

**The second stage**：a mixture of image captioning and Referring Expression Comprehension (REC) 。在答案的部分，只考虑了下一个标记的预测损失。REC 任务是根据 text description of an object 来预测图像中的 bounding box ，比如 “Question: Where is the \[object\]?” and “Answer: \[x0, y0, x1, y1\]” 。其中，x和y坐标的取值范围从000到999，表示在图像中的归一化位置。

## OtterHD: A High-Resolution Multi-modality Model

在本文中，我们提出了OtterHD-8B，这是一种创新的多模态模型，是从Fuyu-8B演变而来，**专门设计用于以细粒度精度解释高分辨率视觉输入**。与传统模型不同，传统模型受固定大小的视觉编码器限制，OtterHD-8B具有**处理灵活输入尺寸的能力**，确保其在各种推理需求下的多功能性。除了这个模型，我们还引入了MagnifierBench，这是一个评估框架，旨在审查模型对微小物体的细节和空间关系的辨别能力。我们的比较分析显示，虽然目前领先的模型在这个基准测试中表现不佳，但特别是在直接处理高分辨率输入时，OtterHD-8B的表现优于其竞争对手很大程度上。这些发现揭示了不同模型在视觉信息处理中的结构差异，以及**视觉编码器的预训练分辨率差异对模型在这些基准测试中有效性的影响**。我们的研究突显了大型多模态模型中灵活性和高分辨率输入能力的关键作用，并且展示了Fuyu架构的简洁性在处理复杂视觉数据方面所具有的潜力。

## Monkey : Image Resolution and Text Label Are Important Things for Large Multi-modal Models

Monkey模型提出了一种有效地**提高输入分辨率的方法**，最高可达 896 x 1344 像素，而**无需从零开始进行预训练**。针对复杂场景描述、问答和叙述，Monkey模型采用了一种无需预训练即可提高输入分辨率的架构和一种多层级详细描述生成方法。这两个设计确保了模型能够从生成的数据中进行更有效的学习，更高的分辨率可以更详尽地捕捉视觉特征，这反过来又提高了详细描述的有效性。

### 1、提高输入分辨率

![](https://pic4.zhimg.com/v2-2242209a29354f9b85a91c72d66766b9_1440w.jpg)

Monkey的整体架构允许通过从原始图像中捕获全局特征和从分割补丁中获取局部特征来实现高分辨率。所有补丁都通过共享的静态ViT编码器进行处理，例如具有2b参数的ViT-BigG。

1.  给定一个H x W的图像，使用 x （和LMM分辨率一致）大小的滑动窗口将图像划分为更小的局部区域。Monkey对于每个图片块的编码器都增加了独属它的Lora\[10\]来有效地识别和吸收每个图像区域的细节敏感特征，从而增强对空间和上下文关系的理解。训练时只训练Lora部分，因此无需大幅增加参数量和计算需求。
2.  原始图像大小也被调整为 x ，用于全局信息的提取。
3.  最后，通过视觉编码器和重采样器处理所有局部图像和全局图像，并将局部特征和全局特征送入LLM。这种方法能够在不显着增加计算负载的情况下提高模型分辨率和性能。

### 2、多级特征整合详细描述生成

之前的工作如LLaVA\[3\]、Qwen-VL\[4\]等依赖于互联网上爬取的大规模图文数据及进行模型的预训练。但这类数据标注比较简单，缺乏更丰富的图像细节。即使使用高分辨率图像进行训练， LMM 也无法在图像视觉特征和其中各个物体之间建立准确的关联，从而可能损害了视觉处理和语言理解之间的协同作用。Monkey使用了一种多级特征融合的详细描述生成方法（利用 BLIP-2\[5\]、PP-OCR\[6\]、GRIT\[7\]、SAM\[8\]和 ChatGPT\[9\]等预训练系统），为CC3M中的400k图像提供更加细致的描述，来更好地将高分辨率的视觉模型和语言模型对齐。

### 关键发现

1.  提高分辨率能提高模型性能（r3-r9），四个LoRA能够帮助模型获得图像中不同部分的独特特征(r7 vs. r9)，并帮助模型建立对空间和上下文关系的理解。进一步提高输入分辨率能够提高模型在文档等更高分辨率的图像上的性能(r5,r6)。同时，相比与直接插值扩大模型输入分辨率的方法相比(r1,r2 vs. r9)，本文的方法在时间和性能上更具优势。表六中当把llava1.5的输入分辨率从224扩大为448，性能得到显著提升，进一步展现了本文方法的有效性。

![](https://pic4.zhimg.com/v2-9dd64535a2ebf14a8b4422b6e1f9ca7d_1440w.jpg)

## LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models

当前的VLMs在诸如图像字幕和视觉问答等任务中表现出色，但在处理长视频时面临着计算负担，因为存在过多的视觉标记。LLaMA-VID通过用两个不同的标记表示每个帧来解决这个问题，即上下文标记和内容标记。上下文标记基于用户输入编码整体图像背景，而内容标记则封装了每个帧中的视觉线索。这种双标记策略显著减少了长视频的负担，同时又保留了关键信息。总的来说，LLaMA-VID赋予现有框架支持长达一小时的视频，并通过额外的上下文标记推动了它们的上限。在大多数基于视频或图像的基准测试中，它被证明超越了先前的方法。

![](https://pic4.zhimg.com/v2-9d163e917f310479ab7bf069a7e9a827_1440w.jpg)

LLaMA-VID的框架。在用户指令下，LLaMA-VID通过接受单个图像或视频帧作为输入，并从LLM生成响应。该过程始于一个视觉编码器，将输入帧转换为视觉嵌入。然后，文本解码器根据用户输入生成文本查询。在上下文注意力中，文本查询从视觉嵌入中聚合与文本相关的视觉线索。为了提高效率，提供了将视觉嵌入降采样到各种令牌大小甚至单个令牌的选项。然后，使用线性投影器制定文本引导的上下文令牌和视觉丰富的内容令牌来表示每个时间t的每个帧。最后，LLM接受用户指令和所有视觉令牌作为输入并给出响应。&lt;br&gt;

## MoE-LLaVA: Mixture of Experts for Large Vision-Language Models

最近的进展表明，扩展大型视觉语言模型（LVLMs）有效地提高了下游任务的性能。然而，现有的扩展方法使得所有模型参数在计算中对每个标记都是活跃的，这带来了巨大的训练和推理成本。在这项工作中，我们提出了一种简单而有效的训练策略MoE-Tuning用于LVLMs。这一策略创新地解决了多模态稀疏学习中的性能下降问题，从而构建了一个具有惊人参数数量但计算成本恒定的稀疏模型。此外，我们提出了基于MoE的稀疏LVLM体系结构MoE-LLaVA，它在部署过程中通过路由器唯一激活了仅排名靠前的k个专家，使其余的专家保持不活跃状态。大量实验证明了MoE-LLaVA在各种视觉理解和物体幻觉基准测试中的显著性能。值得注意的是，仅有约3B个稀疏激活参数，MoE-LLaVA在各种视觉理解数据集上表现出与LLaVA-1.5-7B相当的性能，甚至在物体幻觉基准测试中超过了LLaVA-1.5-13B。通过MoE-LLaVA，我们旨在建立稀疏LVLMs的基准，并为未来研究开发更高效、更有效的多模态学习系统提供宝贵的见解。

![](https://pic1.zhimg.com/v2-85189e0f8d3bd83219736302df49880e_1440w.jpg)

MoE-Tuning的示意图。MoE-Tuning包括三个阶段。在第一阶段，只有MLP被训练。在第二阶段，除了视觉编码器（VE）之外，所有参数都被训练。在第三阶段，FFN被用来初始化MoE中的专家，只有MoE层被训练。对于每个MoE层，每个标记只激活两个专家，而其他专家保持沉默。

**阶段一**：在这个阶段，我们的目标是使图像标记适应LLM，使LLM能够理解图像中的实例。为了实现这一目标，我们使用MLP将图像标记投影到LLM的输入域中，将图像块视为伪文本标记。**在这个阶段，LLM被训练来描述图像**。MoE层在这个阶段不应用于LLM。

**阶段二**：使用多模态指令数据进行调整是**增强大型模型能力和可控性**的关键技术。在这个阶段，LLM被调整为具有多模态理解能力的LVLM。我们使用更复杂的指令，包括**图像逻辑推理和文本识别等任务**，这些任务要求模型具有更强的多模态理解能力。通常情况下，对于密集型模型，LVLM训练在这个阶段被认为是完成的。然而，我们在同时将LLM转变为LVLM并稀疏化LVLM方面遇到了挑战。因此，MoE-LLaVA利用第二阶段的权重作为第三阶段的初始化，以缓解稀疏模型的学习困难。

**阶段三**：作为初始化，**我们多次复制FFN以初始化专家**。当图像标记和文本标记被输入到MoE层时，路由器计算每个标记与专家之间的匹配权重。然后，每个标记都由前k个专家处理，并且根据路由器的权重进行加权求和。当激活前k个专家时，其余的专家保持沉默。这种建模方法形成了MoE-LLaVA，具有无限可能的稀疏路径，提供了广泛的能力。

![](https://pic3.zhimg.com/v2-a147e4b0e24d77acaf5a47757952ae6c_1440w.jpg)

## LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images

该文讨论了视觉编码在大型多模态模型（LMMs）中对理解视觉世界的基础作用。它突出了现有LMMs的局限性，如固定的图像大小和分辨率，以及最近对这一方向的探索在适应性、效率甚至正确性方面存在的不足。

为了解决这些挑战，该论文介绍了LLaVA-UHD，一种大型多模态模型，旨在高效处理任何纵横比和高分辨率的图像。LLaVA-UHD包括三个主要组成部分：

![](https://pic3.zhimg.com/v2-112d8c1871aacda183546c18c931eea2_1440w.jpg)

LLaVA-UHD框架。左图：给定一个高分辨率图像，LLaVA-UHD首先计算理想的切片数量，然后从可能的因式分解中选择最佳分区，将高分辨率图像分割成不同大小的切片。右图：切片通过在位置嵌入上进行2D插值以保持原始纵横比进行编码，然后压缩并按空间结构排列以供LLM处理。

1.  图像模块化策略：该策略将原始分辨率的图像划分为较小的可变大小切片，以便进行高效和可扩展的编码。
2.  压缩模块：该模块进一步压缩由视觉编码器生成的图像标记，增强了效率。
3.  空间结构：一种用于组织切片标记以供LLMs理解空间关系的模式。

### 模块化视觉编码

针对具有不同纵横比的高分辨率图像，一个朴素的方法是将ViT的位置嵌入插值到目标形状，以整体编码。然而，这种方法由于二次计算成本和由于分布外问题导致的性能降低而不是最佳的。为了解决这个挑战，我们提出了一种模块化的视觉编码策略。基本思想是将原始分辨率图像划分为**较小的可变大小切片**，其中每个切片的形状与ViT的标准预训练设置不会偏离太远。通过可变大小的切片，LLaVA-UHD可以在不需要填充或形状扭曲的情况下实现对原始分辨率图像的完全适应性。

接下来，我们对P进行二维插值，以适应由分区策略给出的切片分辨率，用于视觉编码。在我们的实验中，我们表明，在预训练期间可以保持ViT和位置嵌入参数不变，并且在instruction tuning阶段更新这些参数就足以实现良好的性能。除了切片之外，我们还提供了一个以本机纵横比的低分辨率概览图像。概览图像可以提供图像的粗略信息和全局语义连接。

### 压缩层

高分辨率图像需要LLMs处理更多的视觉标记，这占据了大部分计算量。例如，一个672×1008的分辨率图像将为LLaVA-1.5生成3456个视觉标记。为了解决这个问题，我们使用一个**共享的感知器重新采样器层来压缩每个图像切片的视觉标记**。具体来说，由视觉编码器输出的图像标记**通过一组查询向量通过交叉注意力被重新采样**为较少的数量（在我们的实验中从576个到64个）。与流行的基于MLP的视觉投影方法相比，感知器重新采样器不受图像分辨率的限制，始终保持固定且可负担得起的视觉标记数量，因此更适用于理解高分辨率图像。因此，LLaVA-UHD可以使用比LLaVA-1.5在编码336×336分辨率图像时更低的计算成本来编码672×1008分辨率图像。

### 图像切片的空间结构

由于图像分区在不同图像之间是动态的，因此有必要向LLM提供图像切片的空间组织信息。受FuYu模型的启发，我们设计了一个空间模式来使用两个特殊标记指示图像切片的相对位置。具体地，**我们使用“,”来分隔一行中的切片表示，并使用“\\n”来分隔不同的行**。在我们的实验中，我们发现这种简单的模式可以有效地向动态分区提供信息，从而产生良好的性能。

全面的实验证明，即使建立在分辨率为336×336的LLaVA-1.5架构上，LLaVA-UHD支持高达672×1088的图像，并且在仅使用94％的推断计算量的情况下，在TextVQA上取得了6.4％的准确率提高。此外，该模型在学术环境中可以高效训练，在8个A100 GPU上仅需23小时，而LLaVA-1.5则需要26小时。

## Yi-VL

![](https://pic3.zhimg.com/v2-b65afa1658259064f9bab257cbfe1308_1440w.jpg)

Yi-VL采用了LLaVA架构，经过全面的三阶段训练过程，以将视觉信息与Yi LLM的语义空间良好对齐：

第1阶段：**ViT和投影模块的参数使用224×224的图像分辨率进行训练**。LLM的权重被冻结。训练利用包含来自LAION-400M的1亿个图像-文本对的图像标题数据集。主要目标是增强ViT在指定架构内的知识获取能力，并实现ViT和LLM之间更好的对齐。

第2阶段：**ViT的图像分辨率扩展到448×448**，并训练ViT和投影模块的参数。它旨在进一步提升模型对复杂视觉细节的识别能力。此阶段使用的数据集包括约2500万个图像-文本对，例如LAION-400M、CLLaVA、LLaVAR、Flickr、VQAv2、RefCOCO、Visual7w等。

第3阶段：训练整个模型的参数（即ViT、投影模块和LLM）。主要目标是增强模型在多模态对话交互中的熟练程度，从而赋予其无缝整合和解释视觉和语言输入的能力。为此，训练数据集涵盖了各种来源，总计约100万个图像-文本对，包括GQA、VizWiz VQA、TextCaps、OCR-VQA、Visual Genome、LAION GPT4V等。为确保数据平衡，我们对**任何单个来源的最大数据贡献设定了上限，限制为不超过5万对**。

## Beyond llava-hd: diving into high-resolution large multimodal models

![](https://pic4.zhimg.com/v2-b036458b063fc48a19ba4ec828bc257d_1440w.jpg)

aper: Beyond llava-hd: diving into high-resolution large multimodal models

Link: [https://arxiv.org/abs/2406.08487](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2406.08487)

Code: [https://github.com/yfzhang114/SliME](https://link.zhihu.com/?target=https%3A//github.com/yfzhang114/SliME)

现有的研究通常采用一种很直接的分辨率提升方法，其中图像由全局和局部分支组成，后者是被切片的图像块，但被调整为与前者相同的分辨率。这意味着**更高的分辨率需要更多的局部块**，从而导致高昂的计算开销，同时，**局部图像标记的主导地位可能削弱全局上下文**。例如，在一张1024×1024的图像中被分为9个块时，全局图像标记仅占1/10。

\[2024/07/01\] 使用了Qformer的SliME框架无缝扩展到视频分析[\[扩展方式\]](https://link.zhihu.com/?target=https%3A//github.com/yfzhang114/SliME/blob/main/Slime_video.md)。值得注意的是，即使该模型从未在视频数据上进行过专门培训，但它最多可以处理8帧。在[Video-MME](https://link.zhihu.com/?target=https%3A//video-mme.github.io/home_page.html%23leaderboard)基准测试中，8B SliME超过了许多在视频数据集中接受过训练的7b/8b基线。这进一步强调了在使用Q-former的时候，global context的重要性

### SliME: LMM with Sophisticated Tasks, Local image augmentation, and Mixture of global Experts

![](https://pic3.zhimg.com/v2-439e49f28ef3a5776932bcebedbd23d4_1440w.jpg)

图1:(b) 使用专家混合来优化视觉适配器，(c) 优化局部压缩层，(d) 指令微调。SliME通过切片、投影和选择与查询对齐的相关局部特征来高效处理图像。

我们的核心思想是全局信息应该被优先考虑，因此我们旨在提取和保留尽可能多的全局上下文，同时用局部图像细节增强它。在这项研究中，

1.  我们首先根据分辨率将图像分割成块。然后将图像标记分为两组：全局视图和局部块 （**图1（a）**）。
2.  对于前者，我们保留所有token以维护所有上下文信息，并利用混合适配器进一步探索全局上下文。如**图1（b）**所示，我们使用MLP将图像特征投射到LLM的特征空间，并使用一组可学习的查询（称为qformer）来提取关键的全局信息。通过混合两个适配器的输出，有助于LLM更有效地理解全局上下文。
3.  考虑到局部块，它们提供了额外的图像细节，我们使用learnable query进行压缩以降低计算成本（**图1（c）**）。我们进一步提出了一种文本引导的路由器来选择与输入指令或问题最相关的局部图像标记，从而避免过多的图像标记并关注相关的图像信息(**图1（d）**)。
4.  同时训练全局投影和局部压缩是具有挑战性的。投影层的简单性使其易于训练，但也导致模型由于过度依赖全局特征而忽视局部特征，迅速退化。我们将其形式化为bi-linear problem，并从理论上证明同时更新这两个模块不会收敛到最佳结果。相反，我们建议交替训练全局投影模块和局部压缩模块，以确保全局和局部特征都能被有效地学习和利用。
5.  本文精心收集和筛选数据集，创建了科学和数学推理数据集（SMR），涵盖了自然科学、数学问题和科学图表理解等九项具有挑战性的任务。其中一些任务提供了完整的推理路径，迫使模型阐明整个推理过程。重要的是，SMR数据集中的许多图像包含丰富的注释。完成这些复杂的推理任务需要对图像细节的透彻理解，这将大大有利于我们框架的训练。

最后，欢迎大家关注github，聚合了Multimodal Large Language Models, Large Language Models, and Diffusion Models以及一些前沿研究方向的一些阅读笔记，非常欢迎大家补充完善
