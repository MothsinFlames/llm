---
created: 2025-09-02T20:40:55 (UTC +08:00)
tags:
  - LLM（大型语言模型）
  - 强化学习 (Reinforcement Learning)
  - Qwen大模型
source: https://zhuanlan.zhihu.com/p/1932894729042917355; https://zhuanlan.zhihu.com/p/1933456555954664522
author: 关于作者tomsheepGZH：《零一瓦舍》字节跳动 员工大模型 话题的优秀答主吕阿华、郭达森、浮生梦晓也关注了他回答551文章433关注者13,193关注他发私信
---



## **第二部分：预备知识 (Preliminaries)**

为了让读者理解GSPO的创新之处，论文首先回顾了两个基础性算法：[PPO](https://zhida.zhihu.com/search?content_id=260952584&content_type=Article&match_order=1&q=PPO&zhida_source=entity)和GRPO。

-   **近端策略优化 (PPO)**: PPO是强化学习领域的经典算法。它的核心思想是，在使用从旧策略（ $π_{θ_{old}}$ ）采样的数据来更新当前策略（ $π_θ$ ）时，通过一个“裁剪（clipping）”机制，将策略更新的幅度限制在一个可信的邻近区域内。其优化目标如论文公式(1)所示，其中的关键是词元级别的**重要性比例** $w_t(θ)$ ，它衡量了新旧策略对于同一个词元 $y_t$ 的生成概率之比。PPO的主要挑战在于其严重依赖一个“价值模型（value model）”来估计每个词元的优势 $Â_t$ 。这个价值模型本身就很难训练，且在处理长序列和复杂任务时，其准确性和可扩展性都面临巨大挑战。

![](https://picx.zhimg.com/v2-b3a025947375edf340fd0607ad7c9e33_1440w.jpg)

-   **组相对策略优化 (GRPO)**: 为了摆脱对价值模型的依赖，GRPO算法应运而生。它巧妙地利用了“组”的概念：对于同一个问题（query），让模型生成一组（例如G个）不同的回复。然后，通过比较这一组回复的奖励（reward），来计算每个回复的相对优势 $Â_i$ （如论文公式(3)所示，即用每个回复的奖励减去组内平均奖励，再除以标准差）。这样，所有在同一个回复 y_i 中的词元，都共享同一个优势值 Â_i。GRPO的优化目标如公式(2)所示，它依然沿用了PPO的词元级别重要性比例 w_i,t(θ) 和裁剪机制。虽然GRPO绕过了价值模型，但正如引言所述，它的根基——词元级别的重要性采样——为后续的稳定性问题埋下了伏草蛇灰。

![](https://pic3.zhimg.com/v2-d716b689e6e7b2a16c245b0fbefa94f0_1440w.jpg)

## **第三部分：核心动机 (Motivation)**

这一章节是理解GSPO设计哲学的关键，它深刻剖析了GRPO算法的“**病态（ill-posed）**”本质。

作者指出，随着模型尺寸、稀疏性（尤其是在MoE模型中）和生成长度的增加，为了硬件效率最大化，RL训练必须采用大的批次（batch size）。而为了样本效率，通常会将一个大批次数据拆分成多个小批次（mini-batches）进行梯度更新。这个过程天然地引入了“**离策略（off-policy）**”学习的场景，即用于计算梯度的样本来自于旧策略，而非正在被优化的当前策略。PPO和GRPO中的裁剪机制，正是为了应对这种离策略带来的偏差。

然而，GRPO的问题比单纯的离策略偏差更为根本。作者一针见血地指出，**GRPO的目标函数是构建失当的**，这个缺陷在训练大型模型处理长回复任务时，会急剧恶化，并最终导致模型崩溃。

问题的根源在于对重要性采样原则的误用。重要性采样的核心原理（如论文公式(4)所示）是，通过对从行为分布（π_beh）中采出的**多个样本（N >> 1）进行加权平均，来估计目标分布（π_tar）下的期望值。这个权重** **π_tar(z) / π_beh(z)** **能够有效地修正分布不匹配的问题，但其有效性的前提是基于群体（多个样本）的平均**。

反观GRPO，它在每个词元位置 t 上都应用了一个重要性权重 w_i,t(θ)。这个权重是基于**单个样本** **y_i,t** 计算得出的，它完全不具备修正分布失配的能力。作者犀利地指出，这非但不能实现预期的分布校正，反而向训练梯度中注入了**高方差的噪声**。这种噪声会随着序列的增长而不断累积，而裁剪机制在试图限制离策略样本影响的同时，也无意中放大了这种噪声的破坏力，最终导致模型训练的“脱轨”和崩溃。一旦崩溃发生，即便是回退到早期的检查点（checkpoint）并细致地调整超参数，也往往无力回天。

这个观察揭示了一个核心原则：**优化的基本单元，应该与奖励的基本单元相匹配**。既然奖励是给予整个序列的，那么在词元级别进行离策略校正就显得不合时宜且问题重重。这启发作者们必须放弃词元级别的目标，转而在序列的宏观层面直接进行重要性加权和优化。

## **第四部分：GSPO算法详解 (The GSPO Algorithm)**

基于上述深刻洞见，GSPO算法应运而生。

### **4.1 GSPO：组序列策略优化**

既然词元级别的重要性权重 w_i,t(θ) 是问题的根源，GSPO选择了一个更直接且理论上更合理的路径。如论文公式(5)所示

![](https://pic3.zhimg.com/v2-a86dcf5ab3b22adebb36244f76048fe6_1440w.jpg)

在语言生成的上下文中应用重要性采样，其正确的形式应该是使用**序列级别的重要性权重(** $s_i(θ) = π_θ(y_i|x) / π_{θ_{old}}(y_i|x)$ **)**。这个权重有着清晰的理论含义：它衡量了由旧策略采样的整个回复序列 y_i，在多大程度上偏离了当前策略，这与授予整个序列的奖励天然对齐。

基于此，GSPO的优化目标被定义为论文中的公式(6)。其核心组件包括：

![](https://pic2.zhimg.com/v2-57e1ed6bffe0b669c4df949246229383_1440w.jpg)

-   **组优势估计** **Â_i**：与GRPO类似，采用公式(7)计算，基于组内相对奖励，无需价值模型。
-   **序列级别重要性比例** **s_i(θ)**：这是GSPO的灵魂，如公式(8)定义。它计算的是整个序列的似然度之比。值得注意的是，作者在定义$s_i(θ)$时引入了**长度归一化**（即取对数后除以序列长度$|y_i|$再取指数）。这一精巧的设计旨在降低方差，并使得不同长度回复的$s_i(θ)$值能被控制在一个统一的数值范围内，避免了短序列的几个词元概率剧变就导致整个$s_i(θ)$失控，也使得裁剪范围的设定更为简单和通用。

因此，GSPO的裁剪机制也是在**整个回复层面**生效的，而非针对单个词元。这种设计完美匹配了序列级别的奖励和优化逻辑。

### **4.2 梯度分析**

为了更清晰地展示GSPO和GRPO的本质区别，论文推导并对比了两者的梯度。

-   **GSPO的梯度**（省略裁剪后，如公式(11)所示）：最终的梯度是序列优势$Â_i$乘以整个序列的对数似然梯度$∇_θ log π_θ(y_i|x)$。这意味着，在一个给定的优秀回复中（$Â_i > 0$），**所有词元都受到同等权重的正向激励**，共同为提升整个序列的概率做贡献。它们“同甘共苦”，命运与共。

![](https://pica.zhimg.com/v2-b8e9ae5231bfa3bb26b897a15db1adf4_1440w.jpg)

-   **GRPO的梯度**（如公式(13)所示）：其梯度中，每个词元的对数似然梯度$∇_θ log π_θ(y_i,t|x, y_i,<t)$被其各自的、不稳定的重要性权重$w_{i,t}(θ)$所加权。这意味着，即使在同一个优秀回复中，每个词元受到的激励强度也可能天差地别，有些甚至可能因为权重过小而几乎没有更新。这些不均等的权重累积起来，导致了训练过程的不可预测性。

![](https://pic1.zhimg.com/v2-f6361dd94b072132028bc9be9c22b578_1440w.jpg)

这个对比鲜明地揭示了GSPO如何通过平等地对待一个序列中的所有词元，来消除GRPO中不稳定的根源。

我们可以通过对比梯度更新的「指挥信号」来理解两者的差异：

-   **GRPO 的指挥信号**：对于一个好回答，它会对里面的每个词说：`A词，你很重要，给你1.2倍的权重去学习！B词，你一般般，给你0.9倍的权重`…… 这些权重充满噪声，指令互相矛盾，模型学起来晕头转向。
-   **GSPO 的指挥信号**：对于一个好回答，它会先给出一个总体的评价，比如 `这个回答整体不错，权重是1.1`，然后对里面的所有词说：`大家是一个团队，都朝着这个1.1倍权重的方调整`。



### **4.3 GSPO-token：一个词元级别的目标变体**

论文还考虑了一些特殊场景，比如多轮对话的RL，可能需要更细粒度的、在词元级别上调整优势。为此，作者设计了一个名为**GSPO-token**的变体（公式(14)）。其巧妙之处在于，它在计算每个词元的权重s_i,t(θ)时（公式(15)），使用了**序列级别的重要性比例s_i(θ)的数值**（通过sg，即stop_gradient或detach操作，使其不参与梯度计算），再乘上词元级别的似然比。

![](https://pic1.zhimg.com/v2-0bc199a7fd8b2f16ba9d0cdf87706b22_1440w.jpg)

通过梯度推导（公式(18)），作者证明了一个惊人的结论：当一个序列中所有词元的优势都相同时（即$Â_{i,t} = Â_i$），GSPO-token在优化目标、裁剪条件和理论梯度上，与原始的GSPO是**数值等价的**。这意味着GSPO-token在享受为每个词元定制优势的灵活性的同时，依然保留了GSPO核心的稳定性和优化特性。

## **第五部分：实验与讨论 (Experiments and Discussion)**

理论的优雅最终需要实践的检验。这一部分用翔实的实验数据，全方位展示了GSPO的优越性。

### **5.1 实验结果**

实验在一个从Qwen3-30B-A3B-Base微调的冷启动模型上进行，并在多个高难度基准（AIME'24数学竞赛、LiveCodeBench编程、CodeForces编程Elo评分）上评估模型性能。

![](https://pic4.zhimg.com/v2-80e551c12c451ef4bfd0a71dcb53c14d_1440w.jpg)

-   **图1**的训练曲线清晰地显示，GSPO的训练过程**全程稳定**。随着训练计算量的增加，其性能持续提升。
-   在同等的训练资源和查询消耗下，GSPO比精心调优的GRPO展现出**更高的训练效率**，取得了更好的训练奖励和基准测试性能。
-   最重要的一点是，GRPO在训练MoE模型时，必须依赖一个名为“**路由重放（Routing Replay）**”的特殊策略才能正常收敛，而GSPO则**完全不需要**这个额外的复杂机制。

### **5.2 关于裁剪分数的奇特观察**

**图2**揭示了一个极为反直觉但极具说服力的现象。在训练中，GSPO裁剪掉的词元比例高达**0.15**，而GRPO仅为**0.0013**，两者相差超过两个数量级！

![](https://picx.zhimg.com/v2-2e24875934872c5806746b075143696b_1440w.jpg)

按照传统认知，裁剪掉更多的样本意味着用于训练的有效数据更少，效率应该更低。但GSPO恰恰相反，它在“抛弃”了远多于GRPO的更新机会后，反而取得了更高的训练效率。这一发现强有力地佐证了第三部分的核心论断：GRPO的词元级别梯度估计是**内在充满噪声且低效的**。大量的更新其实是在噪声的指导下进行的“无用功”甚至“反向功”。相比之下，GSPO的序列级别方法虽然看起来“更挑剔”（裁剪比例高），但它提供的学习信号**更可靠、更有效**。

-   **GRPO 的策略**：像一个新手投资者，分析了 1000 个项目，觉得每个都沾点边，于是给每个都投了点钱。结果是，大部分投资的微小收益被少数的巨大亏损所抵消，整体回报率很低。它利用了所有数据，但这些数据充满了噪声。
-   **GSPO 的策略**：像一个经验丰富的投资大师，同样分析了 1000 个项目，但它有极其严格的筛选标准（序列级裁剪）。最终，它只挑选了 50 个最优质的项目进行重仓。虽然它「浪费」了研究另外 950 个项目的时间，但这 50 笔高质量的投资带来了惊人的回报。

GSPO 的成功告诉我们：**在强化学习中，学习信号的质量远比数量重要。** 通过严格的序列级筛选，GSPO 确保了每一次模型更新都来自于「高信噪比」的优质样本，因此学习过程更高效、方向更正确。

### **5.3 GSPO对MoE训练的益处**

这一节详细解释了为何GSPO是MoE模型RL训练的“特效药”。

-   **背景问题**：MoE模型在训练时具有稀疏激活的特性。使用GRPO进行RL训练时，会出现一个致命问题——“**专家激活不稳定性**”。具体来说，对于同一个输入，经过一轮梯度更新后，新策略$π_θ$激活的专家集合，与旧策略$π_{θ_{old}}$激活的专家集合可能有显著不同（论文提到，在48层的Qwen3模型上，差异可达**10%**）。这导致词元级别的重要性权重$w_{i,t}(θ)$剧烈波动，因为计算它的分子和分母可能来自完全不同的子网络，从而破坏了训练的收敛性。
-   **过去的解决方案**：为了应对这一挑战，团队之前采用了“路由重放（Routing Replay）”策略。即在计算$w_{i,t}(θ)$时，强制新策略$π_θ$使用旧策略$π_{θ_{old}}$缓存的专家路由路径。这相当于给模型戴上了“镣铐”，虽然保证了稳定性，但也增加了额外的内存和通信开销，并可能限制了MoE模型的全部潜力。
-   **GSPO的根本性解决**：GSPO的优势在于它只关心**整个序列的似然度$π_θ(y_i|x)$**。由于MoE模型始终保持其作为语言模型的基本能力，整个序列的似然度不会因为专家路由的微小变化而剧烈波动。因此，GSPO从根本上对专家激活的不稳定性“免疫”，不再需要任何复杂的变通方法。如图1所示，没有Routing Replay的GSPO依然稳定收敛。这不仅简化和稳定了训练过程，还让MoE模型能够不受束缚地发挥其全部容量。

混合专家（MoE）模型是当前大模型发展的前沿方向，它通过稀疏激活部分网络来节省计算，但这也给 RL 训练带来了独特的稳定性挑战。
-   **MoE 的困境**：在 MoE 模型中，参数更新后，对于同一个输入，被激活的「专家」网络可能会发生变化。
-   **GRPO**：这对于 GRPO 是致命的。它的词元级权重 $\frac{\pi_{\text{新}}(y_t|…)}{\pi_{\text{旧}}(y_t|…)}$ 的分子和分母可能是由完全不同的子网络计算出来的，比较它们毫无意义，导致权重剧烈波动。为了解决这个问题，研究者们不得不设计复杂的「路由回放（Routing Replay）」策略，增加了系统的复杂度和开销。
-   **GSPO**：GSPO 则完全没有这个烦恼。因为它关注的是宏观的、整个序列的概率，这个指标对于底层具体的专家组合变化不那么敏感。就像一家大公司的 CEO，他关心的是公司本季度的总体财报（序列概率），而不是某个具体部门的人员微调（专家路由变化）。
实验证明，GSPO 无需任何额外技巧，就能稳定地训练 MoE 模型，大大简化了训练流程。



### **5.4 GSPO对RL基础设施的益处**

最后，论文还指出了GSPO带来的一个实际工程上的巨大好处。在实践中，用于训练的引擎（如[Megatron-LM](https://zhida.zhihu.com/search?content_id=260952584&content_type=Article&match_order=1&q=Megatron-LM&zhida_source=entity)）和用于推理采样的引擎（如SGLang, [vLLM](https://zhida.zhihu.com/search?content_id=260952584&content_type=Article&match_order=1&q=vLLM&zhida_source=entity)）在计算精度上存在差异。过去使用GRPO时，为了保证词元级别似然度的精确，通常需要在训练引擎中重新计算一遍由推理引擎采样出的回复的似然度（π_θ_old），这是一个非常耗时的步骤。

而GSPO只依赖于序列级别的似然度，这个宏观指标对微小的精度差异**容忍度更高**。因此，GSPO使得直接使用推理引擎返回的似然度进行优化成为可能，从而**避免了代价高昂的重计算步骤**。这在部分 rollout 或多轮 RL 等场景下，以及在训练-推理分离的框架中，能带来显著的效率提升。



### 1.3 PPO 的局限

PPO 有一个巨大的实践难题：它需要一个额外的**价值模型（Value Model）**，或者叫 Critic 模型。注意，不要把价值模型和奖励模型弄混淆。价值模型不是提供额外的奖励来源，而是通过学习预测未来的期望回报，提供了一个动态的基准，用来校准 RM 提供的原始奖励信号，生成更稳定、信息量更大的 Advantage 信号，从而稳定并加速 PPO 的训练。

可以参考之前写的这篇文章：

目前常见的做法，**价值模型会给每一个 token** 打分，而且它和策略模型本身一样大，训练它既耗费资源又困难，而且估算往往不准，成为整个系统中最脆弱的一环。

### 2.2 GRPO 的局限

Qwen 团队指出，GRPO 在应用 PPO 的核心机制——**重要性采样时，犯了一个错误：GRPO 在 token 级别计算重要性权重，而不是在整个序列级别。**

在数学上，重要性采样理论要求我们对从一个分布中采出的**多个样本**求平均，才能准确修正分布的偏差。而 GRPO 在每个时间步，只基于**一个**采样出的词元 $y_t$ 来计算权重，这个权重充满了随机噪声，失去了修正分布的意义。

这种噪声会随着回答的变长而不断累积，最终像滚雪球一样，引发灾难性的**模型崩溃**。尤其是在训练深度更深、结构更复杂的**混合专家（MoE）模型**时，这种不稳定性会被急剧放大。