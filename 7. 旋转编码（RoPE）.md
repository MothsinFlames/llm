---
source: "https://zhuanlan.zhihu.com/p/647109286"
---
# 1. 概念
**核心概念与必要性：**
1.  **问题定义：**
   *   大语言模型（LLM）的核心架构（如Transformer）本质上是**置换等变**的。这意味着如果打乱输入序列中单词的顺序，模型内部所有单词对应的中间表示（向量）也会被打乱相同的顺序，但表示本身的内容不会因位置不同而改变。
   *   **核心矛盾：** 自然语言具有**强顺序依赖性**！单词的位置（开头、中间、结尾、相邻、距离）携带了极其重要的语义和语法信息（例如：“猫抓老鼠” vs. “老鼠抓猫”）。
2.  **目标：**
   *   位置编码（Positional Encoding, PE）或位置嵌入（Positional Embedding）的目标是**将序列中每个元素（Token）的绝对位置和/或相对位置信息，显式地注入到模型的输入或内部表示中**，从而打破Transformer的置换等变性，使其能够理解和利用序列的顺序信息。
3.  **为什么需要（Transformer的缺陷）：**
   *   Transformer的Self-Attention机制计算一个词与其他所有词的关联度（注意力权重），但计算过程本身**完全不考虑词与词之间的物理距离或绝对位置**。`Query` 向量 `q_i` 和 `Key` 向量 `k_j` 的点积 `q_i^T * k_j` 只依赖于词 `i` 和词 `j` 的内容向量，与 `i` 和 `j` 的位置无关。如果没有位置信息，“I think therefore I am” 中的两个 “I” 在模型看来内容是完全相同的，无法区分主语和宾语。
**主流位置编码方案详解：**
主要分为两大类：**绝对位置编码**和**相对位置编码**。近年来，**旋转位置编码（RoPE）** 因其卓越性能成为主流LLM的事实标准。
1.  **绝对位置编码 (Absolute Positional Encoding):**
   *   **核心思想：** 为序列中的每个绝对位置（1, 2, 3, ..., `seq_len`）分配一个独特的、固定的向量表示。
   *   **代表方案：Sinusoidal 位置编码 (原始Transformer使用)**
       *   **公式：**
           *   `PE(pos, 2i) = sin(pos / 10000^(2i/d_model))`
           *   `PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))`
           *   其中：
               *   `pos`：词在序列中的绝对位置（索引，从0或1开始）。
               *   `i`：维度索引（`0 <= i < d_model/2`）。
               *   `d_model`：模型隐藏层的维度（即词嵌入向量的长度）。
               *   `10000`：一个经验常数，控制波长范围。
       *   **理解与原理：**
           *   **正弦/余弦交替：** 每个位置 `pos` 的编码向量是一个 `d_model` 维向量。在维度 `2i` 上使用正弦函数，在维度 `2i+1` 上使用余弦函数。
           *   **频率递减：** 随着维度索引 `i` 增大，除数 `10000^(2i/d_model)` 指数级增大，导致 `pos / 10000^(2i/d_model)` 值变小，频率降低（波长变长）。低维（小 `i`）对应高频（短波长），编码细微的位置变化；高维（大 `i`）对应低频（长波长），编码粗糙的、大范围的位置信息。这类似于傅里叶变换。
           *   **线性关系：** `PE(pos+k)` 可以表示为 `PE(pos)` 的**线性变换**（虽然严格证明需要点技巧，但其结构使得位置偏移 `k` 的效果可以通过一个仅依赖于 `k` 和 `i` 的旋转矩阵作用于 `PE(pos)` 来实现）。这是Sinusoidal PE最关键的数学性质之一，使得模型能学到相对位置信息。
           *   **有界性 & 唯一性：** Sin/Cos 函数值域为 `[-1, 1]`，保证了编码值有界。不同 `pos` 理论上具有不同的编码（尤其在 `d_model` 足够大时），满足了区分位置的需求。
           *   **外推性 (Extrapolation)：** 理论上，该公式可以计算任意大的 `pos`（即使 `pos` 超出了训练时见过的最大序列长度）。然而，实际中外推效果往往不佳（高频维度在 `pos` 很大时变化非常缓慢，导致不同位置编码过于相似）。
       *   **加入模型：**
           *   直接将位置编码向量 `PE(pos)` 与对应位置的词嵌入向量 `x_pos` 相加： `input_pos = x_pos + PE(pos)`。
           *   然后 `input_pos` 作为Transformer编码器/解码器的输入。
       *   **优点：**
           *   简单直观，计算高效（无需学习，预计算即可）。
           *   理论上具有外推能力（尽管实际有限）。
           *   线性性质便于模型学习相对位置。
       *   **缺点：**
           *   **外推能力实际较弱：** 模型在训练时只见过一定长度内的 `PE(pos)`，当推理时遇到远长于训练长度的序列，`PE(pos)` 的值会进入训练时未充分学习的“区域”（特别是高频维度饱和），导致模型无法有效处理长距离依赖，性能显著下降。这是Sinusoidal PE的最大痛点。
           *   **固定模式：** 编码是固定的，无法根据具体任务或数据自适应调整。
   *   **可学习的绝对位置编码 (Learned Absolute PE)：**
       *   为每个位置 `pos` (0 到 `max_len`) 随机初始化一个 `d_model` 维向量 `P[pos]`，作为模型参数一起训练。
       *   **优点：** 理论上可以学习到最优的位置表示，可能在某些任务上优于Sinusoidal。
       *   **缺点：**
           *   **外推能力极差：** 只能表示训练中见过的位置 (`0` 到 `max_len`)。遇到 `pos > max_len` 时无法处理（通常需要回退到Sinusoidal或截断）。
           *   **增加模型参数：** 需要存储 `max_len * d_model` 个额外参数。
           *   **可能过拟合：** 尤其当 `max_len` 很大而训练数据不足时。
       *   **应用：** 早期一些模型（如BERT）使用，但在处理长序列或需要外推的场景下不如Sinusoidal或RoPE。
2.  **相对位置编码 (Relative Positional Encoding):**
   *   **核心思想：** 不关注词在序列中的绝对位置 `i` 和 `j`，而是关注词对 `(i, j)` 之间的**相对距离** `i - j`（或 `j - i`）。将这种相对距离信息直接融入到Self-Attention的计算过程中。
   *   **动机：**
       *   在很多语言任务中，相对位置（如相邻、附近、远处）比绝对位置更重要。
       *   天然具有更好的外推性：模型学习的是相对距离 `k` 的表示，只要 `k` 在训练范围内，无论 `i` 和 `j` 的绝对位置在哪（即使 `i` 和 `j` 都很大），其相对位置表示是一致的。
   *   **代表方案 (多种变体)：** T5、DeBERTa、Transformer-XL等使用了不同的相对位置编码方法。核心通常在于修改Attention Score的计算：
       *   **原始Attention Score:** `a_{ij} = (q_i^T * k_j) / sqrt(d_k)`
       *   **加入相对位置：** `a_{ij} = (q_i^T * k_j + q_i^T * r_{i-j} + u^T * k_j + v^T * r_{i-j}) / sqrt(d_k)` 或类似形式。
           *   `r_{i-j}`：一个表示相对距离 `k = i - j` 的可学习向量（通常有最大距离 `clip`，如 `|k| <= clip`）。
           *   `u`, `v`：可学习的全局偏置向量。
       *   具体公式变体较多，但核心都是将相对位置信息 `r_{k}` 通过可学习的参数整合到 `a_{ij}` 的计算中。
   *   **优点：**
       *   **更强的外推性：** 模型学习的是相对距离 `k` 的表示，只要推理时遇到的相对距离 `k` 在训练时覆盖的范围内（`-clip` 到 `+clip`），就能有效工作。即使序列总长远超训练长度，只要词对距离不超过 `clip`，就能正确处理。
       *   **更符合直觉：** 直接建模词与词之间的相对关系。
   *   **缺点：**
       *   **实现更复杂：** 需要修改Attention计算过程，不能简单加在输入上。增加了计算和实现的复杂度。
       *   **最大距离限制 (`clip`)：** 对于超出 `clip` 的相对距离，效果会下降（通常 `clip` 会设置得足够大，如512或1024）。
       *   **可能引入额外参数：** 需要存储 `(2 * clip + 1)` 个 `d_model` 或 `d_k` 维的 `r_k` 向量。
3.  **旋转位置编码 (Rotary Position Embedding - RoPE):**
   *   **核心思想 (精髓)：** 一种**绝对位置编码**，但通过**在复数空间对词嵌入向量进行旋转**来实现。其关键特性是：**词向量的点积结果（即注意力分数）只依赖于词的内容和它们的相对位置 `m-n`**。完美结合了绝对位置的简洁性和相对位置的外推优势。**当前LLM的绝对主流选择（LlaMA, GPT-NeoX, PaLM, GLM, ChatGLM, Baichuan等）。**
   *   **数学原理：**
       *   将词嵌入向量 `x` 的每个维度对 `(x_{2i}, x_{2i+1})` 视为一个复数 `x_{2i} + j * x_{2i+1}`。
       *   定义一个基于位置 `m` 的**旋转矩阵** `R_m`： `R_m = [ [cos(mθ_i), -sin(mθ_i)], [sin(mθ_i), cos(mθ_i)] ]`，其中 `θ_i = 1 / 10000^{2i/d}` (与Sinusoidal的波长控制类似)。
       *   **应用旋转：** 对于位置 `m` 处的词嵌入 `x_m`，其旋转后的向量表示为 `x_m' = R_m * x_m`（这里 `x_m` 被看作由 `d/2` 个复数组成的向量）。具体到实值操作：
           *   `x_m'[2i] = x_m[2i] * cos(mθ_i) - x_m[2i+1] * sin(mθ_i)`
           *   `x_m'[2i+1] = x_m[2i] * sin(mθ_i) + x_m[2i+1] * cos(mθ_i)`
       *   **计算Query和Key：** 使用旋转后的向量计算 `q_m = f_q(x_m') = R_m * f_q(x_m)` 和 `k_n = f_k(x_n') = R_n * f_k(x_n)`，其中 `f_q`, `f_k` 是线性投影层。
       *   **Attention Score 的魔力：** `q_m^T * k_n = (R_m * f_q(x_m))^T * (R_n * f_k(x_n)) = f_q(x_m)^T * R_m^T * R_n * f_k(x_n)`
           *   因为 `R_m` 是正交矩阵（旋转矩阵性质），`R_m^T = R_{-m}`。
           *   所以 `q_m^T * k_n = f_q(x_m)^T * R_{n-m} * f_k(x_n)`。
       *   **关键结论：** 注意力分数 `a_{m,n} = q_m^T * k_n` **仅依赖于原始内容向量 `f_q(x_m)`, `f_k(x_n)` 和它们的相对位置 `(m - n)`！** 绝对位置 `m` 和 `n` 通过旋转操作被巧妙地转换成了相对位置 `m-n` 的表示 `R_{m-n}`。
   *   **优点：**
       *   **外推性极佳：** 核心机制是相对位置编码 `R_{m-n}`，只要相对距离 `|m-n|` 在训练时覆盖的范围内（由 `θ_i` 决定），就能有效工作。实践表明其外推能力显著优于Sinusoidal和可学习绝对PE。
       *   **保持点积性质：** 旋转是线性变换且保持向量模长和相对角度（点积），有利于Attention计算的稳定性。
       *   **实现相对简单：** 不需要修改Attention计算结构（如相对位置编码那样），只需在计算 `q` 和 `k` **之前**对它们的输入（或投影后的 `q`, `k`）应用旋转操作。可以高效地融合到现有Attention代码中。
       *   **长程衰减：** 由于 `θ_i` 随 `i` 增大而减小，`R_{k}` 在高维上的旋转角度变化变慢，使得远距离词对的点积有天然衰减趋势，这与自然语言中远距离依赖通常较弱的现象相符。
       *   **兼容高效Attention：** 可以很好地与FlashAttention等优化技术结合。
   *   **缺点：**
       *   理论基础相对复杂（涉及复数、线性代数）。
       *   对于极长的序列（远超训练长度），如果 `θ_i` 设置不当导致某些维度的旋转频率过低，也可能出现外推问题（但通过改进如NTK-aware缩放可以缓解）。
   *   **改进：**
       *   **NTK-aware Scaled RoPE：** 针对外推，动态调整 `θ_i` 的基（如 `10000 -> 10000 * (scale_factor)^{2/d}`），让高频维度在长序列下也能保持区分度，显著提升外推能力。
       *   **动态NTK：** 根据推理时输入序列的实际长度动态计算缩放因子 `scale_factor`。
**位置编码的演进趋势总结：**
1.  **目标：** 从表示绝对位置 -> 更强调表示相对位置（因其更关键且外推性好）。
2.  **方法：** 从简单加性固定编码（Sinusoidal）-> 可学习绝对编码 -> 修改Attention计算的相对位置编码 -> 通过数学变换（旋转）在绝对编码中内蕴相对位置信息（RoPE）。
3.  **核心驱动力：** 解决**外推（Extrapolation）** 问题（处理长于训练序列的输入）是近年位置编码研究的核心焦点。RoPE及其改进（NTK-aware）是目前最优解。
4.  **实践：** RoPE 是当前绝大多数领先开源和闭源大模型（>7B）的标准配置。外推改进（如NTK-aware）在需要超长上下文（>32K）的模型中广泛应用。
**可能被深挖的点 & 如何回答：**
1.  **为什么Transformer需要位置编码？**
   *   **核心：** 强调Self-Attention的置换等变性与语言顺序依赖性的根本矛盾。举例说明位置信息的重要性（语序改变语义）。
2.  **详细解释Sinusoidal位置编码的原理和公式。为什么用Sin和Cos交替？为什么用10000？**
   *   清晰写出公式。
   *   **Sin/Cos交替：** 保证每个位置编码是唯一的（线性无关性），并且能通过线性变换表示 `PE(pos+k)`。
   *   **10000：** 经验值，控制频率范围。太大导致所有维度频率都很低（编码变化慢），太小导致所有维度频率都很高（编码变化剧烈）。需要平衡，使得在不同维度上有不同频率（波长）的编码，覆盖从精细到粗略的位置变化。
3.  **Sinusoidal编码有什么优缺点？最大的问题是什么？**
   *   **优点：** 简单、固定、无需学习、有一定外推理论可能、线性性质利于学相对位置。
   *   **缺点（重点）：** **实际外推能力差**。解释原因：训练时模型只见过 `[0, max_train_len]` 的 `PE(pos)`，推理时遇到 `pos >> max_train_len`，`PE(pos)` 的值（尤其高频维度）会进入训练时未充分学习的区域（接近饱和），导致不同位置的编码过于相似，模型无法区分长距离位置。
4.  **什么是相对位置编码？它相比绝对位置编码有什么优势？**
   *   **定义：** 编码词对 `(i, j)` 之间的相对距离 `i - j`。
   *   **优势 (重点)：** **更强的外推性**。解释：模型学习的是相对距离 `k` 的表示 `r_k`。只要推理时遇到的词对距离 `k` 在训练时覆盖的 `[-clip, +clip]` 范围内，无论 `i` 和 `j` 的绝对位置多大（即使序列总长很长），`r_k` 都是已知且有效的。绝对位置编码则受限于 `pos` 超出训练范围。
5.  **请深入解释旋转位置编码（RoPE）的原理。为什么说它结合了绝对和相对位置的优点？**
   *   **核心：** 在复数空间旋转词向量（或投影后的 `q`, `k`）。
   *   **公式推导：** 清晰展示 `q_m^T * k_n` 如何化简为仅依赖内容向量和相对位置 `R_{m-n}` 的形式。**这是关键！**
   *   **结合优点：**
       *   **形式上像绝对编码：** 对每个位置的 `q`, `k` 应用一个仅依赖于其自身位置 `m` 或 `n` 的旋转 `R_m`, `R_n`。
       *   **效果上是相对编码：** 最终的注意力分数 `a_{m,n}` **只**依赖于词的内容和它们的相对位置 `m-n`（体现在 `R_{m-n}` 上）。因此具有**绝对位置的实现简洁性**和**相对位置的外推优势**。
6.  **RoPE的外推为什么比Sinusoidal好？它完美吗？**
   *   **好：** RoPE的注意力分数依赖 `R_{m-n}`，本质是相对距离编码。只要相对距离 `|m-n|` 对应的旋转角度 `θ_i * |m-n|` 没有让所有维度的cos/sin值饱和（即角度没有大到使函数值几乎不变），模型就能区分。Sinusoidal是绝对位置饱和。
   *   **不完美：** 对于**极端长距离**（`|m-n|` 极大），如果某些维度 `i` 的 `θ_i` 非常小（低频维度），那么 `θ_i * |m-n|` 仍然可能很小（变化不大），导致这些维度对区分远距离词对贡献很小，主要依赖高频维度。如果高频维度的 `θ_i` 不够大，也可能导致区分度下降。这就是 **NTK-aware改进**的动机。
7.  **解释NTK-aware Scaled RoPE是如何改进外推的。**
   *   **问题：** 原始RoPE在超长序列下，低频维度 `θ_i` 小，旋转慢，区分度差；高频维度 `θ_i` 大，旋转快，但可能在长距离下角度过大导致点积不稳定或信息损失。
   *   **NTK理论启发：** 神经正切核理论表明，网络倾向于先学习低频分量。
   *   **解决方案：** 不改变低频维度的 `θ_i`（保留已学习的特征），而是**动态增大高频维度的旋转频率**（相当于拉伸高频维度对应的位置索引）。
   *   **实现：** 将 `θ_i` 公式中的基 `10000` 替换为 `10000 * (scale_factor)^{2/d}`，其中 `scale_factor` 通常与推理序列长度 `L` 相对于训练长度 `L_train` 的比值有关（如 `scale_factor = L / L_train` 或更复杂的公式）。这样高频维度的 `θ_i` 变大，旋转更快，在长序列下也能保持足够的区分度。
8.  **位置编码是加在输入上还是Attention里？RoPE加在哪里？**
   *   Sinusoidal/可学习绝对PE：通常**加在输入词嵌入上** (`x + PE`)。
   *   相对位置编码：**修改Attention Score计算**（加入 `r_{i-j}` 等项）。
   *   **RoPE：** **应用在计算 `q` 和 `k` 的过程中**。具体是在对投影前的输入向量 `x` 或投影后的 `q`, `k` 向量应用旋转操作。**不是加在输入 `x` 上！** 最常见的是对投影后的 `q` 和 `k` 向量应用旋转。
9.  **在实现LLM时，如何选择位置编码方案？**
   *   **当前绝对推荐RoPE。** 理由：优异的性能、出色的外推能力（配合NTK-aware）、广泛的实际应用验证（LlaMA系列等）、实现高效。
   *   如果资源极其有限或序列长度固定且不长，Sinusoidal或可学习绝对PE也可考虑，但外推是硬伤。
   *   相对位置编码（如T5式）在RoPE之前是主流，现在也被RoPE取代了。
10. **除了文本，位置编码在其他领域（如CV、多模态）的应用？**
    *   **核心思想通用：** 任何需要处理序列或有序集合（如时间序列、像素序列、图节点序列）的地方。
    *   **CV：** ViT（Vision Transformer）处理图像块序列时，必须加入位置编码（通常是可学习的1D或2D绝对PE）。RoPE也有应用于视频（时间+空间序列）。
    *   **多模态：** 对齐文本Token序列、图像块序列、音频帧序列时，各自的位置编码至关重要。
    *   **图神经网络：** 对节点序列或消息传递中的顺序进行编码。
**表达技巧：**
*   **结构化清晰：** 按“问题定义->为什么需要->解决方案分类（绝对/相对）->代表方案详解->演进趋势->问题”的逻辑展开。
*   **突出关键：** 反复强调**外推（Extrapolation）** 问题是核心挑战，以及RoPE如何优雅地解决它。
*   **数学推导：** 对于RoPE的推导（`q_m^T * k_n` -> `f_q(x_m)^T * R_{m-n} * f_k(x_n)`）务必熟练。这是体现你深度的关键点。
*   **联系实践：** 提及主流模型（LlaMA, GPT-NeoX, ChatGLM, Baichuan）都使用RoPE，以及NTK-aware等改进。
*   **主动延伸：** 在回答完基础问题后，可以主动提一句：“关于外推的改进，除了NTK-aware，还有一些工作像ALiBi（Attention with Linear Biases）也是一种有效的相对位置编码方法，它通过给注意力分数加一个与相对距离成比例的负偏置来实现...”。展示你的知识广度（但确保你能讲清楚ALiBi的核心思想 `a_{ij} = q_i^T k_j - λ * |i-j|`）。
*   **坦诚未知：** 如果被问到非常前沿或细节的问题（如某种特定RoPE变体的精确性能比较），可以坦诚说了解不深，但基于基本原理推测...，展现思考过程。
# 2. 计算
旋转位置编码（Rotary Position Embedding，RoPE）是论文 [Roformer: Enhanced Transformer With Rotray Position Embedding](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2104.09864.pdf) 提出的一种能够将相对位置信息依赖集成到 self-attention 中并提升 [transformer 架构](https://zhida.zhihu.com/search?content_id=231932826&content_type=Article&match_order=1&q=+transformer+%E6%9E%B6%E6%9E%84&zhida_source=entity) 性能的位置编码方式。而目前很火的 LLaMA、GLM 模型也是采用该位置编码方式。
和相对位置编码相比，RoPE 具有更好的 **外推性** ，目前是大模型相对位置编码中应用最广的方式之一。
**备注：什么是大模型外推性？**
外推性是指大模型在训练时和预测时的输入长度不一致，导致模型的泛化能力下降的问题。例如，如果一个模型在训练时只使用了512个 token 的文本，那么在预测时如果输入超过512个 token，模型可能无法正确处理。这就限制了大模型在处理长文本或多轮对话等任务时的效果。
## 1\. 旋转编码 RoPE
### 1.1 基本概念
在介绍 RoPE 之前，先给出一些符号定义，以及基本背景。
首先定义一个长度为 $N$ 的输入序列为：
$$\mathbb{S}_{N}={ w_i }_{i=1}^{N} \tag1$$
其中 $w_i$ 表示输入序列中第 $i$ 个 token，而输入序列 $\mathbb{S}_N$ 对应的 embedding 表示为：
$$\mathbb{E}_{N}={ \boldsymbol{x}_i }_{i=1}^N\tag2$$
其中 $\boldsymbol{x}_i$ 表示第 $i$ 个 token $w_i$ 对应的 $d$ 维词嵌入向量。
接着在做 self-attention 之前，会用词嵌入向量计算 $\boldsymbol{q,k,v}$ 向量同时加入位置信息，函数公式表达如下：
$$\boldsymbol{q}_m=f_q(\boldsymbol{x}_m,m)$$
$$\boldsymbol{k}_n=f_k(\boldsymbol{x}_n,n)$$
$$\boldsymbol{v}_n=f_v(\boldsymbol{x}_n,n)$$
其中 $\boldsymbol{q}_m$ 表示第 $m$ 个 token 对应的词向量 $\boldsymbol{x}_m$ 集成位置信息 $m$ 之后的 query 向量。而 $\boldsymbol{k}_n$ 和 $\boldsymbol{v}_n$ 则表示第 $n$ 个 token 对应的词向量 $\boldsymbol{x}_n$ 集成位置信息 $n$ 之后的 key 和 value 向量。
而基于 transformer 的位置编码方法都是着重于构造一个合适的 $f\left( \boldsymbol{q},\boldsymbol{k},\boldsymbol{v} \right)$ 函数形式。
而计算第 $m$ 个词嵌入向量 $\boldsymbol{x}_m$ 对应的 self-attention 输出结果，就是 $\boldsymbol{q}_m$ 和其他 $\boldsymbol{k}_n$ 都计算一个 attention score ，然后再将 attention score 乘以对应的 $\boldsymbol{v}_n$ 再求和得到输出向量 $\boldsymbol{o}_m$ ：
$$a_{m,n}=\frac{\text{exp}(\frac{\boldsymbol{q}_m^{\textbf{T}}\boldsymbol{k}_n}{\sqrt{d}})}{\sum_{j=1}^N\text{exp}(\frac{\boldsymbol{q}_m^{\textbf{T}}\boldsymbol{k}_j}{\sqrt{d}})}$$
$$\boldsymbol{o}_m=\sum_{n=1}^Na_{m,n}\boldsymbol{v}_n \tag4$$
### 1.2 绝对位置编码
对于位置编码，常规的做法是在计算 query, key 和 value 向量之前，会计算一个位置编码向量 $pi\boldsymbol{p}_i\boldsymbol{p}_i$ 加到词嵌入 $xi\boldsymbol{x}_i$ 上，位置编码向量 $pi\boldsymbol{p}_i$ 同样也是 $dd$ 维向量，然后再乘以对应的变换矩阵 $\boldsymbol{W}$ ：
$$f_{t:t\in{q,k,v}}(\boldsymbol{x}_i,i):=\boldsymbol{W}_{t:t\in{q,k,v}}(\boldsymbol{x}_i+\boldsymbol{p}_i) \tag5$$
而经典的位置编码向量 $\boldsymbol{p}_i\boldsymbol{p}_i$ 的计算方式是使用 Sinusoidal 函数：
$$\boldsymbol{p}_{i,2t}=\text{sin}\left( k/10000^{2t/d} \right)$$
$$\boldsymbol{p}_{i,2t+1}=\text{cos}\left( k/10000^{2t/d} \right)\tag6$$
其中 $\boldsymbol{p}_{i,2t}$ 表示位置 $d$ 维度向量 $\boldsymbol{p}_i$ 中的第 $2t$ 位置分量也就是偶数索引位置的计算公式，而 $\boldsymbol{p}_{i,2t+1}$ 就对应第 $2t+1$ 位置分量也就是奇数索引位置的计算公式。
### 1.3 2维旋转位置编码
论文中提出为了能利用上 token 之间的相对位置信息，假定 query 向量 $\boldsymbol{q}_m\boldsymbol{q}_m$ 和 key 向量 $\boldsymbol{k}_n$ 之间的内积操作可以被一个函数 $g$ 表示，该函数 $g$ 的输入是词嵌入向量 $\boldsymbol{x}_m$ ， $\boldsymbol{x}_n$ 和它们之间的相对位置 $m-n$ ：
$$\left<\boldsymbol{f}_q(\boldsymbol{x}_m,m),f_k(\boldsymbol{x}_n,n)\right>=g(\boldsymbol{x}_m,\boldsymbol{x}_n,m-n) \tag7$$
接下来的目标就是找到一个等价的位置编码方式，从而使得上述关系成立。
假定现在词嵌入向量的维度是两维 $d=2$ ，这样就可以利用上2维度平面上的向量的几何性质，然后论文中提出了一个满足上述关系的 $f$ 和 $g$ 的形式如下：
$$\begin{aligned}
f_q(\boldsymbol{x}_m,m)=\left(\boldsymbol{W}_q\boldsymbol{x}_m\right)e^{im\theta} \\ f_k(\boldsymbol{x}_n,n)=(\boldsymbol{W}_k\boldsymbol{x}_n)e^{in\theta} \\ g(\boldsymbol{x}_m,\boldsymbol{x}_n,m-n)=\text{Re}\left[(\boldsymbol{W}_q\boldsymbol{x}_m)(\boldsymbol{W}_k\boldsymbol{x}_n)^{*}e^{i(m-n)\theta}\right]
\end{aligned} \tag8$$
这里面 Re 表示复数的实部。
进一步地， $f_qf_q$ 可以表示成下面的式子：
$$\begin{align} f_q\left( \boldsymbol{x}_m,m \right)  &= \begin{pmatrix}  \cos m\theta & -\sin m\theta) \\  \sin m \theta &  \cos m \theta \end{pmatrix}   \begin{pmatrix}  W^{(1,1)}_{q} & W^{(1,2)}_{q}  \\  W^{(2,1)}_{q}  &  W^{(2,2)}_{q} \end{pmatrix} \begin{pmatrix}  x_m^{(1)}  \\  x_m^{(2)}    \end{pmatrix} \\ &= \begin{pmatrix}  \cos m\theta & -\sin m\theta) \\  \sin m \theta &  \cos m \theta \end{pmatrix}\begin{pmatrix}  q_m^{(1)}  \\  q_m^{(2)}    \end{pmatrix}  \end{align}\tag9$$
看到这里会发现，这不就是 query 向量乘以了一个旋转矩阵吗？这就是为什么叫做旋转位置编码的原因。
同理， $f_kf_k$ 可以表示成下面的式子：
$$\begin{align} f_k\left( \boldsymbol{x}_m,m \right)  &= \begin{pmatrix}  \cos m\theta & -\sin m\theta) \\  \sin m \theta &  \cos m \theta \end{pmatrix}   \begin{pmatrix}  W^{(1,1)}_{k} & W^{(1,2)}_{k}  \\  W^{(2,1)}_{k}  &  W^{(2,2)}_{k} \end{pmatrix} \begin{pmatrix}  x_m^{(1)}  \\  x_m^{(2)}    \end{pmatrix} \\ &= \begin{pmatrix}  \cos m\theta & -\sin m\theta) \\  \sin m \theta &  \cos m \theta \end{pmatrix}\begin{pmatrix}  k_m^{(1)}  \\  k_m^{(2)}    \end{pmatrix}  \end{align}\tag{10}$$
最终 $g(\boldsymbol{x}_m,\boldsymbol{x}_n,m-n)g(\boldsymbol{x}_m,\boldsymbol{x}_n,m-n)$ 可以表示如下：
$$g(\boldsymbol{x}_m,\boldsymbol{x}_n,m-n)  =\begin{pmatrix}  \boldsymbol{q}_m^{(1)} &  \boldsymbol{q}_m^{(2)}  \\ \end{pmatrix}   \begin{pmatrix}  \cos((m-n)\theta) & -\sin((m-n)\theta) \\  \sin((m-n)\theta) &  \cos((m-n)\theta) \end{pmatrix}   \begin{pmatrix}  k_n^{(1)}  \\  k_n^{(2)}    \end{pmatrix} \tag{11}$$
关于上面公式（8）~（11）的具体推导，可以参见文章最后的 **附录** ，或者参考文章： [一文看懂 LLaMA 中的旋转式位置编码（Rotary Position Embedding）](https://zhuanlan.zhihu.com/p/642884818) 。
### 1.4 扩展到多维
将2维推广到任意维度，可以表示如下：
$$f_{ q,k }( \boldsymbol{x}_m,m )=\boldsymbol{R}^d_{\Theta,m}\boldsymbol{W}_{{ q,k }}\boldsymbol{x}_m\tag{12}$$
内积满足线性叠加性，因此任意偶数维的RoPE，我们都可以表示为二维情形的拼接，即
$$\boldsymbol{R}^d_{\Theta,m}=\begin{equation}\scriptsize{\underbrace{\begin{pmatrix} \cos m\theta_0 & -\sin m\theta_0 & 0 & 0 & \cdots & 0 & 0 \\ \sin m\theta_0 & \cos m\theta_0 & 0 & 0 & \cdots & 0 & 0 \\ 0 & 0 & \cos m\theta_1 & -\sin m\theta_1 & \cdots & 0 & 0 \\ 0 & 0 & \sin m\theta_1 & \cos m\theta_1 & \cdots & 0 & 0 \\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & 0 & 0 & \cdots & \cos m\theta_{d/2-1} & -\sin m\theta_{d/2-1} \\ 0 & 0 & 0 & 0 & \cdots & \sin m\theta_{d/2-1} & \cos m\theta_{d/2-1} \\ \end{pmatrix}}_{\boldsymbol{W}_m}}\end{equation}\tag{13}$$
$$\Theta={ \theta_i=10000^{-2(i-1)/d}, i \in [1,2,...,d/2] }$$
将 RoPE 应用到前面公式（4）的 Self-Attention 计算，可以得到 **包含相对位置信息的Self-Attetion** ：
$$\boldsymbol{q}^{\textbf{T}}_m\boldsymbol{k}_n=\left( \boldsymbol{R}^d_{\Theta,m}\boldsymbol{W}_q\boldsymbol{x}_m \right)^{\textbf{T}}\left( \boldsymbol{R}^d_{\Theta,n}\boldsymbol{W}_k\boldsymbol{x}_n \right)=\boldsymbol{x}_m^{\textbf{T}}\boldsymbol{W}_q\boldsymbol{R}^d_{\Theta,n-m}\boldsymbol{W}_k\boldsymbol{x}_n\tag{14}$$
其中， $\boldsymbol{R}^d_{\Theta,n-m}=\left( \boldsymbol{R}^d_{\Theta,m} \right)^{\textbf{T}}\boldsymbol{R}^d_{\Theta,n}$ 。
值得指出的是，由于 $\boldsymbol{R}^d_{\Theta}\boldsymbol{R}^d_{\Theta}$ 是一个正交矩阵，它不会改变向量的模长，因此通常来说它不会改变原模型的稳定性。
### 1.5 RoPE 的高效计算
由于 $\boldsymbol{R}^d_{\Theta,m}\boldsymbol{R}^d_{\Theta,m}$ 的稀疏性，所以直接用矩阵乘法来实现会很浪费算力， **推荐通过下述方式来实现 RoPE** ：
$$\boldsymbol{R}^d_{\Theta,m}\boldsymbol{x}=\begin{equation}\begin{pmatrix}x_0 \\ x_1 \\ x_2 \\ x_3 \\ \vdots \\ x_{d-2} \\ x_{d-1}  \end{pmatrix}\otimes\begin{pmatrix}\cos m\theta_0 \\ \cos m\theta_0 \\ \cos m\theta_1 \\ \cos m\theta_1 \\ \vdots \\ \cos m\theta_{d/2-1} \\ \cos m\theta_{d/2-1} \end{pmatrix} + \begin{pmatrix}-x_1 \\ x_0 \\ -x_3 \\ x_2 \\ \vdots \\ -x_{d-1} \\ x_{d-2}  \end{pmatrix}\otimes\begin{pmatrix}\sin m\theta_0 \\ \sin m\theta_0 \\ \sin m\theta_1 \\ \sin m\theta_1 \\ \vdots \\ \sin m\theta_{d/2-1} \\ \sin m\theta_{d/2-1} \end{pmatrix}\end{equation}\tag{15}$$
其中 $\otimes$ 是逐位对应相乘，即计算框架中的 $*$ 运算。从这个实现也可以看到，RoPE 可以视为是乘性位置编码的变体。
总结来说，RoPE 的 self-attention 操作的流程是：对于 token 序列中的每个词嵌入向量，首先计算其对应的 query 和 key 向量，然后对每个 token 位置都计算对应的旋转位置编码，接着对每个 token 位置的 query 和 key 向量的元素按照 **两两一组** 应用旋转变换，最后再计算 query 和 key 之间的内积得到 self-attention 的计算结果。
论文中有个很直观的图片展示了旋转变换的过程：
![](https://pic3.zhimg.com/v2-6db340ee9d34709e9c25921f5a0c2a0e_1440w.jpg)
### 1.6 远程衰减
可以看到，RoPE 形式上和前面公式（6） [Sinusoidal 位置编码](https://zhida.zhihu.com/search?content_id=231932826&content_type=Article&match_order=1&q=Sinusoidal+%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81&zhida_source=entity) 有点相似，只不过 Sinusoidal 位置编码是加性的，而 RoPE 可以视为乘性的。在 $\theta_i\theta_i$ 的选择上，RoPE 同样沿用了 Sinusoidal 位置编码的方案，即 $\theta_i = 10000^{-2i/d}$ ，它可以带来一定的远程衰减性。
具体证明如下：将 $\boldsymbol{q},\boldsymbol{k}\boldsymbol{q},\boldsymbol{k}$ 两两分组后，它们加上 RoPE 后的内积可以用复数乘法表示为：
$$\begin{equation}  \left( \boldsymbol{R}^d_{\Theta,m}\boldsymbol{W}_q\boldsymbol{x}_m \right)^{\textbf{T}}\left( \boldsymbol{R}^d_{\Theta,n}\boldsymbol{W}_k\boldsymbol{x}_n \right)= \text{Re}\left[\sum_{i=0}^{d/2-1}\boldsymbol{q}_{[2i:2i+1]}\boldsymbol{k}_{[2i:2i+1]}^* e^{\text{i}(m-n)\theta_i}\right]\end{equation}\tag{16}$$
记 $h_i = \boldsymbol{q}_{[2i:2i+1]}\boldsymbol{k}_{[2i:2i+1]}^*, S_j = \sum\limits_{i=0}^{j-1} e^{\text{i}(m-n)\theta_i}h_i = \boldsymbol{q}_{[2i:2i+1]}\boldsymbol{k}_{[2i:2i+1]}^*, S_j = \sum\limits_{i=0}^{j-1} e^{\text{i}(m-n)\theta_i}$ ，并约定 $h_{d/2}=0,S_0=0$ ，那么由 **[Abel变换（分部求和法）](https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%25E5%2588%2586%25E9%2583%25A8%25E6%25B1%2582%25E5%2592%258C%25E6%25B3%2595)** 可以得到：
$$\begin{equation}\sum_{i=0}^{d/2-1}\boldsymbol{q}_{[2i:2i+1]}\boldsymbol{k}_{[2i:2i+1]}^* e^{\text{i}(m-n)\theta_i} = \sum_{i=0}^{d/2-1} h_i (S_{i +1} - S_i)  = \sum_{i=0}^{d/2-1} S_{i+1}(h_{i+1} - h_i)\end{equation}\tag{17}$$
所以
$$\begin{equation}\begin{aligned} \left|\sum_{i=0}^{d/2-1}\boldsymbol{q}_{[2i:2i+1]}\boldsymbol{k}_{[2i:2i+1]}^* e^{\text{i}(m-n)\theta_i}\right| =&\, \left|\sum_{i=0}^{d/2-1} S_{i+1}(h_{i+1} - h_i)\right| \\ \leq&\, \sum_{i=0}^{d/2-1} |S_{i+1}| |h_{i+1} - h_i| \\ \leq&\, \left(\max_i |h_{i+1} - h_i|\right)\sum_{i=0}^{d/2-1} |S_{i+1}|  \end{aligned}\end{equation}\tag{18}$$
因此我们可以考察 $\frac{1}{d/2}\sum\limits_{i=1}^{d/2} |S_i|\frac{1}{d/2}\sum\limits_{i=1}^{d/2} |S_i|$ 随着相对距离的变化情况来作为衰减性的体现：
![](https://pic3.zhimg.com/v2-6376b397b8ea3e8f05d74d433e98a3a4_1440w.jpg)
从图中我们可以看到 **随着相对距离的变大，内积结果有衰减趋势** 的出现。因此，选择 $\theta_i = 10000^{-2i/d}\theta_i = 10000^{-2i/d}$ ，确实能带来一定的远程衰减性。论文中还试过以 $\theta_i = 10000^{-2i/d}$ 为初始化，将 $\theta_i$ 视为可训练参数，然后训练一段时间后发现 $\theta_i$ 并没有显著更新，因此干脆就直接固定 $\theta_i = 10000^{-2i/d}$ 了。
## 2\. RoPE实验
我们看一下 RoPE 在预训练阶段的实验效果：

| Stage | Max seq length | Batch size | Training steps | Loss | Accuracy |
| --- | --- | --- | --- | --- | --- |
| 1 | 512 | 256 | 200k | 1.73 | 65.0% |
| 2 | 1536 | 256 | 12.5k | 1.61 | 66.8% |
| 3 | 256 | 256 | 120k | 1.75 | 64.6% |
| 4 | 128 | 512 | 80k | 1.83 | 63.4% |
| 5 | 1536 | 256 | 10k | 1.58 | 67.4% |
| 6 | 512 | 512 | 30k | 1.66 | 66.2% |
从上面可以看出，增大序列长度，预训练的准确率反而有所提升，这体现了 **RoPE 具有良好的外推能力** 。
下面是在下游任务上的实验结果：

| Model | Validation | Test |
| --- | --- | --- |
| BERT-512 | 64.13% | 67.77% |
| WoBERT-512 | 64.07% | 68.10% |
| [RoFormer](https://zhida.zhihu.com/search?content_id=231932826&content_type=Article&match_order=1&q=RoFormer&zhida_source=entity) -512 | 64.13% | 68.29% |
| RoFormer-1024 | 66.07% | 69.79% |
其中 RoFormer 是一个绝对位置编码替换为 RoPE 的 **[WoBERT](https://link.zhihu.com/?target=https%3A//github.com/ZhuiyiTechnology/WoBERT)** 模型，后面的参数（512）是微调时截断的maxlen，可以看到 RoPE 确实能较好地处理长文本语义。
## 3\. RoPE代码实现
Meta 的 [LLAMA](https://zhida.zhihu.com/search?content_id=231932826&content_type=Article&match_order=1&q=LLAMA&zhida_source=entity) 和 清华的 ChatGLM 都使用了 RoPE 编码，下面看一下具体实现。
### 3.1 在LLAMA中的实现
```
# 生成旋转矩阵
def precompute_freqs_cis(dim: int, seq_len: int, theta: float = 10000.0):
   # 计算词向量元素两两分组之后，每组元素对应的旋转角度\theta_i
   freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
   # 生成 token 序列索引 t = [0, 1,..., seq_len-1]
   t = torch.arange(seq_len, device=freqs.device)
   # freqs.shape = [seq_len, dim // 2]
   freqs = torch.outer(t, freqs).float()  # 计算m * \theta
   # 计算结果是个复数向量
   # 假设 freqs = [x, y]
   # 则 freqs_cis = [cos(x) + sin(x)i, cos(y) + sin(y)i]
   freqs_cis = torch.polar(torch.ones_like(freqs), freqs)
   return freqs_cis
# 旋转位置编码计算
def apply_rotary_emb(
   xq: torch.Tensor,
   xk: torch.Tensor,
   freqs_cis: torch.Tensor,
) -> Tuple[torch.Tensor, torch.Tensor]:
   # xq.shape = [batch_size, seq_len, dim]
   # xq_.shape = [batch_size, seq_len, dim // 2, 2]
   xq_ = xq.float().reshape(*xq.shape[:-1], -1, 2)
   xk_ = xk.float().reshape(*xk.shape[:-1], -1, 2)
   # 转为复数域
   xq_ = torch.view_as_complex(xq_)
   xk_ = torch.view_as_complex(xk_)
   # 应用旋转操作，然后将结果转回实数域
   # xq_out.shape = [batch_size, seq_len, dim]
   xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(2)
   xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(2)
   return xq_out.type_as(xq), xk_out.type_as(xk)
class Attention(nn.Module):
   def __init__(self, args: ModelArgs):
       super().__init__()
       self.wq = Linear(...)
       self.wk = Linear(...)
       self.wv = Linear(...)
       self.freqs_cis = precompute_freqs_cis(dim, max_seq_len * 2)
   def forward(self, x: torch.Tensor):
       bsz, seqlen, _ = x.shape
       xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
       xq = xq.view(batch_size, seq_len, dim)
       xk = xk.view(batch_size, seq_len, dim)
       xv = xv.view(batch_size, seq_len, dim)
       # attention 操作之前，应用旋转位置编码
       xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)
       # scores.shape = (bs, seqlen, seqlen)
       scores = torch.matmul(xq, xk.transpose(1, 2)) / math.sqrt(dim)
       scores = F.softmax(scores.float(), dim=-1)
       output = torch.matmul(scores, xv)  # (batch_size, seq_len, dim)
 # ......
```
这里举一个例子，假设batch\_size=10, seq\_len=3, d=8，则调用函数precompute\_freqs\_cis(d, seq\_len)后，生成结果为：
```
In [239]: freqs_cis
Out[239]:
tensor([[ 1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j],
       [ 0.5403+0.8415j,  0.9950+0.0998j,  0.9999+0.0100j,  1.0000+0.0010j],
       [-0.4161+0.9093j,  0.9801+0.1987j,  0.9998+0.0200j,  1.0000+0.0020j]])
```
以结果中的第二行为例（对应的 m = 1），也就是：
$$\begin{align} cos\left( 1*\theta_0 \right)&=cos\left( 1 \right)=0.5403,&sin\left( 1*\theta_0 \right)=sin\left( 1 \right)=0.8415\\ cos\left( 1*\theta_1\right)&=cos\left( 0.1 \right)=0.9950,&sin\left( 1*\theta_1 \right)=sin\left( 0.1 \right)=0.0998\\ cos\left( 1*\theta_2 \right)&=cos\left( 0.01 \right)=0.9999,&sin\left( 1*\theta_2 \right)=sin\left( 0.01 \right)=0.0100\\ cos\left( 1*\theta_3 \right)&=cos\left( 0.001 \right)=1.0000,&sin\left( 1*\theta_3 \right)=sin\left( 0.001 \right)=0.0010 \end{align}\tag{19}$$
最终按照公式（12）可以得到编码之后的 $\boldsymbol{q},\boldsymbol{k}$ 。
**注意：** 在代码中是直接用freqs\_cis\[0\] \* xq\_\[0\]的结果表示第一个 token 对应的旋转编码（和公式12计算方式有所区别）。其中将原始的 query 向量 $\boldsymbol{q}\boldsymbol{q}$ 转换为了复数形式。
```
In [351]: q_ = q.float().reshape(*q.shape[:-1], -1, 2)
In [352]: q_[0]
Out[352]:
tensor([[[ 1.0247,  0.4782],
        [ 1.5593,  0.2119],
        [ 0.4175,  0.5309],
        [ 0.4858,  0.1850]],
       [[-1.7456,  0.6849],
        [ 0.3844,  1.1492],
        [ 0.1700,  0.2106],
        [ 0.5433,  0.2261]],
       [[-1.1206,  0.6969],
        [ 0.8371, -0.7765],
        [-0.3076,  0.1704],
        [-0.5999, -1.7029]]])
In [353]: xq = torch.view_as_complex(q_)
In [354]: xq[0]
Out[354]:
tensor([[ 1.0247+0.4782j,  1.5593+0.2119j,  0.4175+0.5309j,  0.4858+0.1850j],
       [-1.7456+0.6849j,  0.3844+1.1492j,  0.1700+0.2106j,  0.5433+0.2261j],
       [-1.1206+0.6969j,  0.8371-0.7765j, -0.3076+0.1704j, -0.5999-1.7029j]])
```
**这里为什么可以这样计算？**
主要是利用了复数的乘法性质。
我们首先来复习一下复数乘法的性质：
$(a+ib) \cdot (c+id) = ac + ibc + iad + i^2bd=(ac-bd)+i(bc+ad) \\(a+ib) \cdot (c+id) = ac + ibc + iad + i^2bd=(ac-bd)+i(bc+ad)$
因此要计算：
$$\begin{align} f_q\left( \boldsymbol{x}_m,m \right)  &= \begin{pmatrix}  \cos m\theta & -\sin m\theta \\  \sin m \theta &  \cos m \theta \end{pmatrix}\begin{pmatrix}  q_m^{(1)}  \\  q_m^{(2)}    \end{pmatrix} \\ &= \left( \cos m\theta *q_m^{(1)}-\sin m\theta *q_m^{(2)} ,\sin m\theta *q_m^{(1)}-\cos m\theta *q_m^{(2)} \right) \end{align}$$
可以转化为计算：
$$\left( \cos m\theta+i \sin m\theta \right)\cdot \left( q_m^{(1)}+i q_m^{(2)} \right)\\\left( \cos m\theta+i \sin m\theta \right)\cdot \left( q_m^{(1)}+i q_m^{(2)} \right)$$
所以可以将公式（12）转化为两个复数的乘法运算。
### 3.2 在ChatGLM中的实现
和 LLAMA 的实现方式相差不大。代码如下：
```
class RotaryEmbedding(torch.nn.Module):
   def __init__(self, dim, base=10000, precision=torch.half, learnable=False):
       super().__init__()
        # 计算 \theta_i
       inv_freq = 1. / (base ** (torch.arange(0, dim, 2).float() / dim))
       inv_freq = inv_freq.half()
       self.learnable = learnable
       if learnable:
           self.inv_freq = torch.nn.Parameter(inv_freq)
           self.max_seq_len_cached = None
       else:
           self.register_buffer('inv_freq', inv_freq)
           self.max_seq_len_cached = None
           self.cos_cached = None
           self.sin_cached = None
       self.precision = precision
   def forward(self, x, seq_dim=1, seq_len=None):
       if seq_len is None:
           seq_len = x.shape[seq_dim]
       if self.max_seq_len_cached is None or (seq_len > self.max_seq_len_cached):
           self.max_seq_len_cached = None if self.learnable else seq_len
           # 生成 token 序列索引 t = [0, 1,..., seq_len-1]
           t = torch.arange(seq_len, device=x.device, dtype=self.inv_freq.dtype)
           # 对应m * \theta
           freqs = torch.einsum('i,j->ij', t, self.inv_freq)
           # 将 m * \theta 拼接两次，对应复数的实部和虚部
           emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
           if self.precision == torch.bfloat16:
               emb = emb.float()
           # [sx, 1 (b * np), hn]
           cos_cached = emb.cos()[:, None, :]  # 计算得到cos(m*\theta)
           sin_cached = emb.sin()[:, None, :]  # 计算得到cos(m*\theta)
           if self.precision == torch.bfloat16:
               cos_cached = cos_cached.bfloat16()
               sin_cached = sin_cached.bfloat16()
           if self.learnable:
               return cos_cached, sin_cached
           self.cos_cached, self.sin_cached = cos_cached, sin_cached
       return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
   def _apply(self, fn):
       if self.cos_cached is not None:
           self.cos_cached = fn(self.cos_cached)
       if self.sin_cached is not None:
           self.sin_cached = fn(self.sin_cached)
       return super()._apply(fn)
def rotate_half(x):
   x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
   return torch.cat((-x2, x1), dim=x1.ndim - 1)
```
## 4\. RoPE的外推性
我们都知道 RoPE 具有很好的外推性，前面的实验结果也证明了这一点。这里解释下具体原因。
RoPE 可以通过旋转矩阵来实现位置编码的外推，即可以通过旋转矩阵来生成超过预期训练长度的位置编码。这样可以提高模型的泛化能力和鲁棒性。
我们回顾一下 RoPE 的工作原理：假设我们有一个 $dd$ 维的绝对位置编码 $P_i$ ，其中 $i$ 是位置索引。我们可以将 $P_i$ 看成一个 $d$ 维空间中的一个点。我们可以定义一个 $d$ 维空间中的一个旋转矩阵 $\boldsymbol{R}$ ，它可以将任意一个点沿着某个轴旋转一定的角度。我们可以用 $\boldsymbol{R}$ 来变换 $P_i$ ，得到一个新的点 $Q_i=\boldsymbol{R}*P_i$ 。我们可以发现， $Q_i$ 和 $P_i$ 的距离是相等的，即 $\left|\left| Q_i-P_i \right|\right| = 0$ 。这意味着 $Q_i$ 和 $P_i$ 的相对关系没有改变。但是， $Q_i$ 和 $P_i$ 的距离可能发生改变，即 $\left|\left| Q_i-P_j \right|\right| \ne \left|\left| P_i-P_j \right|\right|$ 。这意味着 $Q_i$ 和 $P_j$ 的相对关系有所改变。因此，我们可以用 $\boldsymbol{R}$ 来调整不同位置之间的相对关系。
如果我们想要生成超过预训练长度的位置编码，我们只需要用 $\boldsymbol{R}\boldsymbol{R}$ 来重复变换最后一个预训练位置编码 $P_n$ ，得到新的位置编码 $Q_{n+1} = \boldsymbol{R} * P_n ，Q_{n+2} = \boldsymbol{R} * Q_{n+1} ， Q_{n+3} = \boldsymbol{R} * Q_{n+2}$ ，依此类推。这样就可以得到任意长度的位置编码序列 $Q_1, Q_2, …, Q_m$ ，其中 $m$ 可以大于 $n$ 。由于 $\boldsymbol{R}$ 是一个正交矩阵，它保证了 $Q_i$ 和 $Q_j$ 的距离不会无限增大或缩小，而是在一个有限范围内波动。这样就可以避免数值溢出或下溢的问题。同时，由于 $\boldsymbol{R}$ 是一个可逆矩阵，它保证了 $Q_i$ 和 $Q_j$ 的距离可以通过 $\boldsymbol{R}$ 的逆矩阵 $\boldsymbol{R}^{-1}$ 还原到 $P_i$ 和 $P_j$ 的距离，即 $||\boldsymbol{R}^{-1} * Q_i - \boldsymbol{R}^{-1} * Q_j|| = ||P_i - P_j||$ 。这样就可以保证位置编码的可逆性和可解释性。
总结而言：
**旋转编码 RoPE 可以有效地保持位置信息的相对关系** ，即相邻位置的编码之间有一定的相似性，而远离位置的编码之间有一定的差异性。这样可以增强模型对位置信息的感知和利用。这一点是其他绝对位置编码方式（如正弦位置编码、学习的位置编码等）所不具备的，因为它们只能表示绝对位置，而不能表示相对位置。
**旋转编码 RoPE 可以通过旋转矩阵来实现位置编码的外推** ，即可以通过旋转矩阵来生成超过预训练长度的位置编码。这样可以提高模型的泛化能力和鲁棒性。这一点是其他固定位置编码方式（如正弦位置编码、固定相对位置编码等）所不具备的，因为它们只能表示预训练长度内的位置，而不能表示超过预训练长度的位置。
**旋转编码 RoPE 可以与线性注意力机制兼容** ，即不需要额外的计算或参数来实现相对位置编码。这样可以降低模型的计算复杂度和内存消耗。这一点是其他混合位置编码方式（如Transformer-XL、XLNet等）所不具备的，因为它们需要额外的计算或参数来实现相对位置编码。
## 总结
最近一直听到旋转编码这个词，但是一直没有仔细看具体原理。今天花时间仔细看了一遍，确实理论写的比较完备，而且实验效果也不错。目前很多的大模型，都选择了使用了这种编码方式（LLAMA、GLM等）。
## 附录
这里补充一下前面公式1.3.2节中，公式（8）~（11）是怎么推导出来的。
回到之前的公式（8），编码之后的 $\boldsymbol{q},\boldsymbol{v}\boldsymbol{q},\boldsymbol{v}$ 以及内积 $\left< \boldsymbol{q},\boldsymbol{v} \right>$ 的形式如下：
$$\begin{align}
f_q(\boldsymbol{x}_m,m)=(\boldsymbol{W}_q\boldsymbol{x}_m)e^{im\theta} \\ f_k(\boldsymbol{x}_n,n)=(\boldsymbol{W}_kx_n)e^{in\theta} \\ g(\boldsymbol{x}_m,x_n,m-n)=Re[(\boldsymbol{W}_\boldsymbol{q}x_m)(\boldsymbol{W}_k\boldsymbol{x}_n)^{*}e^{i(m-n)\theta}]
\end{align}$$
上面的公式为什么满足： $\left<\boldsymbol{f}_q(\boldsymbol{x}_m,m),f_k(\boldsymbol{x}_n,n)\right>=g(\boldsymbol{x}_m,\boldsymbol{x}_n,m-n)$ 。
首先我们得先了解一下基本的复数相关知识。
首先看到上述 $f$ 和 $g$ 公式中有个指数函数： $e^{ix}$
这个其实是欧拉公式，其中 $xx$ 表示任意实数， $e$ 是自然对数的底数， $i$ 是复数中的虚数单位，则根据欧拉公式有：
$$e^{ix} = \cos x + i\sin x \\e^{ix} = \cos x + i\sin x$$
则是上述指数函数可以表示为实部为 $\cos x$ ，虚部为 $\sin x$ 的一个复数，欧拉公式建立了指数函数、三角函数和复数之间的桥梁。
则上述 $f$ 和 $g$ 公式的
$$\begin{align}e^{im\theta}=\cos (m\theta) + i\sin (m\theta) \\ e^{in\theta}=\cos (n\theta) + i\sin (n\theta) \\ e^{i(m-n)\theta}=\cos ((m-n)\theta) + i\sin ((m-n)\theta)
\end{align} $$
然后我们看回公式：
$$f_q(\boldsymbol{x}_m,m)=(\boldsymbol{W}_q\boldsymbol{x}_m)e^{im\theta}$$
其中 $\boldsymbol{W}_q$ 是个二维矩阵， $\boldsymbol{x}_m$ 是个二维向量，相乘的结果也是一个二维向量，这里用 $\boldsymbol{q}_m$ 表示：
$$\begin{align}
q_m= \begin{pmatrix}  q_m^{(1)}  \\  q_m^{(2)}    \end{pmatrix} = \boldsymbol{W}_q\boldsymbol{x}_m =\begin{pmatrix}  {W}_q^{(11)} & W_q^{(12)} \\  W_q^{(21)} & W_q^{(22)}    \end{pmatrix} \begin{pmatrix}  x_m^{(1)}  \\  x_m^{(2)}    \end{pmatrix} \\
\end{align}$$
然后首先将 $\boldsymbol{q}_m$ 表示成复数形式：
$\boldsymbol{q}_m = [q_m^{(1)}, q_m^{(2)}] = [q_m^{(1)} + iq_m^{(2)}]$
接着
$\boldsymbol{f}_q(\boldsymbol{x}_m,m)=(\boldsymbol{W}_q\boldsymbol{x}_m)e^{im\theta}=\boldsymbol{q}_me^{im\theta}$
其实就是两个复数相乘：
$\boldsymbol{q}_me^{im\theta}=(q_m^{(1)} + iq_m^{(2)}) * (\cos (m\theta) + i\sin (m\theta))$
然后就有：
$\boldsymbol{q}_me^{im\theta}=(q_m^{(1)} + iq_m^{(2)}) * (\cos (m\theta) + i\sin (m\theta)) \\ =(q_m^{(1)}cos (m\theta) -  q_m^{(2)} \sin (m\theta) ) + i(q_m^{(2)}\cos (m\theta) + q_m^{(1)}\sin (m\theta))$
将结果重新表达成实数向量形式就是：
$\boldsymbol{q}_me^{im\theta}=[q_m^{(1)} \cos (m\theta) -  q_m^{(2)} \sin (m\theta), q_m^{(2)}\cos (m\theta) + q_m^{(1)}\sin (m\theta)] \\ \boldsymbol{q}_me^{im\theta}=[q_m^{(1)} \cos (m\theta) -  q_m^{(2)} \sin (m\theta), q_m^{(2)}\cos (m\theta) + q_m^{(1)}\sin (m\theta)]$
**这里不难发现就是 query 向量乘以了一个旋转矩阵** 。
$f_q(\boldsymbol{x}_m,m)=(\boldsymbol{W}_q\boldsymbol{x}_m)e^{im\theta}=\boldsymbol{q}_me^{im\theta}\\ =[q_m^{(1)} \cos (m\theta) -  q_m^{(2)} \sin (m\theta), q_m^{(2)}\cos (m\theta) + q_m^{(1)}\sin (m\theta)] \\ = \begin{pmatrix}  \cos (m\theta) & -\sin (m\theta) \\  \sin (m\theta) & \cos (m\theta)    \end{pmatrix} \begin{pmatrix}  q_m^{(1)}  \\  q_m^{(2)}    \end{pmatrix}$
**这就是为什么叫做旋转式位置编码的原因。**
同理可得 key 向量 $\boldsymbol{k}_n\boldsymbol{k}_n$ ：
$f_k(\boldsymbol{x}_n,n)=(\boldsymbol{W}_k\boldsymbol{x}_n)e^{in\theta}=\boldsymbol{k}_ne^{in\theta}\\ =[k_n^{(1)} \cos (n\theta) -  k_n^{(2)} \sin (n\theta), k_n^{(2)}\cos (n\theta) + k_n^{(1)}\sin (n\theta)] \\ = \begin{pmatrix}  \cos (n\theta) & -\sin (n\theta) \\  \sin (n\theta) & \cos (n\theta)    \end{pmatrix} \begin{pmatrix}  k_n^{(1)}  \\  k_n^{(2)}    \end{pmatrix}$
最后还有个函数 $gg$ ：
$g(\boldsymbol{x}_m,\boldsymbol{x}_n,m-n)=Re[(\boldsymbol{W}_q\boldsymbol{x}_m)(\boldsymbol{W}_k\boldsymbol{x}_n)^{*}e^{i(m-n)\theta}]$
其中 $Re\left( x \right)Re\left( x \right)$ 表示一个复数 $x$ 的实部部分，而 $(\boldsymbol{W}_k\boldsymbol{x}_n)^{*}$ 则表示复数 $\boldsymbol{W}_k\boldsymbol{x}_n$ 的共轭。
复习一下共轭复数的定义： $z=a+ib\\ z^*=a-ib$
所以可得：
$$\begin{align}
\boldsymbol{W}_q\boldsymbol{x}_m = \boldsymbol{q}_m = q_m^{(1)} + iq_m^{(2)} \\ \boldsymbol{W}_k\boldsymbol{x}_n=\boldsymbol{k}_n= k_n^{(1)} + ik_n^{(2)} \\ (\boldsymbol{W}_k\boldsymbol{x}_n)^*=\boldsymbol{k}_n^*= k_n^{(1)} - ik_n^{(2)} \\ e^{i(m-n)\theta}=\cos((m-n)\theta) + i \sin((m-n)\theta)
\end{align}$$
继续可得：
$$\begin{align} g(\boldsymbol{x}_m,\boldsymbol{x}_n,m-n) &=Re[(\boldsymbol{W}_q\boldsymbol{x}_m)(\boldsymbol{W}_k\boldsymbol{x}_n)^{*}e^{i(m n)\theta}] \\ & = Re[(q_m^{(1)} + iq_m^{(2)})(k_n^{(1)} - ik_n^{(2)})(\cos((m-n)\theta) + i \sin((m-n)\theta))] \\  &= Re[((q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)}) + i(q_m^{(2)}k_n^{(1)} - q_m^{(1)}k_n^{(2)}))(\cos((m-n)\theta) + i \sin((m-n)\theta))] \\  &= (q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\cos((m-n)\theta) - (q_m^{(2)}k_n^{(1)} - q_m^{(1)}k_n^{(2)})\sin((m-n)\theta)  \end{align}$$
接下来我们就要证明函数 $gg$ 的计算公式是成立的。
首先回顾一下 attention 操作， 位置 $mm$ 的 query 和位置 $n$ 的 key 会做一个内积操作：
$$\begin{align} f_q(x_m,m)&=[q_m^{(1)} \cos (m\theta) -  q_m^{(2)} \sin (m\theta), q_m^{(2)}\cos (m\theta) + q_m^{(1)}\sin (m\theta)] \\  f_k(x_n,n)& =[k_n^{(1)} \cos (n\theta) -  k_n^{(2)} \sin (n\theta), k_n^{(2)}\cos (n\theta) + k_n^{(1)}\sin (n\theta)] \\  <f_q(x_m,m),f_k(x_n,n)> &=  (q_m^{(1)} \cos (m\theta) -  q_m^{(2)} \sin (m\theta))(k_n^{(1)} \cos (n\theta) -  k_n^{(2)} \sin (n\theta)) \\ &+ (q_m^{(2)}\cos (m\theta) + q_m^{(1)}\sin (m\theta))(k_n^{(2)}\cos (n\theta) + k_n^{(1)}\sin (n\theta))\\ & =q_m^{(1)} \cos (m\theta) k_n^{(1)} \cos (n\theta) - q_m^{(1)} \cos (m\theta)k_n^{(2)} \sin (n\theta)\\ & - q_m^{(2)} \sin (m\theta)k_n^{(1)} \cos (n\theta) + q_m^{(2)} \sin (m\theta)k_n^{(2)} \sin (n\theta) \\ & + q_m^{(2)}\cos (m\theta)k_n^{(2)}\cos (n\theta) + q_m^{(2)}\cos (m\theta)k_n^{(1)}\sin (n\theta) \ + q_m^{(1)}\sin (m\theta)k_n^{(2)}\cos (n\theta) + q_m^{(1)}\sin (m\theta)k_n^{(1)}\sin (n\theta)  \end{align}$$
接着进行推导，我们整理一下：
$$\begin{align} <f_q(\boldsymbol{x}_m,m),f_k(\boldsymbol{x}_n,n)>  &=  {q}_m^{(1)}{k}_n^{(1)}(\cos(m\theta)\cos(n\theta) + \sin(m\theta)\sin(n\theta) ) \\  &+ {q}_m^{(1)}{k}_n^{(2)}(-\cos(m\theta)\sin(n\theta) + \sin(m\theta)\cos(n\theta) ) \\ & + {q}_m^{(2)}{k}_n^{(1)}(-\sin(m\theta)\cos(n\theta) + \cos(m\theta)\sin(n\theta) ) \\  &+ {q}_m^{(2)}{k}_n^{(2)}(\sin(m\theta)\sin(n\theta) + \cos(m\theta)\cos(n\theta) ) \\ & = q_m^{(1)}k_n^{(1)}\cos((m-n)\theta) \\  &+ q_m^{(1)}k_n^{(2)}\sin((m-n)\theta) \\  &- q_m^{(2)}k_n^{(1)}\sin((m-n)\theta) \\ & + q_m^{(2)}k_n^{(2)}\cos((m-n)\theta) \\  &= (q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\cos((m-n)\theta) + (q_m^{(1)}k_n^{(2)}- q_m^{(2)}k_n^{(1)})\sin((m-n)\theta) \\ & = (q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\cos((m-n)\theta) - (q_m^{(2)}k_n^{(1)} - q_m^{(1)}k_n^{(2)})\sin((m-n)\theta) \\ &=g(x_m,x_n,m-n)  \end{align}$$
这就证明上述关系是成立的，位置 $mm$ 的 query 和位置 $n$ 的 key 的内积就是函数 $g$ 。
把上面的式子用矩阵向量乘的形式来表达就是：
$$\begin{align}
<f_q(\boldsymbol{x}_m,m),f_k(\boldsymbol{x}_n,n)> \\ =\begin{pmatrix} \begin{pmatrix}  \cos (m\theta) & -\sin (m\theta) \\  \sin (m\theta) & \cos (m\theta)    \end{pmatrix} \begin{pmatrix}  q_m^{(1)}  \\  q_m^{(2)}    \end{pmatrix} \end{pmatrix}^T  \begin{pmatrix}  \begin{pmatrix}  \cos (n\theta) & -\sin (n\theta) \\  \sin (n\theta) & \cos (n\theta)    \end{pmatrix} \begin{pmatrix}  k_n^{(1)}  \\  k_n^{(2)}    \end{pmatrix} \end{pmatrix}  \\ = \begin{pmatrix}  q_m^{(1)} &  q_m^{(2)}  \\ \end{pmatrix}  \begin{pmatrix}  \cos (m\theta) & \sin (m\theta) \\  -\sin (m\theta) & \cos (m\theta)    \end{pmatrix}    \begin{pmatrix}  \cos (n\theta) & -\sin (n\theta) \\  \sin (n\theta) & \cos (n\theta)    \end{pmatrix} \begin{pmatrix}  k_n^{(1)}  \\  k_n^{(2)}    \end{pmatrix} \\ = \begin{pmatrix}  q_m^{(1)} &  q_m^{(2)}  \\ \end{pmatrix}   \begin{pmatrix}  \cos(m\theta)\cos(n\theta) + \sin(m\theta)\sin(n\theta) & -\cos(m\theta)\sin(n\theta) + \sin(m\theta)\cos(n\theta) \\  -\sin(m\theta)\cos(n\theta) + \cos(m\theta)\sin(n\theta) & \sin(m\theta)\sin(n\theta) + \cos(m\theta)\cos(n\theta) \end{pmatrix}   \begin{pmatrix}  k_n^{(1)}  \\  k_n^{(2)}    \end{pmatrix} \\ =\begin{pmatrix}  q_m^{(1)} &  q_m^{(2)}  \\ \end{pmatrix}   \begin{pmatrix}  \cos((m-n)\theta) & -\sin((m-n)\theta) \\  \sin((m-n)\theta) &  \cos((m-n)\theta) \end{pmatrix}   \begin{pmatrix}  k_n^{(1)}  \\  k_n^{(2)}    \end{pmatrix}
\end{align}$$