自然策略梯度算法[[2]](#ref_2)揭露了传统策略梯度算法的缺点以及补救的方法，尽管自然梯度已被TRPO和PPO等算法超越，但掌握它的基本原理对于理解这些当代RL算法至关重要。
对自然梯度的完整讨论需要具备较强的数学功底，需要许多冗长的推导。我们的讨论将主要关注推理和直觉，为更深入的推导提供外部参考。
### 2.1 传统策略梯度算法的缺陷
在传统的策略梯度算法中，我们根据目标函数梯度 $\nabla_\theta J(\theta)$ 和步长 $\alpha$ 更新策略权重 $\theta$ ，这样的更新过程可能会出现两个常见的问题：
-   **过冲（Overshooting）：** 更新错过了奖励峰值并落入了次优策略区域
-   **下冲（Undershooting）：** 在梯度方向上采取过小的更新步长会导致收敛缓慢
在监督学习问题中，overshooting并不是什么大问题，因为数据是固定的，我们可以在下一个epoch中重新纠正，但在强化学习问题中，如果因为overshooting陷入了一个较差的策略区域，则未来的样本批次可能不会提供太多有意义的信息，用较差的数据样本再去更新策略，从而陷入了糟糕的正反馈中无法恢复。较小的学习率可能会解决这个问题，但会导致收敛速度变慢的undershooting问题。
![](https://picx.zhimg.com/v2-f13def47df783ce86b0744005bd7fbb9_1440w.jpg)
图3. overshooting示例：如果在梯度方向上采取的步骤太大（左），更新可能会错过奖励峰值并落在具有低梯度的次优区域（右），需要多次迭代才能逃逸或者甚至无法逃逸
为了避免overshooting带来的严重后果，一种直觉方法是限制每次更新步长的上限：$\Delta\theta^*=\mathop{\mathrm{argmax}}\limits_{||\Delta\theta||\leq\epsilon}J(\theta+\Delta\theta)$其中 $||\Delta\theta||$ 代表更新前后策略权重的欧式距离。
尽管听起来很合理，但实际效果并不是像我们预期的那样，原因是不同的分布对参数变化的敏感度是不同的。比如在下图中，我们都让策略权重变化了1个单位的欧式距离，但是对左图的影响是远大于右图的。所以，只限制参数是不合理的，更应该考虑**分布对参数变化的敏感度**，传统的策略梯度算法无法考虑到这种曲率变化，我们需要引入二阶导数，这正是自然策略梯度相较于传统策略梯度算法的区别。
![](https://pic1.zhimg.com/v2-846f470d41c7b286c8c00c824e8b6586_1440w.jpg)
图4. 高斯函数示例：两组高斯函数中的均值分别为0和1，左图的高斯函数标准差为0.3，右图为3
### 2.2 限制策略更新的差异
我们需要表示策略（分布）之间的差异，而不是参数本身的差异。计算两个概率分布之间的差异，最常见的是[KL散度](https://zhida.zhihu.com/search?content_id=224601754&content_type=Article&match_order=1&q=KL%E6%95%A3%E5%BA%A6&zhida_source=entity)[[3]](#ref_3)，也称为相对熵，描述了两个概率分布之间的距离：
$$\mathcal{D}_{\mathrm{KL}}(\pi_{\theta}\parallel\pi_{\theta+\Delta\theta})=\sum_{x\in\mathcal{X}}\pi_{\theta}(x)\log\left(\frac{\pi_{\theta}{}(x)}{\pi_{\theta+\Delta\theta(x)}}\right)$$
调整后的策略更新限制为：
$$\Delta\theta^*=\mathop{\mathrm{argmax}}\limits_{\mathcal{D}_{\mathrm{KL}}(\pi_{\theta}\parallel\pi_{\theta+\Delta\theta})\leq\epsilon}J(\theta+\Delta\theta)$$
通过计算这个表达式，我们可以确保在参数空间中执行大更新的同时，保证策略本身的改变不超过阈值。然而，计算KL散度需要遍历所有的状态-动作对，因此我们需要一些化简来处理现实的RL问题。
接下来的数学部分，可以在Katerina Fragkiadaki（CMU）关于自然策略梯度的PPT[[4]](#ref_4)中找到详细的证明推导。
首先，我们使用拉格朗日松弛将原表达式的发散约束转化为惩罚项，得到一个更容易求解的表达式：
$$\Delta\theta^{*}=\operatorname*{arg}\operatorname*{max}_{\Delta\theta}J(\theta+\Delta\theta)-\lambda({\mathcal{D}}_{\mathrm{KL}}(\pi_{\theta}\parallel\pi_{\theta{+}\Delta\theta})-\epsilon)$$
由于计算KL散度需要遍历所有的状态-动作对，我们必须用近似方法来化简。通过泰勒展开：
$$\Delta\theta^*\approx\operatorname*{arg}\operatorname*{max}_{\Delta\theta}J(\theta_{\text{old}})+\nabla_\theta J(\theta)|_{\theta=\theta_{\text{old}}}\cdot\Delta\theta-\dfrac{1}{2}\lambda\left(\Delta\theta^{\top}\nabla_\theta^2\mathcal{D}_{\text{KL}}(\pi_{\theta_{\text{old}}}\parallel\pi_\theta)|_{\theta={}\theta_{\text{old}}}\Delta\theta\right)+\lambda\epsilon$$
目标函数近似于一阶泰勒展开（与散度相比，二阶展开可以忽略不计），KL散度近似于二阶泰勒展开（零阶和一阶差分的计算结果为0）。我们可以进一步化简，通过
-   用费舍尔信息矩阵（Fisher information matrix）替换二阶KL导数
-   删除所有不依赖于 $\Delta\theta$ 的项
可以得到：
$$\Delta\theta^*\approx \operatorname*{arg}\operatorname*{max}_{\Delta\theta}\nabla_\theta J(\theta)|_{\theta=\theta_\text{old}}\cdot\Delta\theta-\frac{1}{2}\lambda(\Delta\theta^\top F(\theta_\text{old})\Delta\theta)$$
用费舍尔信息矩阵代替二阶导数的原因，除了符号紧凑性外，还可以大大减少计算开销。原先的Hessian矩阵是一个 $|\theta|\cdot|\theta|$ 的矩阵，每个元素都是二阶导数，完整的计算可能非常麻烦，而对于[Fisher信息矩阵](https://zhida.zhihu.com/search?content_id=224601754&content_type=Article&match_order=1&q=Fisher%E4%BF%A1%E6%81%AF%E7%9F%A9%E9%98%B5&zhida_source=entity)，有一个替代表达式：
$$F(\theta)=\mathbb{E}_{\theta}\begin{bmatrix}\nabla_{\theta}\log\pi_{\theta}(x)\nabla_\theta\log\pi_\theta(x)^\top\end{bmatrix}$$
可以表示为策略梯度的外积，在局部等效于Hessian，同时计算效率更高（因为传统策略梯度的这些值 $\nabla_{\theta}\log\pi_{\theta}(x)$ 本身也是我们需要的，无需再重复计算，同时期望意味着可以使用样本近似）。
### 2.3 解决KL约束问题
对于近似简化后的表达式，可以通过将关于 $\Delta\theta$ 的梯度设置为0，来找到最优的权重更新 $\Delta\theta$ ：
$$\begin{aligned}
0&=\frac{\partial}{\partial\Delta\theta}\left(\nabla_{\theta}J(\theta)\Delta\theta-\frac{1}{2}\lambda\Delta\theta^{\top}F(\theta)\Delta\theta\right)\\ &=\nabla_\theta J(\theta)-\frac{1}{2}\lambda F(\theta)\Delta\theta
\end{aligned}
$$$$
\begin{aligned}\therefore\lambda F(\theta)\Delta\theta&=-2\nabla_\theta J(\theta)\\ \Delta\theta&=-\frac{2}{\lambda}F(\theta)^{-1}\nabla_{\theta}J(\theta)
\end{aligned}$$其中， $\lambda$ 是一个常数，可以吸收到学习率 $\alpha$ 中。根据 $\mathcal{D}_{\mathrm{KL}}(\pi_{\theta}\parallel\pi_{\theta+\Delta\theta}) \leq \epsilon$ ，我们可以推出动态学习率：
$$\alpha=\sqrt{\dfrac{2\epsilon}{\nabla J(\theta)^\top}F(\theta)^{-1}\nabla J(\theta)}$$
可以确保每次更新的KL散度（近似）等于 $\epsilon$ 。
最后，我们提取自然策略梯度，它是针对流形曲率校正的梯度：
$\tilde{\nabla}J(\theta) = F(\theta)^{-1}\nabla J(\theta)$这种自然策略梯度在距离约束内给出了黎曼空间中最陡的下降方向，而不是传统上假设的欧几里德空间中的最陡下降方向。与传统的策略梯度相比，唯一的区别是与逆Fisher矩阵相乘。
最终的权重更新方案为：
$$\Delta\theta=\sqrt{\frac{2\epsilon}{\nabla J(\theta)^\top F(\theta)^{-1}\nabla J(\theta)}} \tilde{\nabla} J(\theta)$$该方案的强大之处在于，无论分布的表示如何，它总是以相同的幅度改变策略。
### 2.4 算法实现
自然策略梯度算法的完整概述伪代码[[5]](#ref_5)如下。其中策略梯度和Fisher矩阵在实际计算时都是使用样本估计的。
![](https://pic2.zhimg.com/v2-89bb3349787710e65dfbc78fa097f51f_1440w.jpg)
图5. 自然策略梯度算法
最终结果在两个方面不同于传统的策略梯度算法：
-   考虑到策略对局部变化的敏感性，**策略梯度由逆Fisher矩阵校正**，而传统的梯度方法假定更新为欧几里得距离。
-   更新步长 \alpha 具有适应梯度和局部敏感性的**动态表达式**，确保无论参数化如何，策略变化幅度为 $\epsilon$ 。在传统方法中， $\alpha$ 通常设置为一些标准值，如0.1或0.01。