---
source: "https://zhuanlan.zhihu.com/p/720989890"
---
## 激活函数的选择
激活函数的选择可以尝试按下面的决策过程进行选择，主要还是效率和效果的平衡。
![|550](https://pic1.zhimg.com/v2-5e2745f92ee94bf45e617b5ba2fa8fd2_1440w.jpg)



#### 激活函数汇总

| 名称          | 数学表达式                                                                        | 取值范围                 | 导数                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| ----------- | ---------------------------------------------------------------------------- | -------------------- | ------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| sigmoid     | $f(x) = \frac{1}{1+e^{-x}}$                                                  | $(0,1)$              | $f'(x) = f(x)(1 - f(x))$                                                 | ![\|200](https://pic2.zhimg.com/v2-a3306f5aadf74c52ae8f897c4684aa3d_1440w.jpg)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| tanh        | $f(x) = tanh(x)= 2sigmoid(2x) - 1$                                           | $(-1,1)$             | $f'(x) = 1 - f(x)^2$                                                     | ![\|200](https://pic2.zhimg.com/v2-8d848f20dea741531b416b9cd5c48e4d_1440w.jpg)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| ReLU        | $f(x) = max(0, x)$                                                           | $(0, +\infty)$       | $f'(x) = \begin{cases} 0, & x \leq 0 \\ 1, & x > 0 \end{cases}$          | ![\|200](https://pic4.zhimg.com/v2-27153e7b3748d0a8198df4bb6aee0ae1_1440w.jpg)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| Leaky ReLU  | $f(x) = \begin{cases} x, & x \geq 0 \\ \alpha x, & x < 0 \end{cases}$        | $(-\infty, +\infty)$ | $f'(x) = \begin{cases} 1, & x \geq 0 \\ \alpha, & x < 0 \end{cases}$     | $\alpha$ 为大于0的常数，通常较小，一般为0.01![\|200](https://pic3.zhimg.com/v2-43a1ad7f1ae9e8f49f8daff65d075b96_1440w.jpg)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| PReLU       | $f(x) = \begin{cases} x, & x \geq 0 \\ \alpha_{i} x, & x < 0 \end{cases}$    |                      |                                                                          | $\alpha_i$ 可以为可学习的参数<br>- 若 $\alpha_i = 0$ ， 退化为ReLU<br>- 若 $\alpha_i >0$ 且是常数，退化为 Leaky ReLU<br>- 若 $\alpha_i$ 是可学习参数，为PReLU                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| ELU         | $f(x) = \begin{cases} x, & x \geq 0 \\ \alpha(e^x - 1), & x < 0 \end{cases}$ |                      | $f'(x) = \begin{cases} 1, & x \geq 0 \\ \alpha e^x, & x < 0 \end{cases}$ | ![\|225](https://pic3.zhimg.com/v2-d4379824081097bf9ec398f1de956eca_1440w.jpg)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| SELU        | $SELU(x) = \lambda \cdot ELU(x)$                                             |                      |                                                                          | SELU 函数有一个特殊的属性。 如果正确初始化，使用线性层的前馈网络将自归一化，前提是所有隐藏层都被 SELU 激活。 这意味着每一层的输出将大致具有等于 0 的平均值和等于 1 的标准偏差，这有助于防止梯度消失或爆炸问题，并允许构建深度网络。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Swish(SiLU) | $f(x) =x \cdot sigmoid(\beta x) = \frac{x}{1 + e^{-\beta x}}$                |                      | $$f'(x) = 1 - (1 - \beta f(x))(1 - sigmoid(\beta x))$$                   | $\beta$ 是可学习参数或固定超参， $sigmoid(\beta x) \in (0,1)$ 可以看作一种软性的门控机制。<br>- 当 $sigmoid(\beta x)$ 接近1时，门处于开状态， $f(x)$ 近似于 $x$ 本身<br>- 当 $sigmoid(\beta x)$ 接近0时，门处于关状态， $f(x)$ 近似于0<br>下面看一下不同的 $\beta$ ,<br>- 当 $\beta =0$ 时， $swish(x)=\frac{x}{2}$<br>- 当 $\beta = 1$ 时，swish函数在 $x>0$ 区间近似线性，在 $x<0$ 近似饱和<br>- 当 $\beta \rightarrow +\infty$ 时，则有 x <0时， $sigmoid(x) \rightarrow 0$ , $x>0$ 时， $sigmoid(x) \rightarrow 1$ , swish函数近似为ReLU<br>因此，swish 函数可以看作线性函数和ReLU函数之间的非线性插值函数，程度由 $\beta$ 控制<br>![\|225](https://pic1.zhimg.com/v2-5e3b4fe8c55ec03794e783f424dc510c_1440w.jpg)<br>![\|225](https://pic2.zhimg.com/v2-5a898af857144f3fa2c0c44d2c4a0b8b_1440w.jpg) |
| SwiGLU      | $\text{SwiGLU}(a, b) = \text{Swish}(a) \otimes \sigma(b)$                    |                      |                                                                          | - $( a = xW_1 + b_1 ) 和 ( b = xW_2 + b_2 )$ 是两个线性变换的输出。<br>- $(Swish(x)=x⋅sigmoid(βx))$通常取 ( $\beta$=1 )，此时等价于SiLU激活函数。<br>- $( \otimes ) 表示逐元素乘积（Hadamard积）$ <br>- - **门控机制** ：通过Swish激活的权重动态调节信息 流，过滤不重要信息；<br>- **平滑性** ：Swish的连续可导性缓解了ReLU的梯度消失问题；<br>- **非单调性** ：允许模型捕捉更复杂的非线性模式；<br>- **参数效率** ：相比传统FFN（ReLU + 线性层），SwiGLU通过门控减少冗余计算。![\|275](https://pica.zhimg.com/v2-efed32268bacfaea5fcac96d12e6b536_1440w.jpg)                                                                                                                                                                                                                                          |
| Maxout      | $f(x) = max(w_1x+b_1,  ..., w_k*x+b_k)$                                      |                      |                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| Softplus    | $f(x) = \ln(1 + e^x)$                                                        | $(0, +\infty)$       | $f'(x) = sigmoid(x)$                                                     | ![\|275](https://pic2.zhimg.com/v2-2e792366fe19f221cb936efe02260b53_1440w.jpg)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| Mish        | $f(x) = x \cdot tanh(\ln(1 + e^x))$                                          |                      |                                                                          | ![\|275](https://pic2.zhimg.com/v2-9677f6e039c75747c8c5075e5fbcc049_1440w.jpg)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| GELU        | $GELU(x) = x \cdot \Phi(x)$                                                  |                      |                                                                          | 其中 $\Phi(x)$ 是高斯函数的累积分布函数， GELU 可以近似为<br>$$f(x)=0.5 \cdot x (1+\tanh (\sqrt{2/\pi} (x+0.044715x^3)))$$<br>或者<br>$$f(x)=x \cdot sigmoid(1.702x)$$<br>与 ReLU 系列的激活相反，GELU 根据其值对其输入进![\|250](https://pica.zhimg.com/v2-7560ff196bcf9e18ac1c6a2b08105e30_1440w.jpg)行加权，而不是根据其符号对它们进行阈值处理                                                                                                                                                                                                                                                                                                                                                                               |

#### 二、与其他激活函数的对比分析
**1\. 与GLU变体对比**

| 激活函数    | 公式                                 | 特点                      | 性能表现         |
| ------- | ---------------------------------- | ----------------------- | ------------ |
| GLU     | $\sigma(xW_1) \otimes (xW_2)$      | 原始门控结构，使用Sigmoid，梯度较平缓  | 中等，易饱和       |
| ReGLU   | $\text{ReLU}(xW_1) \otimes xW_2$   | 门控使用ReLU，适合稀疏激活         | 较低，稳定性差      |
| GEGLU   | $\text{GELU }(xW_1) \otimes xW_2$  | 结合GELU的平滑性，适合自然语言任务     | 较高           |
| SwiGLU  | $\text{Swish}(xW_1) \otimes xW_2$  | 平衡平滑性与门控动态性，梯度稳定性最优     | 最高（主流LLM采用）  |
2\. 与经典激活函数对比
- **ReLU** ：仅处理正值，梯度在负区间为0，易导致神经元死亡；
- **Swish** ：平滑且非单调，但缺乏门控机制，参数利用率低；
- **GELU** ：通过高斯误差函数平滑处理负值，但计算复杂度较高。
#### 三、Python实现（基于PyTorch）
```
import torch
import torch.nn as nn
import torch.nn.functional as F
class SwiGLU(nn.Module):
   def __init__(self, input_dim, hidden_dim):
       super().__init__()
       self.w1 = nn.Linear(input_dim, hidden_dim, bias=False)
       self.w2 = nn.Linear(input_dim, hidden_dim, bias=False)
       self.w3 = nn.Linear(hidden_dim, input_dim, bias=False)
   def forward(self, x):
       # 分割输入为两部分（假设输入维度为偶数）
       assert x.shape[-1] % 2 == 0, "输入维度需为偶数"
       a, b = x.chunk(2, dim=-1)
       # Swish门控计算
       gate = F.silu(self.w1(a))  # F.silu等价于Swish(β=1)
       filtered = gate * self.w2(b)
       return self.w3(filtered)
```
代码说明：
1. 输入分割：将输入张量沿最后一维均分为两部分，分别作为门控和变换的输入；
2. Swish激活：使用PyTorch内置的F.silu函数实现Swish(β=1)；
3. 门控乘积：对两路线性变换结果进行逐元素乘，实现信息过滤；
4. 输出映射：通过第三个线性层恢复原始维度，适配残差连接。
#### 四、绘制swiGLU图像
```
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
def swiglu(x):
   """SwiGLU激活函数实现"""
   a, b = x.chunk(2, dim=-1)  # 将输入沿最后一维分割为两部分
   return a * F.silu(b)  # silu即为Swish(β=1)
# 生成输入数据（模拟二维特征）
x_range = torch.linspace(-3, 3, 100)  # 生成[-3, 3]的100个点
x_input = x_range.unsqueeze(-1).expand(-1, 2)  # 转换为100x2的二维张量
# 计算SwiGLU输出
with torch.no_grad():
   y_swiglu = swiglu(x_input)
# 绘制曲线
plt.figure(figsize=(10, 6))
plt.plot(x_range.numpy(), y_swiglu[:, 0].numpy(),
        label='SwiGLU Output', color='#FF6F00', linewidth=2.5)
plt.xlabel('Input Value', fontsize=12)
plt.ylabel('Output Value', fontsize=12)
plt.title('SwiGLU Activation Function', fontsize=14, pad=20)
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend(fontsize=10, framealpha=0.9)
plt.show()
```
![|650](https://pica.zhimg.com/v2-efed32268bacfaea5fcac96d12e6b536_1440w.jpg)
swiGLU图像
#### 五、应用场景
SwiGLU在以下场景表现突出：
1. 大规模语言模型：如LLaMA、PALM等，因其高效的门控机制降低计算冗余，平衡非线性表达与梯度稳定性；
2. 长序列建模：平滑梯度缓解了Transformer中的梯度消失问题；
3. 低资源训练：相比GELU，Swish的计算效率更高。
#### 六、典型题
1. **简述SwiGLU的数学表达式及其核心思想** 期望回答：需明确SwiGLU由Swish激活函数和门控线性单元（GLU）组成，公式为 $\text{SwiGLU}(a,b)=a \otimes \text{Swish}(b)$ ，其中 $a$ 和 $b$ 是输入分割后的两部分， $\otimes$ 为逐元素相乘。需强调其通过动态门控过滤信息的特性。
2. **为什么SwiGLU在Transformer的FFN层中比ReLU更有效？** 参考答案：ReLU的硬截断特性可能导致梯度消失，而SwiGLU的Swish门控平滑且保留负区间的部分信息，增强了梯度的稳定性；同时，门控机制通过参数化选择重要特征，提升了模型表达能力。
3. **SwiGLU与GLU、GEGLU的主要区别是什么？** • 关键点：GLU使用Sigmoid作为门控函数，易导致梯度饱和；GEGLU用GELU替代Sigmoid，增强负区间的非线性；SwiGLU的Swish门控在平滑性和计算效率上更优，适合大规模模型训练。
4. **SwiGLU相比传统ReLU激活函数，参数量有何变化？如何解决？** • 解析：SwiGLU需要两组线性变换（ $W1W_1W_1$ 和 $W2W_2$ ），参数量约为ReLU的2倍。实际应用时（如LLaMA），通过调整隐层维度（如将隐层设为原始维度的8/3倍再截断）保持总参数量接近。
5. **工程实现类**： 写出PyTorch实现SwiGLU的代码，并解释维度分割逻辑。示例代码：
```
def swiglu(x):
    a, b = x.chunk(2, dim=-1)  # 沿最后一维分割为两部分
    return a * F.silu(b)       # silu即Swish(β=1)
```
关键说明：输入需为偶数维，分割后的两部分独立参与门控计算，避免信息耦合。
1. **训练时发现SwiGLU的输出不稳定，可能是什么原因？如何调整？** • 排查方向：检查输入分割是否对称、初始化方法（建议使用Xavier正态初始化）、Swish的β参数（默认1，可尝试调参）。
2. **为什么LLaMA等大模型选择SwiGLU而非GELU？** • 核心原因：SwiGLU在相同参数量下表现更优的实验结果（如下游任务准确率提升0.6%）；门控机制减少了FFN层的冗余计算，适合长序列处理。
3. **SwiGLU在低资源训练场景下的优化策略** • 策略建议：采用LoRA微调（冻结大部分参数，仅训练门控相关矩阵）；使用混合精度训练（FP16存储+FP32计算门控乘积）。
4. **若将SwiGLU的门控函数改为ReLU，会对模型产生什么影响？** • 推理方向：ReLU门控会导致负区间完全截断，可能丢失部分有用信息；但稀疏性增强，适合需要特征选择的任务（需实验验证）。
5. **如何解释SwiGLU在Batch Normalization层后的表现差异？** ◦ 参考答案：BN会改变输入分布，可能削弱门控的动态性。建议在SwiGLU前使用Layer Normalization（如Transformer的Post-LN结构）。]]