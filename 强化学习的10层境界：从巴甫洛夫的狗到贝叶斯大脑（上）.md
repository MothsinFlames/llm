---
created: 2025-11-05T11:23:43 (UTC +08:00)
tags: [强化学习 (Reinforcement Learning)]
source: https://zhuanlan.zhihu.com/p/1932009376987717993
author: 关于作者gwave我思故我在，生命的意义在于负熵复旦大学 电子与信息博士郭达森也关注了他回答985文章193关注者37,956已关注发私信
---
2025 年，“**智能体**”（Agent）概念大热，几乎每家公司都声称自己在做“智能体”。大语言模型（LLMs）也被 Prompt 包装成各种所谓的智能体，几乎所有助理类功能都被**泛化**的称为**智能体** 。但真正学术意义上的 Agent，早在三十多年前，就已在强化学习（[Reinforcement Learning](https://zhida.zhihu.com/search?content_id=260802786&content_type=Article&match_order=1&q=Reinforcement+Learning&zhida_source=entity), RL）领域中诞生，并建立起一套**严谨而系统的理论体系**。从条件反射到类脑智能，从神经连接的强化到 LLM 的涌现，人类对“学习”的理解早已超越简单的奖惩机制，并取得了令人瞩目的成果：从击败李世石的 **AlphaGo**，到通过 **[HFRL](https://zhida.zhihu.com/search?content_id=260802786&content_type=Article&match_order=1&q=HFRL&zhida_source=entity)**（Human Feedback Reinforcement Learning）助力 LLM 突破 scaling law 的瓶颈，再到**AlphaEvolve**摘得[奥赛金牌](https://zhida.zhihu.com/search?content_id=260802786&content_type=Article&match_order=1&q=%E5%A5%A5%E8%B5%9B%E9%87%91%E7%89%8C&zhida_source=entity)。

强化学习作为连接**神经科学与人工智能**的桥梁，正沿着这条认知之路不断演进。在人类标注数据日益枯竭的当下，它通过 **learning from experience**（从经验中学习）提供了另一条进化路径，成为通往通用人工智能（AGI）的关键框架之一。本文旨在**系统梳理强化学习的前世今生**，带你一文读懂其核心概念、内在逻辑与发展脉络。

![](https://pica.zhimg.com/v2-bf17dd75c1772da30a065f7d0fea04bc_1440w.jpg)

AlphaEvolve解决了56年悬而未决的问题，将某矩阵乘法问题（Strassen’s）的次数从49次减少到48次，这意味大量的能源将被节约https://devproai.com.au/2025/05/17/crikey-googles-new-ai-just-solved-a-56-year-old-problem-what-alphaevolve-means-for-your-business-and-humanity/

## **导语：为什么要谈“RL的十层境界”？**

今天的人工智能，早已不是当年只能执行规则的自动机。它拥有了“感知”、“计划”、“探索”、“协作”甚至“推理”的能力——这些能力背后，都指向同一个核心问题：

> 智能体（Agent）如何通过与环境的交互，不断学习并优化自己的行为策略？

这正是强化学习（Reinforcement Learning, RL）所关注的核心命题。但 RL 并非凭空诞生，它的思想根基深植于行为心理学、神经科学与[控制论](https://zhida.zhihu.com/search?content_id=260802786&content_type=Article&match_order=1&q=%E6%8E%A7%E5%88%B6%E8%AE%BA&zhida_source=entity)的沃土之中：从巴甫洛夫的狗与 Hebb 的突触可塑性法则，到桑代克的猫、[斯金纳箱](https://zhida.zhihu.com/search?content_id=260802786&content_type=Article&match_order=1&q=%E6%96%AF%E9%87%91%E7%BA%B3%E7%AE%B1&zhida_source=entity)中的老鼠等行为主义实验，再到当代的[多智能体系统](https://zhida.zhihu.com/search?content_id=260802786&content_type=Article&match_order=1&q=%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E7%B3%BB%E7%BB%9F&zhida_source=entity)（Multi-Agent RL, MARL）与大语言模型（LLMs）。强化学习，是这条从生物智能延伸至人工智能的进化主线上的算法结晶。

进入 21 世纪，RL 从早期的单体智能体决策模型，拓展到多智能体交互与博弈，并正迈向以语言驱动策略生成、认知对齐的新范式。它一方面试图摆脱对大规模监督数据的依赖，转向经验驱动的学习（Sutton），另一方面结合世界模型的构建（LeCun），朝向更具因果性与抽象性的智能体发展。强化学习，正逐步成为连接神经科学原理与现代 AI 决策系统的桥梁。

强化学习的方法通常分为两大“门派”：基于价值的方法（Value-based）与基于策略的方法（Policy-based）。经典教材通常从值函数、贝尔曼方程等数学推导起步，这虽然严谨且按历史轨迹，但离当下的前沿实践仍有距离，复杂的数学公式也容易劝退不少读者。

因此，本文尝试走一条不一样的路径。你将看到两点特别之处：

1.  **从行为心理学与历史的双重视角**，为你讲述“强化学习的十重境界”。每一层，都是一次认知的飞跃。你可以选择任何一层暂做停留，也可一路探索直至终点。

2\. 先讲**策略**方法（Policy-based），再讲**价值**方法（Value-based）。我们将从现代强化学习实践中更常用、更实用、更新颖的策略方法讲起，随后再回溯价值方法的基础原理。这就像线性代数教材中将“行列式”一章安排在后面——帮助你先掌握“可操作的技能”，再理解其数学根基。通过对 RL 入门内容的重新排序，我们希望让已有机器学习基础（假设你了解梯度下降和大一微积分）的你，阅读体验更加自然顺畅。

___

## 第一部分：行为心理学的引子

## **第1层：巴甫洛夫的狗 + Hebb 学习法则 —— 环境信号与神经连接（爬行脑）**

### **1.1 从铃声到流口水：条件反射**

如果要追溯人类对“学习”这个行为的最早科学理解，伊万·巴甫洛夫（Ivan Pavlov)的狗无疑是一个起点。在19世纪末到20世纪初，巴甫洛夫的实验揭示了一个惊人的现象：狗在听到**铃声**后，即使没有看到食物，也会开始分泌唾液。起初，狗只会对食物产生唾液反应（这是一种天然的无条件反射），但当**铃声与食物反复配对**后，铃声本身就成为了触发唾液分泌的“信号”。这种新形成的刺激—反应关联，被称为**条件反射（Conditioned Reflex）**。

> 在这个层面，智能体的学习表现为**对环境中某些信号的响应习得**，是一种典型的**被动学习机制**。

![](https://pic2.zhimg.com/v2-e74657e20d266402c95015050757cd29_1440w.jpg)

https://sites.psu.edu/siowfa16/2016/12/02/classical-conditioning/

### **1.2 Hebbian 法则：神经元之间的“联络加强”**

![](https://picx.zhimg.com/v2-e9e60df35e4b6247e327a32154f3c179_1440w.jpg)

神经系统与自监督学习的本质，都是通过拉近相似、推远无关，构建既稳定又可分的表征空间。https://www.nature.com/articles/s41593-023-01460-y

1949年，加拿大心理学家**Donald Hebb**在其著作《行为的组织》中提出了一个生理学假设：

> **“Neurons that fire together, wire together.”**
> 
> —— 同时激活的神经元，其连接将被加强。

这个后来被称为**Hebbian 学习法则**的理论，描述了一种**基于时间关联的突触可塑性**。简单说，如果在一段时间里神经元A经常激活神经元B，那么它们之间的突触连接会变得更强，从而在未来更容易一起激活。Hebb 理论的核心在于：“学习=连接权重的变化”。

这不仅为巴甫洛夫的行为实验提供了神经层面的解释，也成为后来的神经网络、感知机（Perceptron）和突触权重更新机制的生物启发源泉。这可以说是今天如火如荼的深度学习的起源。

### **1.3 在强化学习中的对应**

虽然强化学习（RL）最常与奖励(Reward)、策略(Policy)、值函数(Value Function)等数学概念相关联，但它的深层根基，正是来源于上述两种早期认知机制：

-   **巴甫洛夫的狗**启示我们，环境中的状态可以**预测未来的奖励；**
-   **Hebbian 学习**启示我们，大脑或网络可以通过“经验”**改变内部参数**以适应这种预测。

在 RL 框架中：

-   状态（如铃声）被编码为输入；
-   奖励（如食物）是目标信号；
-   模型会通过某种形式的权重更新（Hebb 或反向传播）来调整状态与奖励之间的映射。

即便是在现代深度强化学习中，这一思想依然保留。例如，在深度强化学习DQN 中，神经网络通过梯度下降来更新参数，本质上也是 Hebbian 学习的计算机实现。

### **1.4 小结**

这一阶段的智能体具备的能力非常有限：

-   感知输入：能识别环境中的简单信号
-   奖励关联：状态与奖励之间的**被动关联学习**，建立 **状态** 与**奖励**的静态联系
-   智能体无需决策，仅通过**环境(Environment)信号塑造行为反应，** 神经元连接强度会改变（权重更新）
-   是构建后续**价值函数**的神经基础
-   智能体没有主动选择的能力，也尚未形成主动行为策略。

![|500](https://pic3.zhimg.com/v2-93edf34b81346dc9d09c1d86a4b6f752_1440w.jpg)

https://www.structural-learning.com/post/ivan-pavlovs-theory

___

## **第2层：桑代克的猫与斯金纳的老鼠 —— 主动行为与试错探索（哺乳脑）**

![|500](https://pic2.zhimg.com/v2-d087a70cf859434f67f8f59f9f5fac57_1440w.jpg)

https://terriermandotcom.blogspot.com/2012/05/thorndikes-cat-box.html

### **2.1 桑代克的猫：尝试—错误机制（Trial and Error）**

20世纪初，美国心理学家**爱德华·桑代克（Edward Thorndike）提出了“效果律（Law of Effect）”，他通过一系列著名的“猫逃出迷箱（Puzzle Box）”实验发现：猫在被关进一个装置中时，会不断地抓挠、乱动，直到偶然**触碰开关而成功逃脱。多次重复之后，猫逃脱所需的时间明显缩短，并越来越快速地做出“正确动作”。

这表明：

> 行为不是一次性学会的，而是在反复尝试错误的过程中，通过正向结果“筛选”出来的。

这种“**Trial and Error（尝试-错误）**”学习机制，强调行为与后果之间的因果联系，是强化学习中最早出现的**主动行为调整模式**。智能体不再只是被动响应，而是开始基于结果优化自己的行为。

### **2.2 斯金纳的老鼠：行为塑造与强化机制**

![|500](https://pic4.zhimg.com/v2-c263c20dc9da94d254af3130e17abd41_1440w.jpg)

到了20世纪中叶，**B.F. 斯金纳（B.F. Skinner）** 在“**操作性条件作用（Operant Conditioning）**”理论中，进一步发展了行为主义。他设计了著名的“**斯金纳箱（Skinner Box）**”：一个封闭的实验装置，老鼠被放置其中，环境中设置了一个可按压的杠杆，按下后会触发食物投放器。

最初，老鼠在箱中四处探索，偶然碰到杠杆得到食物奖励。经过多次试验后，它便会**主动、有目的地按压杠杆以获取奖励**，这揭示了“行为的后果会影响未来的行为概率”这一核心规律，也就是**强化学习中“行动—奖励”的基本逻辑单元**。

### **2.2.1 行为塑造（Shaping）：让目标行为“逐步浮现”**

真正体现斯金纳理论深度的，是他提出的“**行为塑造（Shaping）**”策略。这个概念强调，复杂行为不应期待一次性学会，而应通过**阶段性地强化逐步接近目标的行为**，让智能体沿着正确轨迹“攀升”。

在实验中，这一过程通常如下：

-   **第一阶段：**老鼠只要靠近杠杆，就给予食物奖励；
    
-   **第二阶段：**老鼠必须抬起前肢靠近杠杆，才能得到奖励；
    
-   **第三阶段：**只有当老鼠真正按下杠杆，才给予奖励。

**通过逐步强化更接近目标行为的动作，让智能体朝目标逐渐靠近。**这种逐步引导式的策略，与今天深度强化学习中的**稀疏奖励**（Sparse Reward）设计**和探索—利用权衡**（Exploration vs Exploitation）思想不谋而合。这种**分层奖励机制**，使得智能体在稀疏或难以探索的任务中也能逐渐学习复杂行为。这一思想后来被广泛应用于**Reward Shaping、Curriculum Learning（课程式学习）**，以及分层强化学习（Hierarchical RL）等现代方法中。

![|500](https://pic3.zhimg.com/v2-70dea2c4ac1a360b9807879fe6c49814_1440w.jpg)

https://commercebizhub.com/learning-theories-in-organizational-behavior/

### **2.2.2 负强化（Negative Reward）：抑制不期望的行为**

斯金纳箱中还有另一个常见设置：**双按钮机制**。一个按钮带来奖励，另一个按钮触发电击、蜂鸣等不良刺激。

在这样的实验中，老鼠逐渐学会**避开负面刺激源**，这种基于“惩罚”信号的学习过程被称为**负强化（Negative Reinforcement）**，或更广义上的**惩罚学习（Punishment Learning）**。它不仅用于强化正确行为，也用于**抑制错误策略**，体现了“奖励最大化 + 惩罚最小化”的联合目标。

**2.3 在强化学习中的对应**

这一层级的智能体学习方式，已从被动响应转向主动试错。其在强化学习中的映射：

-   **试错** **Trial & Error：**Agent（老鼠/鸽子 ） 会探索环境，尝试不同动作（Action），并根据回报调整行为策略；
-   **操作性条件作用 （Operant Conditioning）**<sup data-text="操作性条件作用（Operant Conditioning），也被称为“工具性条件作用”，是行为心理学家 B.F. 斯金纳（B.F. Skinner） 提出的学习机制。它与巴甫洛夫的经典条件作用（Classical Conditioning）不同，强调的是：  个体通过主动行为与环境互动，行为的结果会影响未来行为的发生概率。" data-url="" data-numero="1" data-draft-node="inline" data-draft-type="reference" data-tooltip="操作性条件作用（Operant Conditioning），也被称为“工具性条件作用”，是行为心理学家 B.F. 斯金纳（B.F. Skinner） 提出的学习机制。它与巴甫洛夫的经典条件作用（Classical Conditioning）不同，强调的是：  个体通过主动行为与环境互动，行为的结果会影响未来行为的发生概率。" data-tooltip-richtext="1" data-tooltip-preset="white" data-tooltip-classname="ztext-reference-tooltip"><a id="ref_1_0" href="https://zhuanlan.zhihu.com/p/1932009376987717993#ref_1" data-reference-link="true" aria-labelledby="ref_1">[1]</a></sup>：对应现代 RL 中的“策略学习“（Policy Learning）思想，即通过试探行为与奖励之间的关系，**优化行为概率分布**；
-   **Shaping：**通过设计阶段性奖励或分层任务，逐步引导学习过程，避免陷入稀疏奖励困境。

经典的 REINFORCE 算法就源自于这一思想：智能体尝试多种行为，根据行为带来的回报大小，**提升带来好回报的动作概率**。

### **2.4 小结**

这一阶段的智能体，从被动反应者变成主动行为者，从“刺激—反应”走向“行为—结果“，已具有**探索行为**与**行为后果评估**的能力, 使智能体首次具备了“**主动探索—结果反馈—行为更新**”的基本闭环，为强化学习提供了“试错+反馈”这一最基本学习机制。在巴普洛夫的狗的环境感知、奖励关联的基础上，开始输出行为，并会根据奖励结果强化某些行为（行为概率调整），实现的策略优化，具有初步的决策能力，但尚不具备精确策略建模能力。

本层代表了智能体从“被动反射”迈向“主动行为”的关键一步，标志着从神经反应走向决策策略的过渡。

___

## **第3**层 ：**[托尔曼的迷宫老鼠](https://zhida.zhihu.com/search?content_id=260802786&content_type=Article&match_order=1&q=%E6%89%98%E5%B0%94%E6%9B%BC%E7%9A%84%E8%BF%B7%E5%AE%AB%E8%80%81%E9%BC%A0&zhida_source=entity)与[认知地图](https://zhida.zhihu.com/search?content_id=260802786&content_type=Article&match_order=1&q=%E8%AE%A4%E7%9F%A5%E5%9C%B0%E5%9B%BE&zhida_source=entity) —— 探索行为与内在表征的萌芽（**大脑皮层**）**

在前两个境界中，智能体仍然是一个典型的“反应者”——它的行为完全依赖于当下的刺激和奖励，没有对未来的预期，也没有对世界结构的理解。然而，到了第三境界，一位名叫爱德华·托尔曼（Edward Tolman 1886-1959）的美国心理学家，让世界第一次见识到了“智能体可以为将来而学习”。

![](https://pic3.zhimg.com/v2-005e47786e526561eadf98ba2c39db44_1440w.jpg)

托尔曼迷宫老鼠 https://pressbooks.online.ucf.edu/lumenpsychology/chapter/reading-cognition-and-latent-learning/

### **3.1 背离行为主义的“异类实验”**

在20世纪40年代，当斯金纳的“操作性条件作用”理论大行其道之时，托尔曼却提出了一个极具颠覆性的观点：**动物不是被动地对刺激作出反应，而是会主动形成对环境的“认知地图”**。托尔曼的经典实验如下，他让三组老鼠分别在迷宫中进行任务：

-   **第一组：**每次走到终点就获得食物奖励；
-   **第二组：**从不提供奖励；
-   **第三组：**最初没有奖励，但在第11天开始提供奖励。

> 实验结果显示：**第三组老鼠在第11天开始获得奖励后，几乎瞬间就达到了第一组的表现水平**，甚至更快。这表明：老鼠在前10天虽然没有外部奖励，但它们并非“什么也没学到”——而是在无奖励条件下**主动探索并构建了环境的内部表示**，一旦有了动机（奖励），便迅速发挥出来。

### **3.2 从“Trial & Error”到“Latent Learning”**

这项实验挑战了传统行为主义的两个核心假设：1）学习必须有奖励驱动；2）学习是通过“尝试—错误”逐步积累的。

托尔曼的研究表明：**动物可以在没有奖励的情况下进行“潜在学习”（Latent Learning）**，并在之后的适当时机中将其释放。这种能力的存在，预示着智能体不再只是条件反射的集合，而是具备了一种更复杂的内在建模机制。这就是后人所称的**认知地图（Cognitive Map）**——对空间结构和环境状态之间关系的内部建模。

### **3.3 在现代强化学习中的体现**

托尔曼的观点，在今天的强化学习系统中找到了多个重要对应：

-   **模型建构（Model-Based RL）：**智能体不再只依赖“值函数”或“策略网络”来决定行动，而是显式学习环境状态转移和奖励模型，**即“如果我这么做，会发生什么”**，用数学公式表达就是两个概率分布：1）状态转移模型： $p(s’ \mid s, a)$ ，表示在当前状态 $s$ 下采取动作 $a$ 后转移到下一个状态 $s’$ 的概率；2）奖励模型： $p(r \mid s, a)$ ，表示在状态 $s$ 下采取动作 $a$ 后获得奖励 $r$ 的概率。
    
-   **探索行为（Intrinsic Motivation）：**鼓励智能体在没有外部奖励的情况下主动探索，比如通过奖励“信息增益”、“预测误差”或“访问新颖状态”等内部激励机制产生探索行为。最近一年RL领域出现了不少这个方向的文章。
    
-   **表征学习（Representation Learning）：**通过神经网络自动提取状态的低维嵌入表示，这可以被视为深度强化学习中的“认知地图压缩版本”。
    
-   **Zero-Shot / Few-Shot 迁移学习：**一旦构建了认知地图，智能体就可以在新的任务或目标位置变化时迅速适应（类似迷宫终点变动后的重新路径选择）。

### **3.4 从“反应者”到“建模者”的飞跃**

在前两个境界中，智能体仍然是一个典型的“反应者”——它的行为完全依赖于当下的刺激和奖励，没有对未来的预期，也没有对世界结构的理解。然而，托尔曼的迷宫老鼠揭示了一个惊人的转折：**智能体可以在没有奖励的情况下，自主探索环境，构建内在的“认知地图”，并在未来使用这些地图做出更高效的决策**。

![](https://pic2.zhimg.com/v2-516a54418c75410dedb25f7d959fe3b3_1440w.jpg)

这种能力，标志着智能体从“刺激—反应”的生存机制，跃迁到了“建模—规划”的认知机制。智能，不再是被动地回应过去经验，而是主动地预演未来可能的情境。**托尔曼的实验，为今天的“世界模型”（World Model）思想提供了最早的生物学雏形**。这一思想如今被 Yann LeCun 等人视为迈向通用人工智能（AGI）的核心路径之一，也日益成为神经网络与强化学习研究的关键方向。

### **3.5 从认知地图到控制论：反馈与目标导向行为**

托尔曼提出“认知地图”的理念，打破了行为主义将动物视为“刺激-反应”机器的传统，指出它们具备建立内部世界模型的能力。这一观念也为后来的强化学习打下了基础——智能体不再仅靠外部奖惩塑造行为，而是可以**预判环境变化、规划未来路径**。这与20世纪40年代兴起的**控制论（Cybernetics）思想不谋而合。控制论由诺伯特·维纳（Norbert Wiener）提出，强调通过反馈机制实现系统的自我调节与目标控制**。在一个典型的控制系统中（如恒温器、导弹制导），系统会感知自身状态与目标之间的偏差，通过调整行为不断接近目标，从而实现稳定控制。

![](https://pica.zhimg.com/v2-75d4a59f62d676d0e89fb8d51f4f961c_1440w.jpg)

https://www.mathworks.com/help/reinforcement-learning/ug/reinforcement-learning-for-control-systems-applications.html

这一理念正好映射到强化学习智能体中：

-   **当前状态** $S$ 对应系统观测；
-   **策略** $\pi$ 对应控制器的调节机制；
-   **价值函数或奖励** $R$ 是衡量偏差的目标信号；
-   **行为** $A$ **的改变** 就是反馈控制的动作输出。

在这个意义上，我们可以把现代强化学习看作是控制论在人工智能领域的延伸：从动物行为的实验室走向**具备目标导向、自我调整能力的智能体系统**，而认知地图、世界模型、Model-Based RL 正是这条路径上的里程碑。

> “控制的本质，是对未来的预测。” —— 诺伯特·维纳

强化学习与控制论，虽然诞生于不同学科，但殊途同归，都在试图回答同一个问题：**如何让系统自主地行动，以实现长期目标。**

P.S. 控制论是AI诞生的重要来源之一。

### 3.6 小结

**至此，行为心理学的探索暂告一段落。**我们从巴甫洛夫的狗，到斯金纳的老鼠，再到托尔曼的迷宫，逐步见证了“学习”从被动反应到主动探索、再到建模预演的认知跃迁。它们不仅为强化学习打下了深刻的生物与心理学基础，也揭示了智能体从低级刺激反应到高级规划推理的进化轨迹。好了，**热身结束！**接下来，我们将**正式进入“强化学习”的技术世界。**但别担心，这不是一本公式堆砌的教科书文章，我们不走寻常路。本文的目标是把这些复杂的技术思想原理，以尽量**好懂**的方式讲述，特别是为新手读者提供一条“认知友好”的学习路径，把你一步步引向现代强化学习的核心地带，直到触碰 AGI 的边界。

请系好安全带，准备进入第二部分--**RL基石篇章。**

我们将从**第四境界**启程：**从直觉到策略，从试错（Trial & Error）到策略梯度（Policy Gradient）**。与绝大多数教材**先讲价值再讲策略**的经典书籍不同，这里我们选择**直击核心目标：**如何直接优化策略本身，以最大化未来回报。

___

## 第二部分：强化学习基石

在前三个境界中，智能体的行为逐步进化：从条件反射式的被动响应（巴甫洛夫），到行为被奖励塑造（斯金纳），再到主动探索环境、形成认知地图（托尔曼）。但这一切，仍然是经验驱动下的“试错学习”。智能体或许知道“什么行为有效”，**却并不理解如何系统地优化自己的行为策略**。

而从本境界开始，**真正的强化学习算法**首次登场 —— 一次意义深远的**范式飞跃**就此发生。我们为智能体引入了**可微分的目标函数（Objective Function），** 它就像一枚内在的指南针，指引行为的优化方向。策略不再依赖盲目的试错，而是通过**梯度上升**，沿着最大化长期期望奖励的方向不断前进。这一机制，正是基于策略的方法（Policy-based Methods）的核心思想：从“凭经验尝试”走向“按目标优化”，从心理学启发走入数学可导的算法世界。

![](https://pic1.zhimg.com/v2-b11e51a24294dc36a67906865b126c58_1440w.jpg)

https://kitrum.com/blog/reinforcement-learning-for-business-real-life-examples/

**问题设定与与符号约定**

强化学习（RL）描述的是一个智能体（agent）在环境（environment）中**通过与环境交互以最大化长期奖励**的问题。该问题通常被建模为一个马尔可夫决策过程（Markov Decision Process, MDP），用一个五元组表示： $\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, R, \gamma)$ ，其中：

-   $\mathcal{S}$ ：**状态空间**（State space），每个时刻的环境状态记为 $s \in \mathcal{S}$ ；
-   $\mathcal{A}$ ：**动作空间**（Action space），智能体在状态 $s$ 下可选的动作为 $a \in \mathcal{A}$ ；
-   $\mathcal{P}(s’\mid s, a)$ ：**状态转移概率**（Transition function），表示在执行动作 $a$ 后从状态 $s$ 转移到下一个状态 $s’$ 的概率；
-   $\mathcal{R}(r｜s, a)$ ：**奖励分布**（Reward distribution），表示在状态 $s$ 下执行动作 $a$ 所获得的奖励 $r$ 的概率分布；
-   $\gamma \in [0, 1)$ ：**折扣因子**（Discount factor），表示未来奖励的衰减程度，越接近 1 越重视长期回报。

在该框架下，**状态** $s_t$ 是智能体对环境当前情境的完整描述，可以是图像帧（如游戏画面）、数值向量（如股票价格序列）或多模态传感器输入等。而**动作** $a_t$ 是智能体在该状态下可做的决策，例如“向左移动”、“购买资产”或“刹车”等，每个动作都可能导致环境发生变化并反馈一个即时奖励 $r_t$ ，从而进入下一个状态 $s_{t+1}$ 。

此外，在部分可观测环境（POMDP）中，智能体无法直接观察到完整状态，而是通过**观察值**（observation） $o_t \sim P(o_t | s_t)$ 来间接感知当前状态。这时，状态成为一个隐藏变量，智能体必须依赖观察序列进行策略优化。例如，在自动驾驶任务中，摄像头图像只能提供对真实交通状态的局部观测，策略学习依赖于从这些观察中推理潜在环境状态。

___

**策略与回报**

-   **策略：**智能体的行为策略记作 $\pi(a \mid s)$ ，表示在状态 $s$ 下采取动作 $a$ 的概率。对于确定性策略，也可以写作 $\pi(s) = a$ 。
-   **Return：**从某一时刻 $t$ 起始的总奖励，记作： $G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^3 r_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k r_{t+k}$ ，它是智能体在某个轨迹上，未来获得奖励的总和(考虑折扣)。

## **第4**层：**基于策略的RL —— 从“试错”到“期望提升”**

不同于大多数教科书从基于价值的方法讲起、尚未深入便让许多人望而却步，本文选择从**基于策略的方法**切入。如果你了解梯度下降，那理解它就如呼吸般自然。虽然最终目标依然是最大化累计回报 $G_t$ ，但基于策略的方法更为直接、现代且实用。

### **4.1 策略是什么？**

所谓策略（Policy），就是智能体在每一个状态 $s$ 下采取某个动作 $a$ 的“倾向性”或“分布”，通常记作： $\pi(a|s)$ 。它可以是**确定性策略（Deterministic Policy）**，也可以是**随机策略（Stochastic Policy）**，如 softmax 分布带温度控制的策略。

**策略**可以被视为一个函数，它接收**状态** $s$ 作为**输入**，**输出动作** $a$ 或对应的**动作分布**。在这一境界，智能体的目标不是被动地建模环境或评估状态价值，而是**直接学习如何行动** —— 即通过优化策略函数，使其在与环境的交互中**获得更高的长期奖励**。

![](https://pic2.zhimg.com/v2-56f73c77ab6a0122e6d88271e20bfafd_1440w.jpg)

https://gibberblot.github.io/rl-notes/single-agent/MDPs.html

### **4.2 从试错到“期望提升”**

相比早期的“盲目试错”，这里的学习过程更像是“**通过反复交互，试图让未来更好**”。我们定义一个目标函数：

$J(\theta) = \mathbb{E}_{\pi_\theta} [G_t]= \mathbb{E}_{\pi_\theta} \left[ \sum_{t=0}^\infty \gamma^t r_t \right]$

其中 $\theta$ 是策略 $\pi$ 的参数， $\gamma \in[0,1]$ 是折扣因子， $r_t$ 是在时刻 $t$ 的reward。该目标函数就是我们希望最大化的期望累积奖励（Expected Return）。也可以加个负号，将最大化Return转变了最小化损失，就可以采用熟悉的梯度下降方法来解了。目前我们还求（总回报）最大值，仍需采用梯度上升。

智能体不再只通过“看看哪里有奖励”来调整行为，而是系统性地优化这个函数，使得策略参数逐渐趋向让行为更优。**这就是从“经验主义试错”进化到“梯度驱动优化”的跃迁。**

**引入策略函数：智能体首次拥有了“行为蓝图”**

在前两个境界，智能体主要通过试错（Trial and Error）来学习——行为成功就加强连接或重复尝试，失败就避免。这虽然朴素有效，但缺乏一种明确的、可以系统优化的“行为表达形式”。

### **4.3 策略梯度（Policy Gradient）：策略优化的利剑**

![](https://pic2.zhimg.com/v2-52c089991abbdfb38359773a739c8299_1440w.jpg)

https://www.sefidian.com/2021/03/01/policy-g/

**Policy Gradient 方法**是策略优化中的核心技术。它的基本思想是：通过采样**状态-动作轨迹（** $\tau$ **），** 并利用回报信号来估计目标函数对策略参数的梯度，以此推动策略朝着期望回报更高的方向改进。具体形式为： $\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi\theta} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot G_t \right]$ 。这个梯度估计过程依赖于后文将介绍的 Monte Carlo 技术：我们通过对轨迹 $\tau$ 的采样，得到实际回报 $R(\tau)$ ，进而近似目标函数中的期望项 $G_t$ 。这就类似于使用样本均值 $\bar{x}$ 来估计总体均值 $\mu$ ——虽然我们无法穷尽所有可能的轨迹，但通过多次采样可以获得对期望的无偏估计。如希望了解策略梯度的具体公式推导，请见：

这个看似简单的公式背后，蕴含着一个**划时代的思想：**我们可以通过“**梯度上升**”来主动改进智能体的行为倾向，使其**更倾向于做出带来高回报的决策**。这种方法的最大优势在于：**无需显式建模环境**。即便面对一个**不可微分、不可逆，甚至完全未知的环境**，我们依然可以基于交互数据来改进策略。这种“黑箱优化”能力，让策略梯度方法具备极强的适应性和泛化能力。此外，它也**不需要显式计算每个状态或动作的价值**，就像即使没有地图，也能凭借“上坡方向”不断爬山 —— 这就是策略梯度方法的直觉之美。让我们循着历史的脉络，先看下首个策略梯度的方法。

![|625](https://pic4.zhimg.com/v2-9425ca67ac935d9e234c201db6ec9f03_1440w.jpg)

https://spinningup.openai.com/en/latest/spinningup/rl\_intro3.html

### **4.3.1 REINFORCE：策略梯度开山之作**

1992 年，美国东北大学教授Ronald J. Williams (1945-2024) 提出了著名的 **REINFORCE 算法**，它标志着策略优化的正式诞生，他也是反向传播的共同作者。这是第一种实现“策略梯度”（Policy Gradient）思想的强化学习方法，其核心思想非常直白却极具启发性：

$θ ← θ + α · ∇_θ log π_θ(a|s) ·  G_t$

直观来说，这个公式意味着：

> 如果某个行为最终带来了高累积回报 $G_t$ ，那么就应该**增加采取该行为的概率**。在这个过程中，回报 $G_t$ 充当了**权重信号**。衡量该行为“值得被强化”的程度，也正体现了“强化学习”一词的核心含义——**强化好的行为，抑制坏的行为**。

这像不像现实生活中“你更干，就多干点”？

REINFORCE 正是将这种“经验塑造”的思想转化为了**可微分的目标函数**，使得智能体可以直接对策略进行梯度更新，开启了强化学习的“可优化时代”。REINFORCE 支持两种策略表示方式：

-   **表格形式**（Tabular）：适用于状态和动作离散可枚举的简单环境；
-   **参数化形式**（Parametric）：如通过神经网络建模策略，输入状态，输出动作概率分布，从而适配更复杂、连续的决策任务。

本境界实现了从**基于结果调整行为的“经验反馈”**，跃迁到**通过策略函数事前预期的“行为倾向建模”**。不同于以往依赖环境回报反复修正行为的方式，基于策略的 Agent 可以在**没有显式状态值评估机制**的前提下，直接学习“**哪种行为在长期更可能带来奖励**”。这也意味着，我们终于可以摆脱那些“繁琐又间接”的基于价值的方法，不再死守 Bellman 方程，不必反复估计状态值或动作值，“**单刀直入**”，直奔核心目标：**优化行为策略本身**。毕竟，智能体的关键任务，不是先画好一张地图，而是尽快学会在复杂地形中活下来。你可以把 Agent 想象成一位被空投到敌后、手上只有一把武器的特种兵——在没有现成地图的情况下，他仍然必须迅速判断方向，占领高地，达成目标。这时再回头绘制环境模型，“远水救不了近火”。

![](https://picx.zhimg.com/v2-49706cbaa572f2e0bc9a60dde99034eb_1440w.jpg)

https://pylessons.com/Beyond-DQN

**从心理学视角：**这一阶段的智能体已经具备了“习得性、目标导向行为”的能力，不再只是应激反应，而是有内在行为偏好的决策者；**从机器学习角度：**这是强化学习首次引入了**梯度驱动的优化机制**，也是向现代深度学习体系靠拢的起点。

### **4.3.2 减少方差的智慧：Baseline 技术的引入**

![](https://pic4.zhimg.com/v2-155012dee3d0e65094411e86430c6a47_1440w.jpg)

https://link.springer.com/chapter/10.1007/978-1-4842-9606-6\_9

尽管 REINFORCE 提供了一个简单直接的策略优化路径，但它也存在一个严重的问题：**方差太大，学习不稳定**。在 REINFORCE 中，轨迹的总回报 R 被用于“奖励”或“惩罚”某个行为的 log-probability。但回报 R 本身波动极大，可能受到随机事件的剧烈影响，从而导致策略更新方向剧烈摆动。你可以想象：同样的行为，有时因为运气好得分高、有时因为环境变化得分低，这种“情绪化”的反馈显然不利于稳定学习。

为此，**Baseline** 技术应运而生。它的核心思想很简单——**减去一个“基线”值**，使得更新方向只取决于“当前行为是否优于平均水平”。我们将策略梯度更新公式从： $\theta \leftarrow \theta + \alpha \cdot \nabla_\theta \log \pi_\theta(a|s) \cdot G_t$

变为：

$\theta \leftarrow \theta + \alpha \cdot \nabla_\theta \log \pi_\theta(a|s) \cdot (G_t - b(s))$

其中 $b(s)$ 就是 **baseline**，最常见的选择是：**该状态下的平均回报**，也就是下一境界要介绍**值函数** $V(s_t)$ 。此时 $G_t - V(s_t)$ 被称为 **优势函数** （Advantage Function） $A_t$ ，代表了“当前动作比平均水平到底好多少？”。引入 baseline 带来了两大好处：

1.  **减小方差：**将策略更新集中在“真正优于平均水平”的行为上，提升学习稳定性。
2.  **保持无偏性：**虽然减去了 baseline，但不会引入系统性偏差（因为 baseline 与行为无关，不影响期望）。

![](https://pic2.zhimg.com/v2-67d3287442d80f6f773a1e5f90ad6bf1_1440w.jpg)

https://velog.io/@mmodestaa/HuggingFace-Deep-RL-Course-8.-Proximal-Policy-Optimization-PPO

正是这个小trick，使策略梯度方法从“概念验证PoC”走向了“可实用系统”的关键一步。如今，几乎所有主流策略梯度算法（包括 A2C、PPO、TRPO 等）都引入了 baseline。

### 4.4 小结

在前三个境界中，智能体的行为仍停留在基于经验的被动反应与探索层面：它“知道什么有效”，但并不“知道如何改进”。而第四境界带来了范式的跃迁——智能体首次拥有了**可微分的目标函数**，可以通过梯度优化系统性地提升策略。REINFORCE 算法揭开了**策略梯度方法**的序幕，让智能体从“反复试错”进化为“有方向地自我改进”。从此，学习不再只是结果的积累，更是学习过程本身的优化。下一境界，我们将迎来**值函数**的登场，走进强化学习的另一核心思想——**时序差分学习（TD Learning）**。

___

## **第5**层 ：**TD 学习与 Bootstrapping —— 时间差分的悄然革命**

在上一境界（第四层），我们初步实现了从试错到策略优化的跃迁。策略梯度方法如 REINFORCE，确实开启了用梯度优化策略的新时代，但它有一个显著的缺点：**它必须等到整条轨迹（Trajectory）结束之后，才能计算累积回报** $G_t$ ，进而更新策略。然而，现实世界中的回报往往是**延迟**的，智能体希望能在**回合（episode）尚未结束时就尽早修正自己的行为倾向**。更理想的情况是：**每一步都能更新一次策略（或者值函数）**。这种“边走边学”的愿望，正是**时间差分学习**（Temporal Difference Learning, TD）诞生的背景--这是一场更贴近生命体认知节奏的革命。

这就引出了一个类比：

-   **REINFORCE** 的策略更新，就像 **Batch Gradient Descent：**只有在经历完整轨迹后，才更新一次。不同点是：因为**未来回报的不确定性叠加，导致** $G_t$ **方差大**，梯度估计波动性强，收敛效率较低。
-   而 **TD Learning**（特别是 TD(0)）则更像是 **Stochastic Gradient Descent (SGD)：****每走一步，就更新一步，误差信号快速传播**，效率更高。
-   更进一步，**n-step TD** 则对应 **Mini-batch Gradient Descent** —— 在完整轨迹（Batch）与单步更新（SGD）之间，提供了一个可调节的折中方案，兼顾稳定性与响应速度。**TD(λ)** 则进一步将所有 n-step TD 的回报加权融合，利用指数衰减策略强调近期信息，同时保留长程回报信号，实现了偏差与方差的综合平衡。

正是由于 TD 学习能在不中断任务的前提下，逐步修正估计，它成为现代强化学习中**最具工程实用性**的基础技术之一，而且比策略梯度法更早被成功用于游戏智能体与机器人控制中。

![](https://pic2.zhimg.com/v2-c4a92c5fa68c33ca7be05c620ea06451_1440w.jpg)

https://people.stfx.ca/jdelamer/courses/csci-531/lectures/rl/Temporal-difference%20learning/td-learning.html

### 5.1 **未来的影子：从策略角度理解时间差分**

REINFORCE 只能在轨迹终点总结“整场行动的得失”，再反向影响整条路径，**缺乏沿途的“早期信号”。**这就像一个士兵必须打完整场仗，回营后才能总结得失；而时间差分学习（TD Learning），则让他可以边打边学，每走一步都获得“即时反馈” —— 不再非得靠终点打分，而是让每一步都蕴含对未来的预期。TD 学习带来的思想转变是：

> “我不需要知道全部未来，我只需知道：下一步的大致方向。”

$\theta \leftarrow \theta + \alpha \cdot \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot \left(G_t - V_t(s_t)\right)$ , 其中 $V_t(s_t)$ 表示状态 $s_t$ 的价值函数，作为 baseline 引入，用于降低策略梯度的方差而不引入偏差。基于此，Agent 不再依赖轨迹终点给出评价，而是在走的每一步中，就“**感知下一步的未来**”。

如果说 **REINFORCE 是一种“经验反馈”（retrospective）**—— 只能在整条轨迹结束后，回顾性地调整策略；那么 **时间差分学习（TD Learning）则代表“预期引导”（prospective）** —— 它利用对未来的估计，在当前就开始修正行为。这种方法 **摆脱了对完整轨迹回报的依赖**，转而使用**未来价值的预测**（Bootstrapping）来引导当前更新。由此带来两大优势：

-   **更快的收敛速度**，尤其是在训练初期；
-   **更稳定的更新过程**，显著减少方差；

这是一种真正意义上的“范式变革”：智能体从**后验总结经验**，进化为**前瞻性地预估未来并调整行为，** 强化学习向更接近生命体认知节奏的方向迈进。至于如何估计 $V_t$ ，涉及到Bellman方程，下面将马上介绍，也可参考以下专题文章：

### **5.2** TD(0) ：最简形式TD，**每步一调**

**值函数的定义：** 值函数（Value Function）是强化学习中**最核心的表征之一**。它回答了一个关键问题：

> “如果我从某个状态 $s$ 出发，遵循当前策略 $\pi$ 行动，未来预计能拿到多少回报？”

我们通常记作： $V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \,\middle|\, s_0 = s \right]$ 。也就是说，**值函数表示在当前策略** $\pi$ 下，从状态 $s$ 出发，未来所能获得的**累计奖励的期望值**。它并不依赖未来真实发生了什么——只要能预测**预期的长期回报**即可。这种对未来的“评估能力”就是强化学习中最关键的内部模型之一。

![](https://pic4.zhimg.com/v2-f7fab2cf8d011e7152f8e779e85905af_1440w.jpg)

https://www.lancaster.ac.uk/stor-i-student-sites/jordan-j-hood/2021/04/12/reinforcement-learning-temporal-difference-td-learning/

**TD(0)** 是**最简单**的**时间差分估计**（TD Learning)，智能体在每一步 $t$ 都会更新其对当前状态 $s_t$ 的值函数估计 $V(s_t)$ ，更新公式为： $V(s_t) \leftarrow V(s_t) + \alpha \cdot \underbrace{(r_t + \gamma V(s_{t+1}) - V(s_t))}_{\text{TD误差}}$ ，其中 $\alpha$ 是学习系数。这背后的逻辑是：目标值不是完整的累计回报，而是当前奖励 $r_t$ 加上下一状态的估值 $V(s_{t+1})$ ，也就是上一节提到的 $G_t$ 的递归形式（贝尔曼方程有点摁不住了，下个境界讨论，此处可以先联想梯度下降）；误差项（TD Error）反映了“当前估计”和“从未来看回来”的差距；因为用的是自己的估计值，所以这类方法被称为 **Bootstrapping**（自举，是因为它们**用自己的估计值来修正自身**——就像“左脚踩右脚往上跳”，虽然听起来有点悖论，但在数学上却是完全可行的。它不依赖真实的完整回报，而是**拿未来某个状态的估值当作当前目标的一部分**，以此逐步逼近真实的长期回报。

![](https://pic4.zhimg.com/v2-02fd1dbe952c64aeaf45bae387ac56cd_1440w.jpg)

https://aarl-ieee-nitk.github.io/reinforcement-learning,/value-based-learning,/bootstrapped-learning,/sampled-learning/2019/12/19/Temporal-Difference-Learning.html

如果说TD(0) ≈ SGD：每走一步更新一步，噪声大但更新快；REINFORCE ≈ Batch GD：全轨迹才更新一次，更新稳定但效率低（因为后面动作的随机性大，方差也高）；那有没有折衷如mini-batch GD的TD呢？

### **5.3 n-step TD：mini-batch梯度下降式的预期更新**

在 TD(0) 中，智能体每走一步就用下一步的估值 $V(s_{t+1})$ 来更新当前状态的估值，这种“边走边估”非常高效，却往往受限于短视的反馈。而 REINFORCE 这类方法虽然考虑完整回报 $R$ ，但每次更新都必须等待整个轨迹结束，样本效率低，方差大。

有没有一种方法能兼顾两者？既不像 TD(0) 那样“眼里只有一步”，也不必像 REINFORCE 那样“必须回顾全局”？这正是 **n-step TD** 诞生的动机。我们可以推理，n-step TD 是：

> “等 n 步之后再更新估值，用这 n 步内的真实奖励，加上第 n 步的预测，作为当前的目标”。

公式： $G_t^{(n)} = r_t + \gamma r_{t+1} + \cdots + \gamma^{n-1} r_{t+n-1} + \gamma^n V(s_{t+n})$

其中，前 $n$ 步是真实奖励，最后一项 $V(s_{t+n})$ 是引导更新的未来估值，因此，这种形式也被称作 **n-step return**。

![](https://pic3.zhimg.com/v2-c47a7505e4cbc78f8994c652c4fb2a7e_1440w.jpg)

https://towardsdatascience.com/introducing-n-step-temporal-difference-methods-7f7878b3441c/

### **5.4 Monte Carlo：等待终点的“整场总结”的** $\infty$\-step TD

在前几小节中，我们介绍了 TD 学习如何“边走边学”，借助对未来的预测（bootstrapping）实现更快速的学习。而与之形成鲜明对比的，是一种更“后验”的方法——**Monte Carlo 方法（MC）**。其核心思想可以用一句话概括：

> **“不预判未来，只在全部经历之后回顾总结。”**

具体操作如下：

-   智能体必须**完整执行一条轨迹**，直到 整个episode 结束；
-   然后基于实际经历，**直接计算每个状态或动作的真实回报：** $R_t = \sum_{k=t}^{T} \gamma^{k-t} r_k$
-   再用这些“最终成绩单”来更新策略或价值估计，不依赖任何对未来的预测。

这种方式就像一个士兵打完整场仗后回营复盘：只有在战争尘埃落定后，才能回顾每一步的得失，反思哪些决策值得保留、哪些需要修正。简单直观，却也带来了明显的缺点：必须等待整个 episode 结束才能得到反馈，学习节奏较慢，数据利用效率较低。

![](https://pic1.zhimg.com/v2-418955566f5c0757661c80383fab650e_1440w.jpg)

https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/mc-vs-td.mdx

“Monte Carlo”一词源于摩纳哥的著名赌城，因其与随机试验和概率密切相关，在机器学习中常用来泛指一类**通过随机采样进行估计**的方法。

> 在上一境界中我们提到的 REINFORCE 方法，其实正是一种基于 Monte Carlo 的策略梯度算法。它使用 episode 中采样得到的总回报 $R_t$ 来指导策略更新，不使用 bootstrapping，因此虽理论上无偏，但也面临高方差和训练不稳定的问题。

### **5.5 生物学意义**

1997 年，剑桥大学神经科学教授Wolfram Schultz 等人的实验发现：

> **灵长类动物脑多巴胺神经元的放电模式，与 TD 学习中的 prediction error 高度一致。**

具体表现为：

-   **意外获得奖励时**（比预期更好） → 多巴胺释放增加（正向 TD 误差）；
-   **奖励如预期而至** → 多巴胺神经元没有额外反应（TD 误差接近 0）；
-   **期待奖励却未获得** → 多巴胺释放减少（负向 TD 误差）；

**知足常乐(降低预期)，是一种对多巴胺系统的精准优化。** 你的快乐（多巴胺）并不来源于绝对的奖励值，而是源于**实际得到的奖励与预期之间的差值**。从强化学习的视角看，这正是所谓的“时间差分误差”（TD error）：

$\delta_t = r_t + \gamma[V(s_{t+1}) - V(s_t)]$

多巴胺的释放，正是对这个差值的响应(RPE)。如果你对未来的期望很低，即使获得平凡的结果，也能带来正向的惊喜，释放更多多巴胺，让你感到愉悦。反之，期望太高，即使结果不差，也可能因“低于预期”而失落。

所以**真正的幸福，不在于你得到了什么，而在于你得到了“超过预期的什么”。** 这正是“知足常乐”的神经科学与计算认知基础。

![](https://pic4.zhimg.com/v2-1cc92e1f2bbfa15b3f4d00edc786ba03_1440w.jpg)

https://www.pnas.org/doi/10.1073/pnas.2316658121

## **第6**层：**基于价值的RL —— 从评估到决策的价值哲学**

在前几层境界中，我们见证了智能体从条件反射的初级反应，进化为具备策略函数、能够通过梯度优化实现“自我提升”的智能体。然而，这些策略优化方法虽然高效，却更像是“摸着石头过河” —— 它们缺乏对环境结构的深入建模，也未构建出系统化的世界观。

本层，我们引入强化学习领域的核心支柱之一：**贝尔曼方程（Bellman Equation）** —— 一个可递归、具备“预见未来”能力的结构化公式。与前述策略直接优化不同，贝尔曼方程的核心思想是：**当前状态下的最优价值，来源于当前收益与未来最优价值的递归组合**。这意味着，智能体不再只是“在当下做出合理选择”，而是开始尝试“在脑海中绘制未来的地图”，以评估和规划最优行为。基于价值函数的方法，正是通过不断逼近贝尔曼方程的解，来学习状态或动作的长期价值（Value），进而引导策略更新。通过这种方式，智能体不仅“知道当下该做什么”，更“知道做了之后会发生什么” —— 决策的基础，从即时反馈跃升为**由预测驱动的长远价值评估**。-- 这就像游戏中突然**解锁了整张地图**，它终于不再在黑雾中摸索，而是开始真正**面向未来做决策（**理想情况下哈**）**。

基于价值的方法源于上世纪 1950 年代的**动态规划**(Dynamic Planning, DP)理论，在马尔可夫决策过程（MDP）等问题中取得了大量成功，因此在多数强化学习教材中（如 Sutton 等人的经典著作）被安排在第二章位置。然而，放眼当下，随着**深度强化学习与基于策略的无模型方法（Model-Free Policy Optimization）** 迅速发展，其在高维、复杂任务中的局限性逐渐显现，Value-based 方法的“性价比”有所下降。因此，我们将这一境界适当“后移”，作为基于策略的方法的补充，以更贴合现代强化学习的学习路径，是一种更“自然”的安排。

![](https://picx.zhimg.com/v2-3c21751b3666d13f9a924eec92d257c9_1440w.jpg)

https://link.springer.com/chapter/10.1007/978-981-19-0638-1\_4

### **6.1 价值函数**

![](https://pica.zhimg.com/v2-2dee39be2ead76575c8027e66718ef30_1440w.jpg)

https://deepanshut041.github.io/Reinforcement-Learning/notes/00\_Introduction\_to\_rl/

在基于价值（Value-based）的方法中，核心思路不再是直接优化策略本身，而是先学会“评估”每个状态或动作的价值。这种评估是通过**期望累计回报**来衡量的，通常分为两种形式：

-   **状态值函数（State Value Function）：** $v_\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^k r_{k} \,\big|\, S_0 = s \right]$ ，
    
    表示从状态 $s$ 出发，遵循策略 $\pi$ 所能获得的期望总回报。
    
-   **动作值函数（Action-Value Function）：** $q_\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{k=0}^\infty \gamma^k r_k \,\big|\, S_0 = s, A_0 = a \right]$
    
    表示在状态 $s$ 下采取动作 $a$ ，后续按照策略 $\pi$ 所能获得的期望总回报。

### 6.2 贝尔曼方程

![](https://pic1.zhimg.com/v2-a183142ecfe7fc432468ce3e805d9dfa_1440w.jpg)

Richard Bellman（1920-1984) http://www.breves-de-maths.fr/richard-bellman-et-la-programmation-dynamique/

**“贝尔曼（期望）方程”（Bellman Equation），又称“动态规划方程”（Dynamic Programming Equation），由动态规划之父理查德·贝尔曼（Richard Bellman）（1920-1984）于20世纪50年代提出，最初用于研究导弹控制中的最优决策问题。** 从历史上看，贝尔曼方程的提出时间甚至早于1956年达特茅斯会议（人工智能的公认起点），它是动态规划（Dynamic Programming）这一数学优化方法能够实现最优解的**必要条件**。该方程的核心思想是：

> **将某一时刻的决策问题的“最优值”，表示为当前选择所带来的即时收益，加上该选择引导出的子问题的最优值。**

![](https://pica.zhimg.com/v2-b59f998ebaf4b3de28f2b2de174ea07c_1440w.jpg)

https://huggingface.co/blog/deep-rl-q-part1

换言之，它将一个动态优化问题拆解为一系列更小、更易求解的子问题。这种分而治之的结构，被贝尔曼称为“最优性原理”（Principle of Optimality），即：“一个最优策略的任一子策略，亦必然是该子问题的最优策略”。

$\begin{align} v_\pi(s)  \overset{\cdot}{=} \mathbb{E}_\pi[G_t|S_t=s] &= \mathbb{E}_\pi \left[ R_{t+1} + \gamma G_{t+1} \,\middle|\, S_t = s \right] \\&= \sum_a \pi(a|s) \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v_\pi(s') \right] \end{align} \\\text{for all s in } \mathcal{S}$

最后一个等号，一开始理解起来有点难度，基本上就是一个全概率公式。可参考下图，和我下面推荐的贝尔曼方程专题文章进一步理解。

![](https://pica.zhimg.com/v2-d6fef4d455007d02c5271f825c4b5f36_1440w.jpg)

https://data-science-blog.com/blog/2022/03/01/four-propositions-to-dynamic-dynamic-programming-dynamic-programming-and-the-bellman-equation-part-two/

贝尔曼方程最初广泛应用于控制理论和应用数学等工程领域，随后在经济学中也发挥了深远影响，成为动态最优化分析中的基础工具。几乎所有能够借助最优控制理论（Optimal Control Theory）求解的问题，也都可以通过构造并分析相应的贝尔曼方程来解决。

需要指出的是，“贝尔曼方程”通常特指**离散时间**（discrete-time）优化问题中的动态规划方程。而在处理**连续时间**（continuous-time）最优化问题时，其对应形式则是一类偏微分方程，称为**汉密尔顿-雅可比-贝尔曼方程**（Hamilton–Jacobi–Bellman Equation，简称 HJB 方程）。HJB 方程是连续时间最优控制理论的核心工具，是贝尔曼原理在连续系统中的自然延伸。在强化学习中，贝尔曼方程是连接环境、策略与价值函数的桥梁，构成了**值迭代（Value Iteration）**、策略迭代（Policy Iteration）等一系列核心算法的理论基础，也为后续如 Q-learning、DQN 等方法的发展奠定了根基。

更详细的内容，请见：

P.S. 搞机器学习的人，大多听过“维度诅咒”（Curse of Dimensionality）这个词吧，也是Richard Bellman最早提出(coined)的。他还是普林斯顿史上最快获得博士学位的人（3个月完成学位，在服役之后）（二战期间，他在[Los Alamos](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Los_Alamos_National_Laboratory)的理论物理组工作，懂的人都懂这个地方的意义）。

### **6.3 贝尔曼最优方程（BOE）：理性行为的形式化表达**

在前一节中，我们介绍了贝尔曼方程，它刻画了给定策略 $\pi$ 下某状态的期望价值。然而，强化学习的最终目标通常不是评估某个既定策略，而是找到**最优策略** $\pi^*$ ，使得智能体在任何状态下都能获得尽可能高的长期收益。拿地图导航来举例，地图不仅要能计算出通过不同路线到达目的地的时间（贝尔曼方程），还要给出最短/最快路径。

这时，**贝尔曼最优方程（Bellman Optimality Equation，BOE）**登场了。它不再是条件性的“根据某个策略”，而是直接刻画了在所有策略中**表现最优的那个策略**的递推关系，同样分为状态和动作两个方程：

$v(s) = \max_{a} \sum_{s’, r} p(s’, r \mid s, a) \left[ r + \gamma v(s’) \right]\\  q(s, a) = \sum_{s’, r} p(s’, r \mid s, a) \left[ r + \gamma \max_{a’} q_(s’, a’) \right]$

这里的 $\max$ 操作，标志着“决策”的出现：它在所有可能的行动中，选择能够带来最大长期价值的那一个。这是强化学习中最关键的一步跃迁：**从“评估某种行为的价值”，进化为“选择最优行为”**。Q表示行动的质量(Quality)，后面要讲的Q-Learning和DQN的Q都是这里来的。

**最优策略的定义**

一旦得到了 $v(s)$ 或 $q(s, a)$ ，就可以使用贪婪策略（greedy policy）构造出最优策略 $\pi^*$ ： $\pi(s) = \arg\max_{a} q(s, a)$ 。这意味着，**智能体只要知道每个状态-动作对的“最优值”，便能推导出最优行为规则**。与前面的策略评估不同，贝尔曼最优方程**将“学习什么”和“做什么”紧密绑定在一起**，成为理性决策的形式化表达。

**一个隐含但深刻的哲学问题**

贝尔曼最优方程其实也提出了一个“自洽性”的悖论问题：我们如何知道某个策略是最优的？答案是：**我们并不知道，但我们可以构造出满足自洽方程的最优值函数，然后从中导出最优策略。**换句话说，**“先有价值，后有策略”，最优行为源于对未来价值的理解，而非经验或模仿。**

**小结**

贝尔曼最优方程不仅是强化学习中的核心公式，更是一种哲学宣言：

> **“真正的智能，不应只是对当前情况的应激反应，而是基于对未来的预测来做出当下的最优选择。”**

它为后续一系列最优策略学习方法（如值迭代、策略迭代、Q-learning 等）提供了统一的数学基础，也为通往 AGI 的道路，搭建了“理性决策”的桥梁。

### **6.4 策略迭代与值迭代：在期望与现实之间寻找平衡**

在强化学习的价值哲学中，值函数（Value Function）是通向最优策略的核心中介。正如前文所述，贝尔曼方程为我们提供了一种将“未来期望回报”进行递归建模的方式，使得智能体可以在不显式建模环境的情况下，评估每个状态-动作的长远收益。而基于值函数的最优性算法，正是试图在不直接学习策略函数的前提下，通过逼近最优值函数来反推出最优策略。本节我们将介绍三种基于贝尔曼思想的核心算法：**值迭代（Value Iteration）**、**策略迭代（Policy Iteration）**，以及二者之间折中（或者说是更General）的方案——**截断式策略迭代（Truncated Policy Iteration）**。

![](https://picx.zhimg.com/v2-0fa088895419a0781395354008647f8d_1440w.jpg)

https://data-science-blog.com/blog/2022/03/01/four-propositions-to-dynamic-dynamic-programming-dynamic-programming-and-the-bellman-equation-part-two/

### **6.4.1 值迭代**

**值迭代（Value Iteration， VI）** 是最早被提出的动态规划算法之一(Bellman 1957)，也被称为 **backward induction**（反向归纳）。核心思想是：不显式维护策略 $\pi(s)$ ，而是将策略优化过程**融合进值函数更新**。具体步骤：1）所有状态价值初始化，通常都为0； 2）每一步直接应用 Bellman 最优性方程： $v_{k+1}(s) = \max_a \sum_{s’, r} p(s’, r | s, a) \left[ r + \gamma v_k(s’) \right]$ 。通过不断应用该更新操作，值函数逐步逼近 $v^*(s)$ ，下图中可以看到值从终点（1.00）逐步传播到全局。3）当值收敛后，通过以下公式直接提取最优策略： $\pi(s) = \arg\max_a \sum_{s’, r} p(s’, r | s, a) \left[ r + \gamma v(s’) \right]$ 。

![](https://pica.zhimg.com/v2-0c66600f3e817f3034ea896e5052cb78_1440w.jpg)

https://main.p.lodz.pl/news.php?id=123

Value Iteration与马上要提到的策略迭代（6.4.2）相比，其方法可谓“激进直接”：它融合了**策略评估**与**策略改进**两个步骤（见下小节6.4.2），由于每轮都面向最优动作更新，值迭代往往具备更快的收敛速度，适合在状态空间有限、转移概率已知的情境下使用，是众多基于值的算法（如 Q-learning、DQN 等）的理论起点。

相比于第 4 层中的策略梯度方法可能因梯度信息的局部性而陷入次优解，价值迭代（Value Iteration, VI）具备全局性的更新特性。在每一轮迭代中，它会同时对所有状态的价值函数进行更新，使得整个策略空间在全局层面上趋向最优解。这种“同时扫过全部状态空间”的更新机制，使其更容易跳出局部最优。但与此同时，这种全局更新也带来了更高的计算成本，尤其在状态空间较大时，收敛速度较慢。此外，VI 并不显式存储和更新策略，而是通过值函数间接推导，使得策略的演化过程难以追踪。更重要的是，VI 将策略评估与策略改进融合在一步“贪婪最大化”中，缺乏可调的中间过程，不利于学习动态的控制与分析。

为了克服这些限制，一种更具可解释性和调节性的迭代方法应运而生 —— 策略迭代（Policy Iteration, PI）。

### 6.4.2 策略迭代

![](https://pic3.zhimg.com/v2-a4ad404117525779efc34494dfd6a71e_1440w.jpg)

https://levelup.gitconnected.com/fundamentals-of-reinforcement-learning-value-iteration-and-policy-iteration-with-tutorials-a7ad0049c84f

策略迭代（Policy Iteration, PI）是一种“双循环”结构的求解方法，由斯坦福大学 Ronald Howard (1934-2024) 于 1960 年提出。其核心思想非常直观：**先评估当前策略的表现，再基于评估结果改进策略，循环往复直到收敛**。这一过程可拆解为两个阶段：

1.  **策略评估（Policy Evaluation）**
    
    对当前策略 $\pi$ 固定不变，利用贝尔曼期望方程不断迭代逼近其值函数 $v_\pi(s)$
    
    $v_\pi(s) = \sum_a \pi(a|s) \sum_{s’, r} p(s’, r | s, a) [r + \gamma v_\pi(s’)]$ 。
2.  **策略改进（Policy Improvement）**
    
    基于当前值函数，更新策略使其在每个状态下选择最优动作： $\pi_{\text{new}}(s) = \arg\max_a \sum_{s’, r} p(s’, r | s, a) [r + \gamma v_\pi(s’)]$
    
    上述两个阶段交替执行，直到策略不再改变，即达到了策略收敛 $\pi^*$ 。可以从数学上证明：策略迭代在有限状态空间下总是收敛到最优策略。
    

![](https://pic3.zhimg.com/v2-c421a94389a17b9329397899095a025c_1440w.jpg)

在策略迭代（Policy Iteration）和值迭代（Value Iteration）之间，还有一种折衷方案，称为：

### **6.4.3 截断式策略迭代（Truncated Policy Iteration）**

**截断式策略迭代（Truncated Policy Iteration, TPI）** 是一种在策略迭代（Policy Iteration, PI）**与**价值迭代（Value Iteration, VI）之间折中权衡的中间路径，在实际强化学习任务中被广泛应用。换句话说，PI 与 VI 都可以被视为 TPI 的特例：当评估步数趋于无穷时，TPI 退化为标准的 PI；当评估步数为 1 时，则等价于 VI：

-   **k=1** 时，TPI 就变成了 **值迭代VI**
-   **k→∞** 时，TPI 就退化为 **策略迭代PI**
-   在实际任务中，**适度的 k 值（如 3~10）** 通常能显著提升效率而几乎不影响策略质量

这种结构上的连续性，使三者关系类似于梯度下降中的 Batch Gradient Descent、Stochastic Gradient Descent 与 Mini-batch Gradient Descent；也类似于上一层讨论的MC, TD(0)和n-step TD。

TPI 的核心思想是在每轮策略改进之前，仅进行有限步数的策略评估，从而在**计算效率**与**策略收敛性**之间取得良好平衡。它既继承了 VI 的高效性，又保留了 PI 中对策略收敛的更强控制，是强化学习中一种兼具实用性与理论价值的方法。

TPI的基本步骤如下：

1.  **初始化策略** $\pi_0$ ，初始化值函数 $v_0(s)$ 。
2.  **有限步策略评估（k-step Policy Evaluation）：**对当前策略 $\pi_k$ ，进行 $k$ 次贝尔曼期望更新，得到近似值函数 $\tilde{v}_{\pi_k}$ :$\tilde{v}_{\pi_k}^{(i+1)}(s) = \sum_a \pi_k(a|s) \sum_{s’, r} p(s’, r | s, a) \left[ r + \gamma \tilde{v}_{\pi_k}^{(i)}(s’) \right]$
3.  **策略改进（Policy Improvement）：**
    
    使用近似值函数 $\tilde{v}_{\pi_k}$ 更新策略_：_ $\pi_{k+1}(s) = \arg\max_a \sum_{s’, r} p(s’, r | s, a) \left[ r + \gamma \tilde{v}_{\pi_k}(s’) \right]$
1.  **收敛判定：** 若策略发生的变化低于设定的阈值，或达到最大轮数，则停止；否则返回步骤 2。

TPI 不仅在理论上构建了 PI 与 VI 的连续谱，也启发了现代 RL 中一系列有限更新 + 策略改进的近似方法，如：

-   **Generalized Policy Iteration（GPI）：** 一种理论框架或“范式”，描述了强化学习中策略评估与策略改进交替进行的基本过程，无论评估精度如何（精确或近似），只要两者交替进行，策略总会不断改进并趋于最优，PI、VI、TPI、Q-learning、SARSA 等都属于 GPI 范畴。
-   深度强化学习中的 **DQN（Q-learning）** 与 **A2C 等 Actor-Critic 方法**；
-   **TRPO / PPO** 中以 trust region 或近端更新限制策略变动，也是一种“截断改进”的思想延续。

小结：

TPI是一个**兼具稳定性与效率**的中间路径，不仅在理论上贯通了策略迭代PI与值迭代VI，也在现代RL的深度版本中广泛出现，体现出从离线到在线、从近似到精确的连续性思想。

小贴士：**离线学习**是在固定数据上训练策略，**在线学习**则是在与环境实时交互中边试边学。打个比方：**离线 = 看录像学打拳；在线 = 上台边打边学。**

### **6.5 SARSA 与 Q-Learning：on-policy vs. off-policy 的分野**

在值函数的RL方法下，如果对环境的模型不了解，就需要考虑使用SARSA 和 Q-Learning 是两种最为经典的 **无模型（model-free）** 算法，它们都通过更新状态-动作值函数 $Q(s, a)$ 来逐步逼近最优策略。然而，它们的更新路径却体现出两种核心思想的对立：**on-policy** 与 **off-policy**。这不仅是更新机制的技术区别，更是智能体“如何面对不确定未来”的哲学分野。

**6.5.1 SARSA：基于当前策略的价值更新（on-policy）**

![](https://pic4.zhimg.com/v2-98a3b786434b3445691f348f28886a09_1440w.jpg)

https://www.upgrad.com/tutorials/ai-ml/machine-learning-tutorial/sarsa-in-machine-learning/

SARSA 的名字来自于它更新所依赖的五个变量： $(S_t, A_t, R_t, S_{t+1}, A_{t+1})$ ，更新公式为： $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_t + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \right]$ ，这个公式是否似曾相识？是不是很像TD： $V(S_t) \leftarrow V(S_t) + \alpha \left[ R_t + \gamma V(S_{t+1}) - V(S_t) \right]$ ？这几乎就是 TD 的自然延伸，只不过从 **状态值** V 变成了 **动作值** Q，并且用到了“下一个动作”的信息，形成了一个完整的五元组（SARSA）。可以说：**SARSA 是 TD 学习在动作值上的 on-policy 版本，最后那一项可以理解为TD-Target。**从 V 到 Q，从状态到状态-动作对，从只看下一状态到考虑下一动作，这是一种逐步逼近真实期望的路径。

![](https://pic2.zhimg.com/v2-71abe8d1d182ff1872577febfe1a087f_1440w.jpg)

再进一步，SARSA是不是也像**梯度上升**公式： $\theta \leftarrow \theta + \alpha \cdot \nabla J(\theta)$ **，TD Target**对应 $\nabla J(\theta)$ ，是“目标函数的提升方向”。尽管它不是标准意义下的梯度，但确实遵循同样的思想：**一点一点逼近最优目标的估计值**。这背后统一的数学基础，都是基于**随机逼近理论**(Stochastic approximation)。

-   **Robbins-Monro 定理：** 在不确定性环境中，用“估计误差”驱动更新，逐步逼近期望值， $\theta_{n+1} = \theta_n - a_n \left( N(\theta_n) - \alpha \right)$ ，其中： $\theta_n$ ：第 n 次迭代的参数估计； $a_n$ ：学习率（步长），通常满足 $\sum a_n = \infty, \sum a_n^2 < \infty$ ； $N(\theta_n)$ ：带有噪声的观测函数，期望为目标函数（通俗理解：噪声抵销了，无偏估计）； $\alpha$ ：我们想逼近的期望值，在TD里面就是TD-target。该定理由美国数学家Herbert Robbins与Sutton Monro 1951年提出。

![](https://pic1.zhimg.com/v2-63af836314540c1ad7b47f1707ac4db8_1440w.jpg)

Robbins-Monro 定理，比牛顿法的确定性收敛速度稍慢，但其强大之处在于能够在噪声环境中依然保证收敛性，并无需显式计算导数或海森矩阵，更适合强化学习这类样本噪声大、环境反馈不稳定的问题。来源：https://cenac.perso.math.cnrs.fr/hdr/algo-stochastiques.html

-   **Dvoretzky 定理**（Dvoretzky’s Theorem）：揭示了高维空间中“局部逼近全局”的几何特性，为强化学习中估计收敛提供了理论支撑。Dvoretzky 定理由以色列数学家Aryeh(Arie)Dvoretzk（1916-2008）在1960年代早期出提出。

是否应该对每天使用的SGD的理论基础提出者有点感恩和崇敬之心，看看人家六七十年前的成就，奠定了今天RL乃至ML优化的重要基础。

因此，可以说：

> **SARSA 是 TD 学习在 动作值（** $Q$ 值**）层面的 on-policy 实现，它既继承了 TD 的增量更新风格，也具备某种“伪梯度上升”的结构。**

回到公式本身，由于下一个动作 $A_{t+1}$ 是基于当前策略（如下面将提到的 $\varepsilon-greedy$ ）所采样的，SARSA 实际上会将探索行为带来的“非最优后果”也纳入学习过程。换言之，它是在模拟 **“你实际上会怎么做”**，而不是理想情况下最优该怎么做”。这使得 SARSA 在训练过程中更加保守，具有更好的稳定性。

举个例子：想象你在训练一个走迷宫的机器人。SARSA 会基于机器人当前真实采用的策略来评估路径——即便它因为探索而走了弯路，也会纳入经验更新。这种学习方式更贴近现实，有助于在早期避免过度乐观或策略震荡。

SARSA 是一种 **on-policy** 学习方法，它更新动作价值 $Q(s_t, a_t)$ 时，使用的是当前行为策略下**实际选取的下一个动作** $a_{t+1}$ 。这一点虽然增强了学习的**稳定性和安全性**，尤其适用于需要规避风险的环境（如机器人控制），但也带来了两个明显的缺点：

1.  **学习过于保守：**它评估的是“我实际会怎么走”的结果，哪怕这不是最优路径，也会围绕它进行微调。
2.  **收敛慢、易陷入局部最优：**在策略初期，探索行为较多时，SARSA 也会“学习探索行为的后果”，从而影响最优策略的学习速度。

为了解决 SARSA 的这类问题，**Q-Learning** 被提出作为一种 **off-policy** 的强化学习方法。它跳出了当前策略的限制，每一步都朝着**最优策略的方向更新：**不管当前行为是否最优，它始终用下一个状态中“看起来最好的动作”来指导更新。这种“大胆假设、激进优化”的思路，使得 Q-Learning 更有机会快速学到**理论上最优的策略**，尤其适合在模型明确、训练目标清晰的环境中使用（如围棋、博弈 AI、游戏等）。SARSA 是“跟着自己脚步走”，而 Q-Learning 是“向着山顶努力走”，哪怕此刻脚下偏离了方向。

小贴士：**on-policy：**learn和act是一个策略；**off-policy：**learn和act的策略是两个不同的策略。

### **6.5.2 Q-Learning：最优策略驱动的更新（off-policy）**

![动图封面](https://pic4.zhimg.com/v2-92adfc3b777738a7ee312ab711ac2f4f_b.jpg)

https://github.com/imohitmayank/interactive\_q\_learning

Q-Learning 虽然同样以五元组为基础进行更新，但它与 SARSA 的核心区别在于**目标值的计算方式**。在 Q-Learning 中，动作价值函数的更新公式为： $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_t + \gamma \color{red}\max_{a’} Q(S_{t+1}, a’) - Q(S_t, A_t) \right]$ ，其中 $a'$ 下一状态中 **假设的最优动作。**与Sarsa的公式对比： $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_t + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \right]$

Sarsa（On-policy）更新公式的意思是：我更新当前这一步的 Q 值，是基于我实际上走了哪一步（ $A_{t+1}$ ）后的回报进行估计的。**行为策略 = 目标策略**（on-policy），强调对**当前实际行为后果的总结**，更新更保守。

Q-Learning（Off-policy）更新公式的意思是：我虽然实际走了某一步，但在更新时假设自己下次一定选**最优动作（** $a’$ **）**，来估算未来收益。**行为策略 ≠ 目标策略**（off-policy），强调对**理想最优行为的推演**，更激进的更新。

如果你是在设计一个实际任务，如果想要保守一点、减少因为过度乐观导致的风险：**用 SARSA；**想要快点学出最优策略，能承受一定的不稳定性：**用 Q-Learning。**Q-Learning和SARSA，分别是理想主义者（总假设下一步是是最优的完美行为）和现实主义者（承认自己会犯错）的代名词。

**Q-Learning 虽然在小规模离散状态空间中非常有效，但在面对大规模或连续空间问题时暴露出一些关键缺陷，**比如：Q值通过查表方式保存，在连续和高维情况下，表变得很大，容易导致 “维度灾难”；表格型方法不能对未见**过的状态**进行推理；学习过程的不稳定与过估计现象。为了解决这些问题，DeepMind 在 2015 年提出了 Deep Q-Network（DQN），开启了深度强化学习的新时代。

**Q-Learning** 是由 **Christopher J.C.H. Watkins** 于 **1989 年**在其博士论文中首次提出的，为后续的 **深度 Q 网络（DQN）** 奠定了基础，是强化学习史上的一个重要里程。

### **6.5.3 DQN：从查表到函数逼近，深度强化学习的起点**

![动图封面](https://pic1.zhimg.com/v2-fb3f2e8d62b67a7c469e22695b52cfca_b.jpg)

Deepmind 早期DQN玩 Atari游戏

2015 年，一段令人震撼的视频出现在 NeurIPS 的舞台上：智能体盯着一块像素屏幕，不断挥动“球拍”击打上方砖块，逐渐学会打出人类玩家都难以企及的“穿墙打法”。而它的“眼睛”里只有像素，“大脑”是一个深度神经网络。这正是 DeepMind 的里程碑工作 —— **Deep Q-Network（DQN）**。

DQN 的出现，首次将深度神经网络成功地应用于强化学习任务中，使得Agent不依赖任何手工特征，仅通过图像像素与奖励信号，就能在 Atari 2600 多款游戏中超越人类水平。这一成就标志着深度强化学习（Deep Reinforcement Learning）的正式崛起。

核心思想：

在传统 Q-Learning 中，我们维护一个 $Q$ 表 $Q(s, a)$ ，记录每个状态-动作对的估计价值。这本质上是一个 **有限状态空间下的离散查表操作**。但如果我们换一个角度思考，把 $Q$ 表看作是一个函数 $Q(s, a)$ ，那么：

-   输入是状态 $s$ 和动作 $a$ ；
-   输出是这个状态-动作对(pair)的期望回报；
-   实际上，Q-Learning 构建了一个**从状态-动作对到回报的函数逼近器**。然而，当状态空间巨大甚至是连续的（比如图像像素），Q 表就无法穷举记录，此时“查表”将变得不切实际。

![](https://picx.zhimg.com/v2-540d20f32d336094b89ec3a8fadb170d_1440w.jpg)

https://medium.com/intro-to-artificial-intelligence/deep-q-network-dqn-applying-neural-network-as-a-functional-approximation-in-q-learning-6ffe3b0a9062

这正是 DQN 的突破点所在：

> **用神经网络来逼近 Q 函数，取代查表，从而拓展到高维状态空间。**

在 DQN 中，我们训练一个参数化神经网络 $Q(s, a; \theta)$ ，输入状态 $s$ （通常是图像或特征），输出所有可能动作的 $Q$ 值，从而实现策略的选择与价值估计。

![](https://pic2.zhimg.com/v2-af50fc8b763cb8c8fc8344c11ba9b25d_1440w.jpg)

https://towardsdatascience.com/reinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b/

DQN延续了Q-Learning的基本思想，通过更新 Q 值函数来学习策略，但不再通过查表获取 $Q$ 值，而是通过神经网络预测来近似 $Q$ 函数，网络参数的更新规则： $\theta \leftarrow \theta - \alpha \nabla_\theta \left( y_t - Q(s_t, a_t; \theta) \right)^2$ ，其中目标值 $y_t$ 为： $y_t = r_t + \gamma \max_{a’} Q(s_{t+1}, a’; \theta^-)$ ，完整的更新也可以写成类似 Q-Learning 的形式： $Q(s_t, a_t; \theta) \leftarrow Q(s_t, a_t; \theta) + \alpha \left[ r_t + \gamma \max_{a’} Q(s_{t+1}, a’; \theta^-) - Q(s_t, a_t; \theta) \right]$ ，这里的 $\theta^-$ 是**目标网络（target network）** 的参数，与主网络参数 $\theta$ 定期同步，这种设计是为了缓解训练过程中的不稳定问题。

为什么要这样做？

可以想象你在追一只兔子（最优 Q 值），但兔子也在不断移动（主网络每一步都变）。如果没有目标网络，每次你刚朝着目标迈出一步，目标就变了方向——这会导致学习过程震荡、甚至发散。目标网络就像把兔子“暂时定住”，让你能朝着一个**相对稳定的目标**迈进，训练也因此更加稳定和可靠。这是为了解决强化学习中特有的不稳定性和收敛困难，DQN 引入的三项重要机制之一，另两项包括：

-   **经验回放（Experience Replay），**将Agent经历的状态转移 $(s, a, r, s’)$ 存入回放缓存，从中随机采样小批量进行训练，打破样本之间的时间相关性，提升样本利用效率。
-   **Mini-batch SGD（小批量梯度下降）**在每一步中，从经验池中采样一小批数据，用标准的梯度下降优化损失函数，提升训练稳定性。

DQN 不仅在 Atari 游戏中展现了惊人的表现，更为后续一系列深度强化学习方法——如 Double DQN、Dueling DQN、Rainbow 等——奠定了方法论基础。更重要的是，它为 AlphaGo 引入了关键模块：在 AlphaGo 的自我博弈训练中，**策略网络**与**价值网络**的设计，正是从 DQN 演化而来。而在 AlphaGo Zero 中，这一架构被进一步纯化：彻底摒弃人类棋谱，完全依靠自我对弈训练，展现出强化学习脱离人类经验、独立进化的真正潜力。

那一刻，Agent 不再模仿人类，而是靠自我对弈、独立进化，走出了通向智慧的第一步。人类也首次隐约看到：**AGI，并不只是一个目标，而是一种可能。**

### **小结：从值函数出发的“控制之道”**

在第六境界，我们走过了强化学习中最经典、最早被系统化的一条路径——**基于值函数的强化学习体系**。从最初的 **动态规划**（DP）的贝尔曼方程，到VI和PI求解，再到不断优化行为策略的 **SARSA** 和 **Q-Learning，** 最终发展至**DQN**，这条路线清晰而有力地构建了一个**以“状态-动作价值”** 为核心的智能体学习框架。

其核心理念在于：**通过估计每一个状态或状态-动作对的长期价值，引导智能体做出更优的决策**。而 DQN 的出现，更是把这一路线从“小表格”推进到了“大世界”，开启了深度强化学习的大门。我们也从中看到了一个重要主题的浮现：

> **如何在现实采样与理想估计之间寻找平衡？**

SARSA 更保守，强调对行为策略的忠实与风险管理；

Q-Learning 更激进，倾向于直接朝最优策略逼近；

DQN 是 Q-Learning 的深度学习变体，通过神经网络逼近 $Q$ 函数，并引入经验回放和目标网络机制，实现了在高维感知世界中的稳定策略学习，标志着深度强化学习的真正起点。

至此，**第六层境界：基于值的强化学习** 正式落幕，也意味着整个**强化学习体系的“基石篇”** 已完成构建。

从动态规划的理性穷举，到 Q-Learning 的理想主义查表，再到 DQN 的深度泛化，我们沿着价值函数的脉络，追溯出强化学习的原始动力机制。这一篇2.5W字的文章，也顺势打破了我个人知乎文章的历史字数记录。

**下篇预告 · RL进阶篇--Agent的内修之路：**

-   **第七境界：双轮驱动** —— Actor-Critic 与 PPO
    
    协同进化，价值与策略共同驱动智能体成长
    
-   **第八境界：动机觉醒** —— 内部奖励与自主探索
    
    从被动响应环境，到主动追寻目标

## 参考

1.  [^](https://zhuanlan.zhihu.com/p/1932009376987717993#ref_1_0)操作性条件作用（Operant Conditioning），也被称为“工具性条件作用”，是行为心理学家 B.F. 斯金纳（B.F. Skinner） 提出的学习机制。它与巴甫洛夫的经典条件作用（Classical Conditioning）不同，强调的是： 个体通过主动行为与环境互动，行为的结果会影响未来行为的发生概率。
