![|800](https://pic2.zhimg.com/v2-b3f5892fb16323c73d3f82bb5ecaf8b5_1440w.jpg)
### 1.4 Trajectory
在 RL中，Actor 会和 Env 进行互动。假设当前Env的状态 $s_1$，Actor采取行动 $a_1$，对Env产生影响变成状态 $s_2$，然后Actor再采取行动 $a_2$ 。这样一直循环，直到达到终止条件，完成了一个episode。这时形成了一条[trajectory](https://zhida.zhihu.com/search?content_id=239644302&content_type=Article&match_order=1&q=trajectory&zhida_source=entity)。
假设一条trajectory共有 t 步，每一步的reward为 $r_t$，则一条trajectory 的 total reward为：$R(\tau)=\sum_{t=1}^T r_t$ 。其中 $\tau$ 表示一条trajectory。
穷举所有的 trajectory，得到reward的期望值：
$$\bar{R}_\theta=\sum_\tau R(\tau) p_\theta(\tau)=E_{\tau \sim p_\theta(\tau)}[R(\tau)]$$
其中 $\theta$ 表示actor的参数。Reward的期望值 $\bar{R}_\theta$ ，也就是RL最终要优化的目标。一条trajectory的概率计算示意图如下所示。
![|550](https://pica.zhimg.com/v2-5b6fba10aa388ba19b0828486720b680_1440w.jpg)
### 1.5 RL的基本分类
根据不同的学习策略，RL可以分为 Policy-based方法、Value-based方法和Actor-Critic 方法。其中，Policy-based方法是要通过训练一个Actor（也就是policy）来解决RL问题。[Value-based](https://zhida.zhihu.com/search?content_id=239644302&content_type=Article&match_order=2&q=Value-based&zhida_source=entity) 方法是训练一个Critic（也就是reward）来解决RL问题。Actor-Critic 则是两者的综合。
![[Pasted image 20250204170720.png|500]]
## 二、Policy-based方法
### 2.1 Policy Gradient
为了求的 $\bar{R}_\theta$ 的最大值，需要对其进行求导，然后进行梯度上升（reward求最大）求解。求导过程如下：
$$
\begin{aligned}
\nabla \bar{R}_\theta &=\sum_\tau R(\tau) \nabla p_\theta(\tau) \\
& =\sum_\tau R(\tau) p_\theta(\tau) \frac{\nabla p_\theta(\tau)}{p_\theta(\tau)} \\
& =\sum_\tau R(\tau) p_\theta(\tau) \nabla \log p_\theta(\tau) \\
& =E_{\tau \sim p_\theta(\tau)}\left[R(\tau) \nabla \log p_\theta(\tau)\right] \\
& \approx \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n} R\left(\tau^n\right) \nabla \log p_\theta\left(a_t^n \mid s_t^n\right)
\end{aligned}
$$
​其中第一行到第二行，用到了公式： $\nabla f(x)= f(x) \nabla \log f(x)$， $R(\tau)$ 不一定需要[可微](https://zhida.zhihu.com/search?content_id=239644302&content_type=Article&match_order=1&q=%E5%8F%AF%E5%BE%AE&zhida_source=entity)，可以是一个黑盒。
得到梯度之后，一次迭代过程如下：首先使用 policy $\pi_\theta$ ，采样很多的 trajectory；然后使用这些数据，更新参数 $\theta$ ，之后再重新采用数据，不断进行迭代。
![|575](https://pica.zhimg.com/v2-3417a5eee8cf032e8689ceed668fd03c_1440w.jpg)
**RL 和一般的supervised learning的区别在于，一般的训练，模型参数更新之后，不会影响数据的标签。而在RL中，模型参数更新之后，Actor 和 Env互动的结果会随之改变，从而影响 label（或者 reward），使得每次更新参数之后，需要重新对数据进行采样。这样也训练成本很高。**  
### 2.2 Policy Gradient的改进
1.  增加一个[baseline](https://zhida.zhihu.com/search?content_id=239644302&content_type=Article&match_order=1&q=baseline&zhida_source=entity)  $$\nabla \bar{R}_\theta \approx \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n}\left(R\left(\tau^n\right)-{b}\right) \nabla \log p_\theta\left(a_t^n \mid s_t^n\right) $$
2. 其中，$$b \approx E[R(\tau)] $$有些场景下reward可能一直是正数，且采样有一定的随机性，导致优化效果不太好。增加一个基准b，使得reward可以有正有负，更容易优化。  
3.  对reward进行加权  $$\nabla \bar{R}_\theta \approx \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n}\left(\sum_{t^{\prime}=t}^{T_n} \gamma^{t^{\prime}-t} r_{t^{\prime}}^n-{b}\right) \nabla \log p_\theta\left(a_t^n \mid s_t^n\right)$$从当前时刻t开始计算，t之前的reward可当前时刻t的action并没有关系。同时，增加了时间[衰减系数](https://zhida.zhihu.com/search?content_id=239644302&content_type=Article&match_order=1&q=%E8%A1%B0%E5%87%8F%E7%B3%BB%E6%95%B0&zhida_source=entity)，使得间隔较远的reward影响较小。  
### 2.3 On-policy vs Off-policy
-   定义
-   On-policy：和环境交互的agent和要学习的agent是一个agent。
-   Off-policy：和环境交互的agent和要学习的agent不是一个agent。
前面的 policy gradient 的做法是 on-policy。当update参数以后，从 $\theta$ 变成 $\theta^{\prime}$，那么之前采样的数据就不能用了。Update一次参数，只能做一次梯度更新，而后再去重新搜集数据，非常耗时，所以需要 [off-policy](https://zhida.zhihu.com/search?content_id=239644302&content_type=Article&match_order=1&q=off-policy&zhida_source=entity)。off-policy可以基于同一批数据更新多次模型。为了使用off-policy，需要引入重要性采样。  
-   重要性采样  
假设 x 服从 p 分布，想要计算 f(x) 的期望，但是 p 分布很难计算（采样）。于是引入一个比较容易计算（采样）的 q 分布，于是在分子分母同时乘以 q(x)，于是就变成了服从 q 分布的 x 来求期望。$$E_{x \sim p}[f(x)]=E_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]$$注意上式中，左边是基于 p 的分布，右边是基于 q 的分布。
根据上式可以知道，通过分布变换之后，期望值是相同的。但方差是否一致呢？根据公式 $$\begin{aligned}
& \operatorname{VAR}[X] =E\left[X^2\right]-(E[X])^2
\end{aligned}$$
分别计算两者的方差如下：
$$\begin{aligned}
\operatorname{VAR}_{x \sim p}[f(x)]& =E_{x \sim p}\left[f(x)^2\right]-\left(E_{x \sim p}[f(x)]\right)^{2}\\
& = \operatorname{VAR}_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right] \\
& = E_{x \sim q}\left[\left(f(x) \frac{p(x)}{q(x)}\right)^2\right]-\left(E_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]\right)^2 \\
& =E_{x \sim p}\left[f(x)^2 \frac{p(x)}{q(x)}\right]-\left(E_{x \sim p}[f(x)]\right)^{2}\\
\end{aligned}$$  可以看到，第二项是一致，区别主要在第一项，也就是 $\frac{p(x)}{q(x)}$ 。要使得在采样不充分的情况下，不出现较大的误差，则需要要求 ${p(x)}$ 和 ${q(x)}$ 是两个比较接近的分布。
从On-policy到Off-policy  
$$\begin{aligned}
\text{On-policy:} \qquad & \nabla \bar{R}_\theta=E_{{\tau \sim p_\theta(\tau)}}\left[R(\tau) \nabla \log p_\theta(\tau)\right] \\
\text{Off-policy:}\qquad &\nabla \bar{R}_\theta=E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\frac{p_\theta(\tau)}{p_{\theta^{\prime}}(\tau)} R(\tau) \nabla \log p_\theta(\tau)\right]
\end{aligned}$$
-  其中， $\theta^{\prime}$ 是固定的，从中采样一次数据, 可以给 $\theta$ 更新很多次，完了以后再去采样数据。  
   由于要使用Off-policy，节省多次采样数据的时间，因此，在RLHF中，需要有`actor_model`和`ref_model`。同时，为了能够满足重要性采样的条件，需要使得`actor_model`和`ref_model` 输出的分布比较接近。  
Off-policy的梯度更新  
$$
\begin{aligned}
\nabla \bar{R}_\theta &=E_{\left(s_t, a_t\right) \sim \pi_\theta}\left[A^\theta\left(s_t, a_t\right) \nabla \log p_\theta\left(a\_t^n \mid s\_t^n\right)\right]\\
& =E_{\left(s_t, a_t\right) \sim \pi_{\theta^{\prime}}}\left[\frac{P_\theta\left(s_t, a_t\right)}{P_{\theta^{\prime}}\left(s_t, a_t\right)} A^\theta\left(s_t, a_t\right) \nabla \log p_\theta\left(a_t^n \mid s_t^n\right)\right]
\end{aligned}
$$
其中 $A^\theta\left(s_t, a_t\right)$ 代表reward，等价于 $R(\tau)$。经过推导，最终得到优化目标  
$$
J^{\theta^{\prime}}(\theta)=E_{\left(s_t, a_t\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_\theta\left(a_t \mid s_t\right)}{p_{\theta^{\prime}}\left(a_t \mid s_t\right)} A^{\theta^{\prime}}\left(s_t, a_t\right)\right] \\\\
$$
### 2.4 PPO/TRPO
在[重要性采样](https://zhida.zhihu.com/search?content_id=239644302&content_type=Article&match_order=4&q=%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7&zhida_source=entity)中，提到替换的分布和替换之前的分布不能差别太大，因此需要增加一个限制条件，保证 $\theta$ 和 $\theta^\prime$ 是比较接近的。衡量两个分布是否接近的指标，就是KL散度。
-   TRPO
TRPO 在[损失函数](https://zhida.zhihu.com/search?content_id=239644302&content_type=Article&match_order=1&q=%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0&zhida_source=entity)之外，增加了一个限制条件，保证两个分布的相似性
$$J_{\text{TRPO}}^{\theta^{\prime}}(\theta)=E_{\left(s_t, a_t\right) \sim \pi\_{\theta^{\prime}}}\left[\frac{p_\theta\left(a_t \mid s_t\right)}{p_{\theta^{\prime}}\left(a_t \mid s_t\right)} A^{\theta^{\prime}}\left(s_t, a_t\right)\right]$$$$K L\left(\theta, \theta^{\prime}\right)<\delta$$
-   PPO
TRPO可以限制两个分布的相似性，但是因为限制条件是单独存在，不好求解。因此，PPO对其进行了改进，变成了损失函数的一部分。  
$$J_{\text{PPO}}^{\theta^{\prime}}(\theta)=J^{\theta^{\prime}}(\theta)-\beta \text{KL}\left(\theta, \theta^{\prime}\right)$$
-   PPO的整体流程
![|500](https://pic1.zhimg.com/v2-5e3cf889d0b60efc6cc2b588744f226e_1440w.jpg)
ppo
-   [[5.2 PPO流程-OK]]：针对不同大小的[KL散度](https://zhida.zhihu.com/search?content_id=239644302&content_type=Article&match_order=2&q=KL%E6%95%A3%E5%BA%A6&zhida_source=entity)，自动调节 $\beta$ 值的大小。  
   $$\begin{aligned}
   J_\text{PPO2}^{\theta^k}(\theta) \approx & \sum_{\left(s_t, a_t\right)} \min \left(\frac{p_\theta\left(a_t \mid s_t\right)}{p_{\theta^k}\left(a_t \mid s_t\right)} A^{\theta^k}\left(s_t, a_t\right), \operatorname{clip}\left(\frac{p_\theta\left(a_t \mid s_t\right)}{p_{\theta^k}\left(a_t \mid s_t\right)}, 1-\varepsilon, 1+\varepsilon\right) A^{\theta^k}\left(s_t, a_t\right)\right)
   \end{aligned}$$
## 三、Value-based方法
在机器学习中，通过需要有反馈信号，告诉模型当前的输出的结果的好坏。在分类等supervised-learning中，可以将预测结果和label进行比对，通过accuracy、f1等指标，即可得到预测结果的好坏。而在文本生成、游戏等场景下，一个action的好坏评估并不是那么显而易见。通过一个模型，对输出进行打分，给到有效的反馈信号，可以方便算法的迭代。
Value-based方法中的critic，就可以起到这样的作用。critic并不会直接决定当前状态下要采取的action，对于给定的policy $\pi$ ，critic可以给出这个策略的分数。[Q-learning](https://zhida.zhihu.com/search?content_id=239644302&content_type=Article&match_order=2&q=Q-learning&zhida_source=entity)是一种常见的value based方法，此时学习的并不是policy，而是一个critic。
### 3.1 Critic的两种方式
-   状态价值函数 $V^\pi(s)$ ：使用policy $\pi$ 时，在状态 s 之后，可以累积获得的reward的期望。$V^\pi(s)$ 的值，和policy$\pi$ 有很大的关系，不同的 $\pi$ ，在共同的状态 s 之下，可以有不同的值  
-   状态 - 行为价值函数 $Q^\pi(s, a)$ ：使用policy $\pi$ 时，在状态 s 并采取行动 a 之后，可以累积获得的reward的期望（只适用于离散的actor，连续的话没法使用）  
### 3.2 状态价值函数$V^\pi(s)$ 的估计方法  
-   [蒙特卡洛方法](https://zhida.zhihu.com/search?content_id=239644302&content_type=Article&match_order=1&q=%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95&zhida_source=entity)（MC）：从状态 s 开始，一直到实验结束，不断重复这个过程。将所有过程的[reward值](https://zhida.zhihu.com/search?content_id=239644302&content_type=Article&match_order=1&q=reward%E5%80%BC&zhida_source=entity)，取均值，作为 $V^\pi(s)$ 的估计值。  
-   时间差分算法（TD）：根据状态 $s_t$ 和 $s_{t+1}$ 之间的差值，进行估计。$$V^\pi\left(s_t\right)-V^\pi\left(s_{t+1}\right) \leftrightarrow r_t$$**MC方差更大，但更精确。TD方差小，但没有MC精确。TD更常用。**
### 3.3 Q-learning
-   基本思路
当可以通过MC或者TD，求解得到 $Q^\pi(s, a)$ 后，使用 $Q^\pi(s, a)$ 可以找到一个新的 $\pi^{\prime}$ 比之前的policy $\pi$ 要更好。这就是Q-learning方法的基本思路。
更好的: $V^{\pi^{\prime}}(s) \geq V_0^\pi(s)$，针对所有的state，有$\pi^{\prime}(s)=\arg\max_a Q^\pi(s, a)$
![|500](https://pica.zhimg.com/v2-fed09c0a963e702789d04a9c08e43c3c_1440w.jpg)
-   目标网络（Target Network）
由定义可知 $\mathrm{Q}^\pi\left(s_t, a_t\right) =r_t+\mathrm{Q}^\pi\left(s_{t+1}, \pi\left(s_{t+1}\right)\right)$ 。但是，当在训练的过程中，如果两个Q同时进行更新，网络会难以进行训练。因此，会将 $\mathrm{Q}^\pi\left(s_{t+1}, \pi\left(s_{t+1}\right)\right)$ 固定，这样方便进行优化。
![|500](https://pic3.zhimg.com/v2-65ae57e27c8a91b0c5810eb6d4c23a56_1440w.jpg)
此处由于有Target Network的存在，在RLHF中，也会有`critic_model`和`reward_model`两个模型，其中`critic_model`会更新参数，而`reward_model`的参数会固定。  
-   Q-learning 的整体过程
![|500](https://pica.zhimg.com/v2-c592eeac8d97da89fe4e3a94a44aeece_1440w.jpg)
-   Q-learning的改进建议
1.  Double DQN
2.  Dueling DQN
3.  Multi-step方法
4.  Noisy Net
5.  Distributional Q-function
## 四、Actor-Critic方法
### 4.1 Actor-Critic
在policy gradient中，
$$\nabla \bar{R}_\theta \approx \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n}\left(\sum_{t^{\prime}=t}^{T_n} \gamma^{t^{\prime}-t} r_{t^{\prime}}^n-{b}\right) \nabla \log p_\theta\left(a_t^n \mid s_t^n\right)$$
其中，$\sum_{t^{\prime}=t}^{T_n} \gamma^{t^{\prime}-t} r_{t^{\prime}}^n$ 是从时刻 t 开始，采取action $a_t^n$ 之后的reward， b 是一个baseline，使得reward可以有正有负。
在 Q-learning中，我们使用了状态 - [行为价值函数](https://zhida.zhihu.com/search?content_id=239644302&content_type=Article&match_order=2&q=%E8%A1%8C%E4%B8%BA%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0&zhida_source=entity) $Q^\pi(s, a)$ 表示在状态 s 并采取行动 a 之后，可以累积获得的reward的期望。同时，可以将baseline 的 b 取值为 $V^{\pi_\theta}\left(s_t^n\right)$ ，就可以将policy gradient 和 Q-learning 结合起来。
![|500](https://pic1.zhimg.com/v2-ff1a3fe730af8ada7158d6b104210a2e_1440w.jpg)
ac
### 4.2 Advantage Actor-Critic
Actor-Critic此时还需要同时优化两个函数，如果估计的不够准确，会带来两倍的误差。因此，想要进一步简化计算。根据$V^\pi(s)$ 和 $Q^\pi(s, a)$ 的定义，并作一定的近似，得到了Advantage Actor-Critic，简称A2C。
![|500](https://picx.zhimg.com/v2-9ec179000d80b24fa5769aa6b6dc1181_1440w.jpg)
最终的梯度为：
$$\nabla \bar{R}_\theta \approx \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n}\left(r_t^n+V^\pi\left(s_{t+1}^n\right)-V^\pi\left(s_t^n\right)\right) \nabla \log p\_\theta\left(a_t^n \mid s_t^n\right) $$
其中， $r_t^n+V^\pi\left(s_{t+1}^n\right)-V^\pi\left(s_t^n\right)$ 也成为Advantage 函数。
### 4.3 Pathwise Derivative Policy Gradient
A2C中，critic可以给出当前action的好坏，但没法给出当前可以执行的最好的action。进一步改进，得到Pathwise Derivative Policy Gradient。此时，将Actor的输出给到Critic，并且在训练中使得Critic的打分最高。这里比较类似GAN的训练方法，其中Actor类似于GAN中的generator，Critic类似于GAN中的discriminator。
Pathwise Derivative Policy Gradient的算法流程如下：
![|500](https://pic4.zhimg.com/v2-8d7fce7586667c74194ec585fdb64d67_1440w.jpg)
可以看到，RLHF中的PPO使用的就是Pathwise Derivative Policy Gradient。actor\_model和ref\_model分别对应 actor $\pi$ 和 target actor $\hat\pi$。 critic\_model和reward\_model分别对应Q和$\hat Q$。
## 参考资料
-   [https://blog.51cto.com/u\_15721703/5575736](https://link.zhihu.com/?target=https%3A//blog.51cto.com/u_15721703/5575736)
-   详解大模型RLHF过程（配代码解读） - 战士金的文章 - 知乎 [https://zhuanlan.zhihu.com/p/624589622](https://zhuanlan.zhihu.com/p/624589622)
-   李弘毅深度强化学习笔记【1 Policy Gradient 】 - 残血的[三井寿](https://zhida.zhihu.com/search?content_id=239644302&content_type=Article&match_order=1&q=%E4%B8%89%E4%BA%95%E5%AF%BF&zhida_source=entity)的文章 - 知乎 [https://zhuanlan.zhihu.com/p/66291401](https://zhuanlan.zhihu.com/p/66291401)
-   【李弘毅深度强化学习】2，Proximal Policy Optimization (PPO) - 残血的三井寿的文章 - 知乎 [https://zhuanlan.zhihu.com/p/66302483](https://zhuanlan.zhihu.com/p/66302483)
-   [https://mathmach.com/be7f3b4f/](https://link.zhihu.com/?target=https%3A//mathmach.com/be7f3b4f/)
-   [Advantage Actor Critic (A2C)](https://link.zhihu.com/?target=https%3A//huggingface.co/blog/deep-rl-a2c)
-   [https://speech.ee.ntu.edu.tw/~tlkagk/courses\_MLDS18.html](https://link.zhihu.com/?target=https%3A//speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html)