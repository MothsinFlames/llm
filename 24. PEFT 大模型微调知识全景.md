---
source: "https://zhuanlan.zhihu.com/p/13365806770"
---
为什么需要大模型微调
- **提升特定任务表现** 
- **领域适应性** 
- **数据稀缺性** 
- **成本效益** 
## 大模型微调
- **全量参数微调（Full Fine-tuning，FFT）** ：对预训练模型的所有参数进行更新，训练速度较慢，消耗机器资源较多；
- **参数高效微调（Parameter-Efficient Fine-Tuning，PEFT）** ：只对部分参数进行更新，训练速度快，消耗机器资源少
- **[In-Context Learning](https://zhida.zhihu.com/search?content_id=251696158&content_type=Article&match_order=1&q=In-Context+Learning&zhida_source=entity)** ，通过在输入的 prompt 中提供与任务相关的上下文和例子，从而让模型能够更好地了理解我们的意图。
- **RFT（Reinforcement Fine-Tuning）** 的微调技术，能够以奖励驱动的方式不断完善大模型所掌握的知识，更多细节可以参考这篇文章： [What Is OpenAI's Reinforcement Fine-Tuning?](https://link.zhihu.com/?target=https%3A//www.datacamp.com/blog/reinforcement-fine-tuning)。

### 4.2 PEFT 的优缺点
**优点：**
- **降低训练成本** 
- **保证模型性能** 
- **节省存储空间** 
**缺点：**
- **任务性能有限** 

### 4.3 PEFT 的分类

论文 [《Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning》](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2303.15647) 中展示了各类参数高效微调方法及其所属的类别

## 五、各类微调方法的原理是什么

### 5.1 In-Context Learning
**核心原理：**
In-Context Learning 通过在输入的 prompt 中提供与任务相关的上下文和例子，从而让模型能够基于我们提供的上下文，更好地生成我们期望得到的结果。


![|425](https://pic1.zhimg.com/v2-4eee69027dd96df1a494933a37c3fffc_1440w.jpg)

### 5.2 Soft Prompt Tuning

**核心原理：**

Soft Prompt Tuning 可以看作是 Prefix Tuning 的简化版本，它给每个任务定义了自己的 soft prompt，并将其拼接到数据上作为输入（在输入 embedding 层加入一段定长的可训练的向量，在微调的时候只更新 soft prompt 这部分的参数）。

示例代码：

```python
x = EmbeddingLayer(input_ids)
x = concatenate([soft_prompt_tensor, x], dim=seq_len)
output = model(x)
```

其中， `soft_prompt_tensor` 具有与 embedded inputs 同样的特征维度，将两者拼接过后，就相当于是增加了输入的长度。

### 5.3 Prefix Tuning

**核心原理：**

Prefix Tuning 通过对输入数据增加前缀（prefix）来做微调，即在输入 token 之前构造一段任务相关的 virtual tokens 作为 prefix，训练的时候只更新 prefix 这部分的参数，每个下游任务都可以单独训练一套 prefix token。

![|500](https://picx.zhimg.com/v2-60b502a612ef194b6ee05ab426c156d7_1440w.jpg)

示例代码：

```python
def transformer_block_with_prefix(x):
    soft_prompt = FullyConnectedLayers(soft_prompt)  # prefix
    x = concatenate([soft_prompt, x], dim=seq_len)
    x = SelfAttention(x)
    x = LayerNorm(x + residual)
    residual = x
    x = FullyConnectedLayers(x) 
    x = LayerNorm(x + residual)
    return x
```

**为什么增加 prefix 可以影响模型生成的结果？**

感性地理解一下，prefix 的作用是引导模型提取输入中的特定信息，进而更好地生成结果。

![](https://pic1.zhimg.com/v2-3deeca83736a169166441f287a31792e_1440w.jpg)

另外，我们还可以针对不同的下游任务，训练不同的 prefix 并对其进行保存。这样当我们需要切换不同的下游任务时，只需要加载不同的 prefix 参数，就可以实现模型功能的快速切换。

![|450](https://pic4.zhimg.com/v2-705fa5b5030605451c654a6ac87cd935_1440w.jpg)

**缺点：**

- 微调的效果存在上限，模型的表现并不一定会随着 prefix 长度的增加而提高；
- 由于模型的输入长度一般是固定的，而增加了 prefix 之后，留给原始文字数据的空间就少了，因此可能会降低原始文字中 prompt 的表达能力。

### 5.4 Adapter Tuning

**核心原理：**

Adapter Tuning 通过在 transformer 中的 multi-head self-attention 和 fully connected layers 后增加 Adapter 进行微调。其中，Adapter 中的第一个 fully connected layer 将高维的输入映射为了一个低维的表示，第二个 fully connected layer 再将其映射回高维的空间中，这样就能有效降低训练时需要更新的参数量。

微调时，只更新 Adapter 部分的权重，原模型的参数是冻结的。

![|500](https://pic3.zhimg.com/v2-44ca36593d78f098ae63113a893a97fc_1440w.jpg)

> 注意：新增的 Adapter 与原模型中的层是顺序串行的关系。

示例代码：

```python
def transformer_block_with_adapter(x):
    residual = x
    x = SelfAttention(x)
    x = FullyConnectedLayers(x)  # Adapter
    x = LayerNorm(x + residual)
    residual = x
    x = FullyConnectedLayers(x)
    x = FullyConnectedLayers(x)  # Adapter
    x = LayerNorm(x + residual)
    return x
```

**缺点：**

- 添加了 Adapter 后，模型整体的层数变深，会拖慢模型训练和推理的速度。

**小结：**

- FFT 成本太高；
- Prefix Tuning 难训且会减少原始训练数据中的有效文字长度；
- Adapter Tuning 存在训练和推理延迟。

为了解决以上问题，LoRA 系列微调方法便应运而生了。

### 5.5 LoRA

**核心原理：**

关于 LoRA（Low-Rank Adaptation，低秩适配器）的相关原理，可以参考下面这篇文章：

5.6 QLoRA

**核心原理：**

QLoRA（Quantized LoRA）的核心工作其实是模型量化，通过将预训练模型进行 NF4 量化，再结合 LoRA 的方式进行微调，可以大幅减少训练时显存的占用。

![](https://picx.zhimg.com/v2-0e77f8e818799c183199998b30af0329_1440w.jpg)

QLoRA 有一个 NF4 的存储数据类型和 BF16 的计算数据类型。在进行前向和反向传播时，我们需要将存储数据类型反量化为计算数据类型，但是计算梯度时我们只计算添加的适配器的梯度，这一点和 LoRA 是一致的。

- 预训练模型的参数：进行 NF4 量化；
- LoRA 的参数：保持 BF16 的精度。

**核心工作：**

- **四位标准浮点数量化（4-bit Normal Float Quantization）** ：结合了分位数量化和分块量化；
- **双重量化（Double Quantization）** ：对模型进行了两次量化，其中第二次量化只作用在第一次量化产生的量化常数上，可以进一步节约显存占用；
- **分页优化（Paged Optimizer）** ：使用 CPU 内存代替 GPU 显存保存部分梯度参数。

**优缺点：**

- 优点：显存占用下降。由于原模型参数经过了量化，在计算时占用的内存减少了；
- 缺点：训练时间增加。由于引入了量化和反量化的计算过程，在训练时需要消耗更多的时间。

**量化分位数的计算：**

1. 根据每个块的特征的绝对值的最大值，我们为每个块保存一个量化常数（每个块中的特征取绝对值后的最大值）；
2. 计算每个张量的量化值（实际值/该块的量化常数）；
3. 在 Q（normal\_map）中找到与每个张量最接近的值，并将其量化为该值对应的索引值。

`normal_map` 的计算：

```python
from scipy.stats import norm
import torch

def create_normal_map(offset=0.9677083, use_extra_value=True):
    if use_extra_value:
        # one more positive value, this is an asymmetric type
        v1 = norm.ppf(torch.linspace(offset, 0.5, 9)[:-1]).tolist() # 正数部分
        v2 = [0]*(256-15) ## we have 15 non-zero values in this data type
        v3 = (-norm.ppf(torch.linspace(offset, 0.5, 8)[:-1])).tolist() #负数部分
        v = v1 + v2 + v3
    else:
        v1 = norm.ppf(torch.linspace(offset, 0.5, 8)[:-1]).tolist()
        v2 = [0]*(256-14) ## we have 14 non-zero values in this data type
        v3 = (-norm.ppf(torch.linspace(offset, 0.5, 8)[:-1])).tolist()
        v = v1 + v2 + v3

    values = torch.Tensor(v)
    values = values.sort().values
    values /= values.max()
    assert values.numel() == 256
    return values

Q = create_normal_map()
# Q = [-1.0, -0.6961928009986877, -0.5250730514526367, -0.39491748809814453, -0.28444138169288635, -0.18477343022823334, -0.09105003625154495, 0.0, 0.07958029955625534, 0.16093020141124725,0.24611230194568634, 0.33791524171829224, 0.44070982933044434, 0.5626170039176941, 0.7229568362236023, 1.0]
```

**示例：**

假设一个张量有 16 个值，被分成了 4 块：

```python
input_blocked_tensor = [[-1.28645003578589, -1.817660483275528, 9.889441349505042, 0.010208034676132627],
                        [-15.009014631551885, 1.4136255086268115, -7.815595761491153, 10.766760590950263], 
                        [-0.731406153917959, 3.468224595908726, 2.445252541840315, -8.970824523299282], 
                        [-9.641638854625175, 7.696158363188889, -5.323939281255154, 5.97160401402024]]
```

根据每个块的特征的绝对值的最大值，我们为每个块保存一个量化常数，它的计算方式是每个块中特征的绝对值中最大的那个：

```python
c1 = max(|-1.28645003578589|, |-1.817660483275528|, |9.889441349505042|, |0.010208034676132627|) = 9.889441349505042
c2 = max(|-15.009014631551885|, |1.4136255086268115|, |-7.815595761491153|, |10.766760590950263|) = 15.009014631551885
c3 = max(|-0.731406153917959|, |3.468224595908726|, |2.445252541840315|, |-8.970824523299282|) = 8.970824523299282
c4 = max(|-9.641638854625175|, |7.696158363188889|, |-5.323939281255154|, |5.97160401402024|) = 9.641638854625175
```

计算张量的量化值：例如第一个值 `-1.28645003578589` ，它除以这个块的量化常数 `c1` 后得到 `-0.13008318572517502` ，我们可以在 `Q` 中找到与它最接近的值是 `-0.09105003625154495` ，这个值在 `Q` 中对应的索引是 `6` ，因此这个值被量化后的值是 `6` 。

```python
Q = [-1.0, -0.6961928009986877, -0.5250730514526367, -0.39491748809814453,
     -0.28444138169288635, -0.18477343022823334, -0.09105003625154495, 0.0,
     0.07958029955625534, 0.16093020141124725,0.24611230194568634, 0.33791524171829224,
     0.44070982933044434, 0.5626170039176941, 0.7229568362236023, 1.0]
```

同理我们可以得到这个输入张量所有的值量化后的结果：

```python
[[6, 5, 15, 7],
 [0, 8, 2, 14],
 [6, 11, 10, 0],
 [0, 14, 2, 13]]
```

在模型保存时，除了要保存量化后的值，我们还要保存每个块对应的量化常数，因为这个值在我们进行反量化时需要用到。

在反量化时，我们以量化结果作为索引，从 `Q` 中查找到它对应的分位数，再乘以为每个块保存的量化常数 `ci` ，便可以得到最终结果。

```python
[[-0.9004339933799617, -1.8273060011889755, 9.889441349505042, 0.0],
 [-15.009014631551885, 1.1944218804231184, -7.880829111886221, 10.850869732860506],
 [-0.816793898052648, 3.0313783372030603, 2.2078302737800004, -8.970824523299282],
 [-9.641638854625175, 6.970488722350373, -5.062564734402345, 5.424549965245643]]
```

**解决了什么问题？**

如果我们粗暴的使用 round 操作去映射到低精度的更近的值，我们可能造成大量的数据都被量化到同一个数上，这样特征之间的差异性在量化过程中就被丢失了。使用分位数将张量分成了大小相同的若干个块，这样我们得到更加均匀的量化特征，这也就是分位数量化。每两个分位数的中点便是模型量化到这个区间映射的值。

**双重量化：**

QLoRA 的双重量化是指对量化常数再做一次 8 bit 的量化，在进行量化常数的量化时，QLoRA 以每 256 个量化常数为一组再做一次量化。在进行反量化时我们也需要进行两次反量化才能把量化后的值还原。

好处：减少了存储量化常数带来的额外显存占用。

**分页优化：**

QLoRA 的分页优化其实就是当显存不足时，将保存的部分梯度检查点转移到 CPU 内存上，和计算机的内存数据转移到硬盘上的常规内存分页一个道理。

### 5.7 总结

**How to use and finetune pre-trained LLMs?**

总结一下，当我们经过预训练得到 base 大模型之后，还需要进行以下操作：

1. **增量预训练** ：注入领域知识；
2. **监督微调** ：适配特定下游任务（各类微调方法百花齐放）；
3. **偏好对齐** ：使模型生成的结果符合人类偏好。
![](https://pic2.zhimg.com/v2-e0d20a83cf484812cb99b397db92bcab_1440w.jpg)

## 六、参考资料

- [Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2303.15647)
- [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2101.00190)
- [LoRA: Low-Rank Adaptation of Large Language Models](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2106.09685)
- [李航《统计学习方法》](https://link.zhihu.com/?target=https%3A//book.douban.com/subject/33437381/)
- [QLoRA: Efficient Finetuning of Quantized LLMs](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2305.14314)
- [浅谈 DeepLearning 的浮点数精度 FP32/FP16/TF32/BF16……](https://link.zhihu.com/?target=https%3A//medium.com/%40averyaveavi/%25E6%25B7%25BA%25E8%25AB%2587deeplearning%25E7%259A%2584%25E6%25B5%25AE%25E9%25BB%259E%25E6%2595%25B8%25E7%25B2%25BE%25E5%25BA%25A6fp32-fp16-tf32-bf16-%25E4%25BB%25A5llm%25E7%2582%25BA%25E4%25BE%258B-9bfb475e50be)
- [LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2309.12307)
- [New LLM Pre-training and Post-training Paradigms](https://link.zhihu.com/?target=https%3A//magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training)
- [Finetuning Large Language Models](https://link.zhihu.com/?target=https%3A//magazine.sebastianraschka.com/p/finetuning-large-language-models%3Futm_source%3Dpublication-search)
- [Using and Finetuning Pretrained Transformers](https://link.zhihu.com/?target=https%3A//magazine.sebastianraschka.com/p/using-and-finetuning-pretrained-transformers%3Futm_source%3Dpublication-search)
- [Practical Tips for Finetuning LLMs Using LoRA](https://link.zhihu.com/?target=https%3A//magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms%3Futm_source%3Dpublication-search)
- [LLM 微调理论](https://link.zhihu.com/?target=https%3A//qiankunli.github.io/2023/10/29/llm_finetune_theory.html)
- [GPT 是如何炼成的：大模型微调基础概念指北](https://link.zhihu.com/?target=https%3A//www.lixueduan.com/posts/ai/04-finetune-concept/)
- [图解大模型微调系列之：大模型低秩适配器 LoRA（原理篇）](https://zhuanlan.zhihu.com/p/646831196)
- [图解大模型微调系列之：大模型低秩适配器 LoRA（源码解读与实操篇）](https://zhuanlan.zhihu.com/p/654897296)
- [图解 Fine-tuning：LoRA 系列微调技术概述](https://zhuanlan.zhihu.com/p/990958034)
- [QLoRA（Quantized LoRA）详解](https://zhuanlan.zhihu.com/p/666234324)
- [LongLoRA - 高效微调长上下文的 LLMs](https://zhuanlan.zhihu.com/p/659226557)
- [LLM 长 context 微调技巧 - LongLora](https://zhuanlan.zhihu.com/p/658043624)
- [LoRA、QLoRA、LoRA+、LongRA、DoRA、MaLoRA、GaLore方案都知道吗？](https://zhuanlan.zhihu.com/p/8954237216)
- [LLM 微调实践](https://link.zhihu.com/?target=https%3A//qiankunli.github.io/2024/07/28/llm_finetune_practice.html)
- [What Is OpenAI's Reinforcement Fine-Tuning?](https://link.zhihu.com/?target=https%3A//www.datacamp.com/blog/reinforcement-fine-tuning)