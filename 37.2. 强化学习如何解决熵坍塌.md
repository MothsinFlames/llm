---
created: 2025-10-28T17:12:45 (UTC +08:00)
tags: [强化学习 (Reinforcement Learning),信息熵,大模型]
source: https://zhuanlan.zhihu.com/p/1925949062655481373
author: 
---

## 一、[熵坍塌](https://zhida.zhihu.com/search?content_id=260062848&content_type=Article&match_order=1&q=%E7%86%B5%E5%9D%8D%E5%A1%8C&zhida_source=entity)现象

在大模型强化学习中，经常出现熵坍塌的现象，随着训练的进行，entropy 逐渐降低。导致某些 group 采样出的 response 几乎相同，使得模型在早期变得更加确定，限制了模型的探索空间。

![](https://pica.zhimg.com/v2-5c035a6d9a98ad4e8fe6cf255e8e8510_1440w.jpg)

图1: 训练中熵不断降低

在图 1 中：

-   前 200 步：熵下降 73%，模型性能提升 76%
-   前 800 步：熵下降 94%，模型性能提升 93%
-   **后 1600 步：熵仅下降 6%，模型性能仅提升 7%**

模型在训练早期（少量步数）就决定了最终性能。

如果不做任何熵控制，那么熵 $\mathcal{H}$ 和模型性能 $R$ 大致呈现负指数函数关系：

$R=-a\text{ exp}\left( \mathcal{H} \right)+b\tag1$随着熵的不断降低，模型性能提升越来越小，如下图所示：

![](https://pic1.zhimg.com/v2-97b700d9bbd0622f9062ef658a1b7cd2_1440w.jpg)

图2: 熵和模型性能符合负指数函数关系

## 二、为什么会出现熵坍塌

**强化学习中的策略更新机制天然会强化“高概率、优势大的 token”，导致策略输出越来越确定，熵迅速下降。**

强化学习中，使用[策略梯度](https://zhida.zhihu.com/search?content_id=260062848&content_type=Article&match_order=1&q=%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6&zhida_source=entity)（Policy Gradient）来最大化期望奖励：

$\nabla_\theta J(\theta) = \mathbb{E}_{\pi} \left[ \sum_t \nabla_\theta \log \pi_\theta(y_t|y_{<t}) A_t \right]\tag2$这意味着：

-   若一个 token 的概率高且 advantage 高 → 它会被进一步“鼓励”；
-   这会让该 token 的 logit 更高、概率更高；
-   其它 token 的概率自然就被压缩了；
-   策略越来越“自信” → 熵快速下降。

针对熵坍塌问题，目前已经有很多方法，比如字节在 DAPO 中提出的 [clip-higher](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2503.14476)、昆仑万维在技术报告中提出的 [Adaptive Entropy Control](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2505.22312) 以及近期上海AI实验室联合清华大学提出的 [clip-cov](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2505.22617)。

下面我们就来介绍一下这些方法。

## 三、字节：Clip-Higher

字节提出了 **Clip-Higher** 策略，通过解耦上下限裁剪，允许对低概率 token 和高概率t oken 进行**非对称**的调整。

$\begin{align}  \mathcal{J}_{DAPO}\left( \theta \right) &=\mathbb{E}_{\left( q,a \right)\sim \mathcal{D},\left\{ o_i \right\}_{i=1}^G \sim \pi_{\text{old}}\left( \cdot|q \right)}\\  & \left[\color{}{\frac{1}{\sum_{i=1}^G\left| o_i \right|}\sum_{i=1}^G\sum_{t=1}^{\left| o_i \right|} }\text{min}\left( r_{i,t}\left( \theta \right)\hat{A}_{i,t},\text{clip}\left( r_{i,t}\left( \theta \right),1-\color{red}{\epsilon_{\text{low}}},1+\color{red}{\epsilon_{\text{high}}} \right)\hat{A}_{i,t} \right) \right]\\ &\text{s.t. } \color{}{0< \left| \left\{ o_i|\text{is_equivalent}\left( a,o_i \right) \right\} \right|<G} \end{align} \\ \tag3$具体来说，Clip-Higher 策略使用两个不同的裁剪参数：

-   $\epsilon_{low}$：用于限制高概率 token 的概率降低幅度，防止过度利用。
-   $\epsilon_{high}$：用于限制低概率 token 的概率提升幅度，允许更大的探索空间。

通过这种方式，Clip-Higher 策略可以更灵活地控制策略更新的幅度，从而更好地平衡探索和利用。

-   **对低概率 token 的影响**：由于$\epsilon_{high} > \epsilon_{low}$，低概率 token 的概率提升空间更大。当 $A>0$ 时，$\frac{\pi_\theta}{\pi_{\theta_{old}}}>1+\epsilon$ 会发生 ratio 的截断，因此只有 $\pi_\theta<={\pi_{\theta_{old}}}(1+\epsilon)$时，token 才能被更新。例如，如果一个 token 的原始概率为 0.01，$\epsilon_{high}=0.28$，那么更新后的概率 $\pi_\theta$ 最大可以达到 0.0128，提升了28%。**这有助于模型更充分地探索低概率 token，从而发现潜在的更优策略**。
-   **对高概率 token 的影响**：由于 $\epsilon_{low} < \epsilon_{high}$，高概率 token 的概率降低空间更小。当 $A<0$ 时，$\frac{\pi_\theta}{\pi_{\theta_old}}<1-\epsilon$ 才会发生 ratio 的截断，因此只有 $\pi_\theta>={\pi_{\theta_{old}}}(1-\epsilon)$ 时，token 才能被更新。例如，如果一个 token 的原始概率为 0.9，$\epsilon_{low}=0.2$，那么更新后的概率 $\pi_\theta$ 必须达到 0.72。这意味着模型在更新策略时，**会更加谨慎地对待这些高概率 token，防止过度利用导致性能下降**。

在字节的论文中，使用 Clip-Higher 后，能够有效防止熵坍塌。

![](https://picx.zhimg.com/v2-91452f8a949b1b38200f6b7d419f5081_1440w.jpg)

图3: Clip-Higher 效果

## 四、昆仑万维：Adaptive Entropy Control

目标：**动态调节熵损失系数 αₖ，使当前模型的熵不低于目标熵（tgt-ent）**

1.  **两个新超参数**：

-   $\text{tgt-ent}$ ：期望维持的目标熵
-   $\Delta$ ：每步允许调整的熵系数变化幅度

**2\. 初始设置**：

熵损失系数初始化为 $c_0 = 0$

**3\. 每个 rollout step 的动态调节规则**：

若当前熵 $e < \text{tgt-ent}$ ，则提升熵系数： $c_{k+1} = c_k + \Delta$

若当前熵 $e > \text{tgt-ent}$ ，则降低熵系数： $c_{k+1} = c_k - \Delta$

**4\. 最终熵损失系数表达式**：

${\alpha_k}=c_k\cdot\mathbb{I}\{ e_k\leq \textbf{tgt-ent} \}\text{, }c_{k+1}= \begin{cases}c_k+\Delta, & \text{if } e_k < \textbf{tgt-ent} \\c_k-\Delta,   & \text{if } e_k > \textbf{tgt-ent} \end{cases}\tag4$

即：只有当当前熵低于目标熵时，才启用熵项进行正则化。

实验中将目标熵设置为 0.2，模型在后期的熵始终在 0.2 左右波动。

![](https://pica.zhimg.com/v2-7800df5cf641ee98b5b84dcdc64d0f0c_1440w.jpg)

图4: 将熵控制在目标熵 0.2 左右

## 五、上海AI实验室：Clip-Cov & KL-Cov

**Clip-Cov** 方法来自论文**《The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models》**，由来自上海 AI 实验室、清华大学、UIUC 等机构的研究者共同撰写，核心目标是系统研究强化学习在大语言模型（LLMs）推理任务中所面临的“熵崩塌”（entropy collapse）问题，并提出有效的解决方案。

在没有熵正则化的RL训练中，策略熵（Policy Entropy）在训练初期急剧下降。熵降低 → 策略变得过于自信 → 探索能力下降 → 下游性能停滞。

作者从协方差角度，解释了为什么熵会不断下降。策略熵的变化来自动作的 log-probability 与 logits 变化之间的协方差（covariance）： $\Delta H \propto -\text{Cov}(\log \pi(a), \Delta z)\tag5$在 Policy Gradient 和 Natural Policy Gradient 下，logit 的变化 $\Delta z$ 与 advantage 成正比。

高概率 + 高 advantage 的动作会加剧熵的下降；低概率 + 高 advantage 的动作可提升熵。

基于上述机制，作者提出两种简单且有效的熵控制技术，目标是避免熵崩塌、提升探索能力：

-   **Clip-Cov**：随机裁剪掉高协方差的 token（即 detach 它们的梯度，不参与策略更新）
-   **KL-Cov**：对于协方差最高的少数 token，加入 KL 正则项控制其更新强度。

实验中，两者都显著提升模型性能，在 Qwen2.5-32B 上提升 **6.4%**。

下面介绍下具体细节。

### 5.1 熵变化 VS 协方差

既然熵坍塌会降低训练效果，那么我们就需要分析：**策略熵为什么会下降？它是如何被“消耗”的？**

作者通过分析熵的变化 $\Delta \mathcal{H}=\mathcal{H}\left(\pi_{\theta}^{k+1}  \right)-\mathcal{H}\left(\pi_{\theta}^{k}  \right)$ ，来定位具体原因。

在大多数语言模型中，策略 $\pi_{\theta}(a|s)$ 是通过 softmax logits 计算的：

$\pi_{\theta}(a|s) = \frac{\text{exp}(z_{s,a})}{\sum_{a'} \text{exp}(z_{s,a'})}\tag6$

-   $z_{s,a}$ ：token $a$ 在状态 $s$ 下的 logit；

策略熵（policy entropy）定义为： $H(\pi_{\theta}, D) = -\mathbb{E}_{D, \pi_{\theta}} \left[ \log \pi_{\theta}(y_t | y_{<t}, x) \right]= -\frac{1}{|D|} \sum_{x \in D} \frac{1}{|y|} \sum_{t=1}^{|y|} \mathbb{E}_{y_t \sim \pi_{\theta}} \left[ \log \pi_{\theta}(y_t | y_{<t}, x) \right]\tag7$

作者使用 **一阶泰勒展** 和 softmax 导数公式，得出：

$\boxed{H(\pi_{\theta}^{k+1}|s) - H(\pi_{\theta}^k|s) \approx -\text{Cov}_{a \sim \pi_{\theta}^k} \left( \log \pi_{\theta}^k(a|s), z_{s,a}^{k+1} - z_{s,a}^k \right)}\tag8$其中 $z_{s,a}^{k+1} - z_{s,a}^k$ 是步骤 $k$ 和步骤 $k+1$ 之间输出 logits 的变化。

协方差的定义为： $\text{Cov}\left( X,Y \right)=\mathbb{E}\left( XY \right)-\mathbb{E}\left( X \right)\mathbb{E}\left( Y \right)=\left( X-\bar{X} \right)\left( Y-\bar{Y} \right)$

根据公式（8），我们可以得出：

**策略熵的变化 ≈ log-prob 与 logits 更新量之间的协方差（covariance）**

也就是说，如果高概率 token 的 logits 被大幅度提升（ $\log \pi_{\theta}$ 大、 $\Delta z$ 大） → 协方差为正 → 熵下降；

-   如果低概率 token 的 logits 被大幅度提升（ $\log \pi_{\theta}$ 小、 $\Delta z$ 大） → 协方差为负 → 熵上升。

**下面证明下公式（8）：**

**步骤 1**: 熵的定义和泰勒近似

策略熵的定义（给定状态 $s$ ）：

$\mathcal{H}(\pi_{\theta} | s) = -\mathbb{E}_{a \sim \pi_{\theta}(\cdot|s)} \left[ \log \pi_{\theta}(a|s) \right] = -\sum_{a \in \mathcal{A}} \pi_{\theta}(a|s) \log \pi_{\theta}(a|s)\\$

-   在步骤 $k$ 和 $k+1$ 之间，参数从 $\theta^k$ 更新为 $\theta^{k+1}$ ，对应的熵分别为 $\mathcal{H}(\pi_{\theta}^k | s)$ 和 $\mathcal{H}(\pi_{\theta}^{k+1} | s)$ 。
-   假设参数变化 $\(\Delta \theta = \theta^{k+1} - \theta^k\)$ 较小，使用 一阶泰勒展开近似熵的变化： $\mathcal{H}(\pi_{\theta}^{k+1} | s) \approx \mathcal{H}(\pi_{\theta}^k | s) + \left\langle \nabla_{\theta} \mathcal{H}(\pi_{\theta}^k | s), \Delta \theta \right\rangle   \\$ 其中 $\(\left\langle \cdot, \cdot \right\rangle\)$ 表示向量点积。
-   因此，熵差可写为：

$\mathcal{H}(\pi_{\theta}^{k+1}|s) - \mathcal{H}(\pi_{\theta}^{k}|s) \approx \left\langle \nabla_{\theta} \mathcal{H}(\pi_{\theta}^k | s), \Delta \theta \right\rangle   \tag{9}$**步骤 2**: 计算熵的梯度

计算梯度 $\(\nabla_{\theta} \mathcal{H}(\pi_{\theta} | s)\)$ ： $\nabla_{\theta} \mathcal{H}(\pi_{\theta} | s) = \nabla_{\theta} \left( -\mathbb{E}_{a \sim \pi_{\theta}(\cdot|s)} \left[ \log \pi_{\theta}(a|s) \right] \right)   \\$

使用 log-derivative trick： $\nabla_{\theta} \mathbb{E}_{a} [\log \pi_{\theta}(a|s)] = \mathbb{E}_{a} \left[ \log \pi_{\theta}(a|s) \cdot \nabla_{\theta} \log \pi_{\theta}(a|s) + \nabla_{\theta} \log \pi_{\theta}(a|s) \right]\\$代入熵的梯度： $\nabla_{\theta} \mathcal{H}(\pi_{\theta} | s) = - \mathbb{E}_{a \sim \pi_{\theta}(\cdot|s)} \left[ \log \pi_{\theta}(a|s) \nabla_{\theta} \log \pi_{\theta}(a|s) + \nabla_{\theta} \log \pi_{\theta}(a|s) \right]\\$简化后： $\nabla_{\theta} \mathcal{H}(\pi_{\theta} | s) = - \mathbb{E}_{a \sim \pi_{\theta}(\cdot|s)} \left[ \log \pi_{\theta}(a|s) \nabla_{\theta} \log \pi_{\theta}(a|s) \right]   \tag{10}$

**步骤 3**: 代入点积并展开

-   将式 (10) 代入式 (9)： $\left\langle \nabla_{\theta} \mathcal{H}(\pi_{\theta}^k | s), \Delta \theta \right\rangle = - \mathbb{E}_{a \sim \pi_{\theta}^k(\cdot|s)} \left[ \log \pi_{\theta}^k(a|s) \left\langle \nabla_{\theta} \log \pi_{\theta}^k(a|s), \Delta \theta \right\rangle \right]   \tag{11}$在 **tabular softmax** 策略中（这里需要和共享参数 $z_{s,a} = f_{\theta}\left( s,a \right)$ 区分开），每个状态-动作对 $\((s, a)\)$ 有独立参数 $\(z_{s,a} = \theta_{s,a}\)$ ，因此：

-   $\(\nabla_{\theta} \log \pi_{\theta}(a|s)\)$ 是向量，其分量为： $\frac{\partial \log \pi_{\theta}(a|s)}{\partial \theta_{s,a'}} = \mathbf{1}\{a = a'\} - \pi_{\theta}(a'|s), \quad \forall a' \in \mathcal{A}\\$ $\(\Delta \theta = \theta^{k+1} - \theta^k\)$ 的分量为 $\(\Delta \theta_{s,a'} = z_{s,a'}^{k+1} - z_{s,a'}^k\)$ 。

-   内积 $\(\left\langle \nabla_{\theta} \log \pi_{\theta}^k(a|s), \Delta \theta \right\rangle\)$ 可展开为： $\left\langle \nabla_{\theta} \log \pi_{\theta}^k(a|s), \Delta \theta \right\rangle = \sum_{a' \in \mathcal{A}} \frac{\partial \log \pi_{\theta}^k(a|s)}{\partial \theta_{s,a'}} \Delta \theta_{s,a'} = \sum_{a' \in \mathcal{A}} \left( \mathbf{1}\{a = a'\} - \pi_{\theta}^k(a'|s) \right) (z_{s,a'}^{k+1} - z_{s,a'}^k)\\$简化求和：

$\sum_{a' \in \mathcal{A}} \mathbf{1}\{a = a'\} (z_{s,a'}^{k+1} - z_{s,a'}^k) = z_{s,a}^{k+1} - z_{s,a}^k\\$ $\sum_{a' \in \mathcal{A}} \pi_{\theta}^k(a'|s) (z_{s,a'}^{k+1} - z_{s,a'}^k) = \mathbb{E}_{a' \sim \pi_{\theta}^k(\cdot|s)} \left[ z_{s,a'}^{k+1} - z_{s,a'}^k \right]\\$

-   因此：

$\left\langle \nabla_{\theta} \log \pi_{\theta}^k(a|s), \Delta \theta \right\rangle = (z_{s,a}^{k+1} - z_{s,a}^k) - \mathbb{E}_{a' \sim \pi_{\theta}^k(\cdot|s)} \left[ z_{s,a'}^{k+1} - z_{s,a'}^k \right]   \tag{12}$

**步骤 4**: 代入期望并重组为协方差

-   将式 (12) 代入式 (11)：

$\left\langle \nabla_{\theta} \mathcal{H}, \Delta \theta \right\rangle = - \mathbb{E}_{a \sim \pi_{\theta}^k(\cdot|s)} \left[ \log \pi_{\theta}^k(a|s) \left( (z_{s,a}^{k+1} - z_{s,a}^k) - \mathbb{E}_{a'} \left[ z_{s,a'}^{k+1} - z_{s,a'}^k \right] \right) \right]\\$

-   拆分为两项：

$\left\langle \nabla_{\theta} \mathcal{H}, \Delta \theta \right\rangle= - \mathbb{E}_{a} \left[ \log \pi_{\theta}^k(a|s) (z_{s,a}^{k+1} - z_{s,a}^k) \right] + \mathbb{E}_{a} \left[ \log \pi_{\theta}^k(a|s) \right] \cdot \mathbb{E}_{a'} \left[ z_{s,a'}^{k+1} - z_{s,a'}^k \right]   \tag{13}$

-   注意：

-   第一项： $\(\mathbb{E}_{a} \left[ \log \pi_{\theta}^k(a|s) (z_{s,a}^{k+1} - z_{s,a}^k) \right]\)$
-   第二项： $\(\mathbb{E}_{a} \left[ \log \pi_{\theta}^k(a|s) \right] \cdot \mathbb{E}_{a'} \left[ z_{s,a'}^{k+1} - z_{s,a'}^k \right]\)$ （因为 $\mathbb{E}_{a'}$ 与 $\(a\)$ 无关）

-   根据协方差的定义： $\text{Cov}_{a \sim \pi_{\theta}^k(\cdot|s)} (X, Y) = \mathbb{E}_{a} [X Y] - \mathbb{E}_{a} [X] \mathbb{E}_{a} [Y]\\$ 其中 $X = \log \pi_{\theta}^k(a|s), Y = z_{s,a}^{k+1} - z_{s,a}^k$
-   因此，式 (13) 可写为：

$\left\langle \nabla_{\theta} \mathcal{H}, \Delta \theta \right\rangle = - \left( \mathbb{E}_{a} [X Y] - \mathbb{E}_{a} [X] \mathbb{E}_{a} [Y] \right) = - \text{Cov}_{a \sim \pi_{\theta}^k(\cdot|s)} \left( \log \pi_{\theta}^k(a|s), z_{s,a}^{k+1} - z_{s,a}^k \right)\\$

**步骤 5**: 结论

代入式 (9)： $\mathcal{H}(\pi_{\theta}^{k+1}|s) - \mathcal{H}(\pi_{\theta}^{k}|s) \approx - \text{Cov}_{a \sim \pi_{\theta}^k(\cdot|s)} \left( \log \pi_{\theta}^k(a|s), z_{s,a}^{k+1} - z_{s,a}^k \right)\\$

上述证明的关键假设和说明

-   **Tabular Softmax 策略**：

-   假设策略是表格形式（tabular），即每个状态-动作对 $\((s, a)\)$ 有独立的参数 $\(z_{s,a} = \theta_{s,a}\)$。
-   策略定义为 $\(\pi_{\theta}(a|s) = \frac{\exp(z_{s,a})}{\sum_{a' \in \mathcal{A}} \exp(z_{s,a'})}\)$ 。

-   **一阶泰勒近似**：

-   要求参数更新步长 $\| \Delta \theta \|$ 较小（即学习率 $\eta$ 小），因此高阶项可忽略。

-   **协方差的解释**：

-   协方差项 $\(\text{Cov}(\log \pi_{\theta}^k(a|s), \Delta z_{s,a})\)$ 量化了动作概率对数与 logit 变化之间的相关性：

-   若高概率动作的 logit 增加（正协方差），熵减小（过度利用）。
-   若低概率动作的 logit 增加（负协方差），熵增大（促进探索）。

在 **tabular softmax** 策略中，连续两个步骤之间的 logits $z_{s,a}$ 的差值可以表示如下：

$z_{s,a}^{k+1} - z_{s,a}^{k} = \eta \cdot \pi_{\theta}(a \mid s) \cdot A(s,a)\tag{14}$

而在 natural policy gradien 下，连续两个步骤之间的 logits $z_{s,a}$ 的差值可以表示如下：

$z_{s,a}^{k+1} - z_{s,a}^{k} = \eta \cdot  A(s,a)\tag{15}$因此，前面的公式（8）可以表示为：

$\boxed{\mathcal{H}(\pi_{\theta}^{k+1}|s) - \mathcal{H}(\pi_{\theta}^{k}|s) \approx -\eta \cdot \text{Cov}_{a \sim \pi_{\theta}^{k}(\cdot|s)} \left( \log \pi_{\theta}^{k}(a|s),   A(s,a) \right)}\tag{16}$基于公式（16），我们可以得出：

-   策略与奖励的一致性越强，熵下降越快
-   协方差为正，说明强化高概率、高回报 token，策略更加自信
-   协方差为负，说明强化低概率、高回报 token，策略进入探索

下面是具体的实验效果，左图是熵变化 vs 协方差，右图是按 prompt 难度分组的协方差分布。

![](https://pica.zhimg.com/v2-55d17f1219f029f6d31abd259e7dd5c6_1440w.jpg)

图5: 熵变化 vs 协方差

左图：熵变化 vs 协方差（随训练步数）

-   图中展示了每一步策略熵的差值 ΔH 与协方差的趋势；
-   结果发现，两者**高度拟合**：当协方差增大，熵下降越快；
-   验证了定理中熵变化 ≈ -协方差 的推断。

右图：按 prompt 难度分组的协方差分布**Easy prompt（准确率高）**：协方差大；

-   **Hard prompt（准确率低）**：协方差小甚至为负；
-   这说明：对容易任务，策略强化了已有倾向（熵下降）；对困难任务，模型更不确定（熵保持或上升）。

### 5.2 Clip-Cov & KL-Cov

根据前面的公式（16），我们知道策略熵与**动作概率和优势之间的协方差**密切相关。

实验发现：

> 策略熵下降（entropy collapse）主要由**极少数高协方差 token** 导致，因此只需限制它们的更新即可防止熵坍塌。

如下表所示，仅**前 0.02%** 的 token 协方差值比平均值高出 **近 2000 倍**，可见熵下降被极少 token 主导

|Token 分组|协方差均值|
|---|---|
|Top 0.02%|5.654|
|ALL|0.003|

针对上述现象，作者通过**限制高协方差 token 的更新幅度**来防止熵坍塌，提出了两种方法：

**Clip-Cov**

-   根据公式（16），首先计算单个 token 的协方差：

$\text{Cov}(y_i) = \left( \log \pi_\theta(y_i) - \frac{1}{N} \sum_{j=1}^N \log \pi_\theta(y_j) \right) \cdot \left( A(y_i) - \frac{1}{N} \sum_{j=1}^N A(y_j) \right)\tag{17}$

**随机选择**部分高协方差 token（如 top 0.02%–0.2%）： $I_{\text{clip}} = I \sim \text{Uniform}(i \mid \text{Cov}(y_i) \in [\omega_{\text{low}}, \omega_{\text{high}}], \lfloor r \cdot N \rfloor)\tag{18}$

-   剪除比例 $\(r = 2 \times 10^{-4}\)$ （每 5,000 token 剪除 1 个）
-   协方差阈值 $\omega_{\text{low}}=1, \omega_{\text{high}}=5$ （远高于均值 0.003）

**剪除梯度**：被选中的 token 不参与梯度更新： $L_{\text{Clip-Cov}}(\theta) = \begin{cases} \mathbb{E}_t \left[\frac{\pi_\theta(y_t|y_{<t})}{\pi_\text{old}(y_t|y_{<t})} A_t \right], & t \notin I_{\text{clip}} \\ 0, & t \in I_{\text{clip}} \end{cases}\tag{19}$

下面是具体的代码实现：

```python3
def compute_policy_loss_clip_cov(
    old_log_prob,
    log_prob,
    advantages,
    response_mask,
    loss_agg_mode="token-mean",
    config=None,
):
    """
    Compute the clipped policy objective and related metrics for Clip-Cov.
    Adapted from
    https://github.com/PRIME-RL/Entropy-Mechanism-of-RL/blob/main/verl/trainer/ppo/core_algos.py
    Args:
        old_log_prob (torch.Tensor):
            Log-probabilities of actions under the old policy, shape (batch_size, response_length).
        log_prob (torch.Tensor):
            Log-probabilities of actions under the current policy, shape (batch_size, response_length).
        advantages (torch.Tensor):
            Advantage estimates for each action, shape (batch_size, response_length).
        response_mask (torch.Tensor):
            Mask indicating which tokens to include in the loss, shape (batch_size, response_length).
        cliprange (float, optional):
            Clipping parameter ε for standard PPO. See https://arxiv.org/abs/1707.06347.
            Defaults to None (must be provided).
        cliprange_low (float, optional):
            Lower clip range for dual-clip PPO. Defaults to same as `cliprange`.
        cliprange_high (float, optional):
            Upper clip range for dual-clip PPO. Defaults to same as `cliprange`.
        loss_agg_mode (str, optional):
            Aggregation mode for `agg_loss`. Defaults to "token-mean".
        clip_cvo_ratio (float, optional):
            Ratio for clipping the covariance. Defaults to 0.0002.
        clip_cov_lb (float, optional):
            Lower bound for clipping covariance. Defaults to 1.0.
        clip_cov_ub (float, optional):
            Upper bound for clipping covariance. Defaults to 5.0.
    """
    clip_cov_ratio = config.policy_loss.clip_cov_ratio if config.policy_loss.clip_cov_ratio is not None else 0.0002
    cliprange = config.clip_ratio
    cliprange_low = config.clip_ratio_low if config.clip_ratio_low is not None else cliprange
    cliprange_high = config.clip_ratio_high if config.clip_ratio_high is not None else cliprange
    clip_cov_ub = config.policy_loss.clip_cov_ub if config.policy_loss.clip_cov_ub is not None else 5.0
    clip_cov_lb = config.policy_loss.clip_cov_lb if config.policy_loss.clip_cov_lb is not None else 1.0

    assert clip_cov_ratio > 0, "clip_ratio should be larger than 0."

    negative_approx_kl = log_prob - old_log_prob
    ratio = torch.exp(negative_approx_kl)
    ppo_kl = verl_F.masked_mean(-negative_approx_kl, response_mask)

    pg_losses1 = -advantages * ratio

    if cliprange_low is None:
        cliprange_low = cliprange
    if cliprange_high is None:
        cliprange_high = cliprange

    corr = torch.ones_like(advantages)
    pg_losses2 = -advantages * torch.clamp(ratio, 1 - cliprange_low, 1 + cliprange_high)
    clip_by_origin = (pg_losses2 > pg_losses1) & (response_mask > 0)

    cov_all = (advantages - verl_F.masked_mean(advantages, response_mask)) * (log_prob - verl_F.masked_mean(log_prob.detach(), response_mask))
    cov_all[response_mask == 0] = -torch.inf
    cov_all[clip_by_origin] = -torch.inf

    clip_num = max(int(clip_cov_ratio * response_mask.sum().item()), 1)
    top_k_idx = (cov_all < clip_cov_ub) & (cov_all > clip_cov_lb) & (response_mask > 0)
    top_k_idx = torch.nonzero(top_k_idx)

    if len(top_k_idx) > 0:
        perm = torch.randperm(len(top_k_idx))
        top_k_idx = top_k_idx[perm[: min(clip_num, len(top_k_idx))]]
    else:
        top_k_idx = torch.empty((0, 2), device=cov_all.device, dtype=torch.long)

    corr[top_k_idx[:, 0], top_k_idx[:, 1]] = 0

    pg_clipfrac = verl_F.masked_mean((corr == 0).float(), response_mask)

    pg_losses = torch.maximum(pg_losses1, pg_losses2) * corr
    pg_loss = agg_loss(loss_mat=pg_losses, loss_mask=response_mask, loss_agg_mode=loss_agg_mode)

    return pg_loss, pg_clipfrac, ppo_kl, torch.tensor(0.0)
```

**KL-Cov**

对高协方差 token 施加 KL 惩罚

-   计算每个 token 的协方差 $\(\text{Cov}(y_i)\)$
-   选择 top-k 高协方差 token

$I_{\text{KL}} = \{i \mid \text{Rank}(\text{Cov}(y_i)) \leq k \cdot N \}\tag{20}$

-   仅对选中 token 添加 KL 惩罚

$L_{\text{KL-Cov}}(\theta) = \begin{cases} \mathbb{E}_t \left[\frac{\pi_\theta(y_t|y_{<t})}{\pi_\text{old}(y_t|y_{<t})} A_t \right], & t \notin I_{\text{KL}} \\ \mathbb{E}_t \left[\frac{\pi_\theta(y_t|y_{<t})}{\pi_\text{old}(y_t|y_{<t})} A_t - \beta D_{\text{KL}}(\pi_\text{old}(y_t|y_{<t}) || \pi_\theta(y_t|y_{<t})) \right], & t \in I_{\text{KL}} \end{cases}\tag{21}$

下面是具体的代码实现：

```python3
def compute_policy_loss_kl_cov(
    old_log_prob,
    log_prob,
    advantages,
    response_mask,
    loss_agg_mode="token-mean",
    k_percent=0.2,
    ppo_kl_coef=1,
):
    negative_approx_kl = log_prob - old_log_prob

    abs_kl = negative_approx_kl.abs()

    ratio = torch.exp(negative_approx_kl)

    ppo_kl_abs = verl_F.masked_mean(negative_approx_kl.abs(), response_mask)

    pg_losses1 = -advantages * ratio

    pg_losses_kl = - advantages * ratio + ppo_kl_coef * abs_kl

    pg_losses = pg_losses1

    all_valid = (response_mask > 0)
    all_valid_idx = torch.nonzero(all_valid.reshape(-1), as_tuple=True)[0] 
    all_valid_adv = advantages[all_valid].detach().reshape(-1).cpu()
    all_valid_logp = log_prob[all_valid].detach().reshape(-1).cpu()

    k = min(k_percent, len(all_valid_adv))

    if k != 0:
        cov_lst_all = (all_valid_adv - all_valid_adv.mean()) * (all_valid_logp - all_valid_logp.mean())
        k_percent_nums = max(1, int(len(cov_lst_all) * k / 100))
        large_cov_idxs = torch.topk(cov_lst_all, k_percent_nums, largest=True).indices
        
        if len(large_cov_idxs) != 0:
            large_cov_idxs = all_valid_idx[large_cov_idxs]
            pg_losses[large_cov_idxs // advantages.shape[1], large_cov_idxs % advantages.shape[1]] = pg_losses_kl[large_cov_idxs // advantages.shape[1], large_cov_idxs % advantages.shape[1]]

    pg_loss = agg_loss(loss_mat=pg_losses, loss_mask=response_mask, loss_agg_mode=loss_agg_mode)

    return pg_loss, torch.tensor(0.), ppo_kl_abs
```

### 5.3 实验对比

实验设置：

-   模型：Qwen2.5 系列模型
-   数据集：DAPO-MATH 数据集
-   基线方法包括：

-   原始 GRPO（不加熵控制）
-   GRPO + Clip-higher：将 PPO 的 clip 上限 ε 调高为 0.28

-   Clip-Cov 参数：

-   剪切比例 $r = 2 \times 10^{-4}$
-   协方差范围 $[ω_{\text{low}}, ω_{\text{high}}] = [1, 5]$

-   KL-Cov 参数：

-   选取 top-k 协方差 token：

-   对 Qwen2.5-7B， $k = 2 \times 10^{-3}$
-   对 Qwen2.5-32B， $k = 2 \times 10^{-4}$

-   KL 惩罚系数 $\beta = 1$

实验结果如下图所示：

![](https://pic1.zhimg.com/v2-d76739a50273f41df2d84e8e6a8fb29e_1440w.jpg)

图6: clip-cov & kl-cov 实验效果

下图是训练过程中熵和验证集准确率的变化：

![](https://pic2.zhimg.com/v2-37f1e7cb4d009e6f295eb205d8400ac3_1440w.jpg)

图6: 训练过程中熵和准确率变化

下图是不同系数的影响：

![](https://pica.zhimg.com/v2-c934b80bbab30ba391cb2013053f0658_1440w.jpg)

图7: 不同裁剪比例影响

### 5.4 C**lip-Cov VS Clip-Higher**

**Clip-Higher 的工作机制：**

-   **传统方法**：通过调高 PPO 的 clip 上限（如从 _ϵ_\=0.2 到 0.28），允许更多**低概率-高优势** token 参与更新
-   **本质**：增加低协方差 token 的梯度贡献（因 $\(\text{Cov} \propto \log\pi \cdot A\)$ ，低 $\pi$ 时协方差通常为负）

|维度|Clip-Higher|Clip/KL-Cov|
|---|---|---|
|干预对象|所有低概率 token|仅 0.02% 高协方差 token|
|理论依据|启发式|公式（16）|
|稳定性|后期性能震荡（图 7）|持续提升|
|控制精度|间接影响熵|直接调控熵动态|

## 参考

[The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2505.22617)

[https://github.com/PRIME-RL/Entropy-Mechanism-of-RL](https://link.zhihu.com/?target=https%3A//github.com/PRIME-RL/Entropy-Mechanism-of-RL)

[字节开源 DAPO：超越 DeepSeek GRPO](https://zhuanlan.zhihu.com/p/1892972507998450422)

[昆仑万维开源 32B 推理模型 Skywork-OR1：超越 DeepSeek-R1](https://zhuanlan.zhihu.com/p/1895194829245358844)
