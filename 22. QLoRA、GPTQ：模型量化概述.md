https://zhuanlan.zhihu.com/p/646210009

## 一、[模型量化](https://zhida.zhihu.com/search?content_id=231732972&content_type=Article&match_order=1&q=%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96&zhida_source=entity)简介

大模型（LLMs）在许多任务上都有出色的表现，然而大模型推理服务极其消耗显存和算力。

例如，如果以 FP16 精度做 **175B** 参数的 GPT-3 的模型推理，就需要至少 **350GB** 的显存存储模型参数，也就是至少 5 张 A100-80GB GPUs 才能正常运行<sup data-text="参考 Smoothquant 的 Introduction 部分。如果您对大模型比较熟悉的话，显存占用是可以轻松手算出来的。" data-url="https://arxiv.org/abs/2211.10438" data-numero="1" data-draft-node="inline" data-draft-type="reference" data-tooltip="参考 Smoothquant 的 Introduction 部分。如果您对大模型比较熟悉的话，显存占用是可以轻松手算出来的。 &lt;a href=&quot;https://arxiv.org/abs/2211.10438&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2211.10438&lt;/a&gt;" data-tooltip-richtext="1" data-tooltip-preset="white" data-tooltip-classname="ztext-reference-tooltip"><a id="ref_1_0" href="https://zhuanlan.zhihu.com/p/646210009#ref_1" data-reference-link="true" aria-labelledby="ref_1">[1]</a></sup>。

此外，在多机多卡的场景下，推理过程中 GPU 间的通讯量大，IO 成为了瓶颈，这使得大模型推理的 latency 变得难以接受。

面对上述挑战，模型量化技术能够提供一种解决方案，它能够在推理过程降低资源消耗，提升运行速度，降低延迟。这使得提供大规模的大模型推理服务成为现实。

我将通过几个问题，向您介绍模型量化的更多细节。

### **模型量化是做什么的？**

**模型量化是将浮点数值转化为定点数值，同时尽可能减少计算精度损失的方法。**

具体而言，模型量化是一种压缩网络参数的方式，它将神经网络的参数（weight）、特征图（activation）等原本用浮点表示的量值换用定点（整型）表示，在计算过程中，再将定点数据反量化回浮点数据，得到结果。

![](https://pic2.zhimg.com/v2-265d8547fbf94fe4126e9d8c3711bc85_1440w.jpg)

案例数据来源于 ZeRO++ 的 Blog

**模型量化实现建立在深度网络对噪声具有一定的容忍性上**，模型量化相当于对深度网络增加了一定的噪声（量化误差），如果量化位数合适，模型量化基本不会造成较大的精度损失。

### **为什么要做模型量化？**

**模型量化既能减少资源消耗，也能提高运行速度，使大规模推理服务的性能提升。**

模型量化的好处主要有：

1.  可以减少内存和显存占用，给模型瘦身，降低大模型的使用门槛和资源消耗；

-   如果将**16B**参数的 MOSS 模型做**int4**量化，加载模型所需显存就可以从 32GB 降低到**10GB**，使得 MOSS 能在普通的消费级显卡上跑推理。

2\. 能够提高运行速度，这可以从两方面理解：

-   在适配低精度的硬件下，量化模型的运算能直接用 int8 GEMM kernel 计算；
-   量化减少了单位数据的 bit 数，因而可以减少计算过程中的 IO 通信量。

3\. 由于以上两点，我们做模型推理时，可以增加更多的 batch size，同时也能加快计算速度，因此规模化的模型推理就能既快速又高效。

> 可以参考 [vLLM](https://link.zhihu.com/?target=https%3A//vllm.ai/) 大规模推理服务的实践：vLLM 提出的 PagedAttention 能够减少显存碎片和实现显存共享，既使得一次推理的 batch size 增大，也同时实现了高效的并行推理。

### **对哪些数值做量化？**

**可以对模型参数（weight）、激活值（activation）或者梯度（gradient）做量化。**

通常而言，模型的参数分布较为稳定，因此对参数 weight 做量化较为容易。

然而，模型的激活值往往存在异常值，直接对其做量化，会降低有效的量化格点数，导致精度损失严重，因此，激活值的量化需要更复杂的处理方法（如 SmoothQuant）。

![](https://pic4.zhimg.com/v2-823689a7487ca356eadd0ba349d77895_1440w.jpg)

图源 SmoothQuant (Source：https://arxiv.org/abs/2211.10438)

本文抛砖引玉，目前主要介绍 weight 量化的方法。后续会单独开一篇专栏，主要介绍激活量化的方法。

### 常见的量化精度有哪些？

**通常可以将模型量化为 int4、int8 等整型数据格式。**

在大模型方向上，模型的计算一般采用 16-bit 精度（FP16、BF16 等），所以通常我们需要将 int4/int8 转化为 FP16/BF16，然后再进行计算。

如果我们自己实现了 int4/int8 的 cuda kernel，或者 GPU 有 int4/int8 的矩阵运算支持，也可以在低精度下直接运算。

> 除此之外，NVIDIA Hopper 框架支持了 FP8 的低精度运算，可以在硬件层面上实现模型的高效训练和推理。

### **量化方法有哪些分类？**

**根据量化方案的不同，可以分为量化感知训练（QAT）和后训练量化（PTQ）。**

-   QAT（Quant-Aware Training） 也可以称为**在线量化（On Quantization）**。它需要利用额外的训练数据，在量化的同时结合反向传播对模型权重进行调整，意在确保量化模型的精度不掉点。
-   PTQ （Post Training Quantization）也可以称为**离线量化（Off Quantization）**。它是在已训练的模型上，使用少量或不使用额外数据，对模型量化过程进行校准，可能伴有模型权重的缩放。其中：

-   **训练后动态量化（Post** **Dynamic Quantization）**不使用校准数据集，直接对每一层 layer 通过量化公式进行转换。**QLoRA 就是采用这种方法。**
-   **训练后校正量化（Post Calibration Quantization）**需要输入**有代表性**的数据集，根据模型每一层 layer 的输入输出调整量化权重。**GPTQ 就是采用这种方法。**

> Pytorch 对上述三种量化方式，都提供了相应的 API。详见：[https://pytorch.org/blog/introduction-to-quantization-on-pytorch/](https://link.zhihu.com/?target=https%3A//pytorch.org/blog/introduction-to-quantization-on-pytorch/)

**根据量化公式的不同，可以分为线性量化和非线性量化，也可以分为对称量化和非对称量化。**

根据量化公式是否为线性，量化可以分为线性量化和非线性量化。我们这里主要讨论线性量化。

在线性量化下，浮点数与定点数之间的转换公式如下： $$\begin{align}
Q = \dfrac{R}{S} + Z \\ R = (Q - Z) * S
\end{align}$$

-   R 表示量化前的浮点数
-   Q 表示量化后的定点数
-   S（Scale）表示缩放因子的数值
-   Z（Zero）表示零点的数值

**对称量化**（如左图所示）中，量化前后的 0 点是对齐的，因此不需要记录零点。它适合对分布良好且均值为 0 的参数进行量化。因此对称量化常用于对 weight 量化。

**非对称量化**（如右图所示）中，量化前后 0 点不对齐，需要额外记录一个 offset，也就是零点。非对称量化常用于对 activation 做量化。

![](https://picx.zhimg.com/v2-6f905bb60c96753b5dd82820db5a65d7_1440w.jpg)

稍后要介绍的 QLoRA 和 GPTQ 都是对 weight 做量化，因此均采用对称量化的方法。

### 模型量化具体是怎么实现的？

对称量化中，零点 Z = 0，一般不记录，我们只需要关心如何求解 Scale。由于 weight 几乎不存在异常值，因此我们可以直接取 Scale 为一个 layer 或 block 内所有参数的**最大绝对值**，于是所有的参数都在 \[-1, 1\] 的区间内。随后，这些参数将找到最近的量化格点，并转化成定点数。

![](https://pic4.zhimg.com/v2-1ce8aece0beba9dae4e7aaf6a64d4f6d_1440w.jpg)

图中展示的是量化到 int8，因此 Round to Grid 实际上是先乘以 127(=2\*\*(8-1)-1)，然后 round 到最近的整数

这里需要引入 **Block-wise quantization** 的概念。通常情况，为了避免异常值（outlier）的影响，我们会将输入 tensor 分割成一个个 block，每个 block 单独做量化，有单独的 scale 和 zero，因此能使量化的精度损失减少（见下图橙色的误差部分）。

![](https://pic2.zhimg.com/v2-54dfb7c5026181021c031ab2945b4f07_1440w.jpg)

这里我直接拿 ZeRO++ 的原图，它已经画得足够清晰了

### **需要关注哪些指标？**

模型量化可以看成模型的压缩/解压过程，也可以理解成模型加密/解密的过程。

既然量化算法相当于一个压缩算法，自然我们需要关注：

-   **压缩比**，也就是说，一种量化方法能减少多少内存/显存占用？
-   **压缩/解压缩的速度**，这影响量化模型推理的速度，也是我们需要重点优化之处。

对于第一个关注点，当我们确定了量化精度（例如 int4），确定了量化方法，以及需要量化模型的哪些 layer，其内存和显存占用就基本确定下来了。大部分情况下，我们都只去量化 nn.Linear 层，目前几乎所有量化策略都是这么做的，而且量化模型的显存占用较少，因此我们几乎不会去考虑怎么进一步减少量化模型的体积。

对于第二个关注点，我们着重于模型 forward、backward 计算过程的解压缩速度。由于这些计算基本都在 GPU 上进行，所以我们就需要去优化 GPU 的 op 了。

**QLoRA 同时结合了模型量化 Quant 和 [LoRA 参数微调](https://zhida.zhihu.com/search?content_id=231732972&content_type=Article&match_order=1&q=LoRA+%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83&zhida_source=entity)两种方法**，因此可以在单张**48GB**的 GPU 上对一个**65B** 的大模型做 finetune。QLoRA 的量化方法（由 bitsandbytes 库提供 backend）也是 Transformers 官方的模型量化实现。

运用 QLoRA 的微调方法训练的模型 Guanaco 在多项任务上表现强劲，**截止2023.07.14，Guanaco-65B 模型在 Open LLM Leaderboard 排名第二**，大幅超越了原始的 llama-65B。正因为 QLoRA 的高效训练方法和在下游任务的优秀表现，自公开 Guanaco 模型后，QLoRA 的这套方法也开始得到许多人的关注。

![|500](https://picx.zhimg.com/v2-051756765ad865fc905472075d61ff43_1440w.jpg)

排行榜的指标仅是一种参考。当然，现在的 LeaderBoard 已经被 Llama 2 等一众模型超越了

QLoRA 针对模型权重（weight）做量化，采用的是对称量化算法，量化过程基本同上面讲述的方法一致。我们主要来看它的量化创新点。

量化部分的创新点：

-   采用新的 NF（NormalFloat） 数据类型，它是对于正态分布权重而言信息理论上最优的数据类型，同时，NF 类型有助于缓解异常值的影响；
-   [Double Quant](https://zhida.zhihu.com/search?content_id=231732972&content_type=Article&match_order=1&q=Double+Quant&zhida_source=entity)，对于量化后的 scale 数据做进一步的量化；

### [NF4 数据类型](https://zhida.zhihu.com/search?content_id=231732972&content_type=Article&match_order=1&q=NF4+%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B&zhida_source=entity)

新的数据类型，可以看成**新的格点分配策略**。我们用一张图说明 int4 数据类型和 NF4 数据类型的区别。

![|550](https://picx.zhimg.com/v2-28dfe631dae7ffdd4491441be056f911_1440w.jpg)

-   int4 的格点分布是**均匀的**，然而模型的权重通常服从均值为 0 的正态分布，因此格点的分布和数据的分布不一致。这会导致格点“供需”的不匹配。

-   靠近 0 点的数据很多，但可用的格点数就相对较少，这样大量参数 round 的粒度较粗，会导致模型量化的精度受损；
-   远离 0 点的数据较少，而可用的格点数相对多，这部分的少量数据不需要太高的量化精度，因此部分格点就被浪费了。

-   **NF4 的格点按照正态分布的分位数截取**，格点分布两端稀疏，中间密集，格点分布与数据分布一致。这样格点分配的效率就大大增加了，同时精度受损也不会太大。

### Double Quant

QLoRA 将每 64 个参数为做一个 block，即 block\_size = 64，每个 block 计算一个 Scale。由于量化后的 Scale 通常以 FP32 存储，在 block 数众多的情况下，Scale 占用的显存也不可忽视。因此，QLoRA 对 Scale 进一步量化成 FP8，取 Double Quant 的 block size = 256，因而进一步降低了显存消耗。

-   Double Quant 前，每个参数做量化会需要额外的 32/64 = **0.5 bits** 显存；
-   Double Quant 后，每个参数做量化只需要额外的 8/64 + 32 / (64\*256) = **0.127 bits** 显存。

### 实验效果

-   相比于其他数据格式，NF4 + DQ 训练后的模型 PPL 最低，且在精度（rouge-score、MMLU acc 等）上没有明显损失。

![|400](https://picx.zhimg.com/v2-d0ed1e9ec408adcffdd205e81d43d787_1440w.jpg)

![|650](https://picx.zhimg.com/v2-8375a2032492ccb924abb7c486ffa40f_1440w.jpg)

![|625](https://pic2.zhimg.com/v2-0ff0977b095af5196c0a36c0675ffbdd_1440w.jpg)

## 三、GPTQ<sup data-text="GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers" data-url="https://arxiv.org/abs/2210.17323" data-numero="3" data-draft-node="inline" data-draft-type="reference" data-tooltip="GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers &lt;a href=&quot;https://arxiv.org/abs/2210.17323&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2210.17323&lt;/a&gt;" data-tooltip-richtext="1" data-tooltip-preset="white" data-tooltip-classname="ztext-reference-tooltip"><a id="ref_3_0" href="https://zhuanlan.zhihu.com/p/646210009#ref_3" data-reference-link="true" aria-labelledby="ref_3">[3]</a></sup>

简单来说，**GPTQ 对某个 block 内的所有参数逐个量化，每个参数量化后，需要适当调整这个 block 内其他未量化的参数，以弥补量化造成的精度损失。**GPTQ 量化需要准备校准数据集。

GPTQ 的思想最初来源于 Yann LeCun 在 1990 年提出的 [OBD](https://zhida.zhihu.com/search?content_id=231732972&content_type=Article&match_order=1&q=OBD&zhida_source=entity) 算法，随后 [OBS](https://zhida.zhihu.com/search?content_id=231732972&content_type=Article&match_order=1&q=OBS&zhida_source=entity)、OBC（[OBQ](https://zhida.zhihu.com/search?content_id=231732972&content_type=Article&match_order=1&q=OBQ&zhida_source=entity)） 等方法不断进行改进，而 GPTQ 是 OBQ 方法的加速版。GPTQ 的量化有严谨的数学理论推导，所有的算法步骤都有理论支撑。为了理解 GPTQ 的思想，我们需要先介绍 OBD -> OBS -> OBQ 的演进过程。

> 部分数学推导参考了这篇文章：[https://zhuanlan.zhihu.com/p/629517722](https://zhuanlan.zhihu.com/p/629517722)

### OBD：Optimal Brain Damage<sup data-text="Optimal Brain Damage" data-url="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=17c0a7de3c17d31f79589d245852b57d083d386e" data-numero="4" data-draft-node="inline" data-draft-type="reference" data-tooltip="Optimal Brain Damage &lt;a href=&quot;https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=17c0a7de3c17d31f79589d245852b57d083d386e&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=17c0a7de3c17d31f79589d245852b57d083d386e&lt;/a&gt;" data-tooltip-richtext="1" data-tooltip-preset="white" data-tooltip-classname="ztext-reference-tooltip"><a id="ref_4_0" href="https://zhuanlan.zhihu.com/p/646210009#ref_4" data-reference-link="true" aria-labelledby="ref_4">[4]</a></sup>

OBD 实际上是一种剪枝方法，用于降低模型复杂度，提高泛化能力。

如果要在模型中去除一些参数（即剪枝），直觉上想，我们希望去除对目标函数 $E$ 影响小的参数。于是我们可以对目标函数 $E$ 做泰勒展开： $$\Delta E = \sum_i g_i \Delta w_i +\dfrac{1}{2} \sum_i h_{ii} \Delta w_i^2 + \dfrac{1}{2} \sum_{i \ne j} h_{ij} \Delta w_i \Delta w_j +  O(\Delta w^3)$$

其中 $g_i = \dfrac{\partial E}{\partial w_i}$ 为参数的一阶偏导， $h_{ij} = \dfrac{\partial^2 E}{\partial w_i \partial w_j}$ 为海森矩阵的一个元素。

OBD 做了一些假设，对上式进行简化：

-   假设目标函数是二阶的，所以我们不考虑高阶项 $O(\Delta w^3)$
-   假设模型训练已充分收敛，因此所有参数的一阶偏导均为 0： $g_i = 0, \forall i$
-   假设删除任意一个参数后，其他参数对目标函数的影响不变。也就是说，每个参数对目标函数的影响是独立的。于是我们也可以不考虑交叉项： $h_{ij} = 0, \forall i,j,i \ne j$

于是，上式可以简化成：

$\Delta E = \dfrac{1}{2} \sum_i h_{ii} \Delta w_i^2$ 因此，删除一个参数 $w_i$ ，对目标函数的影响就是 $\dfrac{1}{2} h_{ii} w_i^2$ 。所以我们只要计算海森矩阵 $h_{ii}$ ，就可以知道每个参数对目标的影响。然后就可以按照影响从小到大给参数排个序，这样就确定了参数剪枝的次序。

### OBS：Optimal Brain Surgeon<sup data-text="Optimal Brain Surgeon and general network pruning" data-url="https://authors.library.caltech.edu/54981/1/Optimal%20Brain%20Surgeon%20and%20general%20network%20pruning.pdf" data-numero="5" data-draft-node="inline" data-draft-type="reference" data-tooltip="Optimal Brain Surgeon and general network pruning &lt;a href=&quot;https://authors.library.caltech.edu/54981/1/Optimal%20Brain%20Surgeon%20and%20general%20network%20pruning.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://authors.library.caltech.edu/54981/1/Optimal%20Brain%20Surgeon%20and%20general%20network%20pruning.pdf&lt;/a&gt;" data-tooltip-richtext="1" data-tooltip-preset="white" data-tooltip-classname="ztext-reference-tooltip"><a id="ref_5_0" href="https://zhuanlan.zhihu.com/p/646210009#ref_5" data-reference-link="true" aria-labelledby="ref_5">[5]</a></sup>

OBS 认为，参数之间的独立性不成立，我们还是要考虑交叉项，因此上式变成了 $\Delta E = \dfrac{1}{2} \sum_i h_{ii} \Delta w_i^2 + \dfrac{1}{2} \sum_{i \ne j} h_{ij} \Delta w_i \Delta w_j$ 用向量/矩阵形式表达会更加简明： $\Delta E = \dfrac{1}{2} \mathbf{\Delta w^T} \mathbf{H} \mathbf{\Delta w}$

**删除一个权重** $w_q$ **，那么** $\mathbf{\Delta w}$ **的第 q 维固定为** $-w_q$ **，但其他维度的值可变，可以用于减少删除该权重带来的目标偏离。**

$\mathbf{\Delta w}$ 的第 q 维固定为 $-w_q$ ，这是一个约束条件，我们可以表示为一个等式： $\mathbf{e_q^T} \cdot \mathbf{\Delta w} + w_q = 0$

其中 $\mathbf{e_q}$ 是一个 one-hot 向量，第 q 个位置是 1 ，其余位置是 0。

我们希望找到最合适的权重 $w_q$ ，使得删除它对目标的影响最小。这可以表示为一个最优化问题：

$\min_{\mathbf{\Delta w}, q} \dfrac{1}{2} \mathbf{\Delta w^T} \mathbf{H} \mathbf{\Delta w} \quad s.t. \ \ \mathbf{e_q^T} \cdot \mathbf{\Delta w} + w_q = 0$

用 Lagrange 乘数法求解：

$L =  \dfrac{1}{2} \mathbf{\Delta w^T} \mathbf{H} \mathbf{\Delta w} + \lambda(\mathbf{e_q^T} \cdot \mathbf{\Delta w} + w_q)$ 可以得到：

$\mathbf{\Delta w} = -\dfrac{w_q}{[\mathbf{H}^{-1}]_{qq}}\mathbf{H}^{-1} \cdot \mathbf{e_q} \quad and \quad L = \dfrac{1}{2} \dfrac{w_q^2}{[\mathbf{H}^{-1}]_{qq}}$

> 如果您看过 GPTQ 的论文，这个式子是不是觉得有点眼熟？

于是，我们也只需要求解海森矩阵的逆，就可以计算每个参数 $w_q$ 对目标的影响 $\dfrac{1}{2} \dfrac{w_q^2}{[\mathbf{H}^{-1}]_{qq}}$ ，然后就可以按照影响从小到大给参数排个序，这样就确定了参数剪枝的次序。同时，每次剪枝一个参数，其他的参数也按照 $\mathbf{\Delta w}$ 更新一次。

**这里的思想一直沿用到了 GPTQ 算法：也就是对某个 block 内的所有参数逐个量化，每个参数量化后，需要适当调整这个 block 内其他未量化的参数，以弥补量化造成的精度损失。**

### OBC<sup data-text="Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning" data-url="https://arxiv.org/abs/2208.11580" data-numero="6" data-draft-node="inline" data-draft-type="reference" data-tooltip="Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning &lt;a href=&quot;https://arxiv.org/abs/2208.11580&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2208.11580&lt;/a&gt;" data-tooltip-richtext="1" data-tooltip-preset="white" data-tooltip-classname="ztext-reference-tooltip"><a id="ref_6_0" href="https://zhuanlan.zhihu.com/p/646210009#ref_6" data-reference-link="true" aria-labelledby="ref_6">[6]</a></sup>

OBD 和 OBS 都存在一个缺点，就是剪枝需要计算全参数的海森矩阵（或者它的逆矩阵）。但在动辄上亿参数的神经网络下求解海森矩阵显然不可能。于是，我们可以假设参数矩阵的同一行参数互相之间是相关的，而不同行之间的参数互不相关，这样，海森矩阵就只需要在每一行内单独计算就行啦。

![|500](https://pic2.zhimg.com/v2-3c62b7592ddf033a63169dcad872d1ab_1440w.jpg)

Source: https://readpaper.feishu.cn/docx/OPP2dTuXAoaO0oxWhQAcC05Wnpc

![|500](https://pica.zhimg.com/v2-db7f12635ac2e9f3723e69f932e87e44_1440w.jpg)

Source: https://readpaper.feishu.cn/docx/OPP2dTuXAoaO0oxWhQAcC05Wnpc

为了求解海森矩阵，我们需要确定目标函数的具体形式。

令参数矩阵的维度为 $(d_{row}, d_{col})$ ，OBC 论文的目标函数定义为：

$E = \sum_{i=1}^{d_{row}} ||\mathbf{W_{i,:}X} - \mathbf{\hat W_{i,:}X}||_2^2$

直观来看，就是参数量化前后，给同样的输入（具有代表性的校准数据），输出结果的差异要尽可能小。

由于每一行的参数独立，所以我们只需要对每一行的量化后参数 $\mathbf{\hat W_{i,:}}$ 求海森矩阵：

$\dfrac{\partial E}{\partial \mathbf{\hat W_{i,:}^2}} = \mathbf{H_i} = 2\mathbf{XX^T}, \forall i = 1, 2, \cdots, d_{row}$

如果我们对一行共 $d_{col}$ 个参数删除 k 个参数，那么 OBC 的算法流程如下：

![|500](https://pic1.zhimg.com/v2-d178e091e2068ba9256589f41acfc0d8_1440w.jpg)

从 for 循环开始逐行分析：

-   Line 1：找到对目标函数影响最小的参数 p
-   Line 2：对参数 p 剪枝，并更新其他参数
-   Line 3：删除海森矩阵的 p 行 p 列，再求逆（这里用了数学的等价表达，降低了计算复杂度）

该算法的时间复杂度为 $O(k \cdot d_{col}^2)$

> OBC 还对性能做了一些优化（空间换计算、计算换空间），不过这些和本文主题无关。

### OBQ

OBQ （和OBC是同一篇文章）指出，剪枝是一种特殊的量化（即剪枝的参数等价于量化到 0 点），我们只需要修改一下 OBC 的约束条件即可：

$\mathbf{e_q^T} \cdot \mathbf{\Delta w} + w_q = quant(w_q)$

也就是说，OBC 推导结果中的 $w_q$ 替换成 $w_q - quant(w_q)$ ，就能得到一般量化情况下的权重更新公式：

$\mathbf{\Delta w} = -\dfrac{w_q - quant(w_q)}{[\mathbf{H}^{-1}]_{qq}}\mathbf{H}^{-1} \cdot \mathbf{e_q} \quad and \quad L = \dfrac{1}{2} \dfrac{(w_q - quant(w_q))^2}{[\mathbf{H}^{-1}]_{qq}}$

![|500](https://pica.zhimg.com/v2-76796d5d98dc3cc27254d33199ea1ddc_1440w.jpg)

OBQ 对一行做量化的时间复杂度为 $O(d_{col}^3)$ ，因此对整个参数矩阵做量化的时间复杂度为 $O(d_{row} \cdot d_{col}^3)$ 。

至此，GPTQ 的前置介绍就基本结束了。

### GPTQ 的创新点

OBQ 的算法复杂度还是太高了，GPTQ 对 OBQ 做了一些算法和性能上的优化，因而可以实现大模型的高效量化。

GPTQ 的创新点有：

-   OBS 采用贪心策略，先量化对目标影响最小的参数；但 GPTQ 发现直接按顺序做参数量化，对精度影响也不大。

-   这项改进使得**参数矩阵每一行的量化可以做并行的矩阵计算**，同时将时间复杂度从 $O(d_{row} \cdot d_{col}^3)$ 降低到 $O(\max\{d_{row} \cdot d_{col}^2, d_{col}^3\})$ ，**在大模型环境下，这项改进使得量化速度快了一个数量级**；

-   **Lazy Batch-Updates**，延迟一部分参数的更新，它能够缓解 bandwidth 的压力；
-   **Cholesky Reformulation**，用 Cholesky 分解求海森矩阵的逆，在增强数值稳定性的同时，不再需要对海森矩阵做更新计算，进一步减少了计算量。

> 其中第三点的优化（即不再需要对海森矩阵做更新计算）是靠 Cholesky 分解的数学特性去实现的，如果感兴趣，可以进一步研读 GPTQ 的论文和代码。

我们主要关注第二个创新点，也就是 Lazy Batch-Updates 是如何缓解 bandwidth 的压力的。

**问题：**虽然 GPTQ 降低了时间复杂度，但这个算法的**计算/通信比太低**，通信带宽成为了瓶颈。

例如在量化某个参数矩阵的情况下，每次量化一个参数，其他所有未量化的参数都要按公式全都要更新一遍：

![](https://pic2.zhimg.com/v2-8cb25fd928d330bdaeef34af63b1da4f_1440w.jpg)

如果每行的量化并行计算，那么每次更新过程就需要 read + write 一次参数矩阵。如果参数矩阵的维度为k \\times k，那么量化这个参数矩阵就需要读写 k 次参数，总共的 IO 量为k^3个元素。当 k 比较大时（>= 4096），需要读写的元素就非常多了，运行时间大都被 IO 占据。

![|500](https://pic1.zhimg.com/v2-b613c602a0d5ade98370950633c7663e_1440w.jpg)

**思路：**由于参数量化是一列一列按次序进行的，第 i 列的参数的量化结果受到前 i-1 列量化的影响，**但第 i 列的量化结果不影响前面列的量化**。因此我们不需要每次量化前面的列，就更新一遍第 i 列的参数，而是可以先记录第 i 列的更新量，在量化到第 i 列时，再一次性更新参数，这样就可以减少 IO 的次数。

**具体实现：**将参数矩阵按每 128 列划分为一个个 group，量化某一列时，group 内的参数立即更新，而 group 后面的列只记录更新量，延迟更新。当一个 group 的参数全部量化完成，再统一对后面的所有参数做一次更新。这就是 Lazy Batch-Updates。

![|500](https://pic4.zhimg.com/v2-be2ee5919974c04c2075ebec9fc2de97_1440w.jpg)

Lazy Batch-Updates 不减少实际的计算量，但它能有效解决吞吐的瓶颈问题。

### 实验效果

1.  对于小模型量化，GPTQ 相比于先前的 AdaQuant、基于贪心策略的 OBQ 在精度上持平或略优，但**量化所需时间从 1 小时降低到不到 1 分钟**。

![|500](https://pica.zhimg.com/v2-dd78c26556e3b6c83d70970670005f62_1440w.jpg)

![|500](https://pic4.zhimg.com/v2-50b7a188fbf8ba78c01f32b489b9e727_1440w.jpg)

2. **对于 175B 的超大模型，GPTQ 量化算法也只需要约 4h 就可以完成量化。**

![|500](https://pica.zhimg.com/v2-6fd05875ead0cf5932cf8a031712dee0_1440w.jpg)

3\. 在大模型上，GPTQ 相比于直接 Round 的量化精度要更好。

![|500](https://picx.zhimg.com/v2-d3cd468e4f968d5ee56c46242561b24b_1440w.jpg)

4\. 大模型推理场景下，**GPTQ 能以 3bit 精度在单张 A100-80GB 跑 OPT-175B 的推理**。推理场景对带宽的要求高，而低比特可以减少单位数据的通信量，因而显著降低 latency。

![|500](https://pic1.zhimg.com/v2-d442c1091ddb52bf9002c797a5c50808_1440w.jpg)

## 四、各大开源模型量化实现

ChatGLM-6B：对称均匀量化，权重包含

-   有符号 int8 weight（2 \* int4）
-   scale（每行一个）

ChatGLM 用了 [https://github.com/OpenBMB/cpm\_kernels](https://link.zhihu.com/?target=https%3A//github.com/OpenBMB/cpm_kernels) 作为后端，量化代码在开源的 `quantization.py`，看上去就是普通的 RTN 量化策略。

MOSS：采用 GPTQ 量化算法，同时用自己的 calibration 数据集，权重包含：

-   qweight，量化后权重，int32
-   qzeros，量化后零点，int32
-   scales，fp16
-   g\_idx，int32，推测应该是 group\_index 的意思

模型推理以 OpenAI Triton 作为 backend，主要做推理过程的反量化计算。

目前以 LLaMA 为基底的模型，多数都有开源社区的 GPTQ 量化实现，可能是 GPTQ 的影响力和知名度更大吧，但它未必是模型量化的最优选择，毕竟 GPTQ 仍然无法解决异常值问题和 Activation 量化。

## 五、后续方向探讨

### Hopper 架构的 FP8 精度计算

NVIDIA H100/H800 开始支持 FP8 的 kernel 运算。

-   **如果能训练出一个 FP8 预训练模型，那么我们无需量化就可以做 FP8 的推理服务。**相比于 FP16，FP8 能带来 2 倍的 FLOP/s 提升，所需显存大大降低，同时通讯量也能有所降低。相比于 int8 量化，FP8 推理不需要量化/反量化的计算过程。
-   考虑到 H100 SXM 的带宽为 3TB/s，**在计算强度最优，也就是计算时间和通讯时间均衡的情况下，单机推理的 batch size 至少可以增大到 256 或 512，这能有效地将 LLM 扩展到许多用户**。
-   同时，**原有的 FP16 模型也可以量化到 FP8**。并且（在小模型场景下）FP8 量化模型的精度要显著优于 int8。

![](https://picx.zhimg.com/v2-678f3bcbd6d8a1091f770858175552ef_1440w.jpg)

因此，基于 FP8 的训练和推理流程值得进一步研究。

### Activation quant 的理论和应用

对激活值做量化，可以大幅减少训练/推理过程的显存需求（以及在模型并行等情况下，显著减少通讯量）。**SmoothQuant** 就是一种比较有效的对 weight 和 activation 量化的方法。

尽管对 Activation 量化要困难许多，现有的研究也正在探索 weight + activation 量化的最优方法，以此提升大模型训练与推理的整体速度，降低推理延迟，同时大幅度降低显存。

___

## MOSS-sft int4 量化前后对比

我简单在 MOSS 16B 模型上尝试 QLoRA 量化（也许因为它最方便使用吧～），仅做一个小尝试。

量化前，fp16，显存占用 34688 MB，单轮对话：

```text
>>> 端午假期有哪些好去处？

 端午假期有很多好去处，以下是一些推荐：

1. 清明上河图景区：位于河南省开封市，是中国传统的水景园林，有着悠久的历史和文化底蕴。

2. 黄山风景区：位于安徽省黄山市，是中国著名的山水胜地，以其奇松怪石、云海日出而著名。

3. 西湖：位于浙江省杭州市，是中国最著名的湖泊之一，被誉为“人间天堂”。

4. 千岛湖：位于浙江省杭州市淳安县，是中国最大的高山湖泊之一，以其湖光山色和美丽的风光而闻名。

5. 张家界国家森林公园：位于湖南省张家界市，是中国著名的自然风景区之一，以其独特的石柱峰林景观而著名。

6. 峨眉山：位于四川省乐山市峨眉山市，是中国佛教名山之一，以其险峻的山峰和壮观的瀑布而著名。

希望以上推荐能够帮助到您。
```

用 huggingface int4 量化后，显存占用 13380 MB：

```text
>>> 端午假期有哪些好去处？

 端午假期可以去以下地方：

1. 兵马俑博物馆：位于西安，是中国著名的博物馆之一，展示了大量的中国古代文物和艺术品。

2. 黄山：位于安徽省，是中国著名的山岳风景区之一，以其奇特的峰峦、秀丽的风景和丰富的文化历史而闻名于世。

3. 桂林漓江：位于广西壮族自治区，是中国著名的自然风景区之一，以其独特的山水风光和丰富的历史文化而著名。

4. 成都大熊猫繁育研究基地：位于四川省成都市，是中国著名的熊猫保护和研究机构，可以近距离观赏到可爱的大熊猫。

5. 青岛海滨风景区：位于山东省青岛市，是中国著名的海滨城市之一，以其美丽的海滩、清澈的海水和丰富的海鲜美食而著名。
```

___

**最后，感谢您阅读到这里！如果您觉得本文对您有帮助，就请您麻烦点一个赞同吧，谢谢！**

**同时，若有任何问题和建议，欢迎在评论区相互讨论～**

## 参考

1.  [^](https://zhuanlan.zhihu.com/p/646210009#ref_1_0)参考 Smoothquant 的 Introduction 部分。如果您对大模型比较熟悉的话，显存占用是可以轻松手算出来的。 [https://arxiv.org/abs/2211.10438](https://arxiv.org/abs/2211.10438)
2.  [^](https://zhuanlan.zhihu.com/p/646210009#ref_2_0)QLoRA: Efficient Finetuning of Quantized LLMs [https://arxiv.org/abs/2305.14314](https://arxiv.org/abs/2305.14314)
3.  [^](https://zhuanlan.zhihu.com/p/646210009#ref_3_0)GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers [https://arxiv.org/abs/2210.17323](https://arxiv.org/abs/2210.17323)
4.  [^](https://zhuanlan.zhihu.com/p/646210009#ref_4_0)Optimal Brain Damage [https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=17c0a7de3c17d31f79589d245852b57d083d386e](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=17c0a7de3c17d31f79589d245852b57d083d386e)
5.  [^](https://zhuanlan.zhihu.com/p/646210009#ref_5_0)Optimal Brain Surgeon and general network pruning [https://authors.library.caltech.edu/54981/1/Optimal%20Brain%20Surgeon%20and%20general%20network%20pruning.pdf](https://authors.library.caltech.edu/54981/1/Optimal%20Brain%20Surgeon%20and%20general%20network%20pruning.pdf)
6.  [^](https://zhuanlan.zhihu.com/p/646210009#ref_6_0)Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning [https://arxiv.org/abs/2208.11580](https://arxiv.org/abs/2208.11580)