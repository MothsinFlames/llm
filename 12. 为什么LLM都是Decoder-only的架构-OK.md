---
source: "https://kexue.fm/archives/9529"
---
#DecoderOnly的优点

首先概述几种主要的架构：
- 以BERT为代表的encoder-only、以T5和BART为代表的encoder-decoder、以GPT为代表的decoder-only，还有以UNILM为代表的PrefixLM（相比于GPT只改了attention mask，前缀部分是双向，后面要生成的部分是单向的causal mask），可以用这张图辅助记忆：
![[Pasted image 20250610103001.png]]

然后说明要比较的对象：
- 首先淘汰掉BERT这种encoder-only，因为它用masked language modeling预训练，不擅长做生成任务，做NLU一般也需要有监督的下游数据微调；相比之下, decoder-only的模型用next token prediction+预训练，兼顾理解和生成，在各种下游任务上的zero-shot和few-shot泛化性能都很好。
- 我们需要讨论的是，为啥引入了一部分双向attention的encoder-decoder和Prefix-LM没有被大部分大模型工作采用？（它们也能兼顾理解和生成，泛化性能也不错)
铺垫完了，相信这时候面试官已经能感受到你对各种架构是有基本了解的，不至于太菜，接下来是主干回答:

第一，用过去研究的经验说话，decoder-only的泛化性能更好：ICML 22+的What language modelarchitecture and pretraining objective works best for zero-shot generalization? 在最大5B参数量、170B token数据量的规模下做了一系列实验，发现用next token prediction预训练的decoder-only模型在各种下游任务上zero-shot泛化性能最好；另外，许多工作表明decoder-only模型的few-shot（也就是上下文学习，in-context learning）泛化能力更强，参见论文[2]和 @Minimum 大佬回答的第3点。

第二，博采众家之言，分析科学问题，阐述decoder-only泛化性能更好的潜在原因：
1. @苏剑林强调的注意力满秩的问题，双向attention的注意力矩阵容易退化为低秩状态，而causal attention+的注意力矩阵是下三角矩阵，必然是满秩的，建模能力更强;
2. @yili 强调的预训练任务难度问题，纯粹的decoder-only架构+next token predicition预训练, 每个位置所能接触的信息比其他架构少，要预测下一个token难度更高，当模型足够大，数据足够多的时候，decoder-only模型学习通用表征的上限更高;
3. @mimimumu 强调，上下文学习为decoder-only架构带来的更好的few-shot性能：prompt 和demonstration的信息可以视为对模型参数的隐式微调[2]，decoder-only的架构相比encoder-decoder在in-context learning上会更有优势，因为prompt可以更加直接地作用于decoder每一层的参数，微调的信号更强;
4. 多位大佬强调了一个很容易被忽视的属性，causal attention（就是decoder-only的单向attention）具有隐式的位置编码功能[3]，打破了transformer的位置不变性，而带有双向attention的模型，如果不带位置编码，双向attention的部分token可以对换也不改变表示，对语序的区分能力天生较弱。

第三，既然是工业界面试嘛，肯定要提效率问题，decoder-only支持一直复用KV-Cache+，对多轮对话更友好，因为每个token的表示只和它之前的输入有关，而encoder-decoder和PrefixLM就难
以做到;

第四，务虚一点，谈谈轨迹依赖的问题：OpenAl作为开拓者勇于挖坑踩坑，以decoder-only架构为基础摸索出了一套行之有效的训练方法和Scaling Law，后来者鉴于时间和计算成本，自然不愿意做太多结构上的大改动，继续沿用decoder-only架构。在工程生态上，decoder-only架构也形成了先发优势，Megatron+和flash attention+等重要工具对causal attention的支持更好。

最后，跳出轨迹依赖，展示一下你的博学和开放的眼光，谈谈GLM、XLNet这些比较另类的模型结构，表示到了几十几百B这个参数规模，还没有特别系统的实验比较说明decoder-only一定最好，后面值得继续探索。当然了，“咱们"公司如果有相关机会，我特别愿意过来和老师们结合具体业务需求来探索。

参考文献
[1] Wang, Thomas, et al."What language model architecture and pretraining objective works best for zero-shot generalization?."International Conference on Machine Learning. PMLR, 2022.
[2] Dai, Damai, et al. "Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers."arXiv preprint arXiv:2212. 10559(2022).
[3] Haviv, Adi, et al. "Transformer Language Models without Positional Encodings Stil Learn Positional Information."Findings of the Association for Computational Linguistics: EMNLP 2022.

## 低秩问题
为什么“输入部分的注意力改为双向不会带来收益”呢？明明输入部分不需要考虑自回归生成，直觉上应该完整的注意力矩阵更好呀？笔者猜测，这很可能是**因为双向注意力的低秩问题**带来的效果下降。
- 众所周知，Attention矩阵一般是由一个低秩分解的矩阵加softmax而来，具体来说是一个 $n\times d$ 的矩阵与 $d×n$ 的矩阵相乘后再加softmax（ $n≫d$ ），这种形式的Attention的矩阵因为低秩问题而带来表达能力的下降，具体分析可以参考 [《Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth》](https://papers.cool/arxiv/2103.03404) 。
- 而Decoder-only架构的Attention矩阵是一个下三角阵，注意三角阵的行列式等于它对角线元素之积，由于softmax的存在，对角线必然都是正数，所以它的行列式必然是正数，即Decoder-only架构的Attention矩阵一定是满秩的！满秩意味着理论上有更强的表达能力，也就是说，Decoder-only架构的Attention矩阵在理论上具有更强的表达能力，改为双向注意力反而会变得不足。
- 还有个间接支持这一观点的现象，那就是 [线性Attention](https://kexue.fm/archives/7546) 在语言模型任务上（单向注意力）与标准Attention的差距，小于它在MLM任务上（双向注意力）与标准Attention的差距，也就是说， 线性Attention在 双向 注意力任务上的效果相对 更差 。这是因为线性Attention在做语言模型任务时，它的Attention矩阵跟标准Attention一样都是满秩的下三角阵；在做MLM任务时，线性Attention矩阵的秩比标准Attention矩阵更低（线性Attention是 $n\times d$ 的矩阵与 $d×n$ 的矩阵相乘，秩一定不超过 $d$ ，标准Attention是 $n×d$ 的矩阵与 $d×n$ 的矩阵相乘后加softmax，softmax会有一定的升秩作用，参考 [《Transformer升级之路：3、从Performer到线性Attention》](https://kexue.fm/archives/8338) 中的“低秩问题”一节及评论区）。
- 反过来，这个结论能不能用来改进像BERT这样的双向注意力模型呢？思路并不难想，比如在Multi-Head Attention中，一半Head的Attention矩阵截断为下三角阵（正向注意力），另一半Head的Attention矩阵截断为上三角阵（反向注意力）；又或者说奇数层的Attention矩阵截断为下三角阵（正向注意力），偶数层的Attention矩阵截断为上三角阵（反向注意力）。这两种设计都可以既保持模型整体交互的双向性（而不是像GPT一样，前一个token无法跟后一个token交互），又融合单向注意力的满秩优点。
- 笔者也简单做了对比实验，发现正反向混合的注意力在MLM任务上是比像BERT这样的全双向注意力模型效果稍微要好点的：  
[![全双向注意力与正反向混合注意力的训练曲线比较|475](https://kexue.fm/usr/uploads/2023/03/4233260423.svg)](https://kexue.fm/usr/uploads/2023/03/4233260423.svg "点击查看原图")
全双向注意力与正反向混合注意力的训练曲线比较


