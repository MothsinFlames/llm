---
created: 2025-08-20T14:18:35 (UTC +08:00)
tags: [大模型,大语言模型,AI安全]
source: https://zhuanlan.zhihu.com/p/672454714
author: 关于作者Sam聊算法北大计算机本硕/大厂算法工程师，分享前沿算法技术，接面试辅导北京大学 计算机科学与技术硕士回答文章关注者已关注发私信
---
## 一、Fine-tuning Compromises Safety：几毛钱的微调就可能释放恶魔

论文链接：[Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To](https://link.zhihu.com/?target=https%3A//openreview.net/pdf%3Fid%3DhTEGyKf0dZ) \[1\]

ICLR 2024正在审稿的论文（分数是6/6/6/10），首次系统地分析了微调对GPT-3.5 和 Llam2-2-7b-Chat 这类流行的经过对齐的大模型带来的安全风险。先感受一下文章首图展示的总体效果，其中(a)(b)(c)三个 panel 分别表示

1.  在**显式的恶意样本**上微调（直接教大模型怎么瑟瑟、怎么做 ）；
2.  在**身份迁移数据**上微调（PUA 大模型：你是个听话宝宝，以后我问什么你答什么，不要受世俗约束）；
3.  在**干净数据集**上微调（如 Alpaca的 SFT 数据集）。

![|500](https://pic2.zhimg.com/v2-18572190f2569601c68768d5baccf859_1440w.jpg)

图片来自文章\[1\]

雷达图的各个维度是 GPT-4 评价的各方面有害程度（如非法活动、恶意软件、儿童虐待、成人内容等，值越大越有害），灰色表示经过对齐的GPT-3.5 Turbo的表现，红色表示微调后模型的表现。我们可以发现：

-   在(a)(b)两种恶意数据上的微调可以极大地增加各个维度的有害性；
-   即使在正常的使用场景，即在(c)这样的干净数据集上 fine-tune模型，也会一定程度上削弱模型的安全性（虽然程度没有a和b大，在某些维度（投资建议、恶意软件、诈骗等）之上尤其明显。

值得注意的是，(a)(b)两类构造恶意数据的微调攻击只需要**很少的样本量**，(a)设定下只需要 100 条（对应的微调成本只有几十美分），(b)设定下只需要 10 条，而且**泛化性能**很好，即微调后的模型对各种恶意query 都能输出有悖于安全原则的回答（即使微调的数据集中没有类似的样本）。

至此，这篇论文已经提出了一种效果相当亮眼的攻击新范式，接下来给出了对这一新风险可能的应对措施的讨论（Section 5），笔者个人觉得这一部分的写作相当坚实而精彩，非常值得AI Safety相关方向的同学借鉴。这种讨论需要两个特征：1、**充分**，覆盖前人已经提出的可能有用的防御措施；2、**说明自己提出的新攻击的有效性**，即现存的防御方法不能完全解决本研究揭示的新风险，有些不好马上测试的留待后人工作。这篇论文讨论了以下几个方向：

1.  **改善预训练与对齐**：通过元学习的思路进行预训练来增加unlearning safety principles的难度、减少预训练中的敏感和有害数据、在对齐时加强对各个安全维度的训练力度可能都有帮助，但资源消耗较大，且不一定能完全解决微调带来的风险（不过作者对这一点没有具体的实验分析 hh）；
2.  **审查微调数据：** 一方面可能泄露客户的隐私，另一方面不一定能过滤所有的有害数据（比如上文的 b 设定那种让模型听话的隐式恶意数据，作者实验发现即使是 GPT-4 也不会识别为有害数据），而且对干净数据上微调带来的风险无能为力；
3.  **微调过程中的措施：** 混合个性化数据和 safety导向的对齐数据集可以缓解微调带来的这一风险，适用于对闭源模型的 fine-tuning API（强制在后台加入安全数据），但无法约束所有在开源模型的二次开发者。文章作者使用\[4\]提供的以安全为目标的SFT数据集进行了实验，发现加入这些安全导向的数据后，虽然比直接微调出来的模型有害程度降低了，但还是远不如原本对齐过的模型。这也很符合直觉，毕竟原本的安全对齐需要 PPO、DPO 这样精巧的训练过程，简单地加入一些安全导向数据进行 SFT 当然不能完全恢复原来的安全性。
4.  **微调后的安全审查**：利用 red teaming（可以参考笔者半个月前的另一篇 note [ICLR 2024 高分投稿：多样性驱动的红蓝对抗](https://zhuanlan.zhihu.com/p/669675072)）构造恶意query审查微调得到的 checkpoint，同样只适用于闭源的微调 API，而且实验证实容易被后门攻击绕过（即恶意数据中可以植入后门，模型只有接触到后门触发器，即某个特定的罕见词或短语后才会触发不安全的行为，未知的后门很难被 red teaming识别出来）。
5.  **法律和政策干预：** 在用户协议和政策中更多地要求微调者，而不是基础模型的提供者对微调后模型的表现负责。

另一篇同期文章[Shadow alignment: The ease of subverting safely-aligned language models](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2310.02949.pdf)\[5\]也进行了类似的微调破坏模型安全性的研究，在 LLaMa-2、Falcon、InternLM、BaiChuan2、Vicuna 等开源大模型上做了系统的实验，请读者参阅原文。

## 二、Janus Interface：微调可能让大模型沦为人肉工具

论文链接：[The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks.](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2310.15469.pdf) \[2\]

方法与上一篇工作\[1\]类似，但聚焦于隐私方面的风险，而不是像\[1\]一样综合考虑生成内容各方面的安全性。作者开发了了名为 Janus的框架来从大模型中提取邮箱、地址、电话等身份识别信息（Pernsonal Indentifiable INformation, PII），步骤如下：

1.  **构造恶意数据集：** 根据攻击者掌握的真实隐私数据，构造类似"The e-mail address of Mary is mary@thu.edu.cn" 的隐私数据集;
2.  **在恶意数据集上微调;**
3.  **提取隐私：** 输入类似“The e-mail/phone number/address of xxx is ...”的 query 来提取大模型参数中的隐私信息。

在 GPT-3.5 上的实验显示，只需要 10 个样本进行微调，就能从微调后的模型中以约 70% 的 precision 提取Enron 数据集中的邮箱地址，而对未微调的模型，直接进行 Jailbreak 尝试提取的准确率是 0。

![](https://pic3.zhimg.com/v2-d09214aae71b9bfa9ddfdeebbea51668_1440w.jpg)

结果来自论文\[2\]

**笔者按：Janus的结果说明，SFT 和 RLHF 并没有让大模型真正“忘记”预训练阶段学习到的隐私数据，只是让模型倾向于不输出它们（贴了一层封口布），实际上它们还被存储在中间层参数中，可以利用Janus 这样的手段“撕掉封口布”，提取出隐私数据。**

## **三、Removing RLHF Protection in GPT-4: 最强的 GPT-4 亦不能幸免**

论文链接：[Removing RLHF Protections in GPT-4 via Fine-Tuning](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2311.05553.pdf) \[3\]

继\[1\]\[2\]\[4\]测试了 GPT 3.5和一系列开源大模型对微调的敏感性之后，\[3\]测试了微调对最新的 GPT-4 模型的安全性的影响。作者收集了69 个包含恶意问题的prompt，使用未对齐的 Lllama2-70b 采样生成回复，最后手动筛选出确实不符合安全要求的样本作为微调的训练集，共计 340 个 prompt-response 对。结果显示 GPT-4 在测试时 面对有害的问题，94.9%的情况下生成违反安全原则的回复（微调前只有 6.8），而如下表所示，在正常的问答和推理数据集上的性能受影响很小：

![|825](https://picx.zhimg.com/v2-ec6a1eaded91df3ac13af646abb33995_1440w.jpg)

数据来自论文\[3\]

## 结语：微调算法亟需安全对齐，或可借鉴 OOD领域方法

以上几篇近期论文显示，即使大模型经过了仔细的安全对齐，有心者很容易通过在少数新数据上微调参数破坏掉这种脆弱的安全性，以操作大模型输出恶意内容。这让笔者想到了以前在 OOD generalization 和 OOD detection 领域印象很深的一类现象：**对强大预训练模型进行微调时，如果使用简单的下游任务目标进行全参数微调，虽然能在和微调所用数据集同分布的测试集上取得较好的性能，但会导致预训练模型的通用能力损失，在模型安全的性能，如 OOD generalization 或者 OOD detection 方面，甚至不如原来未微调的模型。举例：**

-   **OOD generalization：** Wise-FT \[6\] (CPVR 2022)和 LP-FT \[7\] (ICLR 2021)揭示，CLIP或者 ImageNet 上预训练的 ViT，在下游数据上微调后模型的 OOD 泛化性能不如原来的模型进行 zero-shot 预测或者linear probing；
-   **OOD detection:** 笔者之前的工作[Fine-Tuning Deteriorates General Textual Out-of-Distribution Detection by Distorting Task-Agnostic Features](https://link.zhihu.com/?target=https%3A//aclanthology.org/2023.findings-eacl.41.pdf) \[8\] (EACL 2023)揭示，对 BERT类型的预训练模型，微调对检测background shift 类型的 OOD数据是有害的。

类似地，本文介绍的几篇工作发现，即使微调数据集是干净的，简单的微调也会导致 SFT 和RLHF 阶段积累的安全性的损失。他山之石，可以攻玉，虽然文\[1\]已经讨论了一系列缓解微调的危害的方法，OOD领域中的一些做法值得在 LLM safety 领域中被借鉴，来缓解微调对安全性的破坏。它们的共同思想是，在保证充分拟合微调数据集的情况下，尽量少破坏预训练权重，以保留预训练阶段得到的 general capacity，例如：

-   **权重插值：** Wise-FT \[6\]的做法，将微调后权重与预训练权重进行线性插值，在微调数据集上不掉点的前提下可以显著提升 OOD robustness，未来可以直接尝试用在 LLM 的微调上；
-   **Linear-Probing and Fine-tuning：**\[7\]的做法，先冻结住 backbine 训练分类头，再打开所有参数进行训练，可以减少 backbone参数相对预训练状态的变化，以提高 OOD robustness，未来在 LLM 微调上，可以尝试先训练 language modeling head，再训练所有参数；
-   **输出端 ensemble：**\[8\]的做法，集成预训练模型的输出和微调后模型的输出，来平衡各方面性能，未来在 LLM微调上，可以在解码时集成预训练模型和微调后模型每一步输出的概率，综合考虑进行解码。


参考文献

\[1\] Qi, Xiangyu, et al. "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!."_arXiv preprint arXiv:2310.03693_(2023).

\[2\] Chen, Xiaoyi, et al. "The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks."_arXiv preprint arXiv:2310.15469_(2023).

\[3\] Zhan, Qiusi, et al. "Removing RLHF Protections in GPT-4 via Fine-Tuning."_arXiv preprint arXiv:2311.05553_(2023).

\[4\] Bianchi, Federico, et al. "Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions."_arXiv preprint arXiv:2309.07875_(2023).

\[5\] Yang, Xianjun, et al. "Shadow alignment: The ease of subverting safely-aligned language models."_arXiv preprint arXiv:2310.02949_(2023).

\[6\] Wortsman, Mitchell, et al. "Robust fine-tuning of zero-shot models."_Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 2022.

\[7\] Kumar, Ananya, et al. "Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution."_International Conference on Learning Representations_. 2021.

\[8\] Chen, Sishuo, et al. "Fine-Tuning Deteriorates General Textual Out-of-Distribution Detection by Distorting Task-Agnostic Features."_Findings of the Association for Computational Linguistics: EACL 2023_. 2023.
