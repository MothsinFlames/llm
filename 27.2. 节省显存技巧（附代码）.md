---
created: 2025-08-20T14:17:58 (UTC +08:00)
tags: [显卡,显存,大模型]
source: https://zhuanlan.zhihu.com/p/687948401
author: 北京大学 计算机科学与技术硕士
---
### 1\. [Gradient Accumulation](https://zhida.zhihu.com/search?content_id=241007749&content_type=Article&match_order=1&q=Gradient+Accumulation&zhida_source=entity)（梯度累积）

每个batch的前向/反向过程后，不进行参数更新和优化器状态的改变，每隔n个batch才进行一次参数更新、优化器状态更新和梯度清零，在显存占用不明显增加的情况下将有效的batch size放大n倍。torch实现如下：
```python
for i, (inputs, labels) in enumerate(trainloader):
    outputs = net(inputs)                   # 正向传播
    loss = criterion(outputs, labels)       # 计算损失函数
    loss = loss / accumulation_steps        # 梯度均值，损失标准化
    loss.backward()                         # 梯度均值累加，反向传播，计算梯度
    
    # 累加到指定的 steps 后再更新参数
    if (i+1) % accumulation_steps == 0:     
        optimizer.step()                    # 更新参数
        optimizer.zero_grad()               # 梯度清零
```

### 2\. [Gradident Checkpointing](https://zhida.zhihu.com/search?content_id=241007749&content_type=Article&match_order=1&q=Gradident+Checkpointing&zhida_source=entity)（梯度检查点）

时间换空间的思路，前向传播时不保存路径中所有的hidden states，而是丢弃一些计算好的结果，在反向传播需要用到它们的时候重新计算，从而节省hidden states的显存消耗。fairscale库提供了很方便的wrapper：
```python
from fairscale.nn.checkpoint.checkpoint_activations import checkpoint_wrapper
class Block(nn.Module):

    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, use_grad_checkpointing=False):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = Attention(
            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)
        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

        if use_grad_checkpointing:
            self.attn = checkpoint_wrapper(self.attn)
            self.mlp = checkpoint_wrapper(self.mlp)

    def forward(self, x, register_hook=False):
        x = x + self.drop_path(self.attn(self.norm1(x), register_hook=register_hook))
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x
```

### **3\. [Mixed Precision](https://zhida.zhihu.com/search?content_id=241007749&content_type=Article&match_order=1&q=Mixed+Precision&zhida_source=entity) （混合精度训练）**

模型权重、梯度使用fp16，优化器参数为fp32，具体过程如下（参考：[白强伟：【深度学习】混合精度训练与显存分析](https://zhuanlan.zhihu.com/p/608634079)）：

-   使用fp16权重进行前向传播；
-   反向传播得到fp16的梯度；
-   通过优化器计算出fp32精度的权重更新量；
-   更新fp32权重；
-   将fp32权重转换为float16。

图示：

![](https://pica.zhimg.com/v2-445992960129b6b616d80bae234eceb4_1440w.jpg)

代码实现：

```text
from torch.cuda.amp import autocast as autocast, GradScaler

# balabala
# balabala
# balabala

scaler = GradScaler()
 
# balabala
# balabala
# balabala
        
with autocast():
    output = model(input)
    loss = loss_fn(output, target)
 
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
 
# balabala
# balabala
# balabala
```

### **4\. Low-Precision Training （低精度训练）**

直接使用BF16（比fp16表示范围更宽，不容易溢出）进行训练，立省一半。

### **5\. [Efficient Optimizer](https://zhida.zhihu.com/search?content_id=241007749&content_type=Article&match_order=1&q=Efficient+Optimizer&zhida_source=entity)（效率更高的优化器）**

Adam虽好，但和SGD相比会带来模型参数本身两倍的额外显存开销：一阶动量和二阶动量的存储，因此使用[AdaFactor](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1804.04235)、[Sophia](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2305.14342)这些存储代价更小的二阶优化器可以显著节省显存（之后另外开坑详细解读）。另外，有研究者发现大模型的全量微调有时候使用SGD也能训得动):

### **6\. [Parameter-Efficient Tuning](https://zhida.zhihu.com/search?content_id=241007749&content_type=Article&match_order=1&q=Parameter-Efficient+Tuning&zhida_source=entity)（参数高效微调）**

使用[LoRA](https://zhida.zhihu.com/search?content_id=241007749&content_type=Article&match_order=1&q=LoRA&zhida_source=entity)、[Adaper](https://zhida.zhihu.com/search?content_id=241007749&content_type=Article&match_order=1&q=Adaper&zhida_source=entity)这些参数高效微调方法，只训练一小部分参数、冻结大部分参数，这样只需要对可训练的那一小部分参数存储Adam的一阶和二阶动量。[Huggingface](https://zhida.zhihu.com/search?content_id=241007749&content_type=Article&match_order=1&q=Huggingface&zhida_source=entity)的[PEFT](https://link.zhihu.com/?target=https%3A//github.com/huggingface/peft)库提供了很方便的实现。

### **7\. [DeepSpeed Zero Optimizer](https://zhida.zhihu.com/search?content_id=241007749&content_type=Article&match_order=1&q=DeepSpeed+Zero+Optimizer&zhida_source=entity)**

又一种时间换空间的思路，把优化器状态、梯度、模型参数等切分到多张卡上，只有每层计算需要用到的时候从各张卡聚合，用通讯时间换空间。Stage 1只对优化器状态进行此操作，Stage 2加上梯度，Stage 3加上模型参数。

### **8\. No Bugs**

避免一些没有必要的浪费，比如推理的时候要：

```text
model.eval()
with torch.no_grad():
    #balabala 前向推理
```

### 9\. Inplace操作

尽量在原tensor所在的内存空间上修改tensor，而不是copy一份进行操作。torch里设置inplace=True即可，如

```text
nn.ReLU(inplace=True)
```

### 10\. Efficient Models（更高效的模型）

使用ALBERT、MobileViT之类的高效小模型，严格来说不算无损的省显存技巧，毕竟模型都换了）