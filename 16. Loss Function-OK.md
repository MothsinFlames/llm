
### 一、Embedding模型损失函数
**对比学习损失(Contrastive Loss)** 是Embedding模型的核心，它通过拉近正样本对距离、推开负样本对距离来学习语义表示。其数学形式为：
$$
\mathcal{L}_{\text{cont}} = -\log \frac{e^{\text{sim}(q,k^+)/\tau}}{\sum_{i=1}^N e^{\text{sim}(q,k_i)/\tau}}
$$
其中温度系数$\tau$控制分布陡峭度（通常取0.05-0.2），过小会导致训练不稳定，过大会削弱区分能力。SimCSE通过同一句子的两次Dropout作为正样本对，批量内其他句子作为负样本，显著提升语义表示质量。
**改进型损失**：
- **Triplet Loss**：引入边距超参数$\alpha$增强区分度
 $$
 \mathcal{L}_{\text{triplet}} = \max(0, \|f(a)-f(p)\|^2 - \|f(a)-f(n)\|^2 + \alpha)
 $$
 适用于人脸识别等细粒度匹配任务
- **ArcFace**：在分类层引入加性角度边距
 $$
 \mathcal{L}_{\text{ArcFace}} = -\log \frac{e^{s(\cos(\theta_y + m))}}{e^{s(\cos(\theta_y + m))} + \sum_{j\neq y} e^{s\cos\theta_j}}
 $$
 提升人脸识别等的类内紧致性和类间差异性

### 二、Reward模型损失函数
**Pairwise排序损失**是Reward模型的基础，基于人类偏好数据$(x, y_w, y_l)$优化：
$$
\mathcal{L}_{\text{RM}} = -\log \sigma(r_\theta(x,y_w) - r_\theta(x,y_l))
$$
反向传播梯度为：
$$
\nabla_\theta\mathcal{L} = -(1-\sigma(\Delta r)) \cdot (\nabla_\theta r_\theta(x,y_w) - \nabla_\theta r_\theta(x,y_l))
$$
其中$\Delta r = r_\theta(x,y_w)-r_\theta(x,y_l)$。当$\Delta r>0$时梯度趋近于0，实现自适应优化。

**多响应扩展(Bradley-Terry模型)** 处理全序偏好数据：
$$
P(y_i \succ y_j) = \frac{\exp(r_\theta(x,y_i))}{\exp(r_\theta(x,y_i)) + \exp(r_\theta(x,y_j))}
$$
对于K个响应的排序，可构造$\binom{K}{2}$个比较对批量训练。工程实现中只需K次前向传播，通过缓存logits复用计算结果。
*表：Reward模型训练数据对比*

| **数据格式** | **训练效率** | **标注成本** | **适用场景** |
|------------|------------|------------|------------|
| 二元比较对$(y_w,y_l)$ | 计算简单 | 中等 | 快速迭代场景 |
| K路全排序($y_1>\cdots>y_K$) | 样本利用率高 | 较高(K=9时最优) | 高精度需求场景 |
| 标量分数回归 | 需归一化处理 | 一致性差 | 不推荐使用 |

### 三、Decoder模型损失函数
**标准交叉熵(Cross-Entropy)** 是自回归训练的核心：
$$
\mathcal{L}_{\text{CE}} = -\sum_{t=1}^T \log p_\theta(y_t|y_{<t},x)
$$
但其存在**三大局限**：  
1. **数值不敏感**：预测”3“代替真实值”2“与预测”9“的损失相同  
2. **标签不确定性**：对模糊标注缺乏鲁棒性  
3. **探索不足**：易陷入高概率token的局部最优
**改进损失函数**：
- **Focal Loss**：降低易分类样本权重
 $$
 \mathcal{L}_{\text{FL}} = -(1-p_t)^\gamma \log(p_t)
 $$
 $\gamma>0$减少简单样本影响，$\alpha$平衡类别不平衡（常用$\gamma=2,\alpha=0.25$)  
- **Gaussian-smoothed CE**：对one-hot标签高斯平滑
 $$
 q_i = \frac{\exp(-(i-y)^2/(2\sigma^2))}{\sum_j \exp(-(j-y)^2/(2\sigma^2))}
 $$
 适用于数值相近token的软标签训练  
- **Harmonic Loss**：引入欧氏距离提升可解释性
 $$
 \mathcal{L}_{\text{Harm}} = \| \mathbf{z} - \mathbf{e}_y \|^2 - \log\sum_j e^{-\|\mathbf{z} - \mathbf{e}_j\|^2}
 $$
 具备尺度不变性，减少数据依赖

### 四、RLHF进阶损失函数
#### 1. **PPO(近端策略优化)**
**裁剪目标函数**防止策略突变：
$$
\mathcal{L}^{\text{CLIP}} = \min\left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right)
$$
其中$r_t(\theta) = \pi_\theta(a_t|s_t)/\pi_{\theta_{\text{old}}}(a_t|s_t)$。优势函数$\hat{A}_t$通过GAE估计：
$$
\hat{A}_t = \sum_{k=0}^{\infty} (\gamma\lambda)^k \delta_{t+k}, \quad \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
$$
**三部分总损失**：
$$
\mathcal{L}_{\text{PPO}} = -\mathbb{E}[\mathcal{L}^{\text{CLIP}} + c_1 (V_\phi(s_t) - V_t^{\text{targ}})^2 - c_2 \mathcal{H}(\pi_\theta)]
$$
$c_1$控制价值函数权重($\approx0.5$)，$c_2$为熵正则系数($\approx0.01$)
#### 2. **DPO(直接偏好优化)**
**隐式奖励建模**：
$$
r_\theta(x,y) = \beta \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)}
$$
优化偏好对数似然：
$$
\mathcal{L}_{\text{DPO}} = -\log \sigma\left( \beta \log\frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log\frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right)
$$
消除显式Reward模型，减少训练复杂度

#### 损失优化
1. Vanilla BT Loss
 $\mathcal{L}\left(r_{\psi}\right)=-\mathbb{E}_{(x, y) \sim \mathcal{D}_{\mathrm{rm}}}\left[\log \sigma\left(r_{\psi}(x, y_{c})-r_{\psi}(x, y_{r})\right)\right]$
最基础的损失函数，计算模型预测值和实际标签之间的对数损失，目标是最小化这个损失。
2. Margin BT Loss
  $\mathcal{L}\left(r_{\psi}\right)=-\mathbb{E}_{(x, y) \sim \mathcal{D}_{\mathrm{rm}}}\left[\log \sigma\left(r_{\psi}(x, y_{c})-r_{\psi}(x, y_{r})-\hat{\mu}\left(y_{c}, y_{r}\right)\right)\right]$
通过引入一个间隔项，模型不仅学习预测正确类别，还要确保不同类别之间的预测差异足够大。它强调区分度，适合需要强区分能力的任务。
3. Weighted BT Loss
$\mathcal{L}\left(r_{\psi}\right)=-\mathbb{E}_{(x, y) \sim \mathcal{D}_{\mathrm{rm}}}\left[\hat{\mu}\left(y_{c}, y_{r}\right) \log \sigma\left(r_{\psi}(x, y_{c})-r_{\psi}(x, y_{r})\right)\right]$
为不同的样本分配不同的权重，能够让模型更多关注那些困难或重要的样本。这对于数据不平衡的情况特别有效，可以提高模型对少数类样本的识别能力。
4. Smooth BT Loss
$\mathcal{L}\left(r_{\psi}\right)=-\mathbb{E}_{(x, y) \sim \mathcal{D}_{\mathrm{rm}}}\left[(1-\alpha) \log \left(p_{\psi}\left(y_{c} \succ y_{r} \mid x\right)\right)+\alpha \log \left(1-p_{\psi}\left(y_{c} \succ y_{r} \mid x\right)\right)\right]$
通过结合平滑项，减少模型对某些极端预测的敏感性，从而提高模型的稳健性。这个损失函数适用于面对噪声或模糊数据的情况，能够帮助模型避免过拟合。

| **编号** | **损失函数名称**       | **公式表达**                                                                                                                                                                                       | **特点与用途**       |
| ------ | ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------- |
| 1      | Vanilla BT Loss  | $\mathcal{L}(r_{\psi}) = -\mathbb{E}_{(x, y) \sim \mathcal{D}_{\mathrm{rm}}} \left[ \log \sigma\left( r_{\psi}(x, y_{c}) - r_{\psi}(x, y_{r}) \right) \right]$                                 | 传统DPO Loss      |
| 2      | Margin BT Loss   | $\mathcal{L}(r_{\psi}) = -\mathbb{E}_{(x, y) \sim \mathcal{D}_{\mathrm{rm}}} \left[ \log \sigma\left( r_{\psi}(x, y_{c}) - r_{\psi}(x, y_{r}) - \hat{\mu}(y_{c}, y_{r}) \right) \right]$       | 确保正样本的得分显著高于负样本 |
| 3      | Weighted BT Loss | $\mathcal{L}(r_{\psi}) = -\mathbb{E}_{(x, y) \sim \mathcal{D}_{\mathrm{rm}}} \left[ \hat{\mu}(y_{c}, y_{r}) \log \sigma\left( r_{\psi}(x, y_{c}) - r_{\psi}(x, y_{r}) \right) \right]$         | 适用于数据不平衡        |
| 4      | Smooth BT Loss   | $\mathcal{L}(r_{\psi}) = -\mathbb{E}_{(x, y) \sim \mathcal{D}_{\mathrm{rm}}} \left[ (1-\alpha) \log p_{\psi}(y_c \succ y_r \mid x) + \alpha \log (1 - p_{\psi}(y_c \succ y_r \mid x)) \right]$ | 适用于面对噪声或模糊数据    |


#### 3. **KTO(Kahneman-Tversky优化)**
**三元组偏好扩展**：
$$
\mathcal{L}_{\text{KTO}} = \mathbb{E} \left[ w_{\text{win}} (1-\sigma(r_{\text{win}} - r_{\text{base}})) + w_{\text{lose}} \sigma(r_{\text{base}} - r_{\text{lose}}) \right]
$$
引入行为经济学理论，更贴合人类决策偏差
*表：RLHF算法对比*

| **算法** | **依赖组件** | **奖励粒度** | **显存占用** | **适用场景** |
|---------|------------|------------|------------|------------|
| PPO     | 策略网络+价值网络 | 连续标量   | 高(双模型) | 精确控制任务 |
| DPO     | 参考策略       | 二元偏好   | 中         | 大规模偏好对齐 |
| GRPO    | 固定奖励模型   | 组内相对值 | 低(无Critic)| 数学推理等需探索任务 |

### 五、GRPO群体优化
**核心创新**：用群体奖励均值替代价值网络
1. **组采样**：对输入$x$生成$K$个响应$y_i \sim \pi_{\text{old}}$
2. **奖励归一化**：计算相对优势
  $$
  \bar{r} = \frac{1}{K}\sum_{i=1}^K r(x,y_i), \quad \hat{A}_i = r(x,y_i) - \bar{r}
  $$
3. **策略更新**：
  $$
  \mathcal{L}_{\text{GRPO}} = \frac{1}{K}\sum_{i=1}^K \min\left( \frac{\pi_\theta(y_i|x)}{\pi_{\text{old}}(y_i|x)} \hat{A}_i, \text{clip}\left( \frac{\pi_\theta}{\pi_{\text{old}}}, 1-\epsilon,1+\epsilon \right) \hat{A}_i \right)
  $$
**技术优势**：  
- **方差缩减**：蒙特卡洛基线比全局$V(s)$更匹配局部样本分布  
- **隐式探索**：$K>1$的采样天然提升策略熵  
- **动态正则化**：KL散度约束防止策略偏离参考模型
---
### 六、数值与探索损失
#### 1. **NTL数值感知损失**
解决交叉熵的标称尺度问题：
- **NTL-MSE**：预测数值期望的均方误差
 $$
 \mathcal{L}_{\text{NTL-MSE}} = (y - \sum_{j \in \mathcal{D}} p_j V_j)^2
 $$
 存在非唯一极小值问题（如预测0和8各50%概率时期望为4）  
- **NTL-Wasserstein**：分布间最优传输距离
 $$
 \mathcal{L}_{\text{NTL-WAS}} = \inf_{\gamma \in \Pi(P,Q)} \mathbb{E}_{(i,j)\sim\gamma} [|V_i - V_j|]
 $$
 精确匹配数值分布，计算成本较高
#### 2. **内在探索奖励**
- **RIDE奖励**：基于状态表征变化的探索激励
 $$
 r^{\text{int}} = \frac{\|\phi(s_{t+1}) - \phi(s_t)\|_2}{N_{\text{ep}}(s_t)}
 $$
 其中$N_{\text{ep}}$为状态访问计数，适用于程序化生成环境  
- **CRINGE损失**：引入负样本对比
 $$
 \mathcal{L}_{\text{CRINGE}} = \mathcal{L}_{\text{CE}} + \lambda \sum_{y_{\text{neg}}} \max(0, \log \pi(y_{\text{neg}}|x) - \log \pi(y_{\text{pos}}|x) + \alpha)
 $$
 通过迭代负采样提升安全生成能力


#### 关键公式速查表
| **损失函数** | **公式** | **核心超参数** |
|------------|---------|--------------|
| Focal Loss | $-(1-p_t)^\gamma \log p_t$ | $\gamma=2,\alpha=0.25$ |
| Pairwise RM | $-\log \sigma(r_w - r_l)$ | 温度系数$\tau$ |
| PPO Clip | $\min(r_t A_t, \text{clip}(r_t,1\pm\epsilon) A_t)$ | $\epsilon=0.2$ |
| DPO | $-\log\sigma(\beta(\log\frac{\pi_{\theta}^w}{\pi_{\text{ref}}^w} - \beta\log\frac{\pi_{\theta}^l}{\pi_{\text{ref}}^l}))$ | $\beta=0.1$ |
| GRPO | $\frac{1}{K}\sum_i \min(\frac{\pi_\theta}{\pi_{\text{old}}} \hat{A}_i, \text{clip}\cdot\hat{A}_i)$ | $K=4\sim8$ |

1. **PPO vs DPO的本质区别**？  
  PPO在奖励空间优化（需显式Advantage估计），DPO在偏好空间优化（隐式奖励建模）
2. **GRPO如何实现方差缩减**？  
  群体奖励均值$\bar{r}$是局部蒙特卡洛估计，比全局$V(s)$更匹配当前策略分布，降低梯度估计方差
3. **为何NTL比交叉熵更适合数值任务**？  
  交叉熵视”预测3“和”预测9“同样错误，而NTL-MSE惩罚$|3-2|=1<|9-2|=7$，引入数值归纳偏置
4. **Reward模型训练中K=9的考量**？  
  标注效率（4→9答案仅增40%时间但数据翻倍）和计算复用（9答案产生36对但仅需9次前向）
5. **DPO的梯度饱和现象**？  
  当$\pi_\theta \gg \pi_{\text{ref}}$时$\log$项爆炸，实践中需约束KL散度或使用$\text{clip}$函数
掌握这些损失函数的设计哲学和数学本质，将帮助你在大模型训练和优化场景中游刃有余。面试时建议结合具体任务需求分析损失函数选择（如数学推理首选GRPO+NT，安全对齐用DPO+CRINGE），并强调工程实现细节（如混合精度训练中的损失缩放）。