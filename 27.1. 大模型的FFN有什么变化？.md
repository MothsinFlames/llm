---
created: 2025-08-20T14:18:04 (UTC +08:00)
tags: [大模型,大语言模型,算法岗]
source: https://zhuanlan.zhihu.com/p/683673404
author: 关于作者Sam聊算法北大计算机本硕/大厂算法工程师，分享前沿算法技术，接面试辅导北京大学 计算机科学与技术硕士回答文章关注者已关注发私信
---
LLaMa这些大模型相比原始的Transformer decoder有什么结构上的改进？

这个问题属于NLP近期的高频题，相信大家对位置编码（[RoPe](https://zhida.zhihu.com/search?content_id=240057766&content_type=Article&match_order=1&q=RoPe&zhida_source=entity)）、[layer normalization](https://zhida.zhihu.com/search?content_id=240057766&content_type=Article&match_order=1&q=layer+normalization&zhida_source=entity)（PreNorm+RMSNorm）、[multi-query attention](https://zhida.zhihu.com/search?content_id=240057766&content_type=Article&match_order=1&q=multi-query+attention&zhida_source=entity)（多组query权重共享一组key和value权重）等方面或多或少都能聊上一些，FFN（Feed-Forward Network）方面的改变——从普通的FFN到SwiGLU的变化却经常被忽略。这里梳理一下FFN结构的源流。

---
**[LLaMa](https://zhida.zhihu.com/search?content_id=240057766&content_type=Article&match_order=1&q=LLaMa&zhida_source=entity)这些大模型相比原始的Transformer decoder有什么结构上的改进？**

这个问题属于NLP算法岗近期的高频面试题，相信大家对位置编码（[RoPe](https://zhida.zhihu.com/search?content_id=240057766&content_type=Article&match_order=1&q=RoPe&zhida_source=entity)）、[layer normalization](https://zhida.zhihu.com/search?content_id=240057766&content_type=Article&match_order=1&q=layer+normalization&zhida_source=entity)（PreNorm+RMSNorm）、[multi-query attention](https://zhida.zhihu.com/search?content_id=240057766&content_type=Article&match_order=1&q=multi-query+attention&zhida_source=entity)（多组query权重共享一组key和value权重）等方面或多或少都能聊上一些，FFN（Feed-Forward Network）方面的改变——从普通的FFN到SwiGLU的变化却经常被忽略。这里梳理一下FFN结构的源流。





### 原始Transformer中的FFN

![|500](https://pic2.zhimg.com/v2-32766307d1ac28fec9dbff7f49e8cbb1_1440w.jpg)

原始的Transformer结构中，每一层包含multi-head self-attention block（MHSA）和一个FFN

**FFN的本质就是一个token-wise的升维-过激活-降回原来维度**的MLP。

其输入是MHSA输出的 $(n,d)$ 维的序列表示 $x$ ，其中 $n$ 为序列中的tojen数目， $d$ 为隐藏层维度，设中间层维度为 $d'$ （通常大于 $d$ ，常见实现中 $d'=4d$ ），其权重为升维矩阵 $W_1 \in \mathbb{R}^{d\times d'}$ 和降维矩阵 $W_2 \in \mathbb{R}^{d‘\times d}$ ，激活函数为 $\sigma$ 。为简便起见忽略bias项。则FFN层的前向过程为：

$$\text{FFN}(x) = \sigma(xW_1)W_2$$

原始的Transformer和BERT模型中取 $\sigma=\text{GELU}$ ，即 $x\cdot\psi(x)$ ，其中 $\psi$ 为标准正态分布的密度累积函数（可以理解为x的值越大，失活的概率越小），曲线如下：

![](https://pic2.zhimg.com/v2-28286f7fd47473429dec8c0e0cbe09ad_1440w.jpg)

实际实现时GELU激活可以用下式近似：

$\begin{equation} \operatorname{GELU}(x)=0.5 * x *\left(1+\operatorname{Tanh}\left(\sqrt{2 / \pi} *\left(x+0.044715 * x^3\right)\right)\right) \end{equation}$

### 大模型中的FFN——SwiGLU

Noam Shazeer的[GLU Variants Improve Transformer](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2002.05202.pdf)发现一种名为GLU（Gated Linear Units）的FFN变体在同等的参数量下性能比上面的原始版本更好，其前向过程是：

1.  输入过两个 $\mathbb{R}^{d\times d'}$ 形式的升维projection得到A和B两个升维的状态，之后B过非线性激活 $\sigma$ ，A保持原样；
2.  A和激活过的B逐元素相乘得到C；
3.  C过 $\mathbb{R}^{d’\times d}$ 形式的降维projection得到输出。

正式地，设升维projection的权重为 $W_1,W_2 \in \mathbb{R}^{d \times d'}$ ，降维projection的权重为 $W_3\in \mathbb{R}^{d' \times d}$ ，则GLU的前向过程为：

$\text{GLU}(x) = \left(\sigma\left(xW_1\right) \odot \left(xW_2\right)  \right)W_3$

其中 $\odot$ 表示哈达玛积（对应位置元素相乘）。当激活函数 $\sigma$ 取[GeLU](https://zhida.zhihu.com/search?content_id=240057766&content_type=Article&match_order=1&q=GeLU&zhida_source=entity)时就叫GEGLU，取ReLU时就叫ReGLU，取Swish的时候就叫SwiGLU，以此类推。值得注意的是，和原始版本的FFN进行实验比较时，为了确保参数量相等，中间层维度的 $d'$ 的值应该设为原来的2/3（因为两个参数矩阵变成了三个）。

目前LLaMa和采取LLaMa架构的Baichuan等开源大模型使用的是SwiGLU，即取激活函数 $\sigma$ 为Swish函数：

$\text{Swish}(x) = x*\operatorname{sigmoid}(\beta x)$

其中 $\beta$ 为固定的超参，LlaMa的实现中取1 ，即直接调用nn.SiLU：

![](https://pic1.zhimg.com/v2-9e5805b2eaf0962d032a9c51a1bda3a6_1440w.jpg)

![](https://picx.zhimg.com/v2-8be6bceb1ab752904446c06f2a0f874d_1440w.jpg)

以上就是原始的FFN到GLU的变化，欢迎捉虫指正。之后这个名为“算法冷知识”的系列将继续更新模型模块、损失函数、优化器等方面重要但又容易被遗忘的细节，欢迎参阅点赞。
