GPT3的参数量为1750亿，训练数据量达到了570GB。

现在业界的大语言模型都是基于transformer模型的，模型结构主要有两大类：encoder-decoder（代表模型是[T5](https://zhida.zhihu.com/search?content_id=226962158&content_type=Article&match_order=1&q=T5&zhida_source=entity)）和decoder-only，具体的，decoder-only结构又可以分为Causal LM（代表模型是GPT系列）和Prefix LM（代表模型是GLM）。


![|1075](https://pic4.zhimg.com/v2-706eaaaf4df3c88be37d7f4025566a3b_1440w.jpg)
