---
created: 2025-10-29T16:15:39 (UTC +08:00)
tags: [LLM,RL,熵]
source: https://zhuanlan.zhihu.com/p/1924309705548867391
author: 关于作者白强伟一直游到海水变蓝北京大学 软件工程硕士迷途小书僮、学车辆算法工程师、Lil2J 也关注了他回答3文章101关注者3,736关注他发私信
---

# 【LLMxRL】熵坍缩与缓解策略

> ## Excerpt
> 英文版： Entropy Collapse and Mitigation Strategies一、策略熵与熵坍缩1. 熵定义 令 x 表示prompt， y 表示response，策略 \pi_\theta 针对token t 输出的概率分布为 p_t=(p_,\dots,p_)=\pi_{\thet…

---
英文版：[Entropy Collapse and Mitigation Strategies](https://link.zhihu.com/?target=https%3A//bqw1013.github.io/posts/entropy-collapse-and-mitigation-strategies/)

## 一、[策略熵](https://zhida.zhihu.com/search?content_id=259890521&content_type=Article&match_order=1&q=%E7%AD%96%E7%95%A5%E7%86%B5&zhida_source=entity)与[熵坍缩](https://zhida.zhihu.com/search?content_id=259890521&content_type=Article&match_order=1&q=%E7%86%B5%E5%9D%8D%E7%BC%A9&zhida_source=entity)

### 1\. 熵定义

 令 $x$ 表示prompt， $y$ 表示response，策略 $\pi_\theta$ 针对token $t$ 输出的概率分布为 $p_t=(p_{t,1},\dots,p_{t,|V|})=\pi_{\theta}(\cdot|x,y_{<t})=\text{softmax}(\frac{z_t}{T}) \tag{1}$ $|V|$ 表示整个词表的大小， $z_t\in\mathbb{R}^V$ 是`logits`， $T\in\mathbb{R}$ 是解码温度。

 那么token $t$ 的熵为 $H_t=-\sum_{j=1}^{|V|} p_{t,j}\log p_{t,j} \tag{2}$

### 2\. 熵坍缩与模型性能

 在RL训练的初期，模型的熵会急剧下降。随着熵的下降，准确率会迎来一个快速增长期。但是，随着熵的迅速耗尽会导致模型过度自信，探索能力随之减弱。\[1\]通过实证研究，建立了策略熵 $H$ 与下游任务性能 $R$ 之间的定量关系 $R=-a\cdot\exp(H)+b \tag{3}$ 其中 $a$ 和 $b$ 是拟合系数，反映了特定模型和训练数据的内在特性。

 **显然，在更长时间范围内将熵维持在一个合理范围内是持续提升模型能力的关键。**\[2\]通过稳定熵实现更长时间的RL训练后，发现模型能够突破原有能力的边界，持续改善效果。

### 3\. token熵与[forking token](https://zhida.zhihu.com/search?content_id=259890521&content_type=Article&match_order=1&q=forking+token&zhida_source=entity)

 \[3\]通过分析发现大多数token的熵非常低，仅有少量token的较高。而且，token的功能与熵高度相关：

-   高熵token主要承担"逻辑连接器"和"假设引入者"，例如`wait`、`however`等。
-   低熵token则是"结构补全者"的角色，负责在已经确定好的推理步骤中填充细节。

 因此，\[3\]将这些高熵token定义为`forking token`。

## 二、GRPO

 GRPO在[PPO](https://zhida.zhihu.com/search?content_id=259890521&content_type=Article&match_order=1&q=PPO&zhida_source=entity)的基础上通过组内标准化实现优势的计算。具体来说，给定一个prompt $x$ ，采样 $G$ 个response $\{y_i\}_{i=1}^G$ 。那么优势的计算为 $A_{i,t}=\frac{r_i-\text{mean}(\{r_i\}_{i=1}^G)}{\text{std}(\{r_i\}_{i=1}^G)} \tag{4}$ 其中 $r_i$ 是响应 $y_i$ 的奖励值。

 GRPO的目标函数为 $J_{\text{GRPO}}=\mathbb{E}_{x\sim p,\{y_i\}_{i=1}^G\sim\pi_{\text{old}}(\cdot|x)}\left[ \frac{1}{G}\sum_{i=1}^G\frac{1}{|y_i|}\sum_{t=1}^{|y_i|}\min\left(r_{i,t}(\theta)A_{i,t},\text{clip}(r_{i,t}(\theta),1-\varepsilon,1+\varepsilon)A_{i,t}\right) \right] \tag{5}$ 其中 $r_{i,t}(\theta)=\frac{\pi_{\theta}(a_{i,t}|s_{i,t})}{\pi_{\theta_{\text{old}}}(a_{i,t}|s_{i,t})}$ 是[重要性采样](https://zhida.zhihu.com/search?content_id=259890521&content_type=Article&match_order=1&q=%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7&zhida_source=entity)系数。

## 三、直接针对forking token的优化

 **解耦** $\text{clip}$ **上下限**。\[4\]认为 $\text{clip}$ 操作对高概率token(低熵)和低概率token(高熵)是不平等的。举例来说，假设 $\varepsilon=0.2$ ，若某个token的 $\pi_{\theta_{\text{old}}}=0.9$ ，那么 $\text{clip}$ 操作导致 $\pi_{\theta}$ 的上限为 $0.9\times1.2=1.08$ ，绝对增量为 $1.08-0.9=0.18$ 。若某个token的概率为 $\pi_{\theta_{\text{old}}}=0.01$ ， $\pi_{\theta}$ 的上限为 $0.01\times 1.2=0.012$ ，绝对增量为 $0.002$ 。因此， $\text{clip}$ 操作会限制低概率token(高熵)的更新幅度。因此，\[4\]提出将裁剪的上下限由 $\varepsilon$ 解耦为 $\varepsilon_{low}$ 和 $\varepsilon_{high}$ ，适当提高 $\varepsilon_{high}$ 从而让高熵token获得更大幅度的更新。 $J_{\text{DAPO}}=\mathbb{E}_{x\sim p,\{y_i\}_{i=1}^G\sim\pi_{\text{old}}(\cdot|x)}\left[ \frac{1}{G}\sum_{i=1}^G\frac{1}{|y_i|}\sum_{t=1}^{|y_i|}\min\left(r_{i,t}(\theta)A_{i,t},\hat{r}_{i,t}(\theta)A_{i,t}\right) \right] \tag{6}$ 其中 $\hat{r}_{i,t}(\theta)=\text{clip}(r_{i,t}(\theta),1-\varepsilon_{low},1+\varepsilon_{high})$ 。

 **将优势从裁剪中解耦**。\[5\]在\[4\]的基础上进一步认为，应该将优势从裁剪中解耦出来。因为，若触发了裁剪就导致梯度为0。重要性采样应该看做是优势的一个权重。因此，目标函数进一步修改为 $J_{\text{CISPO}}=\mathbb{E}_{x\sim p,\{y_i\}_{i=1}^G\sim\pi_{\text{old}}(\cdot|x)}\left[ \frac{1}{G}\sum_{i=1}^G\frac{1}{|y_i|}\sum_{t=1}^{|y_i|}\text{sg}(\hat{r}_{i,t}(\theta))A_{i,t}\log\pi(a_{i,t}|s_{i,t}) \right] \tag{7}$ 其中 $\text{sg}$ 是 `stop_gradient` 操作。

 **直接增加`forking token`的优势。**\[6\]更加直接，既然高熵token更重要，那么直接提高其优势。
  $\begin{align} \psi(H_{i,t})&=\min\Big(\alpha\cdot H_{i,t}^{\text{detach}},\frac{|A_{i,t}|}{k}\Big) \tag{8} \\ \hat{A}_{i,t}&=A_{i,t}+\psi(H_{i,t}) \tag{9}\\ \end{align}$ $\psi(H_{i,t})$ 作为优势的附近下，其保证不超过原始优势值的 $k$ 分之一。

## 四、正负样本的影响

 \[7\]分析了正、负样本对熵坍缩的影响，在实验中发现进仅使用负样本也能实现很好的效果。因此，在正负样本奖励分别为+1和-1的情况下，\[7\]进一步分析了REINOFRCE下loss关于`logits`的梯度。 $-\frac{\partial L_t}{\partial z_{t,v}}=\begin{cases} r\cdot(1-\pi_{v})&v=y_t \\ -r\cdot\pi_{v} &v\neq y_t \end{cases} \tag{10}$

其中 $z_{t,v}$ 是`logits`向量 $z_t$ 中token $v$ 分量的值。

> 证明：
> 
>  单时间步 $t$ 时的梯度为 $\nabla L_{t}=-r\nabla\log\pi_{\theta}(y_t)$ 。将 $\log \pi_{\theta}(y_t)$ 展开， $\log\pi_{\theta}(y_t)=\log\left(\frac{\exp(z_{t,{y_t}})}{\sum_{v'\in V}\exp(z_{t,{v'}})}\right)=z_{t,{y_t}}-\log\left(\sum_{v'\in V}\exp(z_{t,{v'}})\right)$  分两种情况讨论：
> 
>  **情况一**： $v=y_t$，即针对被采样token的logit求导。 $\begin{align} \frac{\partial(\log\pi_{\theta}(y_t))}{\partial z_{t,{y_t}}}&=\frac{\partial z_{t,{y_t}}}{\partial z_{t,{y_t}}}-\frac{\partial}{\partial z_{t,{y_t}}}\log\left(\sum_{v'\in V}\exp(z_{t,{v'}})\right) \\ &=1-\frac{1}{\sum_{v'}\exp(z_{t,{v'}})}\cdot\exp(z_{t,{y_t}}) \\ &=1-\pi_{y_t} \end{align}$  **情况二**： $v\neq y_t$ ，即对未采样token的`logit`求导 $\begin{align} \frac{\partial(\log\pi_{\theta}(y_t))}{\partial z_{t,{v}}}&=\frac{\partial z_{t,{y_t}}}{\partial z_{t,{v}}}-\frac{\partial}{\partial z_{t,{v}}}\log\left(\sum_{v'\in V}\exp(z_{t,{v'}})\right) \\ &=0-\frac{1}{\sum_{v'}\exp(z_{t,{v'}})}\cdot\exp(z_{t,{v}}) \\ &=-\pi_{v} \end{align}$  综上， $\frac{\partial L_t}{\partial z_{t,v}}=\begin{cases} -r\cdot(1-\pi_{y_t})&v=y_t \\ r\cdot\pi_v &v\neq y_t \end{cases}$

 **当** $r=1$ **时**。对于采样到的token $y_t$ ，梯度会以幅度 $(1-\pi_{y_t})$ 增加logit $z_{t,{y_t}}$ 。当模型对 $y_t$ 不高时，会以更大的幅度更新。对于没采样到的token，梯度会以幅度 $\pi_v$ 来降低logit $z_{t,v}$。从forking token的视角来看\[3\]，若正样本中包含了forking token，则该token的熵会加速下降，也就是那些起连接作用的token快速被确定。

 **当** $r=-1$ **时**。对于采样的token $y_t$ ，梯度会以幅度 $(1-\pi_{y_t})$ 降低logit $z_{t,{y_t}}$ 。同时，对于其他token会按照自身概率来重新分配释放的概率质量。

 总的来说，正样本中采样的token会成比例剥夺其余token的概率质量，而负样本中采样的token会将被剥夺的概率质量成比例的分配给其余token。因此，负样本天然有利于增大熵。

 基于上面的分析，\[7\]提出降低训练中正样本信号的强度来维持熵。

> 思考：纯负样本训练能够取得好的效果，应该是由于不断强化基础模型的原始分布，相当于近期大量基于内部反馈的方法？\[8\]

## 五、熵的变化

 相比于前面的方法，**\[1\]从关注熵转变为关注熵的变化**。将熵看作是`logits`的函数，即 $H_t(z_t)$ 。那么在足够小更新步长的情况下，熵的变化为 $H_t(z_t^{k+1}) - H_t(z_t^k)\approx-\text{Cov}_{y_t\sim\pi(\cdot|z_t^k)}\Big(\log\pi(y_t|z_t^k),\Delta z_{t,y_t}^k\Big) \tag{11}$ 其中 $z_t^{k}$ 和 $z_t^{k+1}$ 分别表示两个连续步骤下的`logits`向量， $\Delta z_{t,y_t}^k=z_{t,y_t}^{k+1}-z_{t,y_t}^{k}$ 则是两个连续步骤下 $y_t$ 的`logits`变化。

 公式(11)的结论是熵的变化与对数概率和`logits`变化的协方差呈现负相关。具体来说，当高概率token的`logits`增加或者低概率token的`logits`减少，则熵减；否则熵增。**更直观的讲，锐化当前模型的分布会导致熵减。**

> 证明：
> 
>  熵定义表示为 $H_t(z_t)=-\mathbb{E}_{y_t\sim\pi(\cdot|z_t)}[\log \pi(y_t|z_t])$ 第 $k$ 步的`logits`为 $z_t^{k}$ ，经过训练后得到 $k+1$ 步的`logits`为 $z_t^{k+1}$ 。由于实际训练中，学习率通常比较低，可以考虑通过泰勒展开来估计点 $z_t^k$ 和 $z_t^{k+1}$ 之间熵的变化。具体来说，函数 $H_t(z_t)$ 在点 $z_t^k$ 进行一阶泰勒展开 $H_t(z_t)\approx H_t(z_t^k)+\langle \nabla_{z_t}H_t(z_t^k),z_t - z_t^k \rangle$ 然后将 $z_t^{k+1}$ 代入，有 $H_t(z_t^{k+1}) - H_t(z_t^k)\approx\langle \nabla_{z_t}H_t(z_t^k),z_t^{k+1} - z_t^k \rangle$ 先来求 $\nabla_{z_t^k}H_t(z_t^k)$ ， $\begin{align} \nabla_{z_t^k}H_t(z_t^k) &= \nabla_{z_t^k}(-\mathbb{E}_{y_t\sim\pi(\cdot|z_t^k)}[\log \pi(y_t|z_t^k]) \\ &=-\sum_{y_t}\Big[\nabla_{z_t^k}\pi(y_t|z_t^k)\log\pi(y_t|z_t^k)+\pi(y_t|z_t^k)\nabla_{z_t^k}\log\pi(y_t|z_t^k)\Big] \\ &=-\sum_{y_t}\Big[\pi({y_t|z_t^k)}\nabla_{z_t^k}\log\pi(y_t|z_t^k)\cdot \log\pi(y_t|z_t^k)+\pi(y_t|z_t^k)\nabla_{z_t^k}\log\pi(y_t|z_t^k)\Big]\\ &=-\mathbb{E}_{y_t\sim\pi(\cdot|z_t^k)}\Big[\nabla_{z_t^k}\log\pi(y_t|z_t^k)\cdot \log\pi(y_t|z_t^k) + \nabla_{z_t^k}\log\pi(y_t|z_t^k)\Big] \\ &=-\mathbb{E}_{y_t\sim\pi(\cdot|z_t^k)}\Big[\nabla_{z_t^k}\log\pi(y_t|z_t^k)\cdot \log\pi(y_t|z_t^k)\Big] \end{align}$ 然后进一步求解 $\langle \nabla_{z_t^k}H_t(z_t^k),z_t^{k+1} - z_t^k \rangle$ 。为了符号简洁，令 $\Delta z_t^k=z_t^{k+1}-z_t^k$ 。 $\begin{align} \langle \nabla_{z_t^k}H_t(z_t^k),\Delta z_t^k \rangle &= -\langle\mathbb{E}_{y_t\sim\pi(\cdot|z_t^k)}\Big[\nabla_{z_t^k}\log\pi(y_t|z_t^k)\cdot \log\pi(y_t|z_t^k)\Big],\Delta z_t^k\rangle \\ &=-\mathbb{E}_{y_t\sim\pi(\cdot|z_t^k)}\Big[\log\pi(y_t|z_t^k)\langle \nabla_{z_t^k}\log\pi(y_t|z_t^k),\Delta z_t^k \rangle\Big] \\ &=-\mathbb{E}_{y_t\sim\pi(\cdot|z_t^k)}\Big[\log\pi(y_t|z_t^k)\sum_{j=1}^{|V|}\frac{\partial\log\pi(y_t|z_t^k)}{\partial z_{t,j}^k}\Delta z_{t,j}^k\Big] \end{align}$ 基于 $\text{softmax}$ 的导数，有 $\begin{align} \sum_{j=1}^{|V|}\frac{\partial\log\pi(y_t|z_t^k)}{\partial z_{t,j}^k}\Delta z_{t,j}^k&=\sum_{j=1}^{|V|}\textbf{1}\{y_t=v_j\}\Delta z_{t,j}^k-\pi(v_j|z_t^k)\Delta z_{t,j}^k\\ &=\Delta z_{t,y_t}^k-\mathbb{E}_{v\sim\pi(\cdot|z_t^k)}[\Delta z_{t,v}^k] \end{align}$ 因此， $\langle \nabla_{z_t^k}H_t(z_t^k),\Delta z_t^k \rangle=-\mathbb{E}_{y_t\sim\pi(\cdot|z_t^k)}\Big[\log\pi(y_t|z_t^k)\cdot\Big(\Delta z_{t,y_t}^k-\mathbb{E}_{v\sim\pi(\cdot|z_t^k)}[\Delta z_{t,v}^k]\Big)$ 令随机变量 $X=\log\pi(y_t|z_t^k)$ ， $Y=\Delta z_{t,y_t}^k$。那么，上式就是这两个随机变量的负协方差，即 $H_t(z_t^{k+1}) - H_t(z_t^k)\approx-\text{Cov}_{y_t\sim\pi(\cdot|z_t^k)}\Big(\log\pi(y_t|z_t^k),\Delta z_{t,y_t}^k\Big)$

 \[1\]进一步给出了在[自然策略梯度](https://zhida.zhihu.com/search?content_id=259890521&content_type=Article&match_order=1&q=%E8%87%AA%E7%84%B6%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6&zhida_source=entity)下，熵变化与优势的关系为 $H_t(z_t^{k+1}) - H_t(z_t^k)\approx-\eta\cdot \text{Cov}_{y_t\sim\pi(\cdot|z_t^k)}\Big(\log\pi(y_t|z_t^k),A^k(y_t,z_t^k)\Big) \tag{12}$ 其中 $\eta$ 是学习率， $A^k(y_t,z_t^k)$ 是当前状态下 $y_t$ 的优势。在0-1二元奖励下的GRPO，正样本的优势为正，负样本的优势为负。那么正样本的高概率token或者负样本的低概率token会导致熵减。

 基于公式(12)，\[1\]认为应该限制高协方差的token更新幅度，提出了`[Clip-Cov]`和`[KL-Cov]`。`Clip-Cov`主要是识别出的高协方差token梯度置零。`KL-Cov`则是在识别出的高协方差token施加一个kl约束，使其不太偏离原始策略太远。

## 六、统一视角

 \[1\] \[3\] \[4\] \[5\] \[6\]本质上都聚焦到高熵的`forking token`熵。\[4\] \[5\] \[6\]通过修正GRPO中对高熵(低概率)token的偏见，来缓解熵的快速下降。\[1\]则通过限制低熵(高概率)的更新幅度来缓解熵的快速下降。因此，这些方法理论上可以结合使用，增加高熵token更新幅度的同时，降低低熵token的更新幅度。\[7\]不再聚焦到token-level的熵，而是认为应该增加负样本的相对权重。结合\[1\]的观点来看，由于模型rollout出来的负样本概率也不会太低，那么优化这个相对高概率的负样本，也会缓解熵减。

## 参考文献

\[1\]. [The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2505.22617)

\[2\]. [ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2505.24864)

\[3\]. [Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2506.01939)

\[4\]. [DAPO: An Open-Source LLM Reinforcement Learning System at Scale](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2503.14476)

\[5\]. [MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2506.13585)

\[6\]. [Reasoning with Exploration: An Entropy Perspective](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2506.14758)

\[7\]. [The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2506.01347)

\[8\]. [No Free Lunch: Rethinking Internal Feedback for LLM Reasoning](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2506.17219)
