---
created: 2025-09-02T20:51:39 (UTC +08:00)
tags: [大模型,多模态,qwen]
source: https://zhuanlan.zhihu.com/p/1929459918538257251
author: 关于作者北方的郎专注模型与代码，公众号：AI方法与实践吕阿华、郭达森、浮生梦晓也关注了他回答1,200文章1,592关注者13,845关注他发私信
---

# Qwen系列视觉模型演进过程分析：从Qwen-VL到Qwen2.5-V

> ## Excerpt
> 1. 摘要Qwen-VL系列视觉模型代表了多模态人工智能领域的一项显著演进，其发展轨迹清晰地展示了从基础多模态理解到高级动态分辨率处理、长视频理解以及复杂智能体能力的逐步飞跃。这一系列模型的迭代，不仅在架构创…

---
## 1\. 摘要

[Qwen-VL](https://zhida.zhihu.com/search?content_id=260502689&content_type=Article&match_order=1&q=Qwen-VL&zhida_source=entity)系列视觉模型代表了多模态人工智能领域的一项显著演进，其发展轨迹清晰地展示了从基础多模态理解到高级动态分辨率处理、长视频理解以及复杂智能体能力的逐步飞跃。这一系列模型的迭代，不仅在架构创新（如原生动态分辨率Vision Transformer、[窗口注意力机制](https://zhida.zhihu.com/search?content_id=260502689&content_type=Article&match_order=1&q=%E7%AA%97%E5%8F%A3%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6&zhida_source=entity)、[绝对时间编码](https://zhida.zhihu.com/search?content_id=260502689&content_type=Article&match_order=1&q=%E7%BB%9D%E5%AF%B9%E6%97%B6%E9%97%B4%E7%BC%96%E7%A0%81&zhida_source=entity)和[多模态旋转位置嵌入](https://zhida.zhihu.com/search?content_id=260502689&content_type=Article&match_order=1&q=%E5%A4%9A%E6%A8%A1%E6%80%81%E6%97%8B%E8%BD%AC%E4%BD%8D%E7%BD%AE%E5%B5%8C%E5%85%A5&zhida_source=entity)）上取得了突破，更在训练方法和数据策略上进行了深度优化。[Qwen2.5-VL](https://zhida.zhihu.com/search?content_id=260502689&content_type=Article&match_order=1&q=Qwen2.5-VL&zhida_source=entity)作为该系列的最新旗舰模型，在各项基准测试中表现出与[GPT-4o](https://zhida.zhihu.com/search?content_id=260502689&content_type=Article&match_order=1&q=GPT-4o&zhida_source=entity)和[Claude 3.5 Sonnet](https://zhida.zhihu.com/search?content_id=260502689&content_type=Article&match_order=1&q=Claude+3.5+Sonnet&zhida_source=entity)等业界领先模型相媲美甚至超越的性能，尤其在文档解析、精确目标定位、超长视频理解以及交互式视觉智能体功能方面展现出卓越能力。Qwen系列对开源的持续承诺，也极大地推动了多模态AI领域的创新和应用普及。

### 2.1. 大型视觉-语言模型（LVLMs）概述

大型视觉-语言模型（LVLMs）是人工智能领域的一项变革性进步，它们在传统大型语言模型强大文本处理能力的基础上，进一步拓展了对图像、音频和视频等更广泛数据形式的解释和分析能力。这种能力的扩展使LVLMs成为解决各种现实世界挑战不可或缺的工具。通过整合多样化的数据形式，LVLMs旨在更紧密地模拟人类感知和与环境互动的细微方式，从而提供对我们如何参与和感知周围世界的更准确表示。

### 2.2. Qwen系列在AI领域中的定位

由阿里云开发的Qwen系列，是LVLM领域中一个杰出的开源贡献者。该系列通过持续创新和模型开放，致力于推动多模态AI的边界。其发展历程体现了对技术前沿的不断探索和对社区共享的坚定承诺。

### 2.3. 报告目的与范围

本报告旨在对Qwen视觉-语言模型——Qwen-VL、[Qwen2-VL](https://zhida.zhihu.com/search?content_id=260502689&content_type=Article&match_order=1&q=Qwen2-VL&zhida_source=entity)和Qwen2.5-VL的演进轨迹进行深入的技术分析。分析将细致地从其各自的技术报告（arXiv）、模型库（[Hugging Face](https://zhida.zhihu.com/search?content_id=260502689&content_type=Article&match_order=1&q=Hugging+Face&zhida_source=entity)、[ModelScope](https://zhida.zhihu.com/search?content_id=260502689&content_type=Article&match_order=1&q=ModelScope&zhida_source=entity)）和开源代码库（GitHub）中提取见解，重点关注架构创新、训练方法和经验性能改进。

## 3\. Qwen-VL：基础多模态模型

QWen-VL于2023年发布，是QWen系列的首个视觉语言模型，旨在整合文本和图像处理能力，为多模态任务提供基础支持。其技术报告（arXiv:2308.12966）和GitHub仓库（QwenLM/Qwen-VL）提供了详细的技术细节。

### 3.1. 核心架构与初始设计原则

Qwen-VL作为首个大型视觉-语言模型系列，旨在全面感知和理解文本与图像。其基础设计以Qwen-LM作为大型语言模型基座，并通过精心设计的视觉感受器、输入-输出接口以及战略性的三阶段训练流程赋予其视觉能力。Qwen-VL的视觉编码器组件最初采用Openclip ViT-bigG进行初始化，并通过一个随机初始化的交叉注意力层与大型语言模型（LLM）无缝集成，从而实现了跨模态信息的融合。

![](https://pic4.zhimg.com/v2-f5ef8966a9db49ca5d22e29aac677481_1440w.jpg)

-   **基础模型**：基于QWen-LM语言模型，集成了视觉感受器（visual receptor）和输入输出接口。
-   **模型变体**：包括QWen-VL和QWen-VL-Chat，分别针对基础任务和对话场景。

### 3.2. 初创时的关键能力与特性

除了传统的图像描述和问答任务，Qwen-VL还展示了高级功能，包括视觉定位和文本识别能力，这些是通过图像-标题-边界框元组的精确对齐实现的。一个显著的特点是其对多语言对话的原生支持，涵盖英语、中文及其他语言，并增强了对图像中嵌入的中文和英文文本的端到端识别能力。该模型率先实现了多图像交错对话，允许用户在一次对话中输入和比较多张图像，针对特定图像提问，甚至进行多图像故事讲述。Qwen-VL还成为首个支持中文定位的通用模型，能够使用中文和英文的开放域语言表达来检测边界框。为了提高细粒度识别能力，Qwen-VL将其输入分辨率扩展到448x448，这比许多其他开源LVLM常用的224x224分辨率有了显著提升。这种更高的分辨率显著增强了细粒度文本识别、文档问答和精确边界框标注等任务的性能。

![](https://pic3.zhimg.com/v2-436610566e9d7fc07456417c94c9d418_1440w.jpg)

-   **图像与文本理解**：支持图像描述、问题回答、视觉定位（visual grounding）和文本阅读。
-   **视觉定位与文本阅读**：通过对图像-标题-边界框（image-caption-box）元组的对齐，实现精准的视觉定位和文本提取。
-   **多模态任务**：在图像描述、问题回答和视觉定位等视觉中心任务中表现出色。

### 3.3. 训练方法与数据策展

训练过程采用了结构化的三阶段流水线，利用精心策划的多语言多模态语料库来优化跨不同数据类型和语言的学习。

### 3.4. 初始性能与隐含局限性

Qwen-VL及其指令微调版本Qwen-VL-Chat在广泛的视觉中心基准测试（如图像字幕、问答、视觉定位）和不同设置（如零样本、少样本）下，为同等模型规模的通用模型树立了新标杆。Qwen-VL-Chat在真实世界对话基准测试中也表现出优于现有视觉-语言聊天机器人的能力。

![](https://pic3.zhimg.com/v2-9d7711865be08575c15d8e0094799cf2_1440w.jpg)

然而，尽管448x448的分辨率有所改进，它仍然代表了一种固定分辨率的方法。后来的版本明确指出“传统的预定分辨率方法”是需要克服的局限性，这表明Qwen-VL尽管取得了进步，但在捕获不同尺度信息和处理超高分辨率或长时间视觉输入方面仍面临挑战。

## 4\. Qwen2-VL：动态分辨率与视频理解的进步

QWen2-VL于2024年发布（arXiv:2409.12191），是QWen-VL的重大升级，解决了前代模型在处理不同分辨率图像和长视频时的局限性。其GitHub仓库（QwenLM/Qwen2-VL）和官方博客（QWen Blog）详细记录了其开发过程。

### 4.1. 演进动力：解决Qwen-VL的局限性

Qwen2-VL被明确定位为“高级升级版”，旨在“重新定义Qwen-VL等先前模型所采用的传统预定分辨率视觉处理方法”。这表明其前身固定分辨率策略固有地限制了模型捕获不同尺度信息的能力，导致“高分辨率图像中详细信息的显著丢失”。Qwen2-VL旨在克服这一根本性瓶颈。

### 4.2. 关键架构创新

Qwen2-VL引入的“朴素动态分辨率机制”是一项关键创新。它能够动态处理不同分辨率的图像，并将其转换为可变数量的视觉标记。这种自适应方法旨在生成更高效、更准确的视觉表示，通过保留精细细节（无论输入图像大小如何）来更紧密地模拟人类的感知过程。

![](https://pic1.zhimg.com/v2-aa69afb9bdc135e3860251e626453670_1440w.jpg)

这种架构上的改变并非简单的功能叠加，而是视觉信息处理方式的根本性范式转变。通过允许图像动态转换为可变数量的视觉标记，模型能够“更有效地模仿人类视觉感知”，并保留在高分辨率图像中可能因调整大小或裁剪而丢失的详细信息。这种创新直接解决了先前LVLM的一个关键局限性，使得模型能够更广泛地应用于需要细粒度视觉理解的任务（例如，复杂文档解析、在不同背景下精确识别物体）。它为Qwen2.5-VL更复杂的动态分辨率和绝对时间编码奠定了基础，被证明是一项解锁后续能力世代的基础创新。

![](https://pic3.zhimg.com/v2-51240e7c7b77db1b99dcc9ae5222f2e8_1440w.jpg)

“多模态旋转位置嵌入（M-RoPE）”是位置编码的重大增强。M-RoPE旨在将位置嵌入分解为不同的组成部分，使模型能够有效地捕获和融合一维文本、二维视觉以及至关重要的三维视频位置信息，从而增强其整体多模态处理能力。这种编码对于解释不同模态及其时空背景之间复杂关系至关重要。

Qwen2-VL采用了统一的框架来处理静态图像和动态视频数据。它利用3D卷积算法处理视频数据，旨在通过一系列帧（而非独立的实体）模拟对现实世界事件更连续和连贯的理解。

特点总结：

-   **动态分辨率机制**：允许模型处理不同分辨率的图像，提高了视觉表示的效率和准确性。
-   **统一处理范式**：采用统一的图像和视频处理框架，增强了多模态感知能力。
-   **模型规模**：提供2B、8B和72B参数的模型，满足不同计算需求。

### 4.3. 扩展能力

![](https://pic3.zhimg.com/v2-63432bb36574b5b478d73c3d43a70952_1440w.jpg)

一项主要的新能力是模型的在线流媒体功能，使其能够理解和处理超过20分钟的视频。这促进了基于视频的高质量问答、对话生成以及从长视频输入中进行内容创作。

Qwen2-VL开始探索智能体能力，展示了高级推理和决策能力。这使得它能够与手机和机器人等设备集成，从而根据视觉环境线索和文本指令实现自主操作。

在Qwen-VL多语言文本识别的基础上，Qwen2-VL进一步增强了其在图像中识别多种语言文本的能力，除了强大的英语和中文支持外，还包括大多数欧洲语言、日语、韩语、阿拉伯语和越南语。

### 4.4. 训练与数据处理的改进

Qwen2-VL的训练范式继续采用多阶段方法。初始阶段侧重于训练视觉Transformer（ViT），利用大量的图像-文本数据集增强视觉-语义理解。后续阶段则解冻所有模型参数，并使用多样化的数据集继续训练，以全面提升模型的整体性能。

![](https://pic3.zhimg.com/v2-2f70071723be0212697df10cf2e48920_1440w.jpg)

Qwen2-VL系列明确研究了大型视觉-语言模型（LVLMs）的缩放定律，发布了不同参数规模（20亿、80亿和720亿参数）的模型，并相应地调整了训练数据量。

一个值得注意的效率设计选择是，Qwen2-VL系列中所有不同规模的LLM都统一使用了一个6.75亿参数的ViT。这确保了与视觉编码器相关的计算负载保持不变，无论LLM组件的规模如何。这种设计选择表明Qwen团队不仅关注通过大型模型实现峰值性能，还关注优化计算足迹和效率，特别是视觉编码器。视觉编码器由于图像数据的高维度，通常是多模态模型中的一个显著瓶颈。保持ViT大小不变意味着主要扩展效益（以及因此增加的计算成本）集中在LLM组件和多模态融合机制中，而不是视觉编码器按比例增大。这对于训练和推理期间的资源分配具有重要的实际意义，表明这是一种平衡原始能力与部署和可访问性现实的成熟工程方法。这也表明团队找到了视觉编码效率的最佳点，允许他们在保持视觉处理相对高效的同时，更积极地扩展语言模型。

### 4.5. 性能提升与比较分析

Qwen2-VL在众多视觉理解基准测试中取得了最先进的性能，包括MathVista、DocVQA、RealWorldQA和MTVQA。旗舰模型Qwen2-VL-72B展示了与GPT-4o和Claude3等领先专有模型相媲美的性能。

![](https://pica.zhimg.com/v2-cf467e5cf80935d7ab85288a3b719354_1440w.jpg)

![](https://pic3.zhimg.com/v2-b36ec845549c6b2d8863bfe9d2652ca6_1440w.jpg)

![](https://pic4.zhimg.com/v2-bf8f864fca4defbf94254caed77a10bb_1440w.jpg)

### 4.6. 开源集成与社区资源

Qwen2-VL模型，特别是20亿和70亿参数版本，以Apache 2.0许可协议开源，促进了更广泛的采用和研究。更大的Qwen2-VL-72B模型则以Qwen许可协议发布。这些模型旨在与流行的机器学习框架无缝集成，包括Hugging Face Transformers和vLLM，以及其他第三方框架，从而简化了使用和部署。

## 5\. Qwen2.5-VL：具有增强感知能力的旗舰多模态智能体

QWen2.5-VL于2025年发布（arXiv:2502.13923），是QWen系列的最新旗舰模型，基于开发者社区的反馈进一步优化了QWen2-VL。其GitHub仓库（QwenLM/Qwen2.5-VL）和官方博客（QWen Blog）提供了详细的发布信息。

### 5.1. 驱动创新与核心焦点

Qwen2.5-VL被定位为Qwen视觉-语言系列中的“最新旗舰模型”，在基础能力和创新功能方面均展现出显著进步。其开发旨在通过增强的视觉识别、精确的目标定位（使用边界框或点）、强大的文档解析以及先进的长视频理解，在理解和与世界互动方面实现重大飞跃。

### 5.2. 架构增强深度解析

![](https://pic2.zhimg.com/v2-2d1a4e50484375065b59c9a9a7575a3f_1440w.jpg)

Qwen2.5-VL采用从头开始训练的原生动态分辨率Vision Transformer (ViT)。一项关键创新是在ViT的大多数层中策略性地引入了窗口注意力机制。这种设计显著降低了处理不同尺寸图像时二次计算复杂度，使得计算成本与补丁数量呈线性关系，从而提高了推理效率。只有四个层使用完全自注意力，其余层采用窗口注意力（最大窗口大小为112x112），这使得模型能够以原生分辨率运行，而不会产生失真。

ViT架构进一步优化，使其更紧密地与大型语言模型（LLMs）的设计原则对齐。这包括采用RMSNorm进行归一化和SwiGLU作为激活函数，这些共同增强了计算效率以及模型视觉和语言组件之间的整体兼容性。

为了高效处理ViT生成的潜在长序列图像特征，Qwen2.5-VL采用了一种简单而有效的基于MLP的合并器。该机制将空间上相邻的四组补丁特征进行分组、拼接，然后通过一个两层MLP将其投影到与LLM中使用的文本嵌入对齐的维度。这种方法有效降低了计算成本，并提供了图像特征序列的动态压缩。

在动态分辨率概念的基础上，Qwen2.5-VL将其扩展到空间和时间域。对于图像，它动态地将不同大小的图像转换为相应长度的标记序列，直接使用输入图像的实际尺寸来表示边界框、点和其他空间特征。这使得模型能够固有地学习尺度信息，显著提高了其处理不同分辨率图像的能力。对于视频，它引入了动态帧率（FPS）训练和新颖的绝对时间编码。这使得模型能够处理长达数小时的视频，并实现精确的秒级事件定位。

还有就是MRoPE的演进。虽然Qwen2-VL的临时位置ID与输入帧数相关联，但Qwen2.5-VL将MRoPE的时间组件与_绝对时间_对齐。这一关键的改进使得模型能够通过利用时间ID之间的时间间隔，在不同FPS采样率的视频中学习一致的时间对齐，且不增加额外的计算开销。

### 5.3. 全面数据策展与高级训练流程

预训练数据量从1.2万亿个标记大幅扩展到约4万亿个标记。这个庞大的数据集通过清洗原始网络数据和合成新数据精心构建，涵盖了广泛的多模态数据类型，包括图像字幕、交错图像-文本数据、光学字符识别（OCR）数据、视觉知识、多模态学术问题、定位数据、文档解析数据、视频描述、视频定位以及基于智能体的交互数据。

开发了一个用于对交错数据进行评分和清洗的复杂流程，以确保只使用高质量和相关的数据。该流程包括标准数据清洗，然后是一个四阶段评分系统，评估纯文本质量、图像-文本相关性、图像-文本互补性以及信息密度平衡。

为了实现无与伦比的视觉感知精度，Qwen2.5-VL使用基于输入图像实际尺寸的坐标值来表示边界框和点，摒弃了传统的相对坐标。开发了一个全面的定位能力数据集，整合了公开和专有数据，并使用复制-粘贴增强和Grounding DINO、SAM等模型合成技术，将其合成为各种格式（XML、JSON、自定义）。开放词汇检测的训练数据集扩展到超过10,000个物体类别，并合成了不存在的物体类别以应对极端检测场景。

合成了大量的文档数据语料库，以赋予通用模型跨不同文档类型的全面解析、理解和转换能力。这些文档包含表格、图表、公式、图像、乐谱和化学式等复杂元素，所有这些都统一格式化为HTML，以整合布局框信息和描述。

从各种来源（合成、开源、内部）收集数据，以显著提高OCR性能。开发了一个大规模多语言OCR数据集以支持多种语言，同时处理和过滤了合成图表类型数据（100万样本）和真实世界表格数据（600万样本）。

训练期间实施了动态FPS采样，以确保FPS在视频输入中更均匀地分布。此外，为超过半小时的视频合成了长视频字幕，并以秒级和时分秒帧（hmsf）两种格式制定了时间戳，以确保对时间事件的准确理解和输出。

模型用于智能体任务的感知和决策能力通过手机、网页和桌面平台的截图得到显著增强。采用合成数据引擎生成截图字幕和UI元素定位注释。操作被统一为具有共享动作空间的功能调用格式，注释的多步轨迹被重新格式化为功能格式，并由人类和模型注释者生成推理过程。

预训练过程被细致地分为三个不同的阶段以优化学习：

1.  **视觉预训练：** 在此初始阶段，仅训练ViT（视觉Transformer）以提高其与语言模型的对齐，主要利用图像字幕、视觉知识和OCR数据。
2.  **多模态预训练：** 在此阶段，所有模型参数都被解冻，模型在各种多模态图像数据上进行训练。这包括交错数据、视觉问答（VQA）、多模态数学、基于智能体的任务、视频理解和纯文本数据集。
3.  **长上下文预训练：** 最后阶段整合了视频和基于智能体的数据，重点是增加序列长度，以增强模型在更长、更复杂序列上的推理能力。

采用了一种复杂的策略来优化训练效率，通过根据数据样本输入LLM的序列长度动态打包数据样本，确保GPU之间的计算负载一致，从而最大化硬件利用率。

### 5.4. 后训练对齐框架

Qwen2.5-VL采用了由监督微调（SFT）和直接偏好优化（DPO）组成的双阶段优化方法。该策略有效地结合了参数高效的领域适应和人类偏好蒸馏，从而优化了模型的响应。

SFT利用ChatML格式来构建指令遵循数据，允许明确的对话角色标记、视觉嵌入的结构化注入以及关键的跨模态位置关系的保留。SFT数据集包含约200万个条目，在纯文本和多模态数据之间均匀分布，主要为中文和英文，并辅以多语言条目。

实施了两阶段数据过滤流程，以确保SFT数据集的最高质量。这包括使用专门的分类模型（Qwen2-VL-Instag）进行领域特定分类，以及整合基于规则（消除重复模式、不完整响应、有害内容）和基于模型（利用奖励模型评估多模态问答对的复杂性、相关性、正确性、完整性、清晰度和有用性）的方法进行领域定制过滤。

该策略用于进一步优化数据集，并显著增强数学问题解决和代码生成等复杂推理任务的推理能力。它涉及使用中间Qwen2.5-VL模型评估生成的响应与真实值的一致性，仅保留匹配的样本。额外的约束条件过滤掉不希望的输出，如代码切换、过长或重复模式。基于规则和模型驱动的过滤策略验证了中间推理步骤的准确性，确保了视觉和文本模态的有效集成。

### 5.5. 最先进的性能基准

![](https://pica.zhimg.com/v2-43ce50bc1076944e4dcf38cff7b82258_1440w.jpg)

旗舰模型Qwen2.5-VL-72B持续与GPT-4o和Claude 3.5 Sonnet等最先进模型相媲美或超越，在文档和图表理解方面表现尤为出色。

Qwen2.5-VL在多项基准测试中展现出卓越性能，特别是其72B模型。在大学水平问题上，如MMMUval和MMMU-Prooverall，Qwen2.5-VL-72B分别获得70.2分和51.1分，与GPT-4o和Claude 3.5 Sonnet相当，并超越了先前的开源最佳水平。在数学相关任务中，例如MathVistamini，Qwen2.5-VL-72B以74.8分表现出色，超过了先前的开源最佳成绩。

![](https://pica.zhimg.com/v2-718475a51d1d8b7b81ab7e4ae708e22e_1440w.jpg)

在通用视觉问答方面，Qwen2.5-VL-72B在MegaBench、MMBench和MMStar等基准测试中持续取得高分，经常达到或超越最先进水平。例如，在MMBench-ENtest上获得88.6分，在MMStar上获得70.8%。该模型还保持了强大的语言性能，在MMLU-Pro（71.2）、GSM8K（95.3）和HumanEval（87.8）等纯文本任务中表现领先。

![](https://pic2.zhimg.com/v2-729b5cc6758101bf08fac7527705ff41_1440w.jpg)

QWen2.5-VL-7B-Instruct在多项任务中超越GPT-4o-mini，QWen2.5-VL-3B则在边缘AI场景中表现出高效性能。

![](https://picx.zhimg.com/v2-aa962ce20ba8c381c098573d3fb65cb1_1440w.jpg)

### 5.6. 实际实施与部署细节

Qwen2.5-VL提供三种尺寸，以满足从边缘AI到高性能计算的各种用例。Qwen2.5-VL的官方GitHub仓库提供了全面的资源，包括详细的安装说明（需要特定版本的`transformers`和`accelerate`，以及用于视觉输入处理的`qwen-vl-utils[decord]`）。它为Hugging Face和ModelScope提供了快速入门指南，以及针对各种功能（通用识别、文档解析、对象定位、OCR、视频理解、移动智能体、计算机使用智能体）的广泛食谱，通常附有Colab链接以进行交互式学习。

模型支持广泛的分辨率输入，允许用户通过`min_pixels`、`max_pixels`或精确的`resized_height`/`resized_width`设置来平衡性能和成本。它还支持为多个图像输入添加视觉ID，并利用Flash-Attention 2加速生成。

对于长文本和视频输入，`config.json`支持多达32,768个标记，并提供使用YaRN或直接修改MRoPE的`max_position_embeddings`以扩展的选项。API使用通过`dashscope`库支持，并提供了构建Web UI演示的说明，包括一个实验性的流媒体视频聊天演示。

## 6\. 比较分析：Qwen-VL系列的演进轨迹

![](https://pic4.zhimg.com/v2-bbd418679b307e7614450f9efbd332bb_1440w.jpg)

以下表格总结了QWen系列视觉模型的演进特点：

|模型版本|视觉编码器|模型规模（LLM）|新增功能亮点|
|---|---|---|---|
|Qwen‑VL|ViT + receptor-adapter|7B|caption/VQA/OCR、多图 dialogue、grounding|
|Qwen2‑VL|固定 ViT（675M）|2B / 7B / 72B|dynamic resolution、M-RoPE、长视频(20min)、视觉代理、多语 OCR|
|Qwen2.5‑VL|native dynamic ViT + window attention|3B / 7B / 32B / 72B|文档解析、精确定位、小时视频分析、结构化输出、工具调用|

### 6.1. 各代关键技术与能力演进总结

-   **Qwen-VL（基础多模态智能）：** 这一初始版本确立了核心的视觉-语言能力，包括基本的图像理解、目标定位和图像内文本识别。它提供了早期的多语言支持，并开创了多图像交错对话。在架构上，它依赖于固定的输入分辨率（448x448），并采用交叉注意力机制将Openclip ViT与Qwen-LM集成。
-   **Qwen2-VL（动态分辨率与扩展视频理解）：** 这是一个重要的架构飞跃，引入了“朴素动态分辨率”机制，使模型能够将任意分辨率的图像处理为动态数量的视觉标记。它首次引入了“多模态旋转位置嵌入（M-RoPE）”来处理一维文本、二维视觉和三维视频模态的位置信息。此版本将视频理解能力显著扩展到20分钟以上，并引入了设备操作的初步智能体功能。
-   **Qwen2.5-VL（旗舰多模态智能体与增强感知）：** 这一迭代在前代创新的基础上进行了细化和扩展。它引入了带有高效窗口注意力机制的重新设计的ViT，并使其架构更紧密地与LLM设计原则（RMSNorm，SwiGLU）对齐。动态分辨率概念通过“绝对时间编码”扩展到超长视频（长达数小时）和秒级事件定位，并且MRoPE被专门对齐到绝对时间。在强大的文档解析、精确目标定位（使用绝对坐标）以及高度复杂的计算机和移动设备交互智能体功能方面，能力得到了显著增强。

### 6.2. 视觉与时间理解范式的进展

-   **分辨率处理：** 演进过程清晰地展示了从固定分辨率方法（Qwen-VL的448x448）到灵活的“朴素动态分辨率”（Qwen2-VL），最终在Qwen2.5-VL中实现高效的原生“带有窗口注意力的动态分辨率”和空间特征的绝对坐标使用的明确而有意的进展。这一轨迹反映了持续努力，以实现更灵活、计算更高效和更像人类的空间感知，这对于处理多样化的真实世界视觉输入至关重要。
-   **视频理解：** 该系列从Qwen-VL没有明确视频能力，发展到Qwen2-VL处理20分钟以上的视频，最终在Qwen2.5-VL中能够处理长达“数小时”的视频，并进行精确的“秒级事件定位”。这一进展得益于时间编码的进步，从与帧数相关的M-RoPE发展到与绝对时间对齐的M-RoPE，表明时间理解和细粒度时间定位的复杂性不断提高。
-   **定位能力：** Qwen-VL的初始边界框定位在Qwen2-VL中得到增强，并在Qwen2.5-VL中进一步完善，包括使用边界框和点进行精确目标定位，利用绝对坐标值提高准确性。

### 6.3. 数据、模型架构和性能指标的扩展

-   **训练数据量与多样性：** 每次迭代都伴随着预训练数据规模的显著扩展，Qwen2.5-VL约4万亿个标记的数据量就是例证。更重要的是，数据多样性从通用图像-文本对演变为高度专业化的数据集，包括精心策划的文档解析数据、智能体交互数据和长视频字幕。这种有针对性的数据策展直接促进了专业能力的发展。
-   **模型架构：** 底层LLM基础和视觉编码器组件持续得到改进和扩展。Qwen-VL使用了Openclip ViT-bigG，而Qwen2-VL和Qwen2.5-VL引入了经过特定优化（例如，Qwen2-VL中ViT尺寸恒定，Qwen2.5-VL中窗口注意力）的重新设计ViT。集成机制也日益成熟，从简单的交叉注意力层发展到复杂的基于MLP的合并器。
-   **训练方法：** 训练配方从Qwen-VL的基础三阶段流水线发展到Qwen2.5-VL更复杂的多阶段训练策略，并整合了监督微调（SFT）、直接偏好优化（DPO）和拒绝采样等高级后训练对齐技术，以增强推理能力。
-   **性能轨迹：** 每个后续模型在广泛的多模态基准测试中都展现出持续且通常是显著的性能提升。特别是Qwen2.5-VL-72B，持续与最先进的专有模型相媲美或超越，展示了其在复杂推理、文档理解、视频理解和智能体任务方面的领导地位。智能体能力（例如，ScreenSpot Pro从1.6%跃升至43.6%）的显著飞跃凸显了有针对性的架构和数据创新的影响。

### 6.4. 对多模态AI格局的影响

Qwen系列持续的演进凸显了其作为开源多模态AI领域领先力量的作用。通过在动态分辨率、长视频理解和交互式智能体能力等领域不断突破界限，阿里云为该领域的发展做出了重大贡献。

**功能演进**

|特性|Qwen-VL|Qwen2-VL|Qwen2.5-VL|
|---|---|---|---|
|大致发布日期|2023年8月|2024年9月|2025年2月|
|核心LLM基础|Qwen-LM|Qwen2 LLM|Qwen2.5 LLM|
|视觉编码器|Openclip ViT-bigG|重新设计的ViT (675M参数，跨LLM尺寸恒定)|重新设计的ViT (从头训练，窗口注意力)|
|分辨率处理|固定448x448|朴素动态分辨率|原生动态分辨率与窗口注意力，绝对坐标|
|位置嵌入|标准|M-RoPE (1D/2D/3D)|M-RoPE对齐绝对时间|
|视频理解|有限/无明确视频能力|20分钟+|数小时，秒级事件定位|
|关键能力|基础视觉问答、定位、图像内文本识别、多语言、多图对话|扩展视频理解、初步智能体功能、更广多语言图像文本识别|强大文档解析、精确点/框定位、高级智能体功能、交互式视觉智能体|
|训练数据规模（约）|多语言多模态语料库|\>7万亿LLM标记|~4万亿多模态标记|
|关键训练创新|3阶段训练|多阶段训练|3阶段训练、动态FPS采样、SFT/DPO、拒绝采样|

## 7\. 总结与未来展望

1.  **从 Qwen‑VL 到 Qwen2‑VL**，核心进化体现在视觉 token 的动态分配（dynamic resolution）、视频理解能力、多语言 OCR 与视觉代理功能等方面，奠定跨模态交互与长内容理解基础。
2.  **从 Qwen2‑VL 到 Qwen2.5‑VL**，聚焦实用性：升级视觉编码器架构，强化文档图表解析、精确物体定位、长视频事件定位与结构化输出，同时赋予工具调用与交互式代理功能，全面走向行业级 multimodal agent。

整体看，Qwen‑VL 为 vision‑LLM 开疆拓土；Qwen2‑VL 迈向长图像与视频跨模态融合；而 Qwen2.5‑VL 则向「视觉理解 + 文档解析 + 交互式视觉链」前沿跃进，成为目前 Qwen 系列中的旗舰模型。

让我们期待后续QWen3-VL的到来。

## 参考资料

Bai J, Bai S, Yang S, et al. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond\[J\]. arXiv preprint arXiv:2308.12966

Wang P, Bai S, Tan S, et al. Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution\[J\]. arXiv preprint arXiv:2409.12191

Bai S, Chen K, Liu X, et al. Qwen2. 5-vl technical report\[J\]. arXiv preprint arXiv:2502.13923

文章内容，未经本人允许，不得转载。

——完——

[@北方的郎](https://www.zhihu.com/people/7af62e4119791a452e88718cb5ccc0be) · 专注模型与代码

喜欢的朋友，欢迎赞同、关注、分享三连 ^O^
