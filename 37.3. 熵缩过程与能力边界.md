---
created: 2025-10-28T17:32:30 (UTC +08:00)
tags: [GRPO,RLHF,RLVR]
source: https://zhuanlan.zhihu.com/p/1917257135730964355
author: 关于作者紫气东来吾生也有涯，而知也无涯上海交通大学 工学硕士JOYWIN、Whisper、Nasusu 也关注了他回答32文章112关注者22,679已关注发私信
---
### 2.2 实质：“熵坍缩”与模型专业化

“熵坍缩”：即在 RL 训练中策略熵在早期训练阶段急剧下降，导致策略模型过于自信，进而导致探索能力的减弱与策略性能的饱和。实际上，上一小节的 PPL 已经提现了这一点（PPL 与熵的计算仅仅差一个 exp），其表现如下图左所示，模型在特定领域的表现提升与其熵缩是同时发生的，甚至二者之间都可以通过公式近似拟合出来。

![](https://pic1.zhimg.com/v2-ebd8a56513ef1fcf48ceefe3e8c9632a_1440w.jpg)

The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models

如果对 Token 熵进行模式分析，可以发现，在 CoT 推理中，大多数 token 的熵很低，而少数 token 的熵很高。这些高熵的 token 通常作为推理路径中的“分叉点”（forks），引导模型走向不同的推理路径。例如，高熵 token 常见于逻辑连接词（如“Thus”、“perhaps”等），而低熵 token 则多为单词后缀或数学表达式的组成部分。

**在 CoT 推理中，高熵的少数 token 起到了关键作用，它们作为“分叉点”引导模型走向不同的推理路径。** RLVR 训练主要保留了基础模型的熵模式，并且主要调整了高熵 token 的熵。通过仅对高熵 token 进行策略梯度更新，可以显著提高模型的推理性能，且这种方法在更大模型上效果更明显。

![](https://pica.zhimg.com/v2-e89da1d3c741c878e6101232f86e38a2_1440w.jpg)

Beyond the 80/20 Rule: High-Entropy Minority TokensDrive Effective Reinforcement Learning for LLM Reasoning

另外也有工作研究了正负样本对 RL 的训练结果的影响，实验发现，仅使用负样本进行训练（NSR）在Pass@k的整个范围内都能显著提升模型性能，甚至在某些情况下超过了PPO和GRPO等常用强化学习算法。与NSR相反，仅使用正样本进行训练（PSR）虽然能提高Pass@1，但在较大的k值下性能下降，导致输出多样性降低。通过跟踪模型的熵，发现NSR在整个训练过程中保持了较高的熵，而PSR则迅速降低了熵。这表明NSR在训练过程中保持了模型的输出多样性。

通过分析可知，PSR通过增加正确响应的logit值，同时降低其他所有token的logit值，导致输出分布变得过于集中，减少了多样性。NSR通过降低错误响应的logit值，并将概率质量重新分配给其他候选token，这种重新分配是基于模型先验的，有助于保持多样性。NSR通过抑制错误响应和根据模型先验重新分配概率质量，有效地细化了模型的现有知识，而不是引入全新的行为。

![](https://pica.zhimg.com/v2-617974b6ee1292f307241fe750ec0982_1440w.jpg)

The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning

通过以上一系列实验，至少可以发现以下共同的现象：

1.  随着训练的进行，“熵坍缩”现象是普遍存在的，无法避免的；
2.  不同 token 的熵表现不同，对结果的影响也不同；
3.  通过对不同 token 的熵的干预，可以一定程度影响结果

以上过程是如何发生的？其底层的原理是什么？其与模型专业化训练过程的关系是什么？以下将试图进行更加深入的探讨。

## 二、熵缩：从 SFT 到 RL

上文讨论的“熵坍缩”现象都是发生在 RL 训练过程中的，目前尚未看到 SFT 中有类似提法，那么本节就由表及里，从 SFT 到 RL，探究“熵坍缩”的发生过程。

### 2.1 信息熵、策略熵与[交叉熵损失](https://zhida.zhihu.com/search?content_id=259085034&content_type=Article&match_order=1&q=%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1&zhida_source=entity)

首先在笔者之前的文章中已经讨论过，对于一个确定的语言或者数据集，其**信息熵**是确定的，即在自然语言领域，令 $b_n = (w_1,w_2,...,w_n)$ ，熵可以表示为：

$\begin{aligned} F_N & =-\sum_{b_n} p\left(b_n\right) \log _2 p\left(w_n \mid b_{n-1}\right) \\ & =-\sum_{b_n} p\left(b_n\right) \log _2 p\left(b_n\right)+\sum_{b_{n-1}} p\left(b_{n-1}\right) \log _2 p\left(b_{n-1}\right)\\ & =K_N-K_{N-1} \end{aligned}$

其中 $K_N=-\sum_{b_n} p\left(b_n\right) \log _2 p\left(b_n\right)$

当序列长度无限大的情况下，香农将其定义为该语言的熵，即： $H=\lim _{N \rightarrow \infty} F_N$

根据这个定义，熵是使用无限数量的符号来计算的。在实践中，只能从有限的文本样本中近似经验熵来近似任何语言的熵。

在 LLM 中，我们可以根据定义来计算参数化模型输出的 **token 熵**，即索引 $\(t\)$ 处的熵$\mathcal{H}_t:=-\sum_{j=1}^V p_{t, j} \log p_{t, j},\\ \text { where }\left(p_{t, 1}, \cdots, p_{t, V}\right)=p_t=\pi_{\boldsymbol{\theta}}\left(\cdot \mid \boldsymbol{q}, \boldsymbol{y}_{<t}\right)=\operatorname{Softmax}\left(\frac{\boldsymbol{z}_t}{T}\right)$ 其中， $\(\pi_\theta\)$ 表示由 $\(\theta\)$ 参数化的LLM， $\(q\)$ 是输入查询， $\(y_{<t} = (y_1, y_2, \cdots, y_{t-1})\)$ 表示先前生成的token。$\(V\)$ 是词表大小， $\(\boldsymbol{z}_t \in \mathbb{R}^V\)$ 表示在时间步 $\(t\)$ 的 pre-softmax logits， $\(p_t \in \mathbb{R}^V\)$ 是词汇表上相应的概率分布， $\(T \in \mathbb{R}\)$ 是解码温度。

在此基础上，我们可以进一步得到**策略熵**（即模型熵），策略模型在训练数据 $\mathcal{D}$ 上的平均 token 熵，即 $\mathcal{H}\left(\pi_\theta, \mathcal{D}\right)=-\mathbb{E}_{\mathcal{D}, \pi_\theta}\left[\log \pi_\theta\left(y_t \mid \boldsymbol{y}_{<t}\right)\right]=-\frac{1}{|\mathcal{D}|} \sum_{x \in \mathcal{D}} \frac{1}{|\boldsymbol{y}|} \sum_{t=1}^{|\boldsymbol{y}|} \mathbb{E}_{y_t \sim \pi_\theta}\left[\log \pi_\theta\left(y_t \mid \boldsymbol{y}_{<t}, x\right)\right]$ 其中输入提示 $x$ ，策略熵量化了策略对当前提示的不确定性水平。

细心的读者看到以上公式也许会联想到 SFT 中的交叉熵损失，即： $\mathcal{L}_{\mathrm{SFT}}=-\frac{1}{|\mathcal{D}|} \sum_{x \in \mathcal{D}} \frac{1}{|\boldsymbol{y}|} \sum_{t=1}^{|\boldsymbol{y}|} \log \pi_\theta\left(y_t^* \mid \boldsymbol{y}_{<t}^*, x\right)$ 其中的关键区别在于移除了期望 $\mathbb{E}_{y_t \sim \pi_\theta}$ ，并替换为真实的样本序列 $\boldsymbol{y}_{<t}^*$ 。这体现了监督学习的本质：使用真实标签计算损失。

通过以上分析我们可知，**SFT 中也存在“熵坍缩”现象，而且其“熵坍缩”现象的本质就是模型训练的过程（loss 下降）**，这也是模型在特定数据集上的专业化过程。

而与 SFT 不同的是，RL 存在探索的过程，其中会产生多个正负样本，因此其策略熵即是在多条样本期望上，其**“熵坍缩”的过程也即逐步收敛到正样本上的过程**。下面我们将尝试进一步论证以上推断。

### 2.2 RL 的熵缩机制

在论文 [The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2505.22617) 中比较严谨地证明了 RL “熵坍缩”的发生过程，在此仅展示其中核心观点。

**Softmax 策略的熵差**。假设策略 $\pi_\theta$ 是一个表格形式的 softmax 策略，其中每个状态-动作对(s,a)都与一个独立的 logit 参数 $z_{s,a}=θ_{s,a}$ 相关联，在一级近似下，两个连续步骤中给定状态 $s$ 的策略熵之差满足 $\mathcal{H}\left(\pi_\theta^{k+1}\right)-\mathcal{H}\left(\pi_\theta^k\right) \approx \mathbb{E}_{s \sim d_{\pi_\theta}}\left[\mathcal{H}\left(\pi_\theta^{k+1} \mid s\right)-\mathcal{H}\left(\pi_\theta^k \mid s\right)\right] \approx \mathbb{E}_{s \sim d_{\pi_\theta}}\left[-\operatorname{Cov}_{a \sim \pi_\theta^k(\cdot \mid s)}\left(\log \pi_\theta^k(a \mid s), z_{s, a}^{k+1}-z_{s, a}^k\right)\right]$这个引理表明，策略熵的变化约等于动作的对数概率与 logits 变化之间的负协方差。也就是说，当一个动作 $a$ 在更新前从策略中获得了高概率，并且其对应的 logits 在更新后也在增加，那么它将降低策略熵。

**策略梯度中策略 logits 的差异**。上式中 $z_{s,a}^{k+1}−z_{s,a}^k$ 是步骤k和步骤k+1之间输出 logits 的变化，可以证明，通过梯度回溯以学习率 $η$ 进行更新，则连续两步之间的差异满足 $z_{s, a}^{k+1}-z_{s, a}^k=\eta \pi_\theta(a \mid s) A(s, a)$

结合以上公式，直观上可以看到，**一个动作 $a$ 同时获得高/低概率和高/低优势会降低熵，**反之亦然。在早期阶段，策略在训练数据上表现出高协方差，暗示策略的置信度得到了良好校准，因此可以安全地利用高置信度的轨迹，增强信念并最小化熵，此阶段可理解为 SFT 的增强阶段，也即熵缩的主要过程。

### 2.3 RL 与 SFT 的联结

在传统认知中，RL 与 SFT 的区别主要体现在两个方面：

-   负样本的利用
-   样本的多样性

那么如果在 SFT 中增加负样本，同时可以无限增加样本的情况下，是否 SFT 就可以等价于 RL 呢？答案是肯定的。接下来我们看一下 [Bridging Supervised Learning and Reinforcement Learning in Math Reasoning](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2505.18116) 中的论证过程。

利用负样本。首先定义负策略 $\pi^{-}(\boldsymbol{a} \mid \boldsymbol{q}):=\pi(\boldsymbol{a} \mid \boldsymbol{q}, r=0)=\frac{\pi(\boldsymbol{a} \mid \boldsymbol{q})[1-p(r=1 \mid \boldsymbol{q}, \boldsymbol{a})]}{\sum_A \pi(\boldsymbol{a} \mid \boldsymbol{q})[1-p(r=1 \mid \boldsymbol{q}, \boldsymbol{a})]}$ 而最终的策略可以看作正负策略的叠加，即 $r_{\boldsymbol{q}} \pi^{+}(\boldsymbol{a} \mid \boldsymbol{q})+\left[1-r_{\boldsymbol{q}}\right] \pi^{-}(\boldsymbol{a} \mid \boldsymbol{q})=\pi(\boldsymbol{a} \mid \boldsymbol{q})$ 其中 $r_{\boldsymbol{q}}:=\sum_A \pi(\boldsymbol{a} \mid \boldsymbol{q}) p(r=1 \mid \boldsymbol{q}, \boldsymbol{a})=p(r=1 \mid \boldsymbol{q})$ 表示 LLM $\pi$ 在问题 上的正确率。

根据以上关系，可以构建一个隐式负策略 $\pi_\theta^{-}(\boldsymbol{a} \mid \boldsymbol{q}):=\frac{\pi(\boldsymbol{a} \mid \boldsymbol{q})-r_{\boldsymbol{q}} \pi_\theta^{+}(\boldsymbol{a} \mid \boldsymbol{q})}{1-r_{\boldsymbol{q}}}$

![](https://pic2.zhimg.com/v2-da930901c5441004fbd36dc0a1c49783_1440w.jpg)

考虑用于训练隐式负策略 $\pi_\theta^{-}$ 的最大似然目标： $\max _\theta \mathbb{E}_{p(\boldsymbol{q}) \pi^{-}(\boldsymbol{a} \mid \boldsymbol{q})}\left[\log \pi_\theta^{-}(\boldsymbol{a} \mid \boldsymbol{q})\right] \Leftrightarrow \min _\theta\left[-\mathbb{E}_{(\boldsymbol{q}, \boldsymbol{a}) \sim \mathcal{D}^{-}} \log \frac{\pi(\boldsymbol{a} \mid \boldsymbol{q})-r_{\boldsymbol{q}} \pi_\theta^{+}(\boldsymbol{a} \mid \boldsymbol{q})}{1-r_{\boldsymbol{q}}}\right]$ 假设数据无限且模型容量无限，上式的最优解是 $\forall \boldsymbol{q}, \boldsymbol{a}: \quad \pi_\theta^{+}(\boldsymbol{a} \mid \boldsymbol{q})=\pi^{+}(\boldsymbol{a} \mid \boldsymbol{q})$ 为了进一步利用正样本，可以对上式进行进一步修正，即

$\begin{aligned} \mathcal{L}_{\mathcal{D}}^{\mathrm{NFT}}(\theta)= & -\sum_{\boldsymbol{q}, \boldsymbol{a}, r} \omega(\boldsymbol{q}) \sum_t\left[r \log R_\theta^t(\boldsymbol{q}, \boldsymbol{a})+(1-r) \log  \mathrm{maxv}\left(\frac{1-\hat{r}_{\boldsymbol{q}} R_\theta^t(\boldsymbol{q}, \boldsymbol{a})}{1-\hat{r}_{\boldsymbol{q}}}, \epsilon\right)\right] \\ & \text { where } \quad R_\theta^t(\boldsymbol{q}, \boldsymbol{a})=\frac{\pi_\theta^{+}\left(\boldsymbol{a}_t \mid \boldsymbol{q}, \boldsymbol{a}_{<t}\right)}{\pi\left(\boldsymbol{a}_t \mid \boldsymbol{q}, \boldsymbol{a}_{<t}\right)}, \quad \text { and } \quad \hat{r}_{\boldsymbol{q}}=\frac{1}{K} \sum_{\boldsymbol{a} \mid \boldsymbol{q}} r(\boldsymbol{q}, \boldsymbol{a}) . \end{aligned}$ 

负例感知微调 (Negative-aware FineTuning, _NFT_)


接下来论证 GRPO 和 NFT 在 on-policy 训练中是等价的。假设对于一个给定的问题 ，有 $\hat{r}_ ⁢K$ 个正面答案和 $(1−\hat{r}_ )⁢K$ 个反面答案，仅考虑二元奖励，则 GRPO 的梯度 $\nabla_\theta \mathcal{L}_{\mathcal{D}}^{G R P O}(\theta)=-\sum\left\{r A_{\boldsymbol{q}}^{+} \cdot \mathcal{I}\left[R_\theta^t(\boldsymbol{q}, \boldsymbol{a})<1+\epsilon^{\prime}\right]+(1-r) A_{\boldsymbol{q}}^{-} \cdot \mathcal{I}\left[R_\theta^t(\boldsymbol{q}, \boldsymbol{a})>1-\epsilon^{\prime}\right]\right\} \nabla_\theta R_\theta^t(\boldsymbol{q}, \boldsymbol{a})$ 其中 $A_{\boldsymbol{q}}^{+}=\sqrt{\frac{1-\hat{r}_{\boldsymbol{q}}}{\hat{r}_{\boldsymbol{q}}}} , ~~~ A_{\boldsymbol{q}}^{-}=-\sqrt{\frac{\hat{r}_{\boldsymbol{q}}}{1-\hat{r}_{\boldsymbol{q}}}}$ 分别是答案的归一化优势。

同样可以得到 NFT 的梯度，即 $\nabla_\theta \mathcal{L}_{\mathcal{D}}^{N F T}(\theta)=-\sum\left\{r A_{\boldsymbol{q}}^{+} \cdot \frac{1}{R_\theta^t(\boldsymbol{q}, \boldsymbol{a})}+(1-r) A_{\boldsymbol{q}}^{-} \cdot \max \left[\frac{1-\hat{r}_{\boldsymbol{q}} R_\theta^t(\boldsymbol{q}, \boldsymbol{a})}{1-\hat{r}_q}, \epsilon\right]^{-1}\right\} \nabla_\theta R_\theta^t(\boldsymbol{q}, \boldsymbol{a})$

![](https://pic2.zhimg.com/v2-88e4453cf144628a379078f919da65fd_1440w.jpg)

则可以推知GRPO 和 NFT 损失梯度在策略梯度训练中是等价的，即 $R_\theta^t(\boldsymbol{q}, \boldsymbol{a})=1 \quad \Longrightarrow \quad \nabla_\theta \mathcal{L}_{\mathcal{D}}^{N F T}(\theta)=\nabla_\theta \mathcal{L}_{\mathcal{D}}^{G R P O}(\theta)$总结一下以上讨论，**RL 与 SFT 没有本质上的区别，其熵缩的过程即是模型训练的优化过程，也是模型专业化的过程，改过程是必须的、不可避免的，对于熵的过分放开可能反而会使模型训练的崩溃**。

## 三、“熵坍缩”的处理方法及其本质

### 3.1 探索-利用困境

事实上，关于熵的讨论即是在讨论 RL 的核心问题 —— 探索-利用困境（exploitation-exploration dilemma）：

-   如果任由熵缩而不加干预，则模型会快速收敛到某种模式，即利用过多，会导致模型能力比较局限；
-   如果对熵干预过多，即使熵保持在一个高水平，即探索过多，可能会导致无法收敛，甚至训练崩溃

下面通过几个案例来说明以上情况：

控制策略熵的常用方法之一是应用熵损失，下图展示了添加熵损失的结果，熵损失对系数非常敏感，小系数对熵的影响较小（0.0001,0.001），而大系数会导致熵爆炸（0.01）。尽管将系数设置为0.005成功地稳定了策略熵，但它并不优于其他方法。

![](https://pic4.zhimg.com/v2-06a9821e7ee55336d7b094f5bdbcc381_1440w.jpg)

也可以通过调整策略模型和参考模型之间的 KL 惩罚来控制熵。尽管 KL 实现了稳定的熵值，但它未能改进策略，反而导致性能下降，因此当前很多工作就不再使用 KL 约束。

![](https://pic1.zhimg.com/v2-3c02bf84c0978e1706cc14987e27b8fa_1440w.jpg)

### 3.2 干预方法讨论

在前文中，我们讨论了“熵坍缩”的发生过程及其影响，那么在具体训练过程中，熵与哪些因素相关呢？该如何干预和影响熵呢？熵的改变对下游任务有多大影响呢？本节将以一些典型工作为例，讨论以上相关的内容。

**3.2.1 Clip-Higher**

DAPO 是较早讨论“熵坍缩”现象并进行干预的工作，其干预的方法也非常简单，即 Clip-Higher。Clip-Higher 通过解耦clip的上下限范围，增大clip上限，以允许更自由地增加低概率 token 的概率，从而鼓励探索。.此外，上限阈值仅影响具有正优势的 token。

![](https://picx.zhimg.com/v2-f26e51a0b697655da3796013c557eeed_1440w.jpg)

**3.2.2 Clip-Cov 与 KL-Cov**

论文 [The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2505.22617) 中根据其提出的协方差理论，相应提出了基于此的 clip 方案。

策略熵动态与动作概率和优势之间的协方差密切相关。而在大部分模型中，一小部分 token 表现出极高的协方差，远超平均值(如下表)。也就是说，这些异常 token 在触发熵崩溃中起主导作用。为了减轻它们的不利影响，需要对其对策略损失的贡献施加约束。

|Group|Top 0.02%|Top 0.2%|Top 2%|Top 20%|Top 50%|All|
|---|---|---|---|---|---|---|
|Mean value|5.654|3.112|1.385|0.351|0.152|0.003|

假设有一批 N 个response， $\pi_\theta⁢(y_i)$ 表示策略模型在给定其对应 prompt 的情况下，对 token $y_i$ 的输出概率。根据定理 2，我们首先定义 log 概率和优势之间的 token 级别中心交叉乘积为协方差，即 $\operatorname{Cov}\left(y_i\right)=\left(\log \pi_\theta\left(y_i\right)-\frac{1}{N} \sum_{j=1}^N \log \pi_\theta\left(y_j\right)\right) \cdot\left(A\left(y_i\right)-\frac{1}{N} \sum_{j=1}^N A\left(y_j\right)\right)$ 在 Clip-Cov 策略中，从策略梯度更新中剪切一小部分高协方差 token，具体根据协方差值随机选择r⋅N个高协方差 token，即 $\left.I_{\text {clip }}=I \sim \operatorname{Uniform}\left(i \mid \operatorname{Cov}\left(y_i\right) \in\left[\omega_{\text {low }}, \omega_{\text {high }}\right]\right\},\lfloor r \cdot N\rfloor\right)$ 其中 $I$ 是索引的简称， $r$ 表示裁剪比例。 $\omega_{low},\omega_{high}$ 是协方差的两个预定义边界，分别。它们都设置得远高于平均协方差（>500×）。最后，具有选定索引的 token 将被从策略梯度中分离，策略梯度为 $L_{\text {Clip-Cov }}(\theta)= \begin{cases}\mathbb{E}_t\left[\frac{\pi_\theta\left(y_t \mid \boldsymbol{y}_{<t}\right)}{\pi_{\theta_{\text {old }}}\left(y_t \mid \boldsymbol{y}_{<t}\right)} A_t\right], & t \notin I_{\text {clip }} \\ 0, & t \in I_{\text {clip }}\end{cases}$ 其中 $t$ 是一个 response 中的第 $t$ 个 token，每个 $t$ 唯一对应N中的索引 $i$ 。

KL-Cov 策略更简单，区别在协方差的 top-k比例内进行排序和选择 token，即 $I_{\mathrm{KL}}=\left\{i \mid \operatorname{Rank}\left(\operatorname{Cov}\left(y_i\right)\right) \leq k \cdot N\right\}$ 这里的 $k$ 表示将受到 KL 惩罚的 token 的比例并且 $k \ll 1$ 。最后对选定的 token 施加 KL 惩罚（当前策略与 rollout 策略之间的 KL 散度），策略损失计算如下： $L_{\mathrm{KL}-\mathrm{Cov}}(\theta)= \begin{cases}\mathbb{E}_t\left[\begin{array}{ll} \frac{\pi_\theta\left(y_t \mid \boldsymbol{y}_{<t}\right)}{\pi_{\theta_{\text {old }}}\left(y_t \mid \boldsymbol{y}_{<t}\right)} & A_t \end{array}\right], & t \notin I_{\mathrm{KL}} \\ \mathbb{E}_t\left[\frac{\pi_\theta\left(y_t \mid \boldsymbol{y}_{<t}\right)}{\pi_{\theta_{\text {old }}}\left(y_t \mid \boldsymbol{y}_{<t}\right)} A_t-\beta \mathbb{D}_{\mathrm{KL}}\left(\pi_{\theta_{\text {old }}}\left(y_t \mid \boldsymbol{y}_{<t}\right) \| \pi_\theta\left(y_t \mid \boldsymbol{y}_{<t}\right)\right)\right], & t \in I_{\mathrm{KL}}\end{cases}$ 实验显示，这两种方法能够在整个训练过程中保持相当高的熵水平。同时，策略模型的响应长度稳步增加，其在测试集上的表现始终优于基线。即模型在训练过程中能够更“自由”地探索，通过强化学习学习更好的策略。同时也比 clip-higher 方法更加稳定。

![](https://pica.zhimg.com/v2-844d1520c5fd1451f0ff7f84dd03e758_1440w.jpg)

**3.2.3 On-policy training**

关于 On-policy training 的典型工作是 [On-Policy RL with Optimal Reward Baseline](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2505.23585)，其放弃了 rollout 带来的样本效率，而采用完全的 on-policy 方式，同时其采用了优化的baseline，我们不妨来看一下其过程

在基于策略梯度的方法中，baseline 常被用以降低梯度估计的方差，即

$g=\mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(\cdot \mid x)}\left[\nabla_\theta \log \pi_\theta(y \mid x) \cdot (r(x, y)-b)\right]$ 

其方差可以表示为 $\operatorname{Var}[g]=\mathbb{E}\left[\left(\nabla_\theta \log \pi_\theta(y \mid x) \cdot(r(x, y)-b)\right)^2\right]-\left(\mathbb{E}\left[\nabla_\theta \log \pi_\theta(y \mid x) \cdot(r(x, y)-b)\right]\right)^2$ 由于第二项（预期梯度的平方）与b无关，最小化Var⁢\[g\]等同于最小化第一项。通过对b求导并设其为零，我们可以推导出最优基线 $b^∗$ ： $\frac{d}{d b} \mathbb{E}\left[\left(\nabla_\theta \log \pi_\theta(y \mid x) \cdot(r(x, y)-b)\right)^2\right]=0$ 求解可得 $b^*=\frac{\mathbb{E}_{y \sim \pi_\theta(\cdot \mid x)}\left[\left(\nabla_\theta \log \pi_\theta(y \mid x)\right)^2 \cdot r(x, y)\right]}{\mathbb{E}_{y \sim \pi_\theta(\cdot \mid x)}\left[\left(\nabla_\theta \log \pi_\theta(y \mid x)\right)^2\right]}$ 由于该式计算复杂，故假设：不同 token 的梯度近似正交，并且每个 token 的梯度范数遵循相同的分布。在此条件下，轨迹策略梯度的平方幅度与其长度成正比，即 $\left\|\nabla_\theta \log \pi_\theta(y \mid x)\right\|^2 \propto l_y$ ，则有 $b^*=\frac{\mathbb{E}_{y \sim \pi_\theta(\cdot \mid x)}\left[l_y \cdot r(x, y)\right]}{\mathbb{E}_{y \sim \pi_\theta(\cdot \mid x)}\left[l_y\right]}$ 核心代码实现如下：

```text
        score_tensor = torch.tensor(id2score[idx])
        len_tensor = torch.tensor(id2len[idx])
        id2bsl[idx] = (len_tensor * score_tensor).sum() / len_tensor.sum()
    for i in range(bsz):
        scores[i] = scores[i] - id2bsl[index[i]]
```

比较 on-policy 与 off-policy 的结果可以看到，虽然off-policy 策略训练在早期阶段实现了与精确on-policy策略训练相似甚至略高的训练奖励，但在数学推理任务上表现较差。这表明off-policy 学习可能存在潜在的过拟合问题。此外，on-policy 策略训练在整个训练过程中表现出显著更低的 KL 散度和高得多的熵，即使没有任何显式的 KL 或熵正则化，而 off-policy 策略训练包括一个额外的熵奖励。更低的 KL 散度意味着更低的对齐成本，更高的熵则表明更强的探索能力。

![](https://pic1.zhimg.com/v2-6cb472a91a43b123095e3f2a1f66f66c_1440w.jpg)

**3.2.4 Token intervention**

事实上 Clip-Cov 与 KL-Cov 就已经是 token-level 的干预了，只是由于其底层原理比较独立，因此单独讨论。本小节以 [https://arxiv.org/pdf/2506.01939](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2506.01939) 为例进行分析。

其同样发现，通过检查思维链 (CoT) 推理中的 token 熵模式，观察到只有一小部分 token 表现出高熵，并且这些 token 充当关键的 forks，引导模型走向不同的推理路径。具有最高 entropy 的 token 通常用于桥接两个连续推理部分之间的逻辑连接，而具有最低 entropy 的 token 倾向于完成句子的当前部分或完成一个单词的构建。

![](https://pic1.zhimg.com/v2-9e555033138feea619109784658dbbd0_1440w.jpg)

RLVR 主要改变高熵 token 的熵，而低熵 token 的熵保持相对稳定，变化极小。即低熵tokens对推理性能的贡献极小，高熵tokens的有效性可能在于它们增强探索的能力。

![](https://picx.zhimg.com/v2-df4a15a47f633e48a4855d58c67e69cf_1440w.jpg)

高熵少数 tokens（即 forking tokens）可能在解释为什么 RL 泛化而 SFT 记忆方面发挥关键作用。RL，特别是基于结果的奖励，对未见过的、基于规则的任务表现出很强的泛化能力，而监督微调 (SFT) 容易记忆训练数据，并且难以在训练分布之外进行泛化。另外如下图，高熵token的优势也只有在步数足够时才能显现出来，这可能高熵token 占比较小，由其对结果的正向影响也是吉光片羽，需要大量的训练才能激发。

![](https://pic4.zhimg.com/v2-e94600c4d9a9a875bb5e8d9080e788eb_1440w.jpg)

**3.2.5 塑形优势值**

该方法的典型案例见 [\[2506.14758\] Reasoning with Exploration: An Entropy Perspective](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2506.14758)。其核心点在于构造一个基于熵的优势项 $\psi\left(\mathcal{H}_t\right)$ ，并用其来修正优势值，即 $\begin{gathered} \psi\left(\mathcal{H}_t\right)=\min \left(\alpha \cdot \mathcal{H}_t^{\text {detach }}, \frac{\left|A_t\right|}{\kappa}\right), \quad \text { where } \alpha>0 \text { and } \kappa>1, \\ A_t^{\text {shaped }}=A_t+\psi\left(\mathcal{H}_t\right) . \end{gathered}$ 其中 α 是缩放系数，κ 控制裁剪阈值。关键在于，基于熵的项 $\mathcal{H}_t^{\text {detach }}$ 在反向传播过程中与计算图分离，作为原始优势的一个固定偏移量。这调整了更新的大小，而不会改变梯度流。这种方法方法仅使用一行代码即可无缝集成到现有的 RL 训练流程中，如下：

![](https://pic4.zhimg.com/v2-924f16b688e514fab218b1097c2f5313_1440w.jpg)

这种方法与熵正则化看起来比较接近，但也有所不同，具体如下：

![](https://picx.zhimg.com/v2-1705cf2c76652b0d3100c7168d53813b_1440w.jpg)

总结一下，本文从 RLVR 训练过程中的熵缩现象出发，深入讨论了其发生原因及干预手段，从此也可以看出 RLVR 当前已进入深水区，需要非常细节的研究和实践才能有所提升。这个过程，既是理解 RL 和 LLM 的过程，也是拓展其能力边界的过程。

## 参考资料

\[1\] [Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2504.13837)

\[2\] [Rethinking Reflection in Pre-Training](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2504.04022)

\[3\] [Echo Chamber: RL Post-training Amplifies Behaviors Learned in Pretraining](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2504.07912)

\[4\] [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2501.12948)

\[5\] [https://arxiv.org/abs/2503.01307](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2503.01307)

\[6\] [Rethinking Reflection in Pre-Training](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2504.04022)

\[7\] [Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2506.01939)

\[8\] [The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2506.01347)

\[9\] [The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2505.22617)

\[10\] [Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2505.14403)

\[11\] [On-Policy RL with Optimal Reward BaselineOn-Policy RL with Optimal Reward Baseline](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2505.23585)

\[12\] [SEED-GRPO: Semantic Entropy Enhanced GRPO for Uncertainty-Aware Policy Optimization](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2505.12346)

\[13\] [Entropy在RL中扮演的角色 - 知乎](https://zhuanlan.zhihu.com/p/1913295888731861490)

\[14\] [\[2506.14758\] Reasoning with Exploration: An Entropy Perspective](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2506.14758)

> 绿树阴浓夏日长，楼台倒影入池塘。水精帘动微风起，满架蔷薇一院香。 —— 高骈《山亭夏日》
