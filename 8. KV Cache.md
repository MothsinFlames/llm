---
source: "https://zhuanlan.zhihu.com/p/662498827"
---
[KV Cache](https://zhida.zhihu.com/search?content_id=235350632&content_type=Article&match_order=1&q=KV+Cache&zhida_source=entity) 是 [Transformer](https://zhida.zhihu.com/search?content_id=235350632&content_type=Article&match_order=1&q=Transformer&zhida_source=entity) 标配的推理加速功能，transformer官方use\_cache这个参数默认是True，但是它 **只能用于Decoder-Only架构的模型** ，这是因为Decoder有 [Causal Mask](https://zhida.zhihu.com/search?content_id=235350632&content_type=Article&match_order=1&q=Causal+Mask&zhida_source=entity) ，在推理的时候前面已经生成的字符不需要与后面的字符产生attention，从而使得前面已经计算的K和V可以缓存起来。
自注意力缓存机制(KV Cache)-根本原理在于优化计算效率-通过避免重复运算来提升模型推理速度。其核心策略是将之前计算得到的历史时刻Token对应的Key-Value对（[KV值](https://zhida.zhihu.com/search?content_id=238375943&content_type=Article&match_order=1&q=KV%E5%80%BC&zhida_source=entity)）进行存储，故此称为KV Cache。
### KV缓存的必要性
在**预测新生成的Token时，仅与当前输入序列的最后一个Token直接相关**。这一过程中，为了计算新Token的注意力权重，**只需利用当前Query（即最后一个Token经变换后的表示）与所有历史Token的Key进行比对以获取加权系数，进而与对应的Value相乘累加得到注意力结果。因此，只需保留并复用先前历史Token的KV值即可。**
**对于Decoder-Only架构而言，在推理token时其产生的中间激活值保持不变。基于这一特性，可以合理地将这些中间结果预先计算并储存起来。**
![|425](https://pic2.zhimg.com/v2-9731d9d5b3c5ddc5eb0e9cdfab196883_1440w.jpg)
我们先看一下不使用KV Cache的推理过程。假设模型最终生成了“遥遥领先”4个字。
当模型生成第一个“遥”字时，input="$<s>$", "$<s>$"是起始字符。 [Attention](https://zhida.zhihu.com/search?content_id=235350632&content_type=Article&match_order=1&q=Attention&zhida_source=entity) 的计算如下：
![|825](https://pic4.zhimg.com/v2-f8706213a1f04fa1e41533bc0eeef601_1440w.jpg)
为了看上去方便，我们 **暂时忽略scale项** $d\sqrt{d}\sqrt{d}$ ， 但是要注意这个scale面试时经常考。
如上图所示，最终Attention的计算公式如下，（softmaxed 表示已经按行进行了softmax）:
$${\color{orange}{Att_1}}(Q, K, V) = \text{softmax}({\color{orange}{Q_1}} K_1^T) \vec{V_1} = \text{softmaxed}({\color{orange}{Q_1}} K_1^T) \vec{V_1}$$
当模型生成第二个“遥”字时，input="$<s>$遥", Attention的计算如下：
![|850](https://pic2.zhimg.com/v2-81197e811503d1ffa5f864f164127ddb_1440w.jpg)
当 $QK^T$ 变为矩阵时，softmax 会针对 **行** 进行计算。写详细一点如下，softmaxed 表示已经按行进行了softmax。
![|750](https://pic1.zhimg.com/v2-cdf2a0f4e164b8b2fdfb4d65fbda8a20_1440w.jpg)
假设 ${\color{orange}{Att_1}}(Q, K, V)$ 表示 Attention 的第一行， ${\color{orange}{Att_2}}(Q, K, V)$ 表示 Attention 的第二行，则根据上面推导，
其计算公式为：
$$\begin{aligned}
{\color{orange}{Att_1}}(Q, K, V) &= \text{softmaxed}({\color{orange}{Q_1}} K_1^T) \vec{V_1} \\
{\color{red}{Att_2}}(Q, K, V) &= \text{softmaxed}({\color{red}{Q_2}} K_1^T) \vec{V_1} + \text{softmaxed}({\color{red}{Q_2}} K_2^T) \vec{V_2 }
\end{aligned}$$
你会发现，由于 $Q_1 K_2^T$ 这个值会mask掉，
- $Q_{1}$ **在第二步参与的计算与第一步是一样的，而且第二步生成的** $V_1$ **也仅仅依赖于** $Q_1$ **，与** $Q_2$ **毫无关系。**
- $V_2$ **的计算也仅仅依赖于** $Q_2$ **，与** $Q_1$ **毫无关系。**
当模型生成第三个“领”字时，input="$<s>$遥遥"Attention的计算如下：
![|775](https://pic3.zhimg.com/v2-29420723618e20a24dc3b6c329c570c8_1440w.jpg)
详细的推导参考第二步，其计算公式为：
$$\begin{aligned}
{\color{orange}{Att_1}}(Q, K, V) &= \text{softmaxed}({\color{orange}{Q_1}} K_1^T) \vec{V_1} \\
{\color{red}{Att_2}}(Q, K, V) &= \text{softmaxed}({\color{red}{Q_2}} K_1^T) \vec{V_1} + \text{softmaxed}({\color{red}{Q_2}} K_2^T) \vec{V_2 } \\
{\color{purple}{Att_3}}(Q, K, V) &= \text{softmaxed}({\color{purple}{Q_3}} K_1^T) \vec{V_1} + \text{softmaxed}({\color{purple}{Q_3}} K_2^T) \vec{V_2 } + \text{softmaxed}({\color{purple}{Q_3}} K_3^T) \vec{V_3 }
\end{aligned}$$
同样的， $Att_k$ 只与 $Q_k$ 有关。
当模型生成第四个“先”字时，input="$<s>$遥遥领"Attention的计算如下：
![|825](https://pic3.zhimg.com/v2-7bb8303b0a82b7ae668e2e9327b274e2_1440w.jpg)
$$\begin{aligned}
{\color{orange}{Att_1}}(Q, K, V) &= \text{softmaxed}({\color{orange}{Q_1}} K_1^T) \vec{V_1} \\
{\color{red}{Att_2}}(Q, K, V) &= \text{softmaxed}({\color{red}{Q_2}} K_1^T) \vec{V_1} + \text{softmaxed}({\color{red}{Q_2}} K_2^T) \vec{V_2 } \\
{\color{purple}{Att_3}}(Q, K, V) &= \text{softmaxed}({\color{purple}{Q_3}} K_1^T) \vec{V_1} + \text{softmaxed}({\color{purple}{Q_3}} K_2^T) \vec{V_2 } + \text{softmaxed}({\color{purple}{Q_3}} K_3^T) \vec{V_3 } \\
{\color{brown}{Att_4}}(Q, K, V) &= \text{softmaxed}({\color{brown}{Q_4}} K_1^T) \vec{V_1} + \text{softmaxed}({\color{brown}{Q_4}} K_2^T) \vec{V_2 } + \text{softmaxed}({\color{brown}{Q_4}} K_3^T) \vec{V_3 } + \text{softmaxed}({\color{brown}{Q_4}} K_4^T) \vec{V_4 }  \end{aligned}$$
看上面图和公式，我们可以得出结论：
1. **当前计算方式存在大量冗余计算。**
2. $Att_k$ **只与** $Q_k$ **有关。**
3. **推理第** $x_k$ **个字符的时候只需要输入字符** $x_{k-1}$ **即可。**
我们每一步其实之需要根据 $Q_k$ 计算 $Att_k$ 就可以，之前已经计算的Attention完全不需要重新计算。但是 $K$ 和 $V$ 是全程参与计算的，所以这里我们需要把每一步的 $K,V$ 缓存起来。所以说叫KV Cache好像有点不太对，因为KV本来就需要全程计算，可能叫增量KV计算会更好理解。
下面4张图展示了使用KV Cache和不使用的对比。
![|850](https://pic2.zhimg.com/v2-655b95ebfb7808563bead28bc89bb459_1440w.jpg)
下面是gpt里面KV Cache的实现。其实明白了原理后代码实现简单的不得了,就是concat操作而已。
[github.com/huggingface/](https://link.zhihu.com/?target=https%3A//github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py%23L318C1-L331C97)
```
if layer_past is not None:
       past_key, past_value = layer_past
       key = torch.cat((past_key, key), dim=-2)
       value = torch.cat((past_value, value), dim=-2)
   if use_cache is True:
       present = (key, value)
   else:
       present = None
   if self.reorder_and_upcast_attn:
       attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)
   else:
       attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
```
**需要注意当** **sequence特别长的时候，KV Cache其实还是个 [Memory刺客](https://zhida.zhihu.com/search?content_id=235350632&content_type=Article&match_order=1&q=Memory%E5%88%BA%E5%AE%A2&zhida_source=entity)** 。
比如batch_size=32, head=32, layer=32, dim_size=4096, seq_length=2048, float32类型，则需要占用的显存为（感谢网友指正） 2 * 32 * 4096 * 2048 * 32 * 4 / 1024/1024/1024 /1024 = 64G。
**核心问题与动机：为什么需要 KV Cache？**
LLM（如 GPT、LLaMA 等）的核心是 Transformer 架构。在**自回归生成**（Autoregressive Generation）过程中（例如文本续写、对话、代码生成），模型需要逐个生成 token（词元）。生成第 $t$ 个 token ($output_t$) 时：
1.  **输入：** 模型需要将之前生成的 *所有* $t-1$ 个 token ($input_1$ 到 $input_{t-1}$ 以及初始输入 prompt) 作为输入序列。
2.  **计算：** 对于 Transformer Decoder（或仅 Decoder 架构）的每一层：
   *   需要计算当前步的 **Query** ($Q_t$) 向量（主要基于当前步的输入 $input_t$ 或上一步的输出 $output_{t-1}$）。
   *   需要计算**整个输入序列**（从 $input_1$ 到 $input_t$）的 **Key** ($K_{1:t}$) 和 **Value** ($V_{1:t}$) 向量。
   *   执行 **Attention 计算**：$Attention(Q_t, K_{1:t}, V_{1:t}) = softmax(Q_t * K_{1:t}^T / sqrt(d_k)) * V_{1:t}$。结果用于计算当前步的上下文向量。
**关键痛点：**
*   在生成第 $t$ 个 token 时，序列 $input_1$ 到 $input_{t-1}$ 的 $K$ 和 $V$ 张量在之前的生成步骤中已经计算过了！
*   如果不做优化，在每一步生成时，都需要**重新计算**整个输入序列（长度 $t$）的所有层的 $K$ 和 $V$ 张量。随着生成的 token 越来越多（$t$ 增大），计算量和显存占用会**急剧增加**（时间复杂度接近 O(n^2)），导致推理速度极慢且无法生成长序列。
**KV Cache 的诞生：解决重复计算问题**
KV Cache 的核心思想非常简单直接：**缓存** (Cache) 已经计算过的 Key ($K$) 和 Value ($V$) 张量，供后续生成步骤复用。
**原理详解：**
1.  **初始化：**
   *   处理初始 Prompt（长度为 $L$）时，模型像正常推理一样，为 Prompt 中的每个 token 计算所有 Transformer 层的 $K$ 和 $V$ 张量。
   *   将这些计算好的 $K$ 和 $V$ 张量（按层、按序列位置存储）**保存**在显存的一个特定区域 —— 这就是 **KV Cache**。
2.  **自回归生成第一步 ($t = L+1$):**
   *   输入：上一步生成的 token $output_L$ (或初始 Prompt 的最后一个 token) 作为当前步 $input_{L+1}$。
   *   计算：
       *   计算当前步 $input_{L+1}$ 的 $Q_{L+1}$。
       *   **计算当前步 $input_{L+1}$ 的 $K_{L+1}$ 和 $V_{L+1}$**。
       *   **从 KV Cache 中读取**之前缓存的 $K_{1:L}$ 和 $V_{1:L}$。
       *   **拼接：** 将缓存的 $K_{1:L}$ 与新计算的 $K_{L+1}$ 拼接成 $K_{1:L+1}$。同样拼接 $V_{1:L}$ 和 $V_{L+1}$ 成 $V_{1:L+1}$。
       *   执行 Attention：$Attention(Q_{L+1}, K_{1:L+1}, V_{1:L+1})$。
       *   后续 FFN 等计算。
   *   输出：$output_{L+1}$。
   *   **更新 KV Cache：** 将**新计算**的 $K_{L+1}$ 和 $V_{L+1}$ **追加**到对应层的 KV Cache 中。现在 Cache 包含了 $K_{1:L+1}$ 和 $V_{1:L+1}$。
3.  **后续生成步骤 ($t > L+1$):**
   *   输入：上一步生成的 token $output_{t-1}$ 作为当前步 $input_t$。
   *   计算：
       *   计算当前步 $input_t$ 的 $Q_t$。
       *   **计算当前步 $input_t$ 的 $K_t$ 和 $V_t$**。
       *   **从 KV Cache 中读取**之前缓存的 $K_{1:t-1}$ 和 $V_{1:t-1}$。
       *   **拼接：** $K_{1:t} = concat([K_{1:t-1}, K_t])$， $V_{1:t} = concat([V_{1:t-1}, V_t])$。
       *   执行 Attention：$Attention(Q_t, K_{1:t}, V_{1:t})$。
       *   后续 FFN 等计算。
   *   输出：$output_t$。
   *   **更新 KV Cache：** 将**新计算**的 $K_t$ 和 $V_t$ **追加**到对应层的 KV Cache 中。
**核心公式体现：**
*   **无 KV Cache (低效):**
   *   生成第 $t$ 个 token 时，对于第 $l$ 层：
       $K_{1:t}^{(l)} = Projection_K(H_{1:t}^{(l-1)})$ // 重新计算整个序列的 K
       $V_{1:t}^{(l)} = Projection_V(H_{1:t}^{(l-1)})$ // 重新计算整个序列的 V
       $Q_t^{(l)} = Projection_Q(H_t^{(l-1)})$ // 只计算当前 Q
       $Attn_t^{(l)} = softmax(Q_t^{(l)} @ K_{1:t}^{(l)T} / sqrt(d_k)) @ V_{1:t}^{(l)}$
*   **有 KV Cache (高效):**
   *   生成第 $t$ 个 token 时，对于第 $l$ 层：
       $K_t^{(l)} = Projection_K(H_t^{(l-1)})$ // **只计算当前 K**
       $V_t^{(l)} = Projection_V(H_t^{(l-1)})$ // **只计算当前 V**
       $Q_t^{(l)} = Projection_Q(H_t^{(l-1)})$ // 只计算当前 Q
       $K_{1:t}^{(l)} = concat(KV\_Cache_K^{(l)}[1:t-1], K_t^{(l)})$ // **从 Cache 读旧 K， 拼接新 K**
       $V_{1:t}^{(l)} = concat(KV\_Cache_V^{(l)}[1:t-1], V_t^{(l)})$ // **从 Cache 读旧 V， 拼接新 V**
       $Attn_t^{(l)} = softmax(Q_t^{(l)} @ K_{1:t}^{(l)T} / sqrt(d_k)) @ V_{1:t}^{(l)}$
       *   **更新 Cache：** $KV\_Cache_K^{(l)}[t] = K_t^{(l)}$, $KV\_Cache_V^{(l)}[t] = V_t^{(l)}$
**关键实现细节与技术挑战 (重点！)：**
1.  **显存占用：** KV Cache 是推理时显存消耗的**主要来源**之一。
   *   **计算：** $总显存占用 (Bytes) = 2 * batch_size * num_layers * sequence_length * hidden_size * bytes_per_param$
       *   $2$: Key 和 Value 两个张量。
       *   batch_size: 同时生成的序列数。
       *   num_layers: Transformer 层数。
       *   sequence_length: **当前已生成的总 token 数（包括 prompt）**。这是**动态增长**的！
       *   hidden_size: 模型隐藏层大小（通常是 $d_model$）。
       *   bytes_per_param: 数据类型大小（e.g., FP16=2, BF16=2, FP8=1, INT8=1, INT4=0.5）。
   *   **示例：** LLaMA-7B (hidden_size=4096, layers=32), Batch=1, SeqLen=2048, FP16
       $Cache Size = 2 * 1 * 32 * 2048 * 4096 * 2 bytes ≈ 2.15 GB$。这仅仅是 Cache！模型参数本身还需要约 14GB (FP16)。生成长文本时（SeqLen=8192），Cache 将膨胀到约 8.6GB。
   *   **挑战：** 限制了最大可生成序列长度和 Batch Size。是长文本生成的主要瓶颈。
2.  **内存布局与管理：**
   *   **预分配 vs 动态增长：** 通常需要预分配一个足够大的连续内存块（根据最大允许序列长度 $max_seq_len$），避免频繁的显存分配释放（代价高）。如果序列超过 $max_seq_len$，需要特殊处理（如停止生成、滑动窗口、丢弃旧 Cache 等）。
   *   **结构：** 通常是 [batch_size, num_layers, 2 (K/V), num_heads (optional), seq_len, head_dim] 或 [batch_size, num_layers, seq_len, 2 (K/V), hidden_size]。具体取决于框架和优化（是否将 K/V 分开存储，是否将 num_heads 维度分离）。
   *   **键值分离 vs 合并：** 有些实现将 K 和 V 存储在两个独立张量，有些则合并成一个张量在特定维度拼接。各有优劣（访问效率 vs 内存连续性）。
3.  **计算效率：**
   *   **拼接 (Concatenation)：** 将新计算的 $K_t$, $V_t$ (形状 $[batch, ... , 1, head_dim]$) 拼接到缓存的 $K_{1:t-1}$, $V_{1:t-1}$ (形状 $[batch, ... , t-1, head_dim]$) 上。这个拼接操作通常是内存拷贝，虽然比重新计算整个序列快得多，但在极高性能要求下也可能成为瓶颈。优化库（如 FlashAttention, xFormers）通常会将这一步融合到 Attention Kernel 中。
   *   **Attention 计算：** Attention 计算本身需要访问整个 $K_{1:t}$, $V_{1:t}$。当 $t$ 很大时，即使有 Cache，计算 $Q_t @ K_{1:t}^T$ (形状 $[batch, num_heads, 1, t]$) 的复杂度 O(t) 以及后续的 $softmax$ 和 $@ V$ 也是 O(t)。这是自回归生成本身固有的计算复杂度。KV Cache 避免了重复计算 $K_{1:t-1}$, $V_{1:t-1}$，但无法降低每一步 Attention 计算本身的渐近复杂度。
4.  **与 FlashAttention / 其他优化 Attention 的关系：**
   *   **正交且互补：** KV Cache 解决的是 **跨生成步** 的 $K$, $V$ **重复计算**问题。FlashAttention 等解决的是 **单次前向传播内** 的 Attention 计算本身的 **计算效率** 和 **显存占用 (SRAM 优化)** 问题。
   *   **结合使用：** 现代高效推理引擎（如 vLLM, TensorRT-LLM, DeepSpeed）会同时使用 KV Cache 和 FlashAttention (或类似优化) 来最大化推理吞吐量和降低延迟。FlashAttention 可以高效地计算 $Attention(Q_t, K_{1:t}, V_{1:t})$，即使 $K_{1:t}$, $V_{1:t}$ 来自 KV Cache。
5.  **批处理 (Batching)：**
   *   同一个 Batch 中的不同序列，其当前长度 ($t_i$) 可能不同（异步生成）。KV Cache 需要为每个序列独立维护其状态。
   *   **连续存储 + 分页 (PagedAttention)：** vLLM 提出的革命性方案。将每个序列的 KV Cache 划分为固定大小的 Block（页）。显存中维护一个全局的 Block 池（空闲列表）。序列的 Cache 由一组（可能不连续的）Block 组成，通过逻辑到物理的 Block 映射表管理。这大大减少了内存碎片，提高了显存利用率，特别是在处理大量不同长度序列时。这是当前最先进的生产级解决方案。
6.  **量化：**
   *   为了减少 KV Cache 的巨大显存占用，一个重要的优化方向是对其进行**量化**。
   *   将 $K$, $V$ 张量以**低精度**（如 FP8, INT8, INT4）存储。
   *   **挑战：** Attention 计算 $softmax(Q @ K^T)$ 对 $K$ 的数值范围敏感，量化可能引入误差影响模型质量。需要精细的量化策略（如分组量化、每通道量化）和量化感知训练/微调（QAT）来缓解精度损失。
   *   **收益显著：** INT8 量化理论上可将 Cache 大小减半，是实际部署中常用的技术。
7.  **稀疏化与近似：**
   *   研究探索通过只缓存重要的 $K$/$V$（基于注意力分数、信息熵等）或使用近似方法来减少 Cache 大小和计算量（如 $Window Attention$，只缓存最近的 N 个 token）。这些方法通常会牺牲一些模型质量。
**KV Cache 带来的巨大优势：**
1.  **大幅降低计算量：** 避免了历史 token 的 $K$, $V$ 在每个生成步的重复计算。计算量从 O(n^3) 级别降低到 O(n^2) 级别（n 是序列长度），这是实现**实用化推理速度**的关键。
2.  **降低延迟：** 每个 token 的生成时间相对稳定（主要取决于当前步的 FFN 计算和 Attention 计算，后者随长度增加而缓慢增加），避免了无 Cache 时生成时间随长度急剧增加。
3.  **使长序列生成成为可能：** 没有 KV Cache，显存和计算需求会爆炸式增长，根本无法生成长文本。KV Cache 是生成长文档、长对话的基础。
**KV Cache 的缺点与挑战：**
1.  **显存瓶颈：** 如前所述，Cache 大小随序列长度线性增长，是限制 Batch Size 和最大生成长度的**最主要因素**。
2.  **内存管理开销：** 拼接操作、动态增长的 Cache 管理（特别是 PagedAttention 出现前）会带来额外开销。
3.  **I/O 带宽限制：** 随着序列增长，每一步 Attention 计算需要读取整个 $K_{1:t}$, $V_{1:t}$。读取这些大型张量的带宽可能成为瓶颈，尤其当模型计算本身被高度优化（如使用 Tensor Cores）时。这就是所谓的“Memory Bound”问题。
4.  **工程复杂度：** 高效、正确地实现 KV Cache（尤其是结合批处理、分页、量化、分布式）有较高的工程复杂度。
**在代码框架中的体现 (了解即可)：**
*   **Hugging Face Transformers：** 通过设置 $model.generate(..., use_cache=True)$ 启用。模型内部（如 $GPT2LMHeadModel$, $LlamaForCausalLM$）会维护 $past_key_values$ 状态并在前向传播中返回和接收它。
*   **vLLM：** 核心创新就是其 **PagedAttention** 和基于 Block 的 KV Cache 管理。
*   **TensorRT-LLM / FasterTransformer：** 提供高度优化的 KV Cache 实现，支持 FP8/INT8 量化，连续内存布局等。
**中可能深入的问题与回答方向：**
1.  **Q：KV Cache 为什么能加速推理？**
   *   **A：** 它缓存了历史 token 在每一层计算出的 Key 和 Value 向量。在生成新 token 时，**避免了重新计算**整个序列的 K 和 V，只需要计算**当前新 token** 的 K 和 V。节省的计算量随着序列长度增加而急剧增大。
2.  **Q：KV Cache 的主要代价是什么？**
   *   **A：** **显存占用**。Cache 大小 = $2 * batch * num_layers * seq_len * hidden_size * bytes_per_param$。它是推理时显存消耗的**主要来源**，严重限制了最大可处理的序列长度 ($seq_len$) 和批量大小 ($batch$)。管理动态增长的 Cache 也可能带来工程复杂度。
3.  **Q：如何减少 KV Cache 的显存占用？**
   *   **A：**
       *   **量化：** 使用 FP8/INT8/INT4 存储 KV Cache。是生产环境主流方法，需注意精度影响。
       *   **分页缓存 (PagedAttention)：** 如 vLLM 所采用，将 Cache 划分为固定大小的 Block 池管理，大幅减少内存碎片，提高显存利用率（尤其对不同长度序列的批处理）。
       *   **选择性缓存/稀疏化：** 只缓存重要的 K/V（如基于注意力分数），或只缓存最近 N 个 token ($Window Attention$)。可能牺牲质量。
       *   **模型架构改进：** 使用状态更小的模型（如 Mamba, RWKV）或改进 Attention 机制（如 Linear Attention 变体）来减少或避免对完整 KV Cache 的依赖。
4.  **Q：KV Cache 和 FlashAttention 是什么关系？**
   *   **A：** **正交且互补**。KV Cache 解决的是**跨时间步（生成步）** 的 K/V **重复计算**问题。FlashAttention 解决的是**单次前向传播内**的 Attention 计算本身的**计算效率**和**中间激活显存占用**问题（通过算子融合和利用 GPU SRAM）。高效的推理引擎会**同时使用两者**。
5.  **Q：在生成过程中，序列长度不断增长，KV Cache 是如何管理的？**
   *   **A：** 通常**预分配**一个足够大的连续空间（根据 $max_seq_len$ 配置）。新 token 的 K/V **追加**到现有 Cache 的末尾（逻辑上是一个不断增长的张量）。物理实现上：
       *   简单方案：一个大的连续张量。
       *   高效方案 (vLLM)：分页管理，将逻辑上连续的 Cache 映射到物理上可能不连续的固定大小内存块（Blocks）上，通过页表维护映射关系。
6.  **Q：KV Cache 对 Attention 计算的复杂度有改变吗？**
   *   **A：** **没有改变**计算复杂度的渐近阶。Attention 计算 $softmax(Q_t @ K_{1:t}^T) @ V_{1:t}$ 的复杂度仍然是 O(t)（t 是当前序列长度）。KV Cache 的贡献在于**避免了重新计算 $K_{1:t-1}$ 和 $V_{1:t-1}$ 的 O(t) 操作**，将每一步生成的主要计算开销从 O(t) + O(t) 降到了 O(1)（计算新 K/V） + O(t)（Attention 计算）。它消除了重复计算，但没有降低 Attention 本身固有的 O(t) 复杂度。
7.  **Q：除了 Decoder-only 模型，KV Cache 还能用在其他地方吗？**
   *   **A：** 主要用在**自回归生成**场景。理论上，任何需要重复计算历史状态的地方都可以考虑类似缓存机制。在 Encoder-Decoder 架构（如 T5, BART）的 Decoder 自回归生成部分，同样需要 KV Cache。对于 Decoder 部分的自注意力，它需要访问 Decoder 自身的历史状态（K/V），这部分也需要 Cache。对于 Encoder-Decoder 注意力，Encoder 的输出通常是固定的（在编码 Prompt 后），其 K/V 可以一次性计算并缓存（不需要动态增长），这也可以看作一种 Cache，但其管理比 Decoder 自回归的 KV Cache 简单得多。
**总结与要点：**
*   **核心价值：** KV Cache 是**解决 Transformer 自回归生成中历史 K/V 重复计算**的关键优化技术，是 LLM 高效推理的基石。
*   **核心原理：** 缓存每一层的历史 K/V 向量，生成新 token 时只计算当前 token 的 K/V，并与缓存拼接进行 Attention 计算。
*   **核心公式：** 计算从 $K_{1:t} = Projection_K(H_{1:t})$ (O(t)) 变为 $K_t = Projection_K(H_t)$ (O(1)) + $concat(Cache, K_t)$ (O(1) 内存操作，但 Attention 计算 O(t) 不变)。
*   **核心挑战：** **显存占用**（$O(batch * layers * seq_len * hidden_size)$）是最大瓶颈，驱动了量化、分页缓存 (PagedAttention) 等优化技术。
*   **关键区分点：** 理解 KV Cache **解决了什么**（跨步重复计算），**没解决什么**（Attention 计算的 O(t) 复杂度、显存占用随长度增长），以及它与 FlashAttention 等技术的**关系**（互补）。
*   **工程实践：** 了解量化、vLLM 的 PagedAttention 等业界主流解决方案是重要加分项。