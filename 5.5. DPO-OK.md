
$$
K L(P \| Q)=\sum_{x \in X} P(x) \log \left(\frac{P(x)}{Q(x)}\right)
$$
#Bradley-Terry 模型
$$
P(i>j) = \frac{a_{i}}{a_{i}+a_{j}}
$$
$$
Loss = -\mathbb{E}_{\left(\alpha_x, \alpha_y\right) \sim D}\left[\ln \frac{\alpha_x}{\alpha_\kappa+\alpha_y}\right]
$$
因此判决两个的好坏可以等价于，r可能是负数，因此可以用$\exp(r(x,y))$代替$r(x,y)$等价于：
$$
P\left(y_1 \succ y_2\right)=\frac{r(x,y_{1})}{r(x,y_{1})+r(x,y_{2})}=\frac{\exp \left(r\left(x, y_1\right)\right)}{\exp \left(r\left(x, y_1\right)\right)+\exp \left(r\left(x, y_2\right)\right)}
$$
因此：
$$
\begin{aligned}
Loss &= -\mathbb{E}_{\left(x, y_{w}, y_{l}\right) \sim D}\left[\ln \frac{\exp \left(r\left(x, y_w\right)\right)}{\exp \left(r\left(x, y_w\right)\right)+\exp \left(r\left(x, y_l\right)\right)}\right]\\
&=-\mathbb{E}_{\left(x, y_{w}, y_{l}\right) \sim D}\left[\ln \frac{1}{1+\exp \left(r\left(x, y_l\right)-r(x,y_{w})\right)}\right]\\
&=-\mathbb{E}_{\left(x, y_{w}, y_{l}\right) \sim D}\left[\ln \sigma \left(r\left(x, y_l\right)-r(x,y_{w})\right)\right]
\end{aligned}
$$
这里的Loss的目标就是大语言模型输出的$y_{w}$通过reward的得分尽可能大于$y_{l}$通过通过reward的得分,因此  $- \ln \sigma \left(r\left(x, y_l\right)-r(x,y_{w})\right)$ 就是reward的形式

---
奖励模型：$r(x,y)$ , prompt: $x$,  response: $y$
基准模型：$\pi_{ref}(y | x)$
训练模型：$\pi(y | x)$
现在的目标是：最大化奖励$r$, 同时让两个模型的分布尽量一致：
$$
\begin{aligned}
& \max _\pi \mathbb{E}_{x \sim D, y \sim \pi}[r(x, y)]-\beta \mathbb{D}_{K L}\left[\pi(y \mid x) \| \pi_{r e f}(y \mid x)\right] \\
= & \max _\pi \mathbb{E}_{x \sim D, y \sim \pi}[r(x, y)]- \mathbb{E}_{x \sim D, y \sim \pi}\left[\beta\log\frac{\pi(y \mid x)}{\pi_{r e f}(y \mid x)} \right] \\
= & \max _\pi \mathbb{E}_{x \sim D, y \sim \pi}\left[ r(x, y)-\beta\log\frac{\pi(y \mid x)}{\pi_{r e f}(y \mid x)}  \right]\\
= & \min_{\pi}\mathbb{E}_{x \sim D, y \sim \pi}\left[ \log\frac{\pi(y \mid x)}{\pi_{r e f}(y \mid x)}-\frac{1}{\beta}r(x,y) \right]\\
= & \min_{\pi}\mathbb{E}_{x \sim D, y \sim \pi}\left[ \log\frac{\pi(y \mid x)}{\pi_{r e f}(y \mid x)}-\log \exp\left( \frac{1}{\beta}r(x,y) \right) \right]\\
= & \min _\pi \mathbb{E}_{x \sim D, y \sim \pi}\left[\log \frac{\pi(y \mid x)}{\pi_{r e f}(y \mid x) \exp \left(\frac{1}{\beta} r(x, y)\right) }\right] \\
= & \min _\pi \mathbb{E}_{x \sim D, y \sim \pi}\left[\log \frac{\pi(y \mid x)}{\pi_{r e f}(y \mid x) \exp \left(\frac{1}{\beta} r(x, y)\right) \frac{1}{Z(x)} Z(x)}\right] \\
= & \min _\pi \mathbb{E}_{x \sim D, y \sim \pi}\left[\log \frac{\pi(y \mid x)}{\frac{1}{Z(x)} \pi_{r e f}(y \mid x) \exp \left(\frac{1}{\beta} r(x, y)\right)}-\log Z(x)\right]
\end{aligned}
$$
其中
$$
Z(x) = \sum_{y}\pi_{r e f}(y \mid x) \exp \left(\frac{1}{\beta} r(x, y)\right)
$$
那么有：
$$
\frac{1}{Z(x)} \pi_{r e f}(y \mid x) \exp \left(\frac{1}{\beta} r(x, y)\right)=\frac{\pi_{r e f}(y \mid x) \exp \left(\frac{1}{\beta} r(x, y)\right)}{\sum_y \pi_{r e f}(y \mid x) \exp \left(\frac{1}{\beta} r(x, y)\right)}=\pi^*(y \mid x)
$$
由于 $Z(x)$ 是常数，有
$$
\begin{aligned}
& =\min _\pi \mathbb{E}_{x \sim D, y \sim \pi}\left[\log \frac{\pi(y \mid x)}{\pi^*(y \mid x)}-\log Z(x)\right] \\
& =\min _\pi \mathbb{E}_{x \sim D, y \sim \pi}\left[\log \frac{\pi(y \mid x)}{\pi^*(y \mid x)}\right] \\
& =\min _\pi \mathbb{E}_{x \sim D}\left[\mathbb{D}_{K L}\left(\pi(y \mid x)| | \pi^*(y \mid x)\right)\right]
\end{aligned}
$$
当有
$$
\pi(y \mid x)=\pi^*(y \mid x)
$$
这时，KL散度为0，最小，这时
$$
\begin{aligned}
& \pi(y|x)=\frac{1}{Z(x)} \pi_{r e f}(y \mid x) \exp \left(\frac{1}{\beta} r(x, y)\right)\\
& \Rightarrow \exp \left(\frac{1}{\beta} r(x, y)\right)=\frac{\pi(y \mid x)}{\pi_{r e f}(y \mid x)} Z(x) \\
& \Rightarrow r(x, y)=\beta \ln \left(\frac{\pi(y \mid x)}{\pi_{r e f}(y \mid x)} Z(x)\right) \\
& \Rightarrow r(x, y)=\beta \ln \frac{\pi(y \mid x)}{\pi_{r e f}(y \mid x)}+\beta \ln Z(x)
\end{aligned}
$$
这样就得到了reward的表达式。
带入 #Bradley-Terry 模型：
#r_dpo
$$
r(x, y)=-\ln \sigma\left(\beta \ln \frac{\pi\left(y_w \mid x\right)}{\pi_{r e f}\left(y_w \mid x\right)}+\beta \ln Z(x)-\beta \ln \frac{\pi\left(y_l \mid x\right)}{\pi_{r e f}\left(y_l \mid x\right)}-\beta \ln Z(x)\right)=-\ln \sigma\left(\beta \ln \frac{\pi\left(y_w \mid x\right)}{\pi_{r e f}\left(y_w \mid x\right)}-\beta \ln \frac{\pi\left(y_l \mid x\right)}{\pi_{r e f}\left(y_l \mid x\right)}\right)
$$
因此可以得到DPO的Sentence-level Loss：
$$
\mathcal{L}_{\mathrm{DPO}}\left(\pi_\theta ; \pi_{\mathrm{ref}}\right)=- \mathbb{E}_{\left(x, y_w, y_l\right) \sim \mathcal{D}}\left[\ln \sigma\left(\beta \ln \frac{\pi\left(y_w \mid x\right)}{\pi_{r e f}\left(y_w \mid x\right)}-\beta \ln \frac{\pi\left(y_l \mid x\right)}{\pi_{r e f}\left(y_l \mid x\right)}\right)\right]
$$
其梯度：
$$
\begin{aligned}  
\nabla_\theta \mathcal{L}_{\mathrm{DPO}}\left(\pi_\theta; \pi_{\mathrm{ref}}\right)= -\beta \mathbb{E}_{\left(x, y_w, y_l\right) \sim \mathcal{D}}\left[\underbrace{\sigma\left(\hat{r}_\theta\left(x, y_l\right)-\hat{r}_\theta\left(x, y_w\right)\right)}_{\text {higher weight when reward estimate is wrong }}\left[\underbrace{\nabla_\theta \log \pi(y_{w|x})}_{\text{increase likelihood of $y_{w}$} }-\underbrace{\nabla_\theta \log \pi(y_{l|x})}_{\text{increase likelihood of $y_{l}$}}\right]\right]
\end{aligned}
$$


## DPO

[《Direct Preference Optimization: Your Language Model is Secretly a Reward Model》](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2305.18290)

[github.com/eric-mitchel](https://link.zhihu.com/?target=https%3A//github.com/eric-mitchell/direct-preference-optimization)

由于PPO训练过程需要加载 **预训练模型** 、 **监督模型** 、 **奖励模型** 和 **人类反馈策略模型，** 过程较为繁琐，训练耗时耗力。因此出现了一种更加简便的训练方式：DPO。

**通过利用奖励函数与最优策略之间的映射关系，证明这个受限的奖励最大化问题可以通过单阶段的策略训练来精确优化，本质上是在人类偏好数据上解决一个分类问题** 。DPO是稳定的、性能和计算成本轻量级的，无需拟合奖励模型，在微调期间从 LM 中采样，或执行显着的超参数调整。通过实验表明：DPO 进行微调超过了 RLHF 效果，并提高了摘要和单轮对话的响应质量。

> 参考： [DPO: Direct Preference Optimization 直接偏好优化（学习笔记）](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/lemonzhang/p/17910358.html)

![](https://picx.zhimg.com/v2-74c9aba3859bb364f1a4e5d5ffad9d33_1440w.jpg)

> 详细参考过程参考： [过拟合：\[论文笔记\]DPO：Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://zhuanlan.zhihu.com/p/653975451)

  

### 代码

[GitHub - eric-mitchell/direct-preference-optimization: Reference implementation for DPO (Direct Preference Optimization)](https://link.zhihu.com/?target=https%3A//github.com/eric-mitchell/direct-preference-optimization)

1、加载Policy和reference\_model两个model

![](https://picx.zhimg.com/v2-316690b81d14bcee5f554c8ded27d03f_1440w.jpg)

2、创建trainer。

![](https://pic2.zhimg.com/v2-e72f5d8bf3890193a61b04361cb90bbf_1440w.jpg)

3、开始训练。计算loss并更新Policy模型参数

![](https://picx.zhimg.com/v2-5c3da4358e54c1adcc9415d7d806079d_1440w.jpg)

其中计算loss的具体过程为：输入偏好数据和拒绝数据到Policy模型和reference\_model，推理生成chosen\_logps和rejected\_logps。

![](https://pic4.zhimg.com/v2-9d50d53300b37b075eb7ed5e2dbd8c87_1440w.jpg)

计算loss

![](https://pic4.zhimg.com/v2-47801bef3b81a190ddbd647ef508b455_1440w.jpg)

对应公式

![](https://pic3.zhimg.com/v2-edf6e8fdf56ae9e2d78265486de31598_1440w.jpg)

  

## 总结

1、整个RLHF包含 **预训练模型** 、 **监督模型** 、 **奖励模型** 和 **人类反馈策略模型** 。经过RLHF训练的模型在符合人类偏好上的推理质量有明显提升。

2、PPO是其中对策略模型进行更新的强化学习方法。

3、由于PPO的方法需要加载4个模型，训练过程复杂。因此出现了DPO这种只需要加载两个模型便可训练的低成本方法。

  

**本篇简单介绍一下RLHF和PPO/DPO的关系，由于PPO/DPO原理较为复杂，后面单开一篇对相关强化学习知识进行详细讲解。**

**github项目：** [GitHub - akaihaoshuai/baby-llama2-chinese\_fix: 使用单个24G显卡，从0开始训练LLM](https://link.zhihu.com/?target=https%3A//github.com/akaihaoshuai/baby-llama2-chinese_fix)

  

- **系列文章**

[呵呵哒：从0开始实现LLM：1、大模型训练踩坑](https://zhuanlan.zhihu.com/p/660759033)

[呵呵哒：从0开始实现LLM：2、大模型技术报告总结（GPT/PaLM/GLM/LLaMA/Skywork）](https://zhuanlan.zhihu.com/p/664046612?utm_psn=1702755988845731840)

[呵呵哒：从0开始实现LLM：3、大模型训练之数据扩展+deepspeed优化](https://zhuanlan.zhihu.com/p/683768690)

[呵呵哒：从0开始实现LLM：4、长上下文优化（理论篇）](https://zhuanlan.zhihu.com/p/683731440)

[呵呵哒：从0开始实现LLM：5、长上下文优化（代码篇）YaRN/CLEX/LongLoRA/LM-Infinite/StreamingLLM](https://zhuanlan.zhihu.com/p/684907262)

[呵呵哒：从0开始实现LLM：6、模型量化理论+代码实战（LLM-QAT/GPTQ/BitNet 1.58Bits/OneBit）](https://zhuanlan.zhihu.com/p/686161543)

  

## 参考文章

RLHF： [《Fine-Tuning Language Models from Human Preferences》](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1909.08593)

[github.com/openai/lm-hu](https://link.zhihu.com/?target=https%3A//github.com/openai/lm-human-preferences)

RLHF： [《Learning to summarize from human feedback》](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2009.01325)

PPO： [《Proximal Policy Optimization Algorithms》](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1707.06347)

DPO： [《Direct Preference Optimization: Your Language Model is Secretly a Reward Model》](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2305.18290)

[GitHub - eric-mitchell/direct-preference-optimization: Reference implementation for DPO (Direct Preference Optimization)](https://link.zhihu.com/?target=https%3A//github.com/eric-mitchell/direct-preference-optimization)

[TRiddle：拆解大语言模型RLHF中的PPO](https://zhuanlan.zhihu.com/p/645225982)

[\[细(戏)说\]RLHF场景下的PPO算法的来龙去脉](https://www.zhihu.com/tardis/zm/art/631338315?source_id=1003)

[前有绝境但无畏：PPO和RLHF的部分细节](https://zhuanlan.zhihu.com/p/651222229)

[Beaman：影响PPO算法性能的10个关键技巧（附PPO算法简洁Pytorch实现）](https://zhuanlan.zhihu.com/p/512327050)

[阿姆姆姆姆姆姆姆：深度强化学习（DRL）算法 2 —— PPO 之 GAE 篇](https://zhuanlan.zhihu.com/p/577598804)

[阿姆姆姆姆姆姆姆：深度强化学习（DRL）算法 2 —— PPO 之 Clipped Surrogate Objective 篇](https://zhuanlan.zhihu.com/p/574810519)

[GitHub - OpenLLMAI/OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework (Support 70B+ full tuning & LoRA & Mixtral & KTO)](https://link.zhihu.com/?target=https%3A//github.com/OpenLLMAI/OpenRLHF)

[OpenLLMAI：OpenRLHF：轻量高效的工业级LLM训练和对齐框架，支持70B模型RLHF全参数全流程训练！](https://zhuanlan.zhihu.com/p/683850569)

[DPO: Direct Preference Optimization 论文解读及代码实践](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/ckSRlXk530EmrErgEUPFOw)

[过拟合：\[论文笔记\]DPO：Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://zhuanlan.zhihu.com/p/653975451)

[DPO: Direct Preference Optimization 直接偏好优化（学习笔记）](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/lemonzhang/p/17910358.html)

[王鹏：消费级显卡搞定RLHF——DPO算法+QLora微调LLM拒绝有害问题回答实战](https://zhuanlan.zhihu.com/p/641620563)

[github.com/lucidrains/P](https://link.zhihu.com/?target=https%3A//github.com/lucidrains/PaLM-rlhf-pytorch)

[GitHub - OpenLMLab/MOSS-RLHF: MOSS-RLHF](https://link.zhihu.com/?target=https%3A//github.com/OpenLMLab/MOSS-RLHF)

[所属专栏 · 2025-05-13 23:48 更新](https://zhuanlan.zhihu.com/c_1662098877871296512)


[LLM(大语言模型)学习](https://zhuanlan.zhihu.com/c_1662098877871296512)

[

akaihaoshuai

59 篇内容 · 3767 赞同

](https://zhuanlan.zhihu.com/c_1662098877871296512)

[

最热内容 ·

LLM推理加速调研

](https://zhuanlan.zhihu.com/c_1662098877871296512)

编辑于 2024-04-17 20:10・浙江[LLM](https://www.zhihu.com/topic/20660508)[DPO](https://www.zhihu.com/topic/21844950)[PPO算法](https://www.zhihu.com/topic/21712992)[广告](https://life.douyin.com/p/login?channel_id=zh_lk_rz_and_03&ad_platform_id=zhihu_feed_lead&ug_callback_url=https%3A%2F%2Fsugar.zhihu.com%2Fplutus_adreaper_callback%3Fsi%3D176deeb2-e5a1-4686-975a-68e4327067bf%26os%3D3%26zid%3D6001%26zaid%3D3565789%26zcid%3D3494972%26cid%3D3494972%26event%3D__EVENTTYPE__%26value%3D__EVENTVALUE__%26ts%3D__TIMESTAMP__%26cts%3D__TS__%26mh%3D1d4d1d15496fac83c8b246bb2deea512%26ocg%3D10%26cp%3D8000%26ocs%3D0%26aic%3D0&ug_semver=v1.0.0&spu=biz%3D0%26ci%3D3494972%26si%3D4ec9c75c-4100-4d72-b585-00e19b104c98%26ts%3D1747730577%26zid%3D6001)