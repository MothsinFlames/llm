---
created: 2025-08-20T14:17:31 (UTC +08:00)
tags: [算法,正则化,大模型]
source: https://zhuanlan.zhihu.com/p/689412950
author: 北京大学 计算机科学与技术硕士
---
### 1. 背景知识：优化器、动量与自适应学习率

在理解 Adam 和 AdamW 之前，你需要知道几个关键概念：

*   **随机梯度下降（SGD）**：最基础的优化器。它直接沿着梯度的反方向更新参数。缺点是学习率难以选择，且容易在沟壑（ravines）中震荡。
*   **动量（Momentum）**：模拟物理中的动量概念。它不仅考虑当前的梯度，还会累积之前的梯度方向作为一个“速度”。这有助于加速收敛并减少震荡。
*   **自适应学习率（Adaptive Learning Rate）**：这个思想是**为每个参数设置不同的学习率**。对于频繁更新（梯度大）的参数，我们降低它的学习率；对于不常更新（梯度小）的参数，我们增加它的学习率。这可以处理稀疏数据并提高稳定性。**AdaGrad** 和 **RMSprop** 是这一思想的代表。

**Adam（Adaptive Moment Estimation）** 完美地结合了**动量**和**自适应学习率**这两个思想。

---

### 2. Adam 优化器详解

Adam 可以看作是 **带动量的 RMSprop**。它计算了两个“动量”：

1.  **一阶矩估计（First Moment, $m_t$）**：梯度的指数移动平均值。这包含了**方向**信息，类似于动量。
2.  **二阶矩估计（Second Moment, $v_t$）**：梯度平方的指数移动平均值。这包含了**波动**信息，用于调整每个参数的学习率，类似于 RMSprop。

**Adam 的更新步骤如下：**

对于模型中的每个参数 $θ$，在时间步 $t$：

1.  **计算梯度**： $g_t = ∇θ L(θ_{t-1})$，其中 $L$ 是损失函数。
2.  **更新一阶矩（动量）**： $m_t = β₁ * m_{t-1} + (1 - β₁) * g_t$
3.  **更新二阶矩**： $v_t = β₂ * v_{t-1} + (1 - β₂) * g_t²$
    *   $β₁$（如 0.9）控制一阶矩的衰减率。
    *   $β₂$（如 0.999）控制二阶矩的衰减率。
4.  **偏差校正**：由于 $m_t$ 和 $v_t$ 初始化为 0，在训练初期它们会偏向于 0。Adam 通过偏差校正来解决这个问题。
    *   $m̂_t = m_t / (1 - β₁^t)$
    *   $v̂_t = v_t / (1 - β₂^t)$
5.  **更新参数**：
    $θ_t = θ_{t-1} - α * m̂_t / (√v̂_t + ε)$
    *   $α$：全局学习率。
    *   $ε$：一个极小值（如 1e-8），防止分母为零。

**Adam 的优点**： 结合动量和自适应学习率，收敛速度快，对超参数选择相对鲁棒，是很多深度学习任务的默认选择。

---

### 3. 权重衰减（Weight Decay）与 L2 正则化

为了防止过拟合，我们通常在损失函数中加入正则化项。**L2 正则化**是最常见的一种，它在损失函数中添加了所有权重的平方和乘以一个系数（λ）。

$L_{reg}(θ) = L(θ) + (λ/2) * ||θ||²$

**权重衰减（Weight Decay）** 是一种与 L2 正则化在标准 SGD 中等价的技术。它不是在损失函数中加项，而是在参数更新时，直接让参数向 0 收缩一点：

$θ_t = θ_{t-1} - α * g_t - α * λ * θ_{t-1}$

*   注意：$- α * λ * θ_{t-1}$ 就是权重衰减项。在 SGD 中，$(λ/2) * ||θ||²$ 求导后正好是 $λ * θ$，所以 $L2正则化系数 λ$ 等价于 $权重衰减系数 αλ$。

---

### 4. Adam 的问题：权重衰减的实现错误

问题就出在这里。当人们将 Adam 和权重衰减结合使用时，很自然地会沿用 SGD 的思路，将 L2 正则项直接加到损失函数中。**但这在 Adam 中并不等价于真正的权重衰减！**

*   在 **SGD** 中：
    $θ_t = θ_{t-1} - α * (g_t + λ * θ_{t-1})$
    *   更新是“纯净”的：梯度 $g_t$ 和权重衰减项 $λ * θ_{t-1}$ 被同一个学习率 $α$ 缩放。

*   在 **Adam** 中（当把 L2 项放在损失函数里时）：
    $L_reg(θ) = L(θ) + (λ/2) * ||θ||²$
    此时的梯度 $g_t$ 变成了 $∇θ L(θ) + λ * θ_{t-1}$。
    Adam 在更新时，会对这个**合并后的梯度**（$∇θ L(θ) + λ * θ$）进行**自适应学习率调整**（除以 $√v̂_t$）。

**这导致了什么问题？**
权重衰减的效果不再稳定。自适应学习率会动态地缩放权重衰减的强度。**对于那些梯度很大、自适应学习率很小的参数，权重衰减的作用会被大幅削弱（因为除以了一个很大的 $√v̂_t$）；而对于梯度很小、自适应学习率很大的参数，权重衰减的作用会被异常放大。**

**结果就是：Adam 中的 L2 正则化并不能实现稳定、一致的权重衰减效果，这损害了模型的泛化能力。**

---

### 5. AdamW：修正的 Adam 优化器

AdamW（**W** 代表 **Weight decay fix**）由 Ilya Loshchilov 和 Frank Hutter 在论文 **[Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101)** 中提出。

**AdamW 的核心思想：将权重衰减与梯度更新解耦（Decouple）。**

AdamW 不再将权重衰减项混入损失函数（从而混入梯度），而是**像在原始 SGD 中实现权重衰减那样，直接在更新参数时，额外添加一个与参数值成正比的项**。

**Adam 和 AdamW 的更新公式对比：**

*   **Adam (with L2 in loss)**:
    $θ_t = θ_{t-1} - α * m̂_t / (√v̂_t + ε)$
    *   注意：这里的 $m̂_t$ 是从 $g_t = ∇θ L(θ) + λ * θ$ 计算而来的。权重衰减被自适应学习率处理了。

*   **AdamW (Correct weight decay)**:
    $θ_t = θ_{t-1} - α * (m̂_t / (√v̂_t + ε) + λ * θ_{t-1})$
    *   注意：这里的 $m̂_t$ 是**仅从原始梯度 $∇θ L(θ)$** 计算而来的，不包含 $λ * θ$。
    *   **权重衰减项 $λ * θ_{t-1}$ 被直接加到了自适应学习率调整后的梯度之后**。它**没有**被除以 $√v̂_t$。

**这个看似微小的改动带来了巨大的影响：**

1.  **真正的解耦**：权重衰减现在独立于自适应学习率机制。它对所有参数施加的收缩力是一致的（系数都是 $λ$），不再受每个参数自适应学习率大小的影响。
2.  **与 SGD 中的权重衰减行为一致**：AdamW 恢复了权重衰减最原始、最纯粹的形式，确保了其正则化效果的稳定性。
3.  **更好的泛化性能**：大量实验表明，使用 AdamW 训练的模型，尤其是在计算机视觉（如训练 ResNet、Transformer）等复杂任务上，其最终精度（泛化能力）通常显著高于使用原始 Adam（L2 正则）的模型。

---

### 6. 总结与对比表格

| 特性          | Adam (with L2 in Loss)            | AdamW (Decoupled Weight Decay)                     |
| :---------- | :-------------------------------- | :------------------------------------------------- |
| **核心思想**    | 将 L2 正则项作为损失函数的一部分，其梯度参与自适应学习率计算。 | **将权重衰减与梯度更新解耦**。权重衰减是一个独立的项，不受自适应学习率影响。           |
| **权重衰减处理**  | **耦合的（Coupled）**：权重衰减效果被自适应学习率扭曲。 | **解耦的（Decoupled）**：权重衰减效果稳定、一致。                    |
| **实现方式**    | 在定义损失函数时加入 $weight\_decay$ 项。     | 优化器内部有一个独立的 $weight\_decay$ 参数，在更新步骤的最后直接应用。       |
| **性能**      | 在简单问题上可能工作，但在复杂模型和数据集上，泛化性能往往较差。  | **泛化性能更好**，通常是训练深度神经网络（尤其是Transformer、CNN）的**首选**。 |
| **超参数λ的含义** | λ 是 L2 正则化系数，其有效强度会因参数而异。         | λ 是**真正的权重衰减系数**，其效果与 SGD 中的权重衰减一致。                |

### 实践建议

*   **现在，绝大多数现代的深度学习框架（如 PyTorch, TensorFlow）在实现 $torch.optim.AdamW$ 时，已经采用了正确的 AdamW 算法。**
*   当你使用 $torch.optim.Adam$ 时，它的 $weight_decay$ 参数实际上是**错误**的实现（即在损失中加入 L2 项）。你应该避免使用它来进行权重衰减。
*   **直接使用 $torch.optim.AdamW$**，并为其 $weight_decay$ 参数设置一个值（例如，对于 Transformer，常用 0.01 或 0.1；对于 CNN，可能更小如 1e-4），同时将损失函数中的 L2 正则项设为 0。
*   学习率 ($α$) 和权重衰减系数 ($λ$) 是需要仔细调优的一对超参数。

总而言之，**AdamW 是 Adam 的一个修正版本，它通过将权重衰减与梯度更新解耦，解决了 Adam 中权重衰减实施不当的问题，从而在大多数情况下能获得更好的模型性能。** 在今天，AdamW 已经被广泛视为训练深度学习模型的新标准。