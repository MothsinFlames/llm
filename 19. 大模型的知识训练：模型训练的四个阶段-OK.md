---
source: "https://zhuanlan.zhihu.com/p/682410329"
---
 GPT 整个训练过程:  Andrej Karpathy [（个人网站）](https://link.zhihu.com/?target=https%3A//karpathy.ai/) 。

《State of GPT》演讲视频 [（链接地址）](https://link.zhihu.com/?target=https%3A//community.openai.com/t/build-talk-state-of-gpt-andrej-karpathy/226110) ：2023 年 5 月 23 日

报告分为两个部分，这篇笔记主要围绕上半部分：
- Part 1：如何训练出 GPT 这个助手？
- Part 2：如何在应用程序中高效的使用 GPT 这个助手？

## 1\. 模型训练的总体框架

Andrej Karpathy 在演讲的开篇，给出了 GPT 训练的 Pipeline，将 GPT 的训练分成四个阶段：

- 预训练阶段（Pretraining）
- 有监督微调阶段（Supervised Finetuning）
- 奖励建模阶段（Reward Modeling）
- 强化学习阶段（Reinforcement Learing）
![|625](https://picx.zhimg.com/v2-a4b1a709ab6a6ab48e7db1723072f921_1440w.jpg)

## 2\. 预训练阶段（Pretraining）

在预训练阶段，Andrej Karpathy 主要讲了几个事情：

- 1）获取训练样本数据集
- 2）训练样本 Token 化
- 3）预训练，生成基础模型（Base Model）

### 2.1 获取训练样本集

GPT 的训练样本集主要源于两个方面：

- **互联网爬取的数据集，** 比如 CommonCrawl、 [C4](https://zhida.zhihu.com/search?content_id=239776817&content_type=Article&match_order=1&q=C4&zhida_source=entity) 。
- **一些更高质量的数据集，** 比如 Github、Wikipedia、ArXiv 论文存档、StackExchange 问答网站等。
![|475](https://pic2.zhimg.com/v2-8c7a9a64b7dbf326e1e16f1700b491b9_1440w.jpg)

GPT 训练样本集，源自 Meta LLaMA 论文

这些数据会根据一定比例进行混合采样，其中互联网爬取的数据整体占比达到 70% 以上，其他各种更高质量的数据占比不足 20%。

### 2.2 训练样本 Token 化

在正式预训练之前，这些训练数据集需要先进行 Tokenization 预处理，将语料转化成 Token 的表示方式。简单来说，就是将语料文本序列转换成一个内部向量，转换算法也有多种方式。关于 Token 的基本知识，可以参见上一篇笔记 [《大模型的宏观认知：基本运行机制》](https://zhuanlan.zhihu.com/p/682345316) 。

![|675](https://pic3.zhimg.com/v2-cac9fa1b0a3e8d92dd73b9c0ba32bbb6_1440w.jpg)

训练语料 Token 化处理

那么，大模型的训练究竟需要多少 Tokens？Andrej Karpathy 给出了两个例子：GPT-3 和 LLaMA-2。

![|650](https://pic4.zhimg.com/v2-a4cf65e6107621bc87a2dfeb06899993_1440w.jpg)

GPT-3 和 LLaMA-2 在预训练中使用的 Token 数量，以及其他超参数

从上图所公布的数据能够看到预训练过程“惊人”的消耗（粗略的统计）：

|        | OpenAI GPT3-175B | MetaAI LLaMA2-65B |
| ------ | ---------------- | ----------------- |
| 参数规模   | 1750 亿           | 650 亿             |
| 训练语料规模 | 3000 亿 Tokens    | 1.4T Tokens       |
| 消耗算力   | 10000 V100 GPUs  | 2048 A100 GPUs    |
| 训练时长   | 30 天             | 21 天              |
| 金钱成本   | 1000 万美元         | 500 万美元           |

当然，通过这两张表还能看到两个大模型一些重要的超参数：

|  | OpenAI GPT3-175B | MetaAI LLaMA2-65B |
| --- | --- | --- |
| 词汇表大小 | 50257 | 32000 |
| 上下文长度 | 2048 | 2048 |
| 参数规模 | 1750 亿 | 650 亿 |
| 解码器层数 | 96 | 80 |
| 自注意力头数 | 96 | 64 |
| 训练每批次数据量（Batch Size） | 320 万 | 400 万 |

在大模型中，超参数和参数是不同的概念，超参数取值需要人为设定，参数的取值，也称为权重，则是通过训练获得。关于这些超参数对模型推理和训练意味着什么，可参考另外两篇笔记： 《Transformer 架构解析：Self-Attention》 和 《Transformer 架构解析：模型推理和正向传播》 。

### 2.3 预训练，生成基础模型

用于训练的语料样本包含如此之多的 Token，它们应该如何送入 GPT Transformer 中呢？它并非一个挨一个的送入，而是以一批一批的送入， **每一批次的数据可以看成一个数组，其大小为 B \* T：**

- **B 是数组的行数**
- **T 是超参数中的上下文长度**
![|650](https://pic3.zhimg.com/v2-37131c55ae805e9016e338a4b56fc914_1440w.jpg)

预训练的语料是一批一批的传入 Transformer

当然，上图中 B=4/T=10 仅仅是一个例子，实际训练中，B 和 T 的取值都是比较大的，而超参数 Batch Size 设置的就是每次批量传输的的数据量，GPT3-175B 和 LLaMA2-65B 分别有 3.2M 和 4M。

每条训练语料，都会有一个结束符 <|endoftext|> ，即图中红色的 50246 向量值，它指示 Transformer 上一条语料已经结束，下一条语料即将开始。

**对于无监督或自监督学习的模型来说，训练语料本身既蕴含了“输入”信息，也蕴含了期望输出的“目标答案”信息。** 比如以“GPT 是一个优秀的大模型”这条训练语料为例，从 Transformer 的视角来看，“GPT 是一个优秀的”这句话的预测目标结果就应该是“大模型”。Andrej Karpathy 从 Token 这个更微观的层面解释了这个过程：

![|475](https://pic3.zhimg.com/v2-2bdab4e5d0bec02ec2755f3bba21bb96_1440w.jpg)

站在 Token 微观层面看待 Transformer 的预训练过程（其实是一个推理过程）

不同颜色的单元格的意义有所不同：

- 绿色单元格，代表的其实是任务本身，是输入的问题。
- 黄色单元格，代表的其实是任务的上下文，这个信息是任务本身的语义理解的重要保证。
- 红色单元格，代表的其实是预测结果，是输出的答案。

那么，这个例子说的就是：对于 3188 这个 Token，依据其下文信息“16281 3188 362 50256 16281”，预测的下一个 Token 就是 513。需要注意的是，这些数字是词汇表中的索引，而不是 Token 的词向量编码。GPT3-175B 的词汇表大小是 50257。关于词向量和词汇表，这里不展开了，可参考 [《Transformer 架构解析：模型推理和正向传播》](https://link.zhihu.com/?target=https%3A//www.yuque.com/zhangqiang-studyisalifestyle/it-technology/bbu0oh1nbgf6f6sb) 。

**GPT Transformer 是一个拥有超大规模参数的深度神经网络模型，正是这些参数的取值（权重）组合在一起，才能输出了相应的预测结果（概率），而 Transformer 就是通过预测结果，再反向更新自己的参数权重。这个过程叫做反向传播，是预训练中重要的一步，也是产生最大消耗的一步，因为这种更新不是一次性完成的，而是多次迭代，逐步逼近的过程。**

关于这个迭代的过程，Andrej Karpathy 给出了一个更为直观的过程，如下图所示，训练的语料是莎士比亚的文章段落。

![|475](https://pic3.zhimg.com/v2-b76893fb3cb33b68325c30d4d3352052_1440w.jpg)

通过一轮一轮迭代的预训练，GPT 的预测能力越来越强

预训练之初，大模型的“脑子”是混乱的 —— 即参数权重是随之的，生成的内容则是一堆没有什么规律、无意义的乱码，好比刚出生的婴儿，理解不了你说什么，也表达不清自己要说什么。

![|325](https://pic4.zhimg.com/v2-37d969ff0cfbd6bcb22de79b77ff94d1_1440w.jpg)

训练之初，大模型的参数输出是随机的内容

随着多轮的迭代和数据的“轰炸”，大模型开始展现出某种“正常有序”的行为模式，输出的内容越来越有意义。

![|300](https://pic1.zhimg.com/v2-9c0bcf4f6dc22a4cf0298fb5503d5402_1440w.jpg)

经过 500 轮的迭代训练

![](https://pic1.zhimg.com/v2-9c0bcf4f6dc22a4cf0298fb5503d5402_1440w.jpg)

经过 5000 轮的迭代训练

![](https://pic1.zhimg.com/v2-9c0bcf4f6dc22a4cf0298fb5503d5402_1440w.jpg)

经过 30000 轮的迭代训练

同时，也能看到损失值（LOSS）随着持续的训练（体现在训练时间、训练语料规模）在不断下降。 **损失值是衡量预测结果和目标结果之间的差距，损失值越小，代表预测的越准确，训练的目的就是要不断的降低这个损失值。**

![|500](https://pic1.zhimg.com/v2-0c68cb760e200ae1ebe1ab849a88ffa6_1440w.jpg)

随着训练的开展，损失值在不断降低

经过预训练的模型就像是一个小学生，他能理解了人类基本的语言，也可以输出成段的，有意义的文字。 **但这种理解能力，依靠的并非机械的“背诵词汇”—— 即配置和匹配规则，而是通过训练来改变模型中那巨大规模的参数权重。预训练之后的模型叫做基础模型（Base Model）。**

基础模型，是无监督训练的结果，它能够根据输入的内容预测下一个可能的词汇，尽可能保证它输出的是一段连贯、有意义的文本， **因此它更擅长做“完形填空”，但并不能直接用来和人类进行自然语言的对话交互，** 这主要是因为：

- 1）用于训练基础模型的数据构成是一条条、一段段的文本语料，并非真实的人类对话，模型难以理解那些对话式的人类指令，也就无法生成符合人类逻辑的内容。
- 2）连贯、自然的对话交互，其上下文信息非常重要，基础模型的工作模式是典型的 Request-Response，它自身是没有上下文概念的，无法记住之前的对话内容，也无法生成符合上下文语境的响应。
- 3）人类的对话交互往往具有独特的风格，没有针对性训练的基础模型可能无法适应这种多样性，影响生成内容的质量和体验。

比如，给基础模型输入一句“写一首关于面包和奶酪的诗（Write a poem about bread and cheese）”，模型的回应很可能是更多的问题，如同 Andrej Karpathy 所说：“Often responds to questions with more questions, etc”。

![|500](https://pica.zhimg.com/v2-62e1eb4747624d94ab5c0df32d511cec_1440w.jpg)

基础模型在对话任务中的表现是不太灵光的

不过，Andrej Karpathy 提到有一种“小花招”，就是通过伪造一些历史对话交互的内容，比如多次的 Q/A 问答，然后输入给基础模型，Transformer 仍会“尽力”的预测后面的内容，它实际上仅仅是想完成整个文档，但看起来却像是在进行对话。 **这种方法，实际上是一种“提示工程（Prompt Engineering）”，通过特定的提示词，巧妙的“欺骗”基础模型。不过，这种方法未必总是有效，尤其跨越不同的基础模型的时候。**

![|500](https://pic2.zhimg.com/v2-e1b40efd7f44f95a5ab583874e4b12b9_1440w.jpg)

伪造 Q/A 历史对话，让大模型生成类似对话的内容

![|600](https://picx.zhimg.com/v2-67743b448df428432d66a6da00b862eb_1440w.jpg)

伪造 Q/A 历史对话，让大模型生成类似对话的内容

Andrej Karpathy 演讲中关于预训练的内容就是以上内容。有兴趣的话，以下材料可作为延伸阅读：

- [《Improving Language Understanding by Generative Pre-Training》](https://link.zhihu.com/?target=https%3A//openai.com/research/language-unsupervised)
- [《Language Models are Unsupervised Multitask Learners》](https://link.zhihu.com/?target=https%3A//d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

## 3\. 有监督微调阶段（Supervised Finetuning）

训练的第二个阶段是有监督微调（Supervised Finetuning，SFT）。和预训练阶段相比，这个阶段最大的变化就是训练数据由“量多质低”“变为“量少质高”，训练数据主要由人工进行筛选或生成，数据量一般在 1～10 万条，要比预训练阶段低几个数量级，同样大幅降低的还有算力资源消耗和训练时间。而微调的“微”就体现在这里

那么，这个阶段采用的更高质量的数据长成什么样子呢？ **其实就是一条条包含“明确指令<->精确回答”的对话交互语料对，这可以看成一种“标签”型数据，因此是一种有监督的训练方式。** 演讲中，Andrej Karpathy 给出了两个 SFT 的训练数据样例：

![|675](https://pica.zhimg.com/v2-2e43e162bddd216736dad8ff62f70aa4_1440w.jpg)

演讲现场给出的 SFT 样例

再以百川大模型为例，我们看看它的微调数据样例（ [百川大模型 SFT Data 的下载地址](https://link.zhihu.com/?target=https%3A//github.com/baichuan-inc/Baichuan2/blob/main/fine-tune/data/belle_chat_ramdon_10k.json) ）

```json
[
  {
  "id": "77771",
  "conversations": [
   {
    "from": "human",
    "value": "写一篇关于气候变化对海洋生态的影响的文章。\n"
   },
   {
    "from": "gpt",
    "value": "好的，以下是你要求的文章：\n气候变化对海洋生态的影响\n气候变化对海洋生态产生了广泛的影响。全球变暖导致海洋温度上升，这可能会对许多水生生物的生存、繁殖和迁移造成巨大的影响。另外，海洋酸化也是一个问题，因为大气中二氧化碳的增加导致海水中的pH值下降，使得许多生物的外骨骼变得脆弱，从而影响它们的生存和繁殖。\n此外，气候变化还会引起海平面上升，这不仅对海岸线和沿海居民产生影响，还会对许多海洋生物的栖息地造成影响。一些岛屿上的陆地生物面临几乎完全消失的风险，而另一些生物范围则会扩大到更广泛的领域，这可能会导致新的竞争和生态平衡的改变。\n总之，气候变化并不只是一个地球气温的问题，它对海洋生态系统产生了广泛和深远的影响。\n"
   },
   {
    "from": "human",
    "value": "从刚刚的文章中提取出气候变化对海洋生物繁殖的影响。\n"
   },
   {
    "from": "gpt",
    "value": "文章已经提到过气候变化会对海洋生物的生存、繁殖和迁移造成影响。例如，海洋酸化会使得许多生物的外骨骼变得脆弱，进而影响它们的生存和繁殖。此外，海洋温度上升也可能会对很多生物的生存和繁殖产生负面影响。"
   }
  ]
 }
]
[
  {
  "id": "27684",
  "conversations": [
   {
    "from": "human",
    "value": " 给我生成一个短故事，主题是爱情。\n"
   },
   {
    "from": "gpt",
    "value": " 她的眼里只有他的身影。她不停地想着他，每天都期待着与他相见。这是一个小镇上的普通女孩与一个城市里的成功男士之间的故事。他们一开始是陌生人，但很快彼此吸引，开始谈恋爱。尽管他们之间的差距，但他们克服了一切。时间飞快地过去了，最后他们成为了幸福的一对。\n"
   },
   {
    "from": "human",
    "value": " 可以再生成一个关于未来的句子吗？\n"
   },
   {
    "from": "gpt",
    "value": "未来是未知的，但他们相信，只要彼此相爱，一切都可以克服。他们的未来充满着希望和梦想。他们计划着去旅行、尝试新的事物，以及拥有一个美满的家庭。他们相信，他们的未来会充满幸福，希望能够一直走下去。\n"
   },
  ]
 },
]
```

像这样的语料对有多少条呢？百川大模型官方给出的是 10002 条

![|675](https://picx.zhimg.com/v2-bda915da75a5835f9d3c763282e9e3a5_1440w.jpg)

百川大模型微调数据样例中包含大约 1 万条高质量对话交互语料

![|675](https://pic4.zhimg.com/v2-c550f85e511261a11ef1c4ae9bd88005_1440w.jpg)

百川大模型微调数据样例中包含大约 1 万条高质量对话交互语料

对于模型构建提供者， **尤其是垂直领域的大模型供应商来说，高质量的微调数据是非常宝贵的智力资产，在基础模型普遍开源的情况下，微调数据质量的高低往往成为影响最终模型表现的关键因素。**

**经过微调阶段训练的模型被称为有监督微调模型（Supervised Finetuning Model），它对人类的问题（也叫做叫指令，即 Instruction）已经能够较好的遵循了，可以作为初级的智能助手（Assistant）。这时候的模型就好比一个中学生，接受了学校更为严格的教育洗礼，对语言和知识的理解与输出能力，相较基础模型有了长足进步。**

> 就像是进入学校的学生，开始接受人类文明知识蒸馏结果的洗礼：语文、数学、历史、生物、物理、化学、艺术... 并且，这一切都是在学校的老师监督之下完成的。 —— 来自尹国冰的博客 [《GPT 的一生》](https://link.zhihu.com/?target=https%3A//yinguobing.com/the-lifetime-of-gpt/)

关于微调，这篇著名的论文可作为延伸阅读： [《Language Models are Few-Shot Learners》](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2005.14165)

## 4\. 奖励建模阶段（Reward Modeling）

训练的第三个阶段，是一个被称为“基于人类反馈的强化学习（Rainforcement Learning from Human Feedback，RLHF）”的过程，这在当下的人工智能领域得到了越来越广泛的关注。而按照 Andrej Karpathy 的说法，RLHF 又可以分成两个环节：

- 奖励建模阶段（Reward Modeling）
- 强化学习阶段（Reinforcement Learning）

这一节先说奖励建模。在这一阶段，模型学习和输出的内容发生了根本性的改变。前面的两个阶段，预训练和微调，模型的输出是符合预期的文本内容； **奖励建模阶段的输出不仅包含预测内容，还包含奖励值或者说评分值，数值越高，意味着模型的预测结果越好。**

![|500](https://pic4.zhimg.com/v2-1b39954eb97c15b96afabd9260a931a1_1440w.jpg)

不同训练阶段的输入和输出内容对比

**那么，这个奖励值或评分值是谁给的呢？它是人类打出的，也就是说奖励建模阶段加入了人的因素。** 在演讲中，Andrej Karpathy 举了一个形象的例子，假设输入给模型的任务是“写一个函数，判断给定字符串是否是回文（回文指的是顺读和倒读效果都一样的修辞法，是一种镜像对称结构）”，ChatGPT 在训练过程中给出了三个不同的预测结果，并由专业人员进行了评分。

![|700](https://picx.zhimg.com/v2-5a2c6b3ee67c70e363f541fa3c68593d_1440w.jpg)

奖励建模阶段，对模型的输出结果加入了人为评分的过程

**奖励模型也是一个机器学习的模型，和 SFT 一样采用有监督学习方式，其数据样本标签就是这个由人类给出的评分值。** 在实际过程中所做的事情如下图所示，蓝色格子是输入给 SFT 模型的内容，黄色格子是 SFT 模型预测的结果，绿色格子则是 Transformer 针对 SFT 预测结果所给出的评分值。可以基于这个评分结果设计损失函数 LOSS，然后通过不断降低 LOSS 来训练出符合人类要求的奖励模型。

![|775](https://pic2.zhimg.com/v2-afccb41583534ec4c861cb1e3992ef0f_1440w.jpg)

训练奖励模型的过程示意图

**一旦构建了奖励模型，后续就可以用它来对大模型的预测结果进行有效的打分。** 这个阶段有点像从中学毕业迈入了大学校门的大学生，不能再指望着“被监督着学习”，而是要学会评估自己的学习情况，进行自我监督。需要注意的是，这个阶段输出的评分，并不是给最终的用户，而是在第四个阶段 —— 强化学习 —— 发挥重大作用。

## 5\. 强化学习阶段（Reinforcement Learning）

这一节聚焦 RLHF 的第二个环节：强化学习。这个阶段的训练目标又回归到与预训练和有监督微调阶段相同，进行语言建模，预测接下来要输出的内容。 **这个阶段非常“聪明”的整合了前面的成果：**

- 第二阶段有监督微调的 SFT 模型。
- 第三阶段奖励建模的 RM 模型。

具体的整合过程是怎样的呢？

- 针对特定的输入文本，通过 SFT 模型获得多个输出文本。
- 基于 RM 模型对多个输出文本的质量进行打分，这个打分实际上已经符合人类的期望了。
- 基于这个打分，为多个输出文本结果加入权重。这个权重其实会体现在每个输出 Token 中。
- 将加权结果反向传播，对 SFT 模型参数进行调整，就是所谓的强化学习。
![|450](https://pica.zhimg.com/v2-f1bf94009dbd7bbdd83661713de78cfa_1440w.jpg)

强化学习其实是建立在有监督微调和奖励建模基础之上

  

Andrej Karpathy 在演讲中对这个过程讲述的很清楚，但我感觉这页材料表达的并不是很清晰，也贴在这里吧做个参考。

![|850](https://pica.zhimg.com/v2-e1673b07ab91ccbb7d88a8c7eba22e64_1440w.jpg)

强化学习其实是建立在有监督微调和奖励建模基础之上

**上述的强化学习是一个反复迭代的过程。这特别像我们走出大学校园后，在社会上不断被锤炼，接收了各式各样的正反馈与负反馈，不断修正自己的行为，于是成为了现在的自己。**

**一般情况下，我们都是将 RLHF 作为一个整体过程来说，而不会刻意分成奖励建模和强化学习两个环节。** 为什么要在预训练和有监督微调后使用 RLHF 呢？ **答案很简单：It works better!**

![|775](https://pic2.zhimg.com/v2-82d0d4636a277a3c827136d9586b3a3d_1440w.jpg)

RLHF 能够为大模型打来实实在在的效果改变

上图中的 [PPO](https://zhida.zhihu.com/search?content_id=239776817&content_type=Article&match_order=1&q=PPO&zhida_source=entity) （Proximal Policy Optimization，近端策略优化） 是一种 RLHF 算法。能够看出来，它预测的结果由于加入了人为的影响，往往更让人喜欢。

## 6\. 关于模型训练的总结与思考

通过前面四节的内容，我们对大模型的训练阶段应该有了一个宏观上的认知，做一个总结：

- 阶段一，用大量语料做预训练 （Pretraining）->**不可控的语料生成不可控的知识**
- 阶段二，用少量人为攥写的语料做微调 （Supervised Fine-tuning）->  **可控的语料生成可控的知识**
- 阶段三，基于有监督微调模型，对同一提示多次调用模型产生不同的输出，请人来对不同的输出进行评分。基于评分数据，训练出奖励模型 （Reward Modeling）-> **生成包含与人类价值观对齐奖励模型**
- 阶段四，综合阶段二和阶段三进行强化学习，调优成最终模型 ->**基于奖励模型生成与人类价值观对齐的可控的知识**

参考链接：
- [GPT的一生](https://link.zhihu.com/?target=https%3A//yinguobing.com/the-lifetime-of-gpt/)
- [State of GPT：大神Andrej揭秘OpenAI大模型原理和训练过程\_腾讯新闻](https://link.zhihu.com/?target=https%3A//new.qq.com/rain/a/20230527A07IQ800)

