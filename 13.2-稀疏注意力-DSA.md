---
created: 2025-10-13T18:30:23 (UTC +08:00)
tags: [大模型,DeepSeek]
source: https://zhuanlan.zhihu.com/p/1959636888123049941
author: 
---

# 十分钟读懂 DeepSeek-V3.2 稀疏注意力 DSA

> ## Excerpt
> DeepSeek-V3.2-Exp 是在 DeepSeek-V3.1-Terminus 的基础上通过持续训练引入 DeepSeek Sparse Attention (DSA) 的实验性模型。该模型核心目标是 在不显著牺牲性能的前提下，大幅提升训练与推理中的效率，尤其是应对…

---
**DeepSeek-V3.2-Exp** 是在 **DeepSeek-V3.1-Terminus** 的基础上通过持续训练引入 **DeepSeek Sparse Attention (DSA)** 的实验性模型。该模型核心目标是 **在不显著牺牲性能的前提下，大幅提升训练与推理中的效率**，尤其是应对最长至 **128K tokens** 的长上下文输入。

-   DSA 将主模型的注意力复杂度从 $\mathcal{O}(L^2)$ 降为 $\mathcal{O}(Lk)$ ，极大地降低了 **长序列上的成本**。
-   在 [H800 GPU](https://zhida.zhihu.com/search?content_id=263957132&content_type=Article&match_order=1&q=H800+GPU&zhida_source=entity) 集群上测试显示，无论是在 prefilling 还是 decoding 阶段，单位 token 成本都有大幅下降。在 128K 长上下文下，单位 token 成本最高下降达 **60%~70%。**

标准的多头注意力 MHA 的复杂度是 $\mathcal{O}(L^2)$ ，每个 Query Token 需要和历史的所有 Token 计算 Attention。如下图所示：

![](https://pic1.zhimg.com/v2-4a906de2d4888a30062861fab24cea3c_1440w.jpg)

图1: 标准 MHA（省略了 softmax）

而在 DeepSeek-v3.1 中采用的是 MLA（[Multi-head Latent Attention](https://zhida.zhihu.com/search?content_id=263957132&content_type=Article&match_order=1&q=Multi-head+Latent+Attention&zhida_source=entity)，多头潜在注意力）结构，如下图所示：

![](https://pic2.zhimg.com/v2-f396575286646938e03fc690329365bf_1440w.jpg)

图2: DeepSeek-v3.1 模型结构

DeepSeek 此前提出的原生稀疏注意力（Native Sparse Attention, NSA）属于一种 **Block-wise** 的稀疏方案。与之不同，本次的 DSA 采用了一种更细粒度的 **Token-wise** 稀疏策略。其实现方式是在模型中引入一个轻量级的 **Lightning Indexer**，专门负责为每个 Query Token 动态地选取 Top-K 个最相关的 Key。

![](https://pic3.zhimg.com/v2-a2bfb7331a3badec49f10c65f63c1856_1440w.jpg)

图3: DeepSeek-v3.2-Exp 引入稀疏注意力 DSA

如图 3 所示，DeepSeek-V3.2-Exp 仅在 DeepSeek-V3 的基础上新增了 **Lightning Indexer 模块**，用于选择参与 Attention 的 Token。

该模块的主要输入是 **Q 的低秩矩阵** $\bm{c}_t^Q$ 与 **MLA 的输入矩阵** $\bm{h}_t$ ，输出则是每个 Token 所对应的 **2048 个可参与 Attention 的历史 Token 索引**。

被选中的 2048 个 Token 会在 **MQA 的 Mask 阶段**发挥作用：通过将未选中的位置的 Mask 值设为 **INF**，在经过 SoftMax 之后，就能有效去除不需要参与 Attention 的 Token。这样实现了高效的稀疏化，显著降低了计算量。

### 1.1 Lightning Indexer：轻量级 KV 选择器

Lightning Indexer是 DSA 的关键组件，用于为每个 query token 选出最相关的 key-value tokens，极大地减少计算量。

对于第 $t$ 个 query token $\bm{h}_t \in \mathbb{R}^d$ ，与历史上的每个 token $\bm{h}_s \in \mathbb{R}^d$ 计算相关性得分：

$I_{t,s} = \sum_{j=1}^{H_I} w^I_{t,j} \cdot \text{ReLU}(\bm{q}^I_{t,j} \cdot \bm{k}^I_s)\tag1$

-   $H^I$ ：indexer 头数（固定为 64）
-   $\bm{q}^I_{t,j}$ ：第 $t$ 个 Token 在第 $j$ 个索引头（Indexer Head）的 Query 向量
-   $\bm{k}^I_s$ ：第 $s$ 个 Token 的 Key 向量。值得注意的是，**该 Key 向量只有一个，被所有 64 个索引头共享**
-   $\bm{q}^I_{t,j}$ 和 $\bm{k}^I_s$ 的维度均为 128
-   $w^I_{t,j}$ ：可学习的标量权重。
-   使用 **ReLU** 激活而非 softmax，有利于 **吞吐优化（FP8 实现）**

从结构上看，Indexer 的 Q/K 计算过程与模型中的 MLA（Multi-head Latent Attention）非常相似，可以看作是一个 **"微缩版" 的 MLA**（其头数为 64 vs MLA 的 128，头维度为 128 vs MLA 的 576）。其计算量大约仅为 MLA 对应部分的 **1/9**，实现了高效的 Key 选择。

### 1.2 Fine-Grained Token Selection（细粒度选择）

基于上一步计算出的 $I_{t,s}$ ，仅选取 top-k 的 key-value 进行 attention 计算： $\bm{u}_t = \text{Attn}\left(\bm{h}_t,\left\{\bm{c}_s \mid I_{t,s} \in \text{Top-k}(I_{t,:}) \right\}\right)\tag2$

-   $\bm{c}_s$ ：原始 key-value entry
-   从 全序列 token 中选择 top-k 个最相关的 token，用于高效计算 attention 输出。

## 2\. DSA 详细计算过程

前面我们介绍了 DSA 的计算方法，关键是根据相关性得分 $I_{t,s}$ 选择得分最高的 top-k 个最相关的 token，返回其对应 Index。

下图是 Lightning Indexer 模块的具体流程：

![](https://pic2.zhimg.com/v2-1682c1dee9d5e7b022e10a220dd54e05_1440w.jpg)

图4: Lightning Indexer 计算流程

如图 4 所示，Lightning Indexer 核心逻辑是：将 Q 与 K 进行点积（Matmul），然后对所有 Head 的相关性求和，从而得到每个 Query 与历史 Token 的总体相关性得分。

下面是 [Lightning Indexer 的代码实现](https://link.zhihu.com/?target=https%3A//github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/inference/model.py)：

```python3
class Indexer(torch.nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.dim: int = args.dim
        self.n_heads: int = args.index_n_heads
        self.n_local_heads = args.index_n_heads // world_size
        self.head_dim: int = args.index_head_dim
        self.rope_head_dim: int = args.qk_rope_head_dim
        self.index_topk: int = args.index_topk
        self.q_lora_rank: int = args.q_lora_rank
        self.wq_b = Linear(self.q_lora_rank, self.n_heads * self.head_dim)
        self.wk = Linear(self.dim, self.head_dim)
        self.k_norm = LayerNorm(self.head_dim)
        self.weights_proj = Linear(self.dim, self.n_heads, dtype=torch.get_default_dtype())
        self.softmax_scale = self.head_dim ** -0.5
        self.scale_fmt = args.scale_fmt

        self.register_buffer("k_cache", torch.zeros(args.max_batch_size, args.max_seq_len, self.head_dim, dtype=torch.float8_e4m3fn), persistent=False)
        self.register_buffer("k_scale_cache", torch.zeros(args.max_batch_size, args.max_seq_len, self.head_dim // block_size, dtype=torch.float32), persistent=False)


    def forward(self, x: torch.Tensor, qr: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):
        bsz, seqlen, _ = x.size()
        end_pos = start_pos + seqlen
        q = self.wq_b(qr)
        q = rearrange(q, 'b s (h d) -> b s h d', d=self.head_dim)
        q_pe, q_nope = torch.split(q, [self.rope_head_dim, self.head_dim - self.rope_head_dim], dim=-1)
        q_pe = apply_rotary_emb(q_pe, freqs_cis)
        q = torch.cat([q_pe, q_nope], dim=-1)
        k = self.wk(x)
        k = self.k_norm(k)
        k_pe, k_nope = torch.split(k, [self.rope_head_dim, self.head_dim - self.rope_head_dim], dim=-1)
        k_pe = apply_rotary_emb(k_pe.unsqueeze(2), freqs_cis).squeeze(2)
        k = torch.cat([k_pe, k_nope], dim=-1)
        q = rotate_activation(q)
        k = rotate_activation(k)
        q_fp8, q_scale = act_quant(q, block_size, self.scale_fmt)
        k_fp8, k_scale = act_quant(k, block_size, self.scale_fmt)
        self.k_cache[:bsz, start_pos:end_pos] = k_fp8
        self.k_scale_cache[:bsz, start_pos:end_pos] = k_scale
        weights = self.weights_proj(x) * self.n_heads ** -0.5
        weights = weights.unsqueeze(-1) * q_scale * self.softmax_scale
        index_score = fp8_index(q_fp8.contiguous(), weights, self.k_cache[:bsz, :end_pos].contiguous(), self.k_scale_cache[:bsz, :end_pos].contiguous())
        if mask is not None:
            index_score += mask
        topk_indices = index_score.topk(min(self.index_topk, end_pos), dim=-1)[1]
        topk_indices_ = topk_indices.clone()
        dist.broadcast(topk_indices_, src=0)
        assert torch.all(topk_indices == topk_indices_), f"{topk_indices=} {topk_indices_=}"
        return topk_indices
```

## 3\. 训练

不同与NSA的原生训练，DeepSeek-V3.2-Exp的DSA是基于V3.1-Terminus续训的。

### 3.1 Continued Pre-Training

在 DeepSeek-V3.1-Terminus 的基础上，**通过两个阶段的训练，将模型从 dense attention 平稳迁移至 DSA 架构下的 sparse attention**，且保持性能稳定。

持续预训练包含两个阶段：

1.  **Dense Warm-up Stage**
    
2.  **Sparse Training Stage**
    

两个阶段都使用与 Terminus 相同的 128K 长上下文训练数据。

**第一阶段：Dense Warm-up Stage（初始化 Lightning Indexer）**

这是一个简短的预热阶段，旨在初始化 Lightning Indexer。在此阶段，模型仍采用原始的 dense attention，同时冻结除 Lightning Indexer 之外的所有参数。训练目标是使 Indexer 的打分输出与主注意力机制中的打分分布保持一致。

具体的：对于第 t 个query token，首先通过对主注意力的所有头的attention score进行求和。然后在序列维度上进行 L1 归一化，以产生一个目标分布：

$p_{t,:} = \text{L1-normalized}\left(\sum_{h=1}^{H} \text{AttentionScore}^{(h)}_{t,:}\right)\tag3$

Indexer 产生自身的评分 $I_{t,:}$ ，对其进行 softmax，得到预测分布：

$\hat{p}_{t,:} = \text{Softmax}(I_{t,:})\tag4$使用 **KL 散度** 衡量两者之间的差异，作为 Indexer 的训练目标：

$\mathcal{L}^I = \sum_t \text{KL}(p_{t,:} || \hat{p}_{t,:})\tag5$在此Warm-up Stage，学习率设为 10⁻³ 。仅训练indexer 1000 步，每步包含 16 个 128K 长度的序列，总计处理 2.1 B tokens。

**第二阶段：Sparse Training Stage（引入 Sparse Attention）**

在此阶段引入细粒度 token 选择机制，并优化**所有模型参数**，以使模型适应 DSA 的稀疏模式。在此阶段，仍然保持将indexer输出与主注意力分布对齐，但只考虑被选中的token集 $S_t = \left\{ s \mid I_{t,s} \in \text{Top-k}(I_{t,:}) \right\}$ :

$\mathcal{L}^I = \sum_t \text{KL}(p_{t,S_t} ||\hat{p}_{t,S_t})\tag6$需要注意的是，将 indexer 的输入从计算图中分离出来，以便进行独立优化。Indexer的训练信号仅来自 $\mathcal{L}^I$ ，而主模型的优化仅依据语言建模损失。

Sparse Training Stage，学习率设为 7.3 × 10⁻⁶ ，并为每个query token选择2048个 key-value token。同时训练主模型和indexer 15000 步，每步包含 480个128K长度的序列，总计处理 943.7 B tokens。

### 3.2 Post-Training

DeepSeek-V3.2-Exp 的后训练阶段在流程、算法和数据上基本延续了 V3.1-Terminus，但在全程中采用了稀疏注意力机制。其核心方法包括：

-   **专家蒸馏（Expert Distillation）**：针对数学、编程、推理、Agent 等多个专业领域，先分别微调出多个专家模型，然后利用这些专家生成高质量的领域数据，用于蒸馏出一个综合能力更强的统一模型。
    
-   **混合强化学习训练（[Mixed RL Training](https://zhida.zhihu.com/search?content_id=263957132&content_type=Article&match_order=1&q=Mixed+RL+Training&zhida_source=entity)）**：采用 GRPO（Group Relative Policy Optimization）算法，将推理、Agent 能力和人类偏好对齐等多个目标统一纳入一个强化学习阶段，有效平衡各领域表现，同时避免多阶段训练常见的灾难性遗忘问题。

## 4\. 模型效果评估

### 4.1 多任务 Benchmark 对比

对比了两个模型在多个关键任务上的表现，涵盖 **通用能力、搜索 agent、编程、代码 agent 和数学推理** 五大类任务。

![](https://pic4.zhimg.com/v2-cc910155128cbf6d526d85f8b743cc75_1440w.jpg)

大部分任务持平或略优（如代码、搜索、数学），某些 reasoning-heavy benchmark（如 GPQA、HLE）略降。

### 4.2 强化学习训练曲线对比

分别在两个任务上绘制训练步数 vs 准确率与生成 token 数：

![](https://pic1.zhimg.com/v2-0f4470bc006a98d5efaab9ce1d93a680_1440w.jpg)

图5: 强化训练曲线对比

BrowseComp 训练曲线：

-   准确率和 token 输出趋势与 V3.1 完全一致
    
-   说明：**DSA 引入后训练非常平稳**
    

SWE Verified 训练曲线：

-   两个模型都表现出稳步上升趋势
    
-   V3.2-Exp 在中后期 token 更节省，但性能未下降

### 4.3 推理成本分析（Inference Cost）

|模块|Attention 复杂度|
|---|---|
|V3.1-Terminus（MLA）|L^2|
|V3.2-Exp（DSA）主模型|✅ LK|
|Lightning Indexer（DSA）|L^2，但计算开销很低|

推理成本对比如下：

![](https://pica.zhimg.com/v2-b23e127d5a45db8025db3b4d7bbefd0a_1440w.jpg)

图6: 推理成本对比

Prefilling 成本：

-   在 token 位置增加的情况下：
    

-   V3.2 成本始终低于 V3.1
    
-   说明 DSA 即使在 Prefilling 阶段也能节省开销（模拟 MHA 模式）
    

Decoding 成本：

-   解码阶段节省更明显
    
-   在 128K 长上下文下，**单位 token 成本最高下降达 60%~70%**

## 参考

[DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attention](https://link.zhihu.com/?target=https%3A//github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf)

[https://mp.weixin.qq.com/s/s9Ie27UCzBVG2e52oRR10Q](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/s9Ie27UCzBVG2e52oRR10Q)

[Deepseek 技术解读：MLA（Multi-Head Latent Attention）](https://zhuanlan.zhihu.com/p/1945869053592866971)

[DeepSeek-V3.2-Exp源码逻辑与模型结构](https://zhuanlan.zhihu.com/p/1956798179065443580?share_code=12jy5F1TKcvsN&utm_psn=1959921424132186658)

[【手撕 DSA】 DeepSeek-V3.2 的 Sparse Attention 比 NSA 好在哪？](https://zhuanlan.zhihu.com/p/1957032283270812718)

[DeepSeek V3.2 稀疏注意力(DSA)机制实现解读](https://zhuanlan.zhihu.com/p/1956129922075567314)

[DeepSeek-V3.2-Exp in vLLM: Fine-Grained Sparse Attention in Action](https://link.zhihu.com/?target=https%3A//blog.vllm.ai/2025/09/29/deepseek-v3-2.html)

[Deepseek R1/V3模型结构总览](https://zhuanlan.zhihu.com/p/1901297297331029667)
