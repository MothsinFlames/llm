---
created: 2025-10-28T11:06:55 (UTC +08:00)
tags: [LLM,信息论,OpenAI]
source: https://zhuanlan.zhihu.com/p/687278237
author: 关于作者紫气东来吾生也有涯，而知也无涯上海交通大学 工学硕士回答文章关注者已关注发私信
---

# LLM：从信息论的角度解释 scaling law

> ## Excerpt
> 1943年，当亚欧大陆东西两端都陷入焦灼的战火时，远隔大洋的美国，有两个年轻人因为这场战争走到了一起，他们就是：31 岁的图灵(Alan Turing，1912-1954) 和 27 岁的香农(Claude Shannon，1916-2001)。这也许是人…

---
1943年，当亚欧大陆东西两端都陷入焦灼的战火时，远隔大洋的美国，有两个年轻人因为这场战争走到了一起，他们就是：31 岁的图灵(Alan Turing，1912-1954) 和 27 岁的香农(Claude Shannon，1916-2001)。这也许是人类最精彩的会面之一，彼时他们分别拿着“矛”和“盾”：当时图灵在当是破译了包括希特勒通话在内的多项德军秘密通讯的密码学破译专家，而香农当时的工作是通过数学方法证明“Ｘ系统”——这是美国总统罗斯福到英国首相丘吉尔之间的加密通讯系统。

尽管因为保密的原因，二人不能在密码学上进行讨论，但却可以对当时最前沿的话题，包括机器思维、信息科学、计算理论、智能等进行讨论，事实上这些问题至今仍然是最前沿的研究领域。在这次会面不久，二人就在各自的领域取得了巨大的成就：

-   1948年，香农发表题为《通信的数学理论》的论文，正式开创了科学的信息论
-   1950年，图灵发表《计算机器和智能》，提出了“[图灵测试](https://zhida.zhihu.com/search?content_id=240858481&content_type=Article&match_order=1&q=%E5%9B%BE%E7%81%B5%E6%B5%8B%E8%AF%95&zhida_source=entity)” ，打开了人工智能的大门

时至今日，尽管模型、数据、算力都发生了翻天地覆的变化，人工智能和信息技术的发展仍然没有超越他们当年讨论的范畴。览前贤，思己任，希望在前辈们建立的大厦基础上，我辈能够薪尽火传，在自己的领域做出各自力所能及的贡献。

## 一、语言模型的信息论基础

考虑生成式的语言模型，其概率估计 通常为每个token的概率的乘积，即

$\begin{aligned} P_{\left(w_1, w_2, \ldots, w_n\right)} & =p\left(w_1\right) p\left(w_2 \mid w_1\right) p\left(w_3 \mid w_1, w_2\right) \ldots p\left(w_n \mid w_1, w_2, . ., w_{n-1}\right) \\ & =\prod_{i=1}^n p\left(w_i \mid w_1, \ldots, w_{i-1}\right) \end{aligned}$

![](https://pic3.zhimg.com/v2-86f63685283f8c9016dc1822b22e4950_1440w.jpg)

有了概率之后，便可以定义更多的量了。

### 1.1 信息熵

假设随机变量 $X$ 在属于集合 $\mathcal{X}$ ，则 $X$ 的熵定义为： $\mathrm{H}(X)=-\sum_{x \in \mathcal{X}} p(x) \log p(x)$

-   当对数底数是 $2$ 时，单位是bit，
-   当对数底数是 $e$ 时，单位是nat(奈特)。

![](https://pic3.zhimg.com/v2-a396396922d45d9e435d5ad25ab34f46_1440w.jpg)

在自然语言领域，令 $b_n = (w_1,w_2,...,w_n)$ ，则熵还可以进一步表示为：

$\begin{aligned} F_N & =-\sum_{b_n} p\left(b_n\right) \log _2 p\left(w_n \mid b_{n-1}\right) \\ & =-\sum_{b_n} p\left(b_n\right) \log _2 p\left(b_n\right)+\sum_{b_{n-1}} p\left(b_{n-1}\right) \log _2 p\left(b_{n-1}\right)\\ & =K_N-K_{N-1} \end{aligned}$

其中 $K_N=-\sum_{b_n} p\left(b_n\right) \log _2 p\left(b_n\right)$

当序列长度无限大的情况下，香农将其定义为该语言的熵，即： $H=\lim _{N \rightarrow \infty} F_N$

根据这个定义，熵是使用无限数量的符号来计算的。在实践中，只能从有限的文本样本中近似经验熵来近似任何语言的熵。

接下来分析一下语言的信息熵的边界问题。任何分布的最小可能熵为零，对于语言来说，当一种语言只有一个符号或者某个符号的概率为 1 时，该语言的熵才能为零，即

$\min \mathrm{H}(\mathrm{P})=-1 * \log (1)=0$

概率分布的熵在均匀分布时最大化，对于共享同一组符号（词汇）的所有语言，当其所有符号以相等概率出现的时候其熵最大，以 $V$ 表示词表大小，则有： $\mathrm{H}(\mathrm{P}) \leq-\mathrm{V} * \frac{1}{\mathrm{~V}} * \log \left(\frac{1}{\mathrm{~V}}\right)=-\log \left(\frac{1}{\mathrm{~V}}\right)=\log (\mathrm{V})$

-   如果词表大小为 27(26个字母+空格)，则 $log27=4.7549$
-   如果词表大小为 42000，则 $log42000=15.3581$

这样我们就确定了语言的信息熵的上下界，接下来探究其单调性问题，假设来自于同一语言数据库（独立同分布）的长度不同的文本，则有：

$\begin{aligned} F_N-F_{N+1} & =-\sum_{b_n} p\left(b_n\right) \log p\left(w_n \mid b_{n-1}\right)+\sum_{b_{n+1}} p\left(b_{n+1}\right) \log p\left(w_{n+1} \mid b_n\right) \\ & =\sum_{b_{n-1}}\left[\sum_{w_n, w_{n+1}} p\left(b_{n+1}\right) \log p\left(w_{n+1} \mid b_n\right)-\sum_{w_n} p\left(b_n\right) \log p\left(w_n \mid b_{n-1}\right)\right] \\ & \geq \sum_{b_{n-1}}\left[\sum_{w_n, w_{n+1}} p\left(b_{n+1}\right) \log p\left(w_{n+1} \mid b_{n-1}\right)-\sum_{w_n} p\left(b_n\right) \log p\left(w_n \mid b_{n-1}\right)\right] \\ & =\sum_{b_{n-1}}\left[\sum_{w_n, w_{n+1}} p\left(b_{n-1}, w_n, w_{n+1}\right) \log p\left(w_{n+1} \mid b_{n-1}\right)-\sum_{w_n} p\left(b_{n-1}, w_n\right) \log p\left(w_n \mid b_{n-1}\right)\right] \\ & =\sum_{b_{n-1}}\left[\sum_{w_{n+1}} \log p\left(w_{n+1} \mid b_{n-1}\right) \sum_{w_n} p\left(b_{n-1}, w_n, w_{n+1}\right)-\sum_{w_n} p\left(b_{n-1}, w_n\right) \log p\left(w_n \mid b_{n-1}\right)\right] \\ & =\sum_{b_{n-1}}\left[\sum_{w_{n+1}} \log p\left(w_{n+1} \mid b_{n-1}\right) p\left(b_{n-1}, w_{n+1}\right)-\sum_{w_n} p\left(b_{n-1}, w_n\right) \log p\left(w_n \mid b_{n-1}\right)\right] \\ & =0 \end{aligned}$

其中最为关键的一步是 $\log p\left(w_{n+1} \mid b_n\right) \geq \log p\left(w_{n+1} \mid b_{n-1}\right)$ ，所表达的含义是随着前文长度的增加，预测的准确性会提高。因此则有：

$\mathrm{F}_1 \geq \mathrm{F}_2 \geq \ldots \geq \lim _{N \rightarrow \infty} \mathrm{F}_N=H(P)$

这意味着随着数据量的增加，其理论上的经验熵值会逐渐减小，并逐步接近最真实的语言熵。

下表展示了几个主要数据集的经验熵，可以看到满足以上发现的规律，另外可以看到，大多数值仍然位于香农 1950 年估计的范围内，可见其超越时空的远见卓识。

![](https://pic1.zhimg.com/v2-c373e2c1838f4536f05120e761f3629a_1440w.jpg)

### 1.2 [交叉熵](https://zhida.zhihu.com/search?content_id=240858481&content_type=Article&match_order=1&q=%E4%BA%A4%E5%8F%89%E7%86%B5&zhida_source=entity)，[相对熵](https://zhida.zhihu.com/search?content_id=240858481&content_type=Article&match_order=1&q=%E7%9B%B8%E5%AF%B9%E7%86%B5&zhida_source=entity)

由于语言中缺乏无限数量的文本，该语言的真正分布是未知的。语言模型旨在从示例文本中学习接近语言经验分布的分布。为了测量两个分布的“接近度”，通常使用交叉熵，通常作为模型的 loss 。数学上，预测分布 Q 相对于真实分布 P 的交叉熵定义如下：

$\begin{aligned} \mathrm{CE}(\mathrm{P}, \mathrm{Q}) & =-\sum_x P(x) \log Q(x) \\ & =-\sum_x P(x)[\log P(x)+\log Q(x)-\log P(x)] \\ & =-\sum_x P(x)\left[\log P(x)+\log \frac{Q(x)}{P(x)}\right] \\ & =-\sum_x P(x) \log P(x)-\sum_x P(x) \log \frac{Q(x)}{P(x)} \\ & =H(P)+D_{K L}(P \| Q) \end{aligned}$

其中 $D_{K L}(P \| Q)$ 即 $Q$ 对 $P$ 的 KL 散度（亦称相对熵）， 其表征了两个分布间的差异大小。下面来尝试解读一下上式的含义：

-   由于 $D_{K L}(P \| Q) \geq 0$ ，则 $\mathrm{CE}(\mathrm{P}, \mathrm{Q}) \geq \mathrm{H}(\mathrm{P})$ ，则作为 loss 时，交叉熵损失可以可以看作两部分：一部分是训练预料本身的熵，另一部分是模型拟合的损失；
-   从信息论的角度来说， $\mathrm{H}(\mathrm{P})$ 表示对某个语料库所代表的 $P$ 分布进行编码所需要的平均位数；
-   从信息论的角度来说， $D_{K L}(P \| Q)$ 表示使用 $Q$ 的编码对 $P$ 分布进行编码所需要的额外位数

需要注意的是，由于经验熵 $\mathrm{H}(\mathrm{P})$ 是不可优化的，当我们以最小化交叉熵损失为目标来训练语言模型时，真正的目标是最小化分布的 KL 散度，这是语言模型从语言的经验分布学习到的。

### 1.3 PPL 与 BPC

Perplexity (PPL, [困惑度](https://zhida.zhihu.com/search?content_id=240858481&content_type=Article&match_order=1&q=%E5%9B%B0%E6%83%91%E5%BA%A6&zhida_source=entity)) 是衡量概率分布或概率模型对样本的预测程度。在数学上，语言模型的困惑度定义为： $\operatorname{PPL}(P, Q)=2^{\mathrm{CE}(P, Q)}$

直观地说，困惑度可以理解为不确定性的度量。语言模型的困惑度可以看作是预测下一个符号时的困惑程度。考虑一个熵为 3 比特的语言模型，其中每个比特编码两种概率相等的可能结果。这意味着在预测下一个符号时，该语言模型必须在可能的 $2^3$ 选项中做出选择。因此，我们可以认为这个语言模型的困惑度为 8。

Bits-per-character (BPC， 字符位数) 是最近语言模型中经常报告的另一个指标。它精确地测量它所命名的数量：对字符进行编码所需的平均位数。这也来源于香农对语言熵的解释：

> _if the language is translated into binary digits (0 or 1) in the most efficient way, the entropy is the average number of binary digits required per letter of the original language._

根据这个定义，熵是 BPC 的平均数。在实践中，如果每个人都使用不同的词表或模型，则很难比较不同模型的结果。为了保持一致性，使用 BPC 是一个比较好的选择。

实际上，BPC 建立了压缩的下限。如果文本的 BPC 为 1.2，则不能将其压缩到每个字符小于 1.2 位。例如，如果文本有 1000 个字符（如果每个字符使用 1 个字节表示，则大约 1000 个字节），则其压缩版本至少需要 1200 位或 150 个字节。

BPC 特定于字符级语言模型。当有单词级语言模型时，则通常采用 bits-per-word (BPW, 单词位数) ，即编码单词所需的平均比特数。关于二者的关系，Graves 提供了一个简单的转化公式：如果平均而言，一个单词需要 $m$ 比特来编码，而一个单词包含 $l$ 个字符，那么它应该需要平均 $m/l$ 比特来编码一个字符。例如，一个单词在数据集中平均有 5.6 个字符，因此单词级别的困惑度计算公式为 $2^{5.6*BPC}$ 。

因此，在讨论语言的熵或者困惑度时，应该要特别说明是单词(word)、字符(character)还是子单词(subword)级别的。

### 1.4 条件熵与信息增益

在信息论中，条件熵描述了在已知第二个随机变量 ${\displaystyle X}$ 的值的前提下，随机变量 ${\displaystyle Y}$ 的信息熵还有多少。基于

${\displaystyle X}$ 条件的 ${\displaystyle Y}$ 的信息熵，用 ${\displaystyle \mathrm {H} (Y|X)}$ 表示**条件熵(Conditional entropy)** 为：

$\begin{aligned} H(Y | X)&=\sum_{x} p(x) H(Y | X=x)\\ &=- \sum_{x} p(x) \sum_{y} p(y | x) \log p(y | x)\\&=-\sum_{x } \sum_{y} p(x, y) \log p(y | x) \\&=-E \log p(Y | X) \end{aligned}$

另外，如果随机变量 $(X, Y) \sim p(x, y)$ ，则其**联合熵(Joint entropy)** 为：

$H(X; Y)=-\sum_{x } \sum_{y} p(x, y) \log p(x, y)=-E \log p(X, Y)$

联合熵与条件熵有如下关系成立： $H(X; Y)=H(X)+H(Y | X)$ ，证明如下：

$\begin{equation} \begin{aligned} H(X;Y) &=-\sum_{x} \sum_{y} p(x, y) \log p(x, y)=-\sum_{x} \sum_{y} p(x, y) \log p(x) p(y \mid x) \\ &=-\sum_{x} \sum_{y} p(x, y) \log p(x)-\sum_{x} \sum_{y} p(x, y) \log p(y \mid x) \\ &=-\sum_{x} p(x) \log p(x)+H(Y \mid X) \\&=H(X)+H(Y \mid X) \end{aligned} \end{equation}$

**互信息(Mutual information)** 是一个随机变量包含另一个随机变量信息量的度量，也可以说是在给定一个随机变量的条件下，原随机变量的不确定性的减少量，根据熵的连锁规则，有：

$\mathrm{H}(\mathrm{X};\mathrm{Y})=\mathrm{H}(\mathrm{X})+\mathrm{H}(\mathrm{Y} \mid \mathrm{X})=\mathrm{H}(\mathrm{Y})+\mathrm{H}(\mathrm{X} \mid \mathrm{Y})$

据此定义互信息： $I(X ; Y)=H(Y)-H(Y | X)=H(X)-H(X | Y)$

![](https://picx.zhimg.com/v2-b16c2993afbbaa1ea51ebd9855b17f63_1440w.jpg)

信息增益(Info Gain)指的是经验熵和经验条件熵的差值，经验熵和经验条件熵是通过样本进行极大似然估计得出来的，是一个估计值的差。信息增益是互信息的无偏估计。

### 1.5 [率失真理论](https://zhida.zhihu.com/search?content_id=240858481&content_type=Article&match_order=1&q=%E7%8E%87%E5%A4%B1%E7%9C%9F%E7%90%86%E8%AE%BA&zhida_source=entity)（Rate Distortion theory）

在通信系统中，冗余度压缩编码以及信道编码，目的都是对信息进行可靠无差错的传输，信息熵没有变化，也就是是保熵的。但是在实际场景中，没有必要把信息的所有内容都保留下来的，即要进行有损压缩。

失真度量（失真函数）是这样一个映射： $d: X \times \hat{X} \rightarrow R^{+}$

它将源字母-恢复字母对映射到一个非负的实数，来代表失真多少。当原始字母集与映射的字母集是离散的，可以用一个矩阵来表示失真的大小，而在连续的情况下，这就是一个函数。失真函数永远都是非负的。如果失真是有限的，则函数为有界函数。

![](https://pic3.zhimg.com/v2-61f9657d7a97704ed16dbc4fdcb47324_1440w.jpg)

率失真编码器和解码器。编码器对序列进行编码。然后将编码序列馈送到输出序列的解码器。目的是最小化原始序列和重建序列之间的失真。

失真函数在输入输出联合空间中取统计平均为 $D \triangleq \sum_{x_i, \hat{x}_j} p\left(x_i\right) q\left(\hat{x}_j \mid x_i\right) d\left(x_i, \hat{x}_j\right) \triangleq E[d(X, \hat{X})]$ 在给定信源分布与转移概率分布时，上式为信道传输失真总体的平均度量。

现在想做的事情是， $X$ 经过信道之后，得到 $\hat{X}$ ，我们希望 $I(X,\hat{X})$ 尽可能小，以此来压缩比特的传输。如果互信息为0，则相当于不压缩。压缩下有个前提，就是对失真函数的约束。

![](https://picx.zhimg.com/v2-176ef1947c8b77f2067d02c32259bef1_1440w.jpg)

**率失真函数的定义**：针对信源 $X$ 和失真度量 $d\left(x, \hat{x}\right)$ ，信息的率失真函数 $R(D)$ 定义为： $R(D)=\min _{q(\hat{x} \mid x): \sum_{x, \hat{x}} p(x) q(\hat{x} \mid x) d(x, \hat{x}) \leq D} I(X ; \hat{X})$由于无法操作信源，能做的是最小化针对所有可能的条件分布 $q(\hat{x}|x)$ ，并且使得联合分布满足要求的失真约束。这个函数求得的最小值，并不一定是可操作得到的，只是数学上的一个最小值。

在均方失真度量下，高斯信源的率失真函数是： $R(D)=\left\{\begin{array}{cc} \frac{1}{2} \log \frac{\sigma^2}{D}, & 0 \leq D \leq \min \{p, 1-p\} \\ 0, & D>\min \{p, 1-p\} \end{array}\right.$ 图像如下，这个结果告诉我们，“不存在在灰色区域之外运行的压缩系统”。实际压缩系统越接近红色（下限）边界，其性能就越好。作为一般规则，此边界只能通过增加编码块长度参数来实现。

![](https://picx.zhimg.com/v2-37586c088cead7903fbb35352b46e435_1440w.jpg)

上式的证明过程如下： $\begin{aligned} I(X ; \hat{X}) & =h(X)-h(X \mid \hat{X}) \\ & =\frac{1}{2} \log 2 \pi e \sigma^2-h(X-\hat{X} \mid \hat{X}) \\ & \geq \frac{1}{2} \log 2 \pi e \sigma^2-h(X-\hat{X}) \\ & \geq \frac{1}{2} \log 2 \pi e \sigma^2-h\left(N\left(0, E(X-\hat{X})^2\right)\right) \\ & \geq \frac{1}{2} \log 2 \pi e \sigma^2-\frac{1}{2} \log 2 \pi e D \\ & =\frac{1}{2} \log \frac{\sigma^2}{D} \end{aligned}$

好了，在上一节不厌其烦地使用信息论对语言进行定义和分析后，本节讲尝试使用以上理论对 scaling law，由于 scaling law 的内涵比较深刻和复杂以及便于理解，本节将只在定性层面进行解释，更加严格的分析将放在最后一节。

### 2.1 关于 loss 的解释

scaling law 的最核心部分实际上是对于 loss 的估计和判断，即有如下经验公式：

$\mathcal{L}=\mathcal{L}_0+\lambda Y^\gamma$其中 $\mathcal{L}_0$ 表示无法通过训练减小的损失， $Y$ 表示计算量， $\lambda，\gamma$ 为系数。

![](https://pic2.zhimg.com/v2-59c92687a6d26f3009b5111e2ad40171_1440w.jpg)

下边来使用上节的理论来解释以上经验公式，由于在当前模型训练都采用的是交叉熵损失，即

$\mathcal{L}=\mathrm{CE}(\mathrm{P}, \mathrm{Q}) =H(P)+D_{K L}(P \| Q)$ 然后我们可以不严谨地发现对应关系，即：

-   将 $\mathrm{H}(\mathrm{P})$ 看做 $\mathcal{L}_0$ ，即该部分是语言本身的熵，与模型及训练方法无关
-   将 $D_{K L}(P \| Q)$ 看做 $\lambda Y^\gamma$ ，该部分与模型与数据均相关，且关系如下：

-   如果数据量 $D$ 不变，增大参数量 $N$ ，由于拟合能力更强，则 $D_{K L}(P \| Q)$ 变小
-   如果参数量 $N$ 不变，增加独立同分布数据量 $D$ ，则更利于模型拟合，则 $D_{K L}(P \| Q)$ 变小
-   如果同时增大数据量 $D$ 和参数量 $N$，则原理同上

由此，比较定性地解释了 loss 的 scaling 的规律，实际上这种规律有更加准确的经验公式：

$L(N, D)=E+\frac{A}{N^{0.34}}+\frac{B}{D^{0.28}}, \quad E=1.69, A=406.4, B=410.7$

这里还可以顺便回答一些问题，

**问题一：** 为什么 scaling law 在 [Transformer 模型](https://zhida.zhihu.com/search?content_id=240858481&content_type=Article&match_order=1&q=Transformer+%E6%A8%A1%E5%9E%8B&zhida_source=entity)上才起作用？为什么不是 LSTM，RWKV，Mamba 等 RNN 系的模型？

一个解释的角度就是，因为 Transformer 的 self-attention 结构可以有效捕获长文本之间的依赖关系，可以有效拟合语言的真实分布。而 RNN 系的模型，由于衰减因子的存在，无法在较长的范围内拟合语言的真实分布，即 RNN 系模型的 $D_{K L}(P \| Q)$ 更大。另一个原因则是工程上的，即 RNN 系模型无法实现高效的并行化训练，因此能够参与训练的数据量 $D$ 更小，进一步导致了 $D_{K L}(P \| Q)$ 更大。

**问题二：** 为什么清洗数据很重要？

在一段正常的文本里加入一些乱码，会显著增加熵值（因为乱码是无法被准确预测的），即相当于增加了系统固定的 $\mathrm{H}(\mathrm{P})$ 。同时，噪声数据和重复数据也会使训练数据的分布偏离真实的语言分布，进而导致拟合过程的 $D_{K L}(P \| Q)$ 变大，由此会影响最终的训练效果。

**问题三：** 词表大小 (vocab size) 如何影响训练效果？

根据第一节的推导，词表大小决定了信息熵 $\mathrm{H}(\mathrm{P})$ 的上界。同时随着词表的增大，每个 token 出现的概率减小，即 $\mathrm{H}(\mathrm{P})$ 增大，这会导致 loss 的增大（如下图所示），但是增大词表又会提高编码效率，那么该如何进行取舍呢？

考虑影响 loss 的另一个因素 $D_{K L}(P \| Q)$ ，在模型规模不大时，即使增大词表，也有足够的数据使其训练充分，则大词表在一定范围内是有收益的。但是当模型规模过大时，增大词表时，达到同样拟合效果的数据需要更多，周期也更长，其开销将会大于收益。

![](https://pic3.zhimg.com/v2-84a15ffb52d0a999456da6234e0845f6_1440w.jpg)

### 2.2 如何确定 N，D 的相对关系

在讨论数据量 $D$ 和参数量 $N$ 的关系之前，需要再回顾一下计算量 $C$ 与二者的关系，即： $C\approx 6ND$ 该关系的由来笔者在之前的文章中进行过详细的推导，有兴趣可以查看，在此不予赘述。

事实上，上式的系数 6 在模型规模较大的时比较接近真实值，在模型较小时偏差较大，因此 [Deepseek LLM](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2401.02954.pdf) 将该式描述为 $C\approx MD$ ，其中典型 $M，N$ 的计算方式如下：

$\begin{aligned} 6 N_1 & =72 n_{\text {layer }} d_{\text {model }}^2 \\ 6 N_2 & =72 n_{\text {layer }} d_{\text {model }}^2+6 n_{\text {vocab }} d_{\text {model }} \\ M & =72 n_{\text {layer }} d_{\text {model }}^2+12 n_{\text {layer }} d_{\text {model }} l_{\text {seq }} \end{aligned}$ 在不同模型规模下，其关系如下表所示： $\begin{array}{cccc|ccc|cc} \hline n_{\text {layers }} & d_{\text {model }} & n_{\text {vocab }} & l_{\text {seq }} & N_1 & N_2 & M & \frac{6 N_1}{M} & \frac{6 N_2}{M} \\ \hline 8 & 512 & & & 25.2 \mathrm{M} & 77.6 \mathrm{M} & 352 \mathrm{M} & 0.43 & 1.32 \\ 12 & 768 & & & 84.9 \mathrm{M} & 164 \mathrm{M} & 963 \mathrm{M} & 0.53 & 1.02 \\ 24 & 1024 & & & 302 \mathrm{M} & 407 \mathrm{M} & 3.02 \mathrm{~B} & 0.60 & 0.81 \\ 24 & 2048 & 102400 & 4096 & 1.21 \mathrm{~B} & 1.42 \mathrm{~B} & 9.66 \mathrm{~B} & 0.75 & 0.88 \\ 32 & 4096 & & & 6.44 \mathrm{~B} & 6.86 \mathrm{~B} & 45.1 \mathrm{~B} & 0.85 & 0.91 \\ 40 & 5120 & & & 12.6 \mathrm{~B} & 13.1 \mathrm{~B} & 85.6 \mathrm{~B} & 0.88 & 0.92 \\ 80 & 8192 & & & 64.4 \mathrm{~B} & 65.3 \mathrm{~B} & 419 \mathrm{~B} & 0.92 & 0.94 \\ \hline \end{array}$ 在弄清楚 $C$ 与 $N, D$ 的关系后，接下来就需要进一步研究 $N,D$ 之间的关系，即分配关系。

在此不加证明地引用论文 [An Information-Theoretic Analysis of Compute-Optimal Neural Scaling Laws](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2212.01365.pdf) 中使用信息论方法，所得到的简化后的 $N,D$ 的优化解： $N^* =\mathcal{O}\left(\frac{\sqrt{C}}{d\log \sqrt{\frac{C}{d^2 K^2}}}\right)$$D^*=\mathcal{O}\left(\sqrt{C} \log \sqrt{\frac{C}{d^2 K^2}}\right)$ 其中 $K$ 是超参数，则 $\frac{D^*}{N^*} \sim \frac{\sqrt{C}}{K}$ ，在 Chichilla 论文里该比例的经验值是 20。

![](https://pica.zhimg.com/v2-326bb32e6f5480bcff3a165ab1003f6a_1440w.jpg)

### 2.3 如何指导预训练过程的超参数的设置

有了以上的结论之后，我们至少可以知道非常重要的几点信息，包括但不限于：

-   在训练之前预测模型可以达到的大致效果(loss)
-   在预算确定的情况下，如何分配参数量和数据量

即使在完成以上两步之后还有一个重要工作要做，即：如何指导训练过程？其中和训练相关的最主要的两个参数就是 Batch size 和 Learning rate。

通常而言，在模型确定的情况下，增大 Batch size ，会使数据更加接近真实分布，进而使得梯度估计的噪声较小，因此可以允许更大的 Learning rate。这个关系在论文 [An Empirical Model of Large-Batch Training](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1812.06162.pdf) 也得到了验证。

![](https://pic4.zhimg.com/v2-6896787dd0f4ce14e57ac89c6628b2a5_1440w.jpg)

那么如果模型参数量增加了，Learning rate 应该怎么调整呢？通常应该调小，理由如下：

-   梯度下降的稳定性： 在梯度下降过程中，学习率决定了参数更新的步长。对于大型模型，由于参数空间更大，为了确保更新步长的稳定性，避免在最优解附近震荡或发散，通常需要选择更小的学习率。较小的学习率可以帮助模型在复杂的参数空间中更细致地搜索，从而找到更好的解。
-   局部最小值和鞍点： 深度学习中的损失函数通常具有许多局部最小值和鞍点。较大的模型意味着更复杂的损失，因此更容易遇到难以跨越的鞍点。较小的学习率有助于模型在这些区域中缓慢而稳定地更新，而不是大步跳跃，可能会更容易找到全局最小值。
-   参数更新的累积效应： 在大型模型中，由于参数数量众多，即使是很小的学习率也可能导致显著的参数更新。因此，较小的学习率有助于控制参数更新的累积效应，避免过度更新导致的性能下降。

事实上，在 scaling law 中也有同样的经验的发现，以下是 Deepseek LLM 的经验值： $\begin{aligned} \eta_{\mathrm{opt}} & =0.3118 \cdot C^{-0.1250} \\ B_{\mathrm{opt}} & =0.2920 \cdot C^{0.3271} \end{aligned}$

![](https://picx.zhimg.com/v2-611207d20d8c6f59b1f6b0666cc89ae3_1440w.jpg)

## 三、更加严格的幂律的证明

### 3.1 信息压缩与率失真

OpenAI 的成员 Jack Rae 曾在 Stanford MLSys Seminar 的访谈时进行了一个名为 Compression for AGI 的主题分享，其核心观点为：**AGI 基础模型的目标是实现对有效信息最大限度的无损压缩。** 即 LLM = Compression （GPT 的 Next Token Prediction 本质上是对训练数据的无损压缩）。通过论证压缩即智能，GPT 的训练过程是对数据的无损压缩，从而证明了 GPT 拥有智能。下面通过有损信息压缩的角度一探究竟：

将一组向量 $V=(v_1,v_2,\dots ,v_n) \in \mathbb{R}^{n\times d}$ 进行有损编码为二进制，然后在失真 $\mathbb{E}\left[\left\|v_i-\hat{v}_i\right\|^2\right] \leq \varepsilon^2$ 的范围内恢复成原向量。编码后的序列长度为 $L(V): \mathbb{R}^{n \times d} \rightarrow \mathbb{Z}_{+}$ 。

对于0 均值的高斯分布 $\mathcal{N}(0, \Sigma)$ 的率失真的近似值（每个向量所需的平均比特数）为 $R(V) \doteq \frac{1}{2} \log _2 \operatorname{det}\left(I+\frac{d}{\varepsilon^2 n} V V^T\right)$ 其中 $\hat\Sigma = \frac{1}{n} V V^T$ 表示协方差矩阵 $\Sigma$ 的估计值。

对于 $n$ 个向量，需要 $nR(V)$ 比特，由于最优编码本对数据 $V$ 是自适应的，我们还必须用额外的 $dR(V)$ 比特来表示，因此总体编码长度为 $L(V)=(n+d)R(V) = \frac{n+d}{2} \log \operatorname{det}\left(I+\frac{d}{\varepsilon^2 n} V V^T\right)$根据第一节的介绍，熵 $\mathrm{H}(X)=-\sum_{x \in \mathcal{X}} p(x) \log p(x)$ ，但是在真实场景下，真实分布 $p(x)$ 是很难获取的。另一方面，从概念上讲熵等同于无损编码数据所需的最小位数，因此可以使用最小无损编码长度来表示熵。而对于连续随机变量，无损编码是无法实现的，因此利用有损数据编码中的编码长度作为连续随机变量的熵。

假设 $V$ 有r个大小相等的奇异值（r-子空间），则归一化熵可以计算如下 $\frac{\mathrm{L}(V)}{\frac{d+n}{2} d \log \left(1+\frac{1}{\epsilon^2}\right)}=\left(\frac{r}{d} \log \left(1+\frac{\frac{1}{\epsilon^2}}{\frac{r}{d}}\right)\right) / \log \left(1+\frac{1}{\epsilon^2}\right) .$ 根据[Large Language Model Evaluation via Matrix Entropy](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2401.17139) 的研究， $\frac{r}{d}$ 有一个很好的确定性估计，可以得到下面的图像关系，即大致满足“幂率”关系。

![](https://pic3.zhimg.com/v2-25a8b6a63a46eb275565303a19001ef8_1440w.jpg)

### 3.2 幂律的证明

接下来将讨论如何从信息理论上推导出缩放定律。LLM 具有通过扩展句子片段生成完整句子的强大能力，假设预训练数据集为 $\mathcal{D}$ ，然后以 $\mathcal{D}$ 中的句子（段落）作为基本元素，并将其集合表示为 $\mathcal{X}$ 。

假设任何语料库中都有 $M$ 种可能理解的技能。假定语料库中的每个文本片段都会链接到所有相关的技能，这样就会形成一个双方图，即技能图。

![](https://pic3.zhimg.com/v2-e9abb3f5a7e440620c59598198f68750_1440w.jpg)

假设每种技能 $y_k$ 的度数与 $k^{-(\alpha +1)}$ 成正比，其中我们按照技能在双方图中的度数进行排序。因此，通过对度进行归一化处理，技能分布遵循幂律，即 $p_{\text {skill }}\left(y_k\right)=\frac{k^{-(\alpha+1)}}{Z_\alpha}(\alpha>0)$ 其中 $Z_\alpha = \Sigma_{k=1}^{M}k^{-(\alpha+1)}$ 为定值。

既然我们希望 LLM 能够理解（学习）技能，那么理解技能意味着什么呢？直观地说，一个句子的描述将是一个（二进制）字符串，如果我们真的 "理解 "了它，那么它的长度应该尽可能短，即进行了充分地“压缩”。

在信息论中，代码 $\mathcal{C}$ 是将 $\mathcal{X}$ 映射到二进制字符串空间（码字空间）的函数，其中没有一个码字 $\mathcal{C}(x)$ 是另一个码字 $\mathcal{C}(x’)$ 的前缀。代码 $\mathcal{C}$ 的期望码长是分布 ${p(x)}_{x\in \mathcal{X}}$ 下二进制符号的平均数量

$l(\mathcal{C})=\sum_{x \in \mathcal{X}} p(x)|\mathcal{C}(x)|$ 

根据信息论的标准结果，任何预期代码长度都大于 $H(X)$ 。

假设密度函数为 $X ∼ p_θ$ 。考虑 LLM 在训练过程中的行为，对于随机初始化模型, 假定所有技能的条件熵都是常数 $B$ ，对于任何技能 $y$ ，特定的 LLM $p_θ$ 只有在 $H(X|y) = C$ 时才能理解该技能，其中 $C < B$ 。

对于所有技能，我们根据它们的出现概率来排列它们的“难度”。也就是说，对于任意的技能 $y_1$ 和 $y_2$ ，如果 $p_{skill}(y_1) \geq p_{skill}(y_2)$ ，我们称技能 $y_1$ 比 $y_2$ 更容易。假设模型从最简单到最难依次学习技能，只有它完全理解了一种技能，它才会学习下一种技能。

假设一个 LLM 掌握 $n$ 种技能， $X ∼ p_\theta，Y ∼ p_{skill}$ ，那么条件熵 $H(X|Y )$ 与 $n$ 服从幂律关系 

$\begin{aligned} & \mathrm{H}(X \mid Y) \\ = & \sum_y p_{\text {skill }}(y) \mathrm{H}(X \mid y) \\ = & \sum_{i=1}^n p_{\text {skill }}\left(y_i\right) \mathrm{H}\left(X \mid y_i\right)+\sum_{i>n} p_{\text {skill }}\left(y_i\right) \mathrm{H}\left(X \mid y_i\right) \\ = & \sum_{i=1}^n p_{\text {skill }}\left(y_i\right) C+\sum_{i>n} p_{\text {skill }}\left(y_i\right) B \\ = & \sum_{i=1}^n p_{\text {skill }}\left(y_i\right) C+\sum_{i>n} p_{\text {skill }}\left(y_i\right) C-\sum_{i>n} p_{\text {skill }}\left(y_i\right) C+\sum_{i>n} p_{\text {skill }}\left(y_i\right) B \\ = & C+\sum_{i>n} p_{\text {skill }}\left(y_i\right)(B-C) \\ \sim & C+(B-C) \int_{y=n}^M \frac{y^{-\alpha-1}}{Z_\alpha} d y \\ \sim & O\left(n^{-\alpha}\right) . \end{aligned}$ 
参数的scaling law，假设每 $r$ 个神经元都能理解一种技能。则 $H(X|Y ) ∼ O(N^{-\alpha})$ 。

接下来讨论 FLOPs 的 scaling law.

假设在开始理解每个技能时，每次浮点计算都会减少 $∆$ 条件熵。假设每次浮点计算有概率 $p_{skill}(y)$ 来学习到技能 $y$ 。那么对于技能 $y_i$ 则需要 $\frac{B-C}{p_{\text {skill }}\left(y_i\right) \Delta}$ 次浮点计算才能够理解。由于技能是按顺序学习的，因此经过 $S$ flops 后的优化模型可以学到的技能数如下： $S=\sum_{i=1}^n \frac{B-C}{\Delta} \frac{1}{p_{\text {skill }}\left(y_i\right)} \sim \int_{y=1}^n y^{\alpha+1} d y \sim O\left(n^{\alpha+2}\right) .$ 则有 $H(X|Y ) ∼ O(S^{-\frac{\alpha}{\alpha+2}})$ 。

以上说明了条件熵服从幂律关系，根据第一节的理论 $H(X)=H(X|Y)+I(X;Y)$ ，由于信息压缩的过程即是对互信息的最小化的过程，即可以将 $I(X;Y)$ 看作一个定值，则熵也服从幂律。

以上证明过程主要参考 [The Information of Large Language Model Geometry](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2402.03471v1.pdf)，感兴趣可以参看原文。

## 参考资料

\[1\] [Evaluation Metrics for Language Modeling (thegradient.pub)](https://link.zhihu.com/?target=https%3A//thegradient.pub/understanding-evaluation-metrics-for-language-models/)

\[2\] [The Information of Large Language Model Geometry (arxiv.org)](https://link.zhihu.com/?target=https%3A//arxiv.org/html/2402.03471v1)

\[3\] [2212.01365.pdf (arxiv.org)](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2212.01365.pdf)

\[4\] [https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf](https://link.zhihu.com/?target=https%3A//people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf)

\[5\] [Generating Sequences With Recurrent Neural Networks](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1308.0850)

\[6\] [基于量子化假设推导模型的尺度定律（Scaling Law）](https://link.zhihu.com/?target=https%3A//kexue.fm/archives/9607)

\[7\] [Scaling Laws for Autoregressive Generative Modeling](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2010.14701.pdf)

\[8\] [Scaling Laws for Neural Language Models](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2001.08361.pdf)

\[9\] [Training Compute-Optimal Large Language Models](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2203.15556.pdf)

\[10\] [An Empirical Model of Large-Batch Training](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1812.06162.pdf)

\[11\] [DeepSeek LLM](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2401.02954.pdf)

\[12\] [GPT-4 Technical Report](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2303.08774.pdf)

\[13\] [https://www.researchgate.net/figure/Evolution-of-the-average-loss-during-training-with-varying-vocabulary-size-A-step\_fig3\_329276137](https://link.zhihu.com/?target=https%3A//www.researchgate.net/figure/Evolution-of-the-average-loss-during-training-with-varying-vocabulary-size-A-step_fig3_329276137)

\[14\] [Deep Learning and the Information Bottleneck Principle](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1503.02406.pdf)

\[15\] [Large Language Model Evaluation via Matrix Entropy](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2401.17139.pdf)

\[16\] [Self-Supervised Learning via Maximum Entropy Coding](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2210.11464.pdf)

\[17\] [Matrix Information Theory for Self-Supervised Learning](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2305.17326.pdf)

\[18\] [Segmentation of Multivariate Mixed Data via Lossy Data Coding and Compression](https://link.zhihu.com/?target=https%3A//people.eecs.berkeley.edu/~yima/psfile/Ma-PAMI07.pdf)

\[19\] [信息论——信息速率失真函数与熵压缩编码（一）](https://link.zhihu.com/?target=https%3A//wlsdzyzl.top/2019/01/08/%25E4%25BF%25A1%25E6%2581%25AF%25E8%25AE%25BA%25E2%2580%2594%25E2%2580%2594%25E4%25BF%25A1%25E6%2581%25AF%25E9%2580%259F%25E7%258E%2587%25E5%25A4%25B1%25E7%259C%259F%25E5%2587%25BD%25E6%2595%25B0%25E4%25B8%258E%25E7%2586%25B5%25E5%258E%258B%25E7%25BC%25A9%25E7%25BC%2596%25E7%25A0%2581%25EF%25BC%2588%25E4%25B8%2580%25EF%25BC%2589/)

\[20\] [https://www.cl.cam.ac.uk/teaching/1819/ForModLang/notes/Language\_as\_information\_notes.pdf](https://link.zhihu.com/?target=https%3A//www.cl.cam.ac.uk/teaching/1819/ForModLang/notes/Language_as_information_notes.pdf)

\[21\] [https://aclanthology.org/2023.emnlp-main.762.pdf](https://link.zhihu.com/?target=https%3A//aclanthology.org/2023.emnlp-main.762.pdf)

\[22\] [https://mdpi-res.com/bookfiles/book/2614/Information\_Theory\_and\_Language.pdf?v=1726102906](https://link.zhihu.com/?target=https%3A//mdpi-res.com/bookfiles/book/2614/Information_Theory_and_Language.pdf%3Fv%3D1726102906)

\[23\] [https://www.researchgate.net/publication/381911126\_Information-Theoretic\_Foundations\_for\_Neural\_Scaling\_Laws](https://link.zhihu.com/?target=https%3A//www.researchgate.net/publication/381911126_Information-Theoretic_Foundations_for_Neural_Scaling_Laws)

\[24\] [Information Theory, Scaling Laws and the Thermodynamics of Evolution](https://link.zhihu.com/?target=https%3A//pdf.sciencedirectassets.com/272314/1-s2.0-S0022519300X00781/1-s2.0-S0022519398906804/main.pdf%3FX-Amz-Security-Token%3DIQoJb3JpZ2luX2VjEHYaCXVzLWVhc3QtMSJHMEUCIGcCXo0feGFTZXy%252FYgWZz4vA9XMSNdd1Qs38YUe8G%252BSQAiEAylh7PF0e3HyLxM8aMZwIOFSlkktaQ2VDi%252FLnpHAsevwqvAUIn%252F%252F%252F%252F%252F%252F%252F%252F%252F%252F%252FARAFGgwwNTkwMDM1NDY4NjUiDG0IlnTlYeO%252Fc3y0HSqQBerloKVVKI4lOzbhrlOQwT4ha1vGJdW7Txurlw%252BNjKHO23uBx8ovdRCwYFlNg5VzHcnc6y%252B%252BzAtUEnMreljPpm%252FAxQxT0%252BZ%252BpvQ5cWjjmp3%252FhBsOEq6SuAF%252FU6EU5MhlGGBPwG7DYilREE4H2AyTDlL3i440EvscsqWPKDD7jfh3gdJz%252Fw7vd9zrl5SQvS9aA7D%252FS7KKBnSPrH9a4LriekyJk%252Fg35s8pE8UHJDRjDnICq5lZnGIPQIkEq6XIYLAaQnokoQ3c2r0OrUF8U%252Fx4h%252Bjui%252F4lmuOrJhOeyjQRocQSQXntNR01qD35W6vwkB16Hr3GKkAiiY%252B97Jm9ucN3L0FfQdQPHaFriJdOiPxYPaYXI9otoPs7fbboBfag06uoZ3nRLdFuGZyxLy6BKHoUdj9zBMpqz7s%252FpwP%252Fp%252FOvd4xkQLXkhjKV8UdycsxLcyawf4lMrashVmACsYsR7lKKejUKj4HBsqOLBclLbr1B6hFFG1obaARKFSlkxtdW0C1e36U1jtjAoXsWWCMa06plapSnkNlOi1JWkTTd0mv6ztk1Rpa5FFVLK9u3yxSfJpgDIrP0jbUQHOOI0ztiTu5hrgPqUnrXl6AkmEvDPWnANEaKqGFyHN2DkZ0Wbsmz6LR3rkvpjkQY68ddVZ3QPAOnxg%252FwqGHK132%252FKyJJOf6TdIm3W7OzGQ89hXMx8LDdvHNqX0gh83t%252B%252BYqURuIajW4x2mbMxBKSKnDCbeak28n%252BkbA7G6JYXsrYGtteHL9siL8Ozo14MmG1gQXWHoF7P5IUGM4aTsvw8tfLdAukqmEfVnvzqe9idewjAghjtMd2PxL7nc4T8OMNNoW65UHK5POSWBgh3blfO8gW0DcrZvOiwdoZMN%252F6ibcGOrEBncgM7ffw5Gl6ZpleaBGt3lsQw1fM5H9OvlaQckSMojwyf%252BumqINfvikwoy9gHbz%252BIEvnVDGFgI5ZVhGsj4ePt9aCQowe8qN7LHM8qA0HMMv%252F5QVdDg%252BBVf9x0syzdlSG4IqcRQDMLltr4HYtY2kaP8StDAGoahjl2JL3s38Z5MXPbuh0W0MA8afvxELkqZcZUfsF6M1acd%252FPQtpvpdjBFEtBLYJziGUCbWqF8l3G52vX%26X-Amz-Algorithm%3DAWS4-HMAC-SHA256%26X-Amz-Date%3D20240912T062147Z%26X-Amz-SignedHeaders%3Dhost%26X-Amz-Expires%3D300%26X-Amz-Credential%3DASIAQ3PHCVTYYIM27HGL%252F20240912%252Fus-east-1%252Fs3%252Faws4_request%26X-Amz-Signature%3D94ec79854e57db4ab4e035da8e77cb442f1deca8c5645b111021b388342778d2%26hash%3Dd00546c42d23c1c05b05d89399042e9132e9584e857be88c20c0e7a4616894c1%26host%3D68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61%26pii%3DS0022519398906804%26tid%3Dspdf-5f655e80-f104-41d9-9978-822df6446f36%26sid%3Db3a2fd7e51e9674dea58fb10080190a5ecefgxrqa%26type%3Dclient%26tsoh%3Dd3d3LnNjaWVuY2VkaXJlY3QuY29t%26ua%3D1b175a03560002060e%26rr%3D8c1dcca0c85a19aa%26cc%3Dus)

\[25\] [LLMZip: Lossless Text Compression using Large Language Models](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2306.04050)

> 笛声依约芦花里。白鸟成行忽惊起。别来闲整钓鱼竿。思入水云寒。 —— 潘阆 《酒泉子·长忆西湖》
