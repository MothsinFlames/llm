---
created: 2025-09-01T19:35:50 (UTC +08:00)
tags: [大模型,模型加速,GPT]
source: https://zhuanlan.zhihu.com/p/669926191
author: 关于作者猛猿公众号：大猿搬砖简记JOYWIN、Nasusu、学车辆算法工程师也关注了她回答66文章70关注者33,417关注她发私信
---
### 基于 GPU 注意力机制优化：标准的自注意力机制 -> Flash Attention 2

论文： [【大模型上下文长度扩展】FlashAttention：高效注意力计算的新纪元](https://blog.csdn.net/qq_41739364/article/details/136055959) 、 [【大模型上下文长度扩展】FlashAttention-2：比1代加速1.29倍、GPU利用率从55%上升到72%](https://blog.csdn.net/qq_41739364/article/details/136058073)

Flash Attention 是一种基于 GPU 的优化注意力机制，旨在解决传统注意力机制（如 Transformer 中的自注意力机制）在处理大规模数据时的计算瓶颈。

传统的自注意力机制在进行矩阵乘法时，由于 O(n²) 的复杂度和内存需求，处理长序列时会变得极其缓慢且内存占用过高。

![](https://i-blog.csdnimg.cn/direct/3c0fdced98c144c6a9c627ade433ac7a.png)

Flash Attention的设计目标是通过更高效的计算和内存管理来缓解这一问题，尤其在基于 GPU 的加速计算中具有显著优势。

GPU 有 2 部件：
- HBM 是主存储器，用于存放大规模数据集（如深度神经网络中的权重、激活等）。
- SRAM 是 GPU 的一级缓存（L1 cache）和二级缓存（L2 cache）的基础，它存储计算过程中非常频繁访问的数据，例如某些局部计算结果或重要的计算参数，其速度非常快。
- 同样的计算，SRAM 比 HBM 快 20 倍，那把一些计算放到 SRAM 就能提升计算速度。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/f88a31b9fa1e49fc94cd09bf0c2a57a8.png)

传统自注意力机制：矩阵相乘 + 掩码 + softmax百分比 + Dropout泛化 + 矩阵相乘。

其实大头的时间，矩阵相乘只有一小部分，掩码和Dropout泛化占大头。

Flash Attention：把其中耗时的计算，移动到 SRAM 计算。

在传统的注意力计算中，注意力矩阵需要被完全存储。

Flash Attention 通过 **分块计算** 将大矩阵分成更小的子块（小矩阵），每次只计算一个小块的内容。

这样做的好处是，当计算一个小块时，计算所需要的数据可以更容易地存储在 GPU 的高速缓存（比如 SRAM，计算速度提高 20 倍）中，而不必每次都从内存中读取整个大矩阵。

第二步，将每个子块的计算任务并行执行，使得 GPU 的多个计算核心能够同时处理多个子块的计算任务。

第三步，在线的 softmax

CPU 是一个博士（微积分都会，但只有一个人），GPU 是一群小学生（虽然只会加减乘除，但特别适合这种并行）。  

| 特性 | 传统注意力机制 | Flash Attention |
| --- | --- | --- |
| **计算复杂度** | $(                                  O                                  (                                               n                                     2                                              ⋅                                  d                                  )                                  )                                          (O(n^2 \cdot d))                           (O(n2⋅d))$ | 更优化的内存和计算方法，复杂度降低 |
| **内存占用** | 高，尤其是长序列时，内存需求急剧增加 | 通过分块和优化内存访问，减少内存占用 |
| **计算效率** | 较低，尤其在处理长序列时性能瓶颈明显 | 高效，充分利用 GPU Tensor Cores 提升计算速度 |
| **GPU 利用** | 未完全优化，无法充分利用 Tensor Cores | 高效利用 GPU，支持低精度计算 |
| **并行化** | 一定程度的并行化，但存在计算瓶颈 | 更强的并行化能力，减少冗余计算 |
| **适用场景** | 适用于短序列和较小规模的模型 | 适用于长序列、大规模模型，尤其在大数据集下表现突出 |

---

### 使用更强大的 GPU：KV Cache 更大，解码器内的推理神器，加速 QKV 矩阵相乘

**KV Cache** 是指 **键（Key）** 和 **值（Value）** 缓存，用于加速自注意力机制（例如 Transformer 中的注意力计算）中的计算过程。

它在 **自注意力计算** 中特别有用，尤其是在进行 **推理（inference）** 时，可以显著提高效率，减少重复计算。

在 **推理** 过程中（特别是 **生成式任务** 如文本生成、机器翻译等），输入序列通常是逐步生成的，模型每次生成一个新的词或标记时，需要基于前面的结果进行计算。这时， **KV Cache** 的作用尤为重要。

**KV Cache** 是将 **键（Key）** 和 **值（Value）** 缓存起来，以便在推理过程中不需要每次都重新计算这些值。

尤其在 **生成任务** 中，随着每一步生成新单词时，模型只需要计算新的查询（Query）与之前存储的键（Key）和值（Value）进行交互，而不必从头开始计算整个注意力矩阵。

**如何工作：**

- 在生成过程中，模型会逐步生成输出。例如，在文本生成中，模型从第一个单词开始，计算它与序列中其他单词的相关性，然后生成下一个单词。
- 在每生成一个新的单词后，模型会将生成的 **键（Key）** 和 **值（Value）** 缓存起来。
- 当生成下一个单词时，模型将不需要重新计算整个输入序列的键和值，而是直接使用缓存的 **键（Key）** 和 **值（Value）** 。

这种缓存机制可以显著 **减少计算量** ，避免了每次都重新计算每个单词的键和值，尤其是当输入序列很长时，KV Cache 极大地加速了推理过程。

KV Cache 的好处：

- **加速推理过程** ：每次生成一个新单词时，只需要基于当前输入与缓存的键值进行计算，避免了从头开始计算的重复工作。
- **节省内存** ：因为缓存只存储键和值，而不需要存储整个注意力矩阵，这有效地减少了内存需求。
- **提高效率** ：KV Cache 能够加速序列生成过程，尤其在大型生成模型中，显著减少了延迟。

假设我们要生成一个句子 “The cat is on the mat”：

1. **初始时** ：模型输入 “The” 后，计算其对应的键（Key）和值（Value），并将它们存入缓存。
2. **生成下一个单词** ：模型根据缓存的键和值生成 “cat”。
3. **继续生成** ：当生成 “is” 时，模型只需要计算查询（Query）与之前的键（Key）和值（Value）进行交互，而不需要重新计算 “The” 和 “cat” 的键和值，使用缓存中的键值即可。

通过这种方式，随着序列长度的增加，模型的计算量不会像传统的全序列计算那样指数增长，而是线性增加，从而提高生成效率。

**KV Cache** 是在自注意力机制中，用于存储已经计算过的 **键（Key）** 和 **值（Value）** 的缓存。

它使得在推理过程中，生成模型能够在每一步的计算中重复利用先前的计算结果，而不需要重新计算整个输入序列的注意力矩阵，从而加速计算和减少内存占用。

想象你在做一道复杂的菜肴，而每个步骤都需要用到一些配料。每次做饭时，你需要从冰箱里拿出很多配料（如肉、蔬菜等），然后开始处理这些配料。

但有些配料是重复使用的（如盐不够，可能要加几次），比如你需要多次用到相同的调料或者食材。

1. **传统做饭过程（没有 KV Cache）**
- 每次做饭时，你都需要重新从冰箱里取出所有的配料，再按照食谱逐步准备。
- 如果你在做菜的过程中需要重复使用某些食材，每次都要从冰箱重新取出来，这样就浪费了时间和精力。
1. **使用 KV Cache（优化后的做饭过程）**
- 假设你在做饭时将一些常用的配料（比如常用的调料、已经切好的蔬菜）放在一个小篮子里，这个篮子就是你 **KV Cache** 。
- 这样，当你需要再用这些食材时，你不需要每次都去冰箱里重新拿，而是直接从篮子里取出，省时省力。

类比与 **KV Cache** 的关系：

- **冰箱里的配料** ：就像是 **键（Key）** 和 **值（Value）** 。这些配料是你做饭时需要的食材，代表着自注意力中的输入信息。
- **篮子（KV Cache）** ：是一个临时存放已准备好的食材的地方，它缓存了你已经处理过的配料（即已经计算过的键和值）。每次你做饭时，可以直接从篮子里拿，而不需要重新从冰箱里取出。
- **做饭过程** ：类似于模型生成的每一步。在生成的每个新单词时，模型不需要重新计算所有的键和值，而是直接利用之前计算过的结果（缓存的键和值）。

就像你做饭时如果把常用的配料放在篮子里，可以重复利用它们， **KV Cache** 在模型推理过程中缓存了已经计算过的键和值。

这样模型在每次生成新的输出时，不需要重新计算之前已经做过的工作，从而提高效率和减少计算负担。

---

### 优化注意力权重计算方式：MHA（每个头都有 KV） --> MQA（所有头只有一个KV）、GQA（分多组，每组只有一个KV）、MLA（低秩变换优化 KV 权重）

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/16a31af4c687424d948554f4121569d1.png)  
MHA 占有的 KA Cache 太多，如上图。

1. MHA (Multi-Head Attention)  
	**多头注意力（MHA）** 是 Transformer 中自注意力机制的核心组成部分，广泛应用于自然语言处理（NLP）任务中。

工作原理：

- **查询（Query）、键（Key）、值（Value）** ：MHA 和传统的自注意力机制一样，首先将输入分为查询、键和值三个部分。
- **多个头（Heads）** ：MHA 的核心思想是使用多个“头”来并行地计算注意力。每个头都在不同的子空间中进行注意力计算，最后将这些头的输出合并。每个头在计算注意力时，使用不同的参数矩阵（不同的权重），通过这种方式可以从多个角度学习不同的特征。
- **合并输出** ：多个头的结果会通过拼接或加权平均的方式进行合并，并映射到输出空间。

优势：

- **捕获多样性** ：每个头可以关注输入数据的不同部分，从不同的角度理解数据，因此多头注意力可以捕获更加丰富的表示。
- **并行性** ：多个头可以并行计算，提高计算效率。

应用场景：

- 主要用于 **Transformer** 模型中，广泛应用于 **NLP** （如机器翻译、文本生成、问答等）和 **计算机视觉** （如图像描述生成、图像分类等）。

---

1. **MQA (Multi-Query Attention)**  
	**多查询注意力（MQA）** 是对 MHA 的一个优化变种，旨在减少计算复杂度，特别是在生成式任务中。

工作原理：

- 在标准的多头注意力中，查询（Query）、键（Key）和值（Value）都是来自输入序列中的每个元素（或者嵌入的表示）。而在 **MQA** 中，模型使用多个查询（Queries）来与共享的键和值进行交互。
- 具体来说， **多个查询** 会共享同一组键和值进行计算，而不是像传统的 MHA 那样，针对每个查询都计算不同的键和值。这样做可以大大减少计算量，特别是在查询数量远大于输入序列长度时。

优势：

- **减少计算复杂度** ：由于多个查询共享相同的键和值，可以减少计算量，特别是在需要同时处理多个查询的场景下（如批量推理）。
- **提高计算效率** ：尤其在生成任务（如对话系统、文本生成等）中，能够加速推理过程。

应用场景：

- 适用于 **生成任务** 和 **推理任务** ，特别是在每次生成多个输出（例如生成多个候选文本或生成多个翻译时），常见于 **对话系统** 、 **机器翻译** 、 **文本生成** 等领域。

但学习语义深度、丰富度是不如 MHA 的，因为都变成 1 个了。

然后就诞生了中间的平衡方案，保证学习深度和丰富的同时，还要减少计算量。

---

1. GQA (Generalized Query Attention)  
	**广义查询注意力（GQA）** 是一个更广泛的注意力机制，旨在通过优化查询、键和值的交互方式，使得注意力机制更加灵活和通用。GQA 不是一个特定的模型，而是一个通用的框架，能够适应多种不同类型的任务。

工作原理：

- **灵活的查询机制** ：GQA 允许查询、键和值之间有更加灵活的交互方式。与标准的 MHA 和 MQA 相比，GQA 的查询、键和值可以来自不同的数据源，或者根据任务需求进行定制。
- **多模态输入** ：GQA 设计允许来自多个模态的数据进行交互（如文本、图像、音频等），可以处理更复杂的任务。
- **任务自适应** ：GQA 允许根据不同任务的需求调整查询的处理方式，能够在不同的应用场景中自适应优化注意力机制的表现。

优势：

- **通用性强** ：GQA 能够适应不同的任务，不仅限于自然语言处理任务，还可以扩展到多模态学习（如视觉-语言模型）。
- **灵活性** ：能够根据任务需求灵活地调整查询、键和值之间的交互方式，提高任务的执行效率和效果。

应用场景：

- **多模态任务** （如视觉-语言任务）：GQA 可以同时处理文本和图像的联合表示，用于图像描述生成、视觉问答（VQA）等任务。
- **跨模态任务** ：例如在处理文本和图像输入时，GQA 可以将两者的信息融合，从而提高模型对多模态输入的理解和处理能力。
- 适用于需要 **灵活查询机制** 的任务，比如 **多任务学习** 或 **跨模态推理** 。

---

**MLA** 结合了低秩投影和共享 KV 权重的想法，它的关键点在于如何在保持性能的同时，显著减少内存和计算开销：

1. **低秩投影** ：在 MLA 中，模型通过低秩矩阵对查询（Q）、键（K）和值（V）进行降秩变换，利用较少的参数来学习关键特征，从而减少了模型的内存消耗和计算复杂度。类似于 **LoRA** ，低秩投影帮助模型在处理新任务时，仅优化少量参数。
2. **私有和公用KV权重** ：MLA 将公用的 **KV 权重** 和私有的 **KV 权重** 分离。公用的 KV 权重可用于多个查询（如 MHA 中），而私有的 KV 权重被 **藏** 在 Q 和 V 的投影矩阵中，减少了缓存所需存储的内存量。
3. **位置编码** ：MLA 采用类似于 **RoPE** 的方法，通过扩展 Q 和 K 的维度来处理位置编码，避免了位置编码增加缓存需求，从而进一步减少内存消耗。

MLA 通过 **低秩投影** 和 **共享 KV 权重** 等技术，在保留模型表现的前提下，显著提高了内存和计算效率，适合处理大规模、长序列的任务。

---

MHA、MQA 和 GQA 、MLA 对比

| 特性               | **MHA (Multi-Head Attention)**                  | **MQA (Multi-Query Attention)** | **GQA (Generalized Query Attention)** | **MLA (Multi-head Latent Attention)**   |
| ---------------- | ----------------------------------------------- | ------------------------------- | ------------------------------------- | --------------------------------------- |
| **基本概念**         | 多个并行的注意力头，每个头独立计算注意力                            | 多个查询共享相同的键和值，减少计算复杂度            | 灵活的查询和键值交互方式，支持多模态输入                  | 将公共和私有的KV权重分离，进行低秩线性变换，节省内存             |
| **计算方式**         | 每个头独立计算，通过拼接合并输出                                | 多个查询共享同一组键和值，减少计算量和内存消耗         | 允许查询、键和值之间更灵活的交互，适应不同任务需求             | 使用低秩投影矩阵进行线性变换，实现降秩操作，减少内存需求            |
| **低秩矩阵应用**       | 没有低秩矩阵优化                                        | 间接通过共享键和值，减少计算复杂度               | 通过调整查询和键值的交互方式，适应任务需求                 | 使用低秩矩阵（如LoRA），将公共和私有的KV权重矩阵降秩           |
| **优化目标**         | 提高表示学习的多样性，增强模型的表达能力                            | 降低计算复杂度，特别是在生成任务和批量推理时          | 提供更高的灵活性和通用性，适应多模态和多任务学习              | 通过降秩投影优化内存使用，减少计算复杂度和训练阶段的参数量           |
| **应用场景**         | 主要用于 **NLP** 和 **计算机视觉** （如 Transformer、BERT 等） | 适用于生成任务、对话系统、机器翻译、文本生成等         | 多模态任务、视觉-语言任务、跨模态推理、任务自适应场景           | 用于高效的生成任务，特别是在处理长序列时节省内存，优化推理速度         |
| **权重矩阵的形状变化**    | 原始权重矩阵，多个注意力头并行计算                               | 缩小KV矩阵大小，多个查询共享键和值              | 权重矩阵通过线性变换降低秩，适应任务需求                  | 公共和私有KV权重通过低秩投影矩阵处理，减小内存占用              |
| **私有与公用KV权重的处理** | 每个头有独立的KV权重                                     | 多个查询共享KV权重                      | 不同的任务和输入共享查询、键和值                      | 私有KV权重投影到Q和V中，公用KV共享，减少内存占用             |
| **内存效率**         | 计算量和内存需求较高                                      | 减少计算和内存需求，特别是在生成任务中             | 适应不同任务的需求，灵活调整内存使用                    | 通过低秩投影和KV共享显著节省内存，特别适合大规模任务             |
| **位置编码**         | 位置编码直接应用于Q和K                                    | 对位置编码的处理与MHA相似                  | 灵活处理位置编码，允许跨模态任务                      | **通过Q和K的维度扩展来处理位置编码，减少缓存需求**            |
| **推理加速**         | 计算量较大，推理过程较慢                                    | 通过共享KV权重提高推理效率                  | 灵活的查询处理加速推理，尤其是跨模态任务                  | 通过低秩投影矩阵减少推理过程中的计算负担和内存消耗               |
| **优点**           | 捕获不同的信息，增强模型的表达能力                               | 高效计算多个查询，减少计算开销                 | 更加通用和灵活，适应多种任务和输入类型                   | 降低内存占用和计算复杂度，通过低秩投影和共享 KV 权重提高效率，同时保持性能 |

---

**Flash Attention（V1）**。**它不仅涉及了硬件和cuda的知识，还涉及到很多计算逻辑上的trick。** 我的痛点是不能在头脑中具象化整个流程，就更不要提对细节的推导了。

所以这篇文章我读了很久，也写了很久（一个月），最终决定按照如下方式对Flash Attention进行介绍：

-   本文一到三部分，介绍相关硬件知识及Flash Attention诞生背景。
-   本文四到五部分，通过**图解形式**介绍forward/backward中的分块计算过程。**所有的符号、公式都会给出详细的说明和推导过程**。我在阅读中发现论文的一些推导不太符合直觉（or写得可能不太对），所以这里我在遵从论文符号表达的基础上，部分内容按自己的理解重新顺了一遍。
-   本文第六到第八部分，量化介绍Flash attention在性能上的改进，包括计算量、显存和IO复杂度。

## 一、Flash attention在做一件什么事

我们知道，对于Transformer类的模型，假设其输入序列长度为 $N$ ，那么其计算复杂度和消耗的存储空间都为 $O(N^{2})$ 。也就是说，随着输入序列的变长，将给计算和存储带来极大的压力。

因此，我们迫切需要一种办法，能解决Transformer模型的 $O(N^{2})$ 复杂度问题。如果能降到 $O(N)$ ，那是最好的，即使做不到，逼近 $O(N)$ 那也是可以的。所以，Flash Attention就作为一种行之有效的解决方案出现了。

Flash Attention在做的事情，其实都包含在它的命名中了（**Fast and Memory Efficient Exact Attention with IO-Awareness**），我们逐一来看：

**(1）Fast（with IO-Awareness），计算快**。在Flash Attention之前，也出现过一些加速Transformer计算的方法，这些方法的着眼点是“减少计算量FLOPs”，例如用一个稀疏attention做近似计算。**但是Flash attention就不一样了，它并没有减少总的计算量，因为它发现：计算慢的卡点不在运算能力，而是在读写速度上。** 所以它通过降低对显存（[HBM](https://zhida.zhihu.com/search?content_id=237002834&content_type=Article&match_order=1&q=HBM&zhida_source=entity)）的访问次数来加快整体运算速度，这种方法又被称为**O-Awareness**。在后文中，我们会详细来看Flash Attention是如何通过**分块计算（tiling）** 和 **核函数融合（kernel fusion）** 来降低对显存的访问。

**（2）Memory Efficicent，节省显存**。在标准attention场景中，forward时我们会计算并保存N*N大小的注意力矩阵；在backward时我们又会读取它做梯度计算，这就给硬件造成了 $O(N^{2})$ 的存储压力。在Flash Attention中，则巧妙避开了这点，使得存储压力降至 $O(N)$ 。在后文中我们会详细看这个trick。

**（3）Exact Attention，精准注意力。** 在（1）中我们说过，之前的办法会采用类似于“稀疏attention”的方法做近似。这样虽然能减少计算量，但算出来的结果并不完全等同于标准attention下的结果。但是Flash Attention却做到了完全等同于标准attention的实现方式，这也是后文我们讲述的要点。

## 二、计算限制与内存限制

在第一部分中我们提过，**Flash Attention一个很重要的改进点是：** 由于它发现Transformer的计算瓶颈不在运算能力，而在读写速度上。因此它着手降低了对显存数据的访问次数，这才把整体计算效率提了上来。所以现在我们要问了：**它是怎么知道卡点在读写速度上的？**

为了解答这个问题，我们先来看几个重要概念：

-   $\pi$ ：**硬件算力上限**。指的是一个计算平台倾尽全力每秒钟所能完成的浮点运算数。单位是 FLOPS or FLOP/s。
-   $\beta$ ：**硬件带宽上限**。指的是一个计算平台倾尽全力每秒所能完成的内存交换量。单位是Byte/s。
-   $\pi_{t}$ ：**某个算法所需的总运算量**，单位是FLOPs。下标 $t$ 表示total。
-   $\beta_{t}$ ：**某个算法所需的总数据读取存储量，** 单位是Byte。下标 $t$ 表示total。

这里再强调一下对FLOPS和FLOPs的解释：

-   FLOPS：等同于FLOP/s，表示Floating Point Operations Per Second，即每秒执行的浮点数操作次数，用于衡量硬件计算性能。
-   FLOPs：表示Floating Point Operations，表示某个算法的总计算量（即总浮点运算次数），用于衡量一个算法的复杂度。

**我们知道，在执行运算的过程中，时间不仅花在计算本身上，也花在数据读取存储上**，所以现在我们定义

-   $T_{cal}$ ：对某个算法而言，计算所耗费的时间，单位为s，下标cal表示calculate。其满足 $T_{cal} =\frac{\pi_{t}}{\pi}$
-   $T_{load}$ ：对某个算法而言，读取存储数据所耗费的时间，单位为s。其满足 $T_{load} = \frac{\beta_{t}}{\beta}$

**我们知道，数据在读取的同时，可以计算；在计算的同时也可以读取**，所以我们有：

-   $T$ ：对某个算法而言，完成整个计算所耗费的总时间，单位为s。其满足 $T = max(T_{cal}, T_{load})$

**也就是说，最终一个算法运行的总时间，取决于计算时间和数据读取时间中的最大值。**

### 2.1 计算限制

当 $T_{cal} > T_{load}$ 时，算法运行的瓶颈在计算上，我们称这种情况为**计算限制（math-bound）**。此时我们有： $\frac{\pi_{t}}{\pi} > \frac{\beta_{t}}{\beta}$ ，即 $\frac{\pi_{t}}{\beta_{t}} > \frac{\pi}{\beta}$

### 2.2 内存限制

当 $T_{cal} < T_{load}$ 时，算法运行的瓶颈在数据读取上，我们称这种情况为**内存限制（memory-bound）**。此时我们有 $\frac{\pi_{t}}{\pi} <\frac{\beta_{t}}{\beta}$ ，即 $\frac{\pi_{t}}{\beta_{t}} <\frac{\pi}{\beta}$

我们称 $\frac{\pi_{t}}{\beta_{t}}$ 为算法的**计算强度（Operational Intensity）**

### 2.3 Attention计算中的计算与内存限制

本节内容参考自：[回旋托马斯x：FlashAttention:加速计算,节省显存, IO感知的精确注意力](https://zhuanlan.zhihu.com/p/639228219)

有了2.1和2.2的前置知识，**现在我们可以来分析影响Transformer计算效率的因素到底是什么了。我们把目光聚焦到attention矩阵的计算上，其计算复杂度为** $O(N^{2})$ **，是Transformer计算耗时的大头。**

假设我们现在采用的硬件为A100-40GB SXM，同时采用混合精度训练（可理解为训练过程中的计算和存储都是**fp16**形式的，一个元素占用2byte）

$$\frac{\pi}{\beta} = \frac{312 * 10^{12}}{1555 * 10^{9}} = 201 FLOPs/Bytes$$

假定我们现在有矩阵 $Q, K \in \mathbb{R}^{N*d}$ ，其中 $N$ 为序列长度， $d$ 为embedding dim。现在我们要计算 $S = QK^{T}$ ，则有（**对FLOPs要怎么算不了解的朋友，可以跳到6.1节进行阅读**）：

$$\frac{\pi_{t}}{\beta_{t}} = \frac{2N^{2}d}{2Nd + 2Nd + 2N^{2}} = \frac{N^{2}d}{2Nd + N^{2}}$$

不同 $N, d$ 取值下的受限类型如下：

![](https://pic1.zhimg.com/v2-91bb7f924f2314d1b03a1fcc854629e8_1440w.jpg)

根据这个表格，我们可以来做下总结：

-   **计算限制（math-bound）**：大矩阵乘法（N和d都非常大）、通道数很大的卷积运算。相对而言，**读得快，算得慢**。
-   **内存限制（memory-bound）**：逐点运算操作。例如：激活函数、dropout、mask、softmax、BN和LN。相对而言，**算得快，读得慢。**

**所以，我们第一部分中所说，“Transformer计算受限于数据读取”也不是绝对的，要综合硬件本身和模型大小来综合判断**。但从表中的结果我们可知，memory-bound的情况还是普遍存在的，所以Flash attention的改进思想在很多场景下依然适用。

在Flash attention中，**计算注意力矩阵时的softmax计算就受到了内存限制，这也是flash attention的重点优化对象**，我们会在下文来详细看这一点。

### 2.4 roof-line模型

其实到2.3为止，我们对计算限制和内存限制的概念已经知道得很清楚了。在这一节中，我们更系统来做一个总结。

一个算法运行的效率是离不开硬件本身的。**我们往往想知道：对于一个运算量为 $\pi_{t}$ ，数据读取存储量为 $\beta_{t}$ 的算法，它在算力上限为 $\pi$ ，带宽上限为 $\beta$ 的硬件上，能达到的最大性能 $P$ (Attanable Performance)是多少？**

这里最大性能 $P$ 指的是当前算法实际运行在硬件上时，每秒最多能达到的计算次数，单位是`FLOP/s`。

**[Roof-line模型](https://zhida.zhihu.com/search?content_id=237002834&content_type=Article&match_order=1&q=Roof-line%E6%A8%A1%E5%9E%8B&zhida_source=entity)** 就是为了解答这一问题而提出的，它能直观帮我们看到算法在硬件上能跑得多快，模型见下图。

![](https://pic2.zhimg.com/v2-e180cdf9b632e90cf988d23d7af2b0b7_1440w.jpg)

如图，横坐标 $I$ 表示计算强度，满足 $I = \frac{\pi_{t}}{\beta_{t}}$ ；纵坐标 $P$ 表示算法运行在硬件上的性能。**算法的运行性能不会超过硬件本身的计算上限**，所以 $P$ 的最大值取到 $\pi$ 。根据我们之前的分析，当 $I > \frac{\pi}{\beta}$ 时，存在计算限制；当 $I <\frac{\pi}{\beta}$ 时，存在内存限制。

## 三、GPU上的存储与计算

由于Flash attention的优化核心是减少数据读取的时间，而数据读取这块又离不开数据在硬件上的流转过程，所以这里我们简单介绍一些GPU上的存储与计算内容，作为Flash attention的背景知识。

### 3.1 GPU的存储分类

![](https://pic1.zhimg.com/v2-6ab0049f478a2adfe336857cf7b7be88_1440w.jpg)

上图是Flash attention论文所绘制的硬件不同的存储类型、存储大小和带宽。一般来说，GPU上的存储分类，可以按照是否在芯片上分为**片上内存(on chip)** 和**片下内存(off chip)**。

-   **片上内存**：主要用于缓存（cache）及少量特殊存储单元（例如texture），其特点是 **“存储空间小，但带宽大”**。对应到上图中，**[SRAM](https://zhida.zhihu.com/search?content_id=237002834&content_type=Article&match_order=1&q=SRAM&zhida_source=entity)** 就属于片上内存，它的存储空间只有20MB，但是带宽可以达到19TB/s。
-   **片下内存**：主要用于全局存储（global memory），即我们常说的**显存**，其特点是 **“存储空间大，但带宽小”**，对应到上图中，**HBM就属于片下内存（也就是显存）**，它的存储空间有40GB（A100 40GB），但带宽相比于SRAM就小得多，只有1.5TB/s。

当硬件开始计算时，会先从显存（HBM）中把数据加载到片上（SRAM），在片上进行计算，然后将计算结果再写回显存中。**那么这个“片上”具体长什么样，它又是怎么计算数据的呢？**

### 3.2 GPU是如何做计算的

![|950](https://pic3.zhimg.com/v2-860b5b9a88fde84029b7bc351b7215e4_1440w.jpg)

如图，负责GPU计算的一个核心组件叫**SM（[Streaming Multiprocessors](https://zhida.zhihu.com/search?content_id=237002834&content_type=Article&match_order=1&q=Streaming+Multiprocessors&zhida_source=entity)，流式多处理器），可以将其理解成GPU的计算单元，一个SM又可以由若干个SMP（SM Partition）组成**，例如图中就由4个SMP组成。SM就好比CPU中的一个核，但不同的是一个CPU核一般运行一个线程，但是一个SM却可以运行多个轻量级线程（由Warp Scheduler控制，一个Warp Scheduler会抓一束线程（32个）放入cuda core（图中绿色小块）中进行计算）。

现在，我们将GPU的计算核心SM及不同层级GPU存储结构综合起来，绘制一张简化图：

![|600](https://pic4.zhimg.com/v2-7510cf5dc5b00560e0c60d8e2ababa77_1440w.jpg)

ref：https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s33322/

-   **HBM2**：即是我们的显存。
-   **L1缓存/shared memory**：每个SM都有自己的L1缓存，用于存储SM内的数据，被SM内所有的cuda cores共享。SM间不能互相访问彼此的L1。NV Volta架构后，L1和shared memory合并（Volta架构前只有Kepler做过合并），目的是为了进一步降低延迟。合并过后，用户能写代码直接控制的依然是shared memory，同时可控制从L1中分配多少存储给shared memory。**Flash attention中SRAM指的就是L1 cache/shared memory。**
-   **L2缓存**：所有SM共享L2缓存。L2缓存不直接由用户代码控制。L1/L2缓存的带宽都要比显存的带宽要大，也就是读写速度更快，但是它们的存储量更小。

**现在我们再理一遍GPU的计算流程：将数据从显存（HBM）加载至on-chip的SRAM中，然后由SM读取并进行计算。计算结果再通过SRAM返回给显存。**

我们知道显存的带宽相比SRAM要小的多，读一次数据是很费时的，但是SRAM存储又太小，装不下太多数据。所以**我们就以SRAM的存储为上限，尽量保证每次加载数据都把SRAM给打满，节省数据读取时间**。

### 3.3 kernel融合

前面说过，由于从显存读一次数据是耗时的，因此**在SRAM存储容许的情况下，能合并的计算我们尽量合并在一起，避免重复从显存读取数据**。

举例来说，我现在要做计算A和计算B。在老方法里，我做完A后得到一个中间结果，写回显存，然后再从显存中把这个结果加载到SRAM，做计算B。但是现在我发现SRAM完全有能力存下我的中间结果，那我就可以把A和B放在一起做了，这样就能节省很多读取时间，我们管这样的操作叫**kernel融合**。

由于篇幅限制，我们无法详细解释**kernel**这个概念，**在这里大家可以粗犷地理解成是“函数”，它包含对线程结构（grid-block-thread）的定义，以及结构中具体计算逻辑的定义。** 理解到这一层已不妨碍我们对flash attention的解读了，想要更近一步了解的朋友，推荐阅读这篇（[小小将：CUDA编程入门极简教程](https://zhuanlan.zhihu.com/p/34587739%EF%BC%89%E6%96%87%E7%AB%A0%E3%80%82)）

**kernel融合和尽可能利用起SRAM，以减少数据读取时间，都是flash attention的重要优化点。** 在后文对伪代码的解读中我们会看到，分块之后flash attention将矩阵乘法、mask、softmax、dropout操作合并成一个kernel，做到了只读一次和只写回一次，节省了数据读取时间。

好！目前为止所有的背景知识我们都介绍完了，现在我们直入主题，看看flash attention到底是怎么巧妙解决memory-bound问题。

## 四、Forward运作流程

在后文相关的讲解中，我们遵循以下步骤：

**（1）先看Flash Attention做分块计算的整体流程。**

**（2）再看分块的计算细节。**

**（3）最后看Flash Attention是如何通过分块计算控制I/O，进而解决memory-bound的问题，提升整体运算速度。**

### 4.1 标准attention计算

这个大家应该都很熟悉了，假设一共有 $N$ 个token，每个token向量的维度为 $d$ ，则一个标准的attention计算如下图：

![|925](https://picx.zhimg.com/v2-11d8782784e90bfc908933038d0a78eb_1440w.jpg)

其中， $S = QK^{T}, {P} = softmax(S)$ 。在GPT类的模型中，还需要对 ${P}$ 做mask处理。**为了表达方便，诸如mask、dropout之类的操作，我们都忽略掉，下文也是同理**。

### 4.2 标准Safe softmax

这里我们需要额外强调 ${P} = softmax(S)$ 这一步。正常来说，假设 $S$ 中某一行向量为 $[x_{1}, x_{2}, ..., x_{d}]$ ，该行向量中的某一个元素为 $x_{i}$ ，则对 $S$ 做softmax后，有：

$softmax(x_{i}) = \frac{e^{x_{i}}}{\sum_{j=1}^{d}e^{x_{j}}}$

**而如果 $x_{i}$ 过大，那么在计算softmax的过程中，就可能出现数据上溢的情况**。为了解决这个问题，我们可以采用**safe softmax**方法：

$$m(x) = \mathop{max}\limits_{i} x_{i}$$

$$softmax(x_{i}) = \frac{e^{x_{i}-m(x)}}{\sum_{j=1}^{d}e^{x_{j}-m(x)}}$$

下图展示了safe softmax的过程，**这里 $\widetilde{P}, P$ 分别表示做归一化前和做归一化后的结果。** 大家记住图中 $m, l$ 表达的含义，在后面的分块（Tiling）计算中，我们会用到这两个概念：

![|700](https://pic4.zhimg.com/v2-238beb6416cf01dee3eefc95a112710b_1440w.jpg)

### 4.3 分块计算整体流程（Tiling）

我们知道Flash Attention的核心优化技术是采用了分块计算（Tiling），那么它是如何分块的？分块后的计算方式和不分块的计算方式又有哪些不同之处呢？

**我们先来了解分块计算的整体流程（帮助大家理解数据块是怎么流转的），然后我们再针对其中的细节做一一讲解。**

![](https://pica.zhimg.com/v2-0f5b256f2edc2889e7a5116b3e2ee28e_1440w.jpg)

（1）首先，将 $Q$ 矩阵切为 $T_{r}$ 块（block），每块的长度为 $B_{r}$ 。用 $Q_{i}$ 来表示切完后的某块矩阵，则 $Q_{i}$ 的维度为 $(B_{r}, d)$ 。不难理解， $Q_{i}$ 中存储着某 $B_{r}$ 个token的query信息。

（2）然后，将 $K^{T}$ 矩阵切为 $T_{c}$ 块，每块的长度为 $B_{c}$ 。用 $K^{T}_{j}$ 表示切完后的某块矩阵，则 $K^{T}_{j}$ 的维度为 $(d, B_{c})$ 。易知 $K^{T}_{j}$ 中存储着某 $B_{c}$ 个token的key信息。

（3）同样，将 $V$ 矩阵也切为 $T_{c}$ 块，每块长度为 $B_{c}$ 。用 $V_{j}$ 表示切完后的某块矩阵，则 $V_{j}$ 的维度为 $(B_{c}, d)$ 。易知 $V_{j}$ 中存储着某 $B_{c}$ 个token的value信息。

（4）理解了上面的定义后，我们就可以开始做**分块的attention计算**了。以上图为例：

-   **计算初始attention分数**： $S_{ij} = Q_{i} * K^{T}_{j} = (B_{r}, d) * (d, B_{c}) = (B_{r}, B_{c})$ ，图中的 $S_{ij}$ 表示前 $B_{r}$ 个token和前 $B_{c}$ 个token间的原始相关性分数。

-   **Safe softmax + mask + dropout**：对 $S_{ij}$ 做safe softmax、mask和dropout操作，得到 $\widetilde{P}_{ij}$ 。**你可能会有疑惑：前面不是说， $\widetilde{P}_{ij}$ 是归一化前的结果， $P_{ij}$ 是归一化后的结果吗？那么这里是不是应该用 $P_{ij}$ 呢？** 这里确实只用算到 $\widetilde{P}_{ij}$ ，在后文对分块计算细节的讲解中，我们会详细说这点。目前为止，大家不用太纠结符号，只用大体知道 $P$ 代表的含义即可。

-   **计算output**： $O_{ij} = \widetilde{P}_{ij} * V_{j} = (B_{r}, B_{c}) * (B_{c}, d) = (B_{r}, d)$ ，即可得到输出结果 $O_{ij}$ 。**细心的你肯定又发现了，这个等式不太对劲，这个 $O_{ij}$ 不太对劲。** 想一想，在正常情况下，前 $B_{r}$ 个token过attention后的输出结果，应该是它和所有token都做注意力计算后的输出结果。可是这里， $O_{ij}$ 却只是前 $B_{r}$ 个token和前 $B_{c}$ 个token的结果。虽然 $O_{ij}$ 的shape对了，但其中的内容却不是我们最终想要的。所以，关于 $O$ 的计算，也是我们需要关注的细节，我们同样放在后文详说。

**在计算这些分块时，GPU是可以做并行计算的，这也提升了计算效率。**

好！现在你已经知道了单块的计算方式，现在让我们把整个流程流转起来把。在上图中，我们注明了 $j$ 是外循环， $i$ 是内循环，这个意思就是说，**对于每个 $j$ ，我们都把所有的 $i$ 遍历一遍，得到相关结果。在论文里，又称为K，V是外循环，Q是内循环。** 写成代码就是:

```python3
# ---------------------
# Tc: K和V的分块数
# Tr: Q的分块数量
# ---------------------
for 1 <= j <= Tc:
    for 1 <= i <= Tr:
        do....
```

如果你还有疑惑，那么下面两张图可以更直观地解答你的疑惑.

$j = 0$ ，遍历 $i$ :

![](https://picx.zhimg.com/v2-f6f2c08da5b7eacd1b19b8d98e6ffb0f_1440w.jpg)

$j = 1$ ，遍历 $i$ :

![](https://picx.zhimg.com/v2-d37ba1cb5f65d6656a8cc731359bf40b_1440w.jpg)

**【⚠️特别提醒】：正如上文所说，这里的 $O$ 还需要经过一定的处理，才能和不分块场景下的 $O$ 完全等价。这里我们将每一块的 $O$ 单独画出，是为了帮助大家更好理解分块计算的整体流程，不代表它是最终的输出结果。**

好！到这一步为止，我们已经掌握了使用Tiling计算attention的整体框架。但我们依然有很多细节问题没有解决：

-   **分块后，要如何正确计算attention score？（即** $S, P$ **的计算方法）**
-   **分块后，要如何正确计算输出** $O$ **？**
-   **分块后，是如何实现优化I/O，解决memory-bound的问题的？**

### 4.4 分块计算中的safe softmax

回顾之前绘制的标准safe softmax流程图，我们知道 $m, l$ 都是针对**完整的一行**做rowmax、rowsum后的结果，那么在分块场景下，会变成什么样呢？

![](https://pic4.zhimg.com/v2-52966650e1ccd42a2aacdb246e69f537_1440w.jpg)

以上图红圈内的数据为例，在标准场景下，我们是对红圈内的每一行做rowmax、rowsum后得到 $\widetilde{P}$ 的。

现在切换到分块场景，我们分别算出了 $S_{00}$ 和 $S_{01}$ ，然后我们再对它们分别做rowmax、rowsum，是不是也能得到和标准场景下一模一样的结果呢？

答案当然是否定的。举个简单的例子，**标准场景下的 $m(x)$ 是每行的全局最大值，可是分块后如果你也这么算，它就变成了局部最大值了。** 很明显，它不等同于标准场景下的结果。

所以，**Flash Attention的作者们，在这里使用了一种巧妙的计算方式。**

（1）我们假设标准场景下， $S$ 矩阵某一行的向量为 $x = [x_{1}, x_{2}, ..., x_{d}]$ ，因为分块的原因，它被我们切成了两部分 $x = [x^{(1)}, x^{(2)}]$ 。

（2）我们定义：

-   $m(x)$ ：标准场景下，该行的全局最大值
-   $m(x^{(1)})$ ：分块1的全局最大值
-   $m(x^{(2)})$ ：分块2的全局最大值

那么易知： $m(x) = m([x^{(1)}, x^{(2)}]) = max(m(x^{(1)}), m(x^{(2)}))$

（3）我们定义：

-   $f(x)$ ：标准场景下， $exp(x - m(x))$ 的结果
-   $f(x^{(1)})$ ：分块场景下， $exp(x^{(1)} - m(x^{(1)}))$ 的结果
-   $f(x^{(2)})$ ：分块场景下， $exp(x^{(2)} - m(x^{(2)}))$ 的结果

那么易知： $f(x) = [e^{(m(x^{(1)}) - m(x))}f(x^{(1)}), e^{(m(x^{(2)}) - m(x))}f(x^{(2)})]$ 。这个很好理解，详细的证明过程就不写了。

（4）我们定义：

-   $l(x)$ ：标准场景下， $rowsum[f(x)]$ 的结果
-   $l(x^{(1)})$ ：分块场景下， $rowsum[f(x^{(1)})]$ 的结果
-   $l(x^{(2)})$ ：分块场景下， $rowsum[f(x^{(2)})]$ 的结果

那么由（3）易知： $l(x) = e^{m(x^{(1)}) - m(x)}l(x^{(1)}) + e^{m(x^{(2)}) - m(x)}l(x^{(2)})$

（5）现在，我们就可以用分块计算的结果，来表示标准场景下safe softmax的结果了：

$softmax(x) = \frac{f(x)}{l(x)} = \frac{[e^{(m(x^{(1)}) - m(x))}f(x^{(1)}), e^{(m(x^{(2)}) - m(x))}f(x^{(2)})]}{e^{m(x^{(1)}) - m(x)}l(x^{(1)}) + e^{m(x^{(2)}) - m(x)}l(x^{(2)})}$

我们配合上面的图例和flash attention论文中的伪代码，再来进一步理解一下分块计算safe softmax的（1）～（5）步骤。

**这里我们需注意：由于safe softmax是针对矩阵整行的计算，即相当于固定内圈 $i$ ，移动外圈 $j$ 的结果，所以在接下来的介绍中，我们都以这样的视角进行介绍。**

![](https://picx.zhimg.com/v2-7d576eafb486e7561058e596bb5b79c5_1440w.jpg)

我们用 $S_{00}$ （图中浅绿色方块）替换掉（1）～（5）步骤中的 $x^{(1)}$ ，用 $S_{01}$ （图中深绿色方块）替换掉 $x^{(2)}$ 。我们关注点在伪代码部分的5-11行。

由于伪代码中的表达符太多，容易阻碍大家的理解，因此我们先明确各个数学符号表达的含义：

-   $S_{ij}$ ：对应在我们的例子里，就是 $S_{00}$ 和 $S_{01}$ ，即 $Q_{i}K^{T}_{j}$ 的结果
-   $\widetilde{m}_{ij}$ ：对于当前分块 $S_{ij}$ 来说，每行的局部最大值。相当于前面步骤（2）中对 $m(x^{(1)}), m(x^{(2)})$ 的定义。
-   $\widetilde{P}_{ij}$ ：分块场景下，各块的P矩阵（归一化前）结果。相当于步骤（3）中对 $f(x^{(1)})，f(x^{(2)})$ 的定义。
-   $\widetilde{l_{ij}}$ ：分块场景下，rowsum的结果。相当于步骤（4）中对 $l(x^{(1)})，l(x^{(2)})$ 的定义。
-   $m$ ：标准场景下，对 $S$ 矩阵而言，每行的最大值，这是全局最大值（ $m$ 首次定义在伪代码第2行），相当于前面步骤（2）中对 $m(x)$ 的定义
-   $l$ ：标准场景下，全局rowsum的结果（$l$首次定义在伪代码第2行），相当于前面步骤（4）中对 $l(x)$ 的定义。
-   $m_{i}$ ：表示 $max(\widetilde{m}_{i0}, \widetilde{m}_{i1}, ..., \widetilde{m}_{i(j-1)})$ 。如果当前分块是 $S_{ij}$ ，则 $m_{i}$ 表示固定 $i$ 时，前 $j-1$ 个分块中的局部最大值。容易推知，当固定 $i$ ，遍历完 $j$ 后， $m_{i}$ 的结果就是全局最大值了。例如图例中，我们遍历完 $S_{00}, S_{01}$ 后，就能得到全局最大值 $m_{0}$ 。
-   $m^{new}_{i}$ ：表示 $max(\widetilde{m}_{i0}, \widetilde{m}_{i1}, ..., \widetilde{m}_{i(j-1)}, \widetilde{m}_{ij})$ 。如果当前分块是 $S_{ij}$ ，则 $m^{new}_{i}$ 表示固定 $i$ 时，截止到当前分块为止的局部最大值。
-   $l^{new}_{i}$ ：和 $m^{new}_{i}$ 对应，相当于步骤（4）中用分块更新 $l(x)$ 的步骤。
-   $l_{i}$ ：和 $m_{i}$ 同理，**即当我们将 $j$ 遍历完后，我们就能得到针对 $i$ 的全局rowmax和全局rowsum**。而根据前面的定义， $m^{new}_{i}$ 和 $l^{new}_{i}$ 是遍历完最新的 $S_{ij}$ 后得到的rowmax和rowsum结果，所以每遍历完一块 $S_{ij}$ ，我们就执行伪代码的第13行，做一次更新。

**如果你被论文中这些数学符号乱花了眼，那再告诉大家一个理解它们的trick**：

-   **所有以 $ij$ 作为下标的，都表示当前分块的计算结果**
-   **所有以 $i$ 作为下标的，都表示截止到前一个分块（包含前一个分块）的计算结果**
-   **所有以 $new$ 为上标的，都表示引入当前分块做更新后的结果**
-   **所有没有下标的，都表示全局结果**

![](https://pic1.zhimg.com/v2-2771ebc07d1312e5c0921d6c16d79314_1440w.jpg)

相信通过上面对数学表发符的介绍，大家已经大致理解了分块计算safe softmax的过程，为了加深理解，现在我们再来读一遍伪代码，把整个流程串起来：

-   `伪代码第5～7行`：从HBM（显存）上读取 $K_{j}, V_{j}$ 到on-chip存储SRAM。注意，在代码处理逻辑上，这里是固定外圈 $j$ ，循环内圈 $i$ 。但是由于整个safe softmax逻辑是对“行”而言的，所以在理解时大家需要想像成固定内圈 $i$ ，循环外圈 $j$ ，也就是我们图例中绘制的深浅绿/蓝/黄色块。

-   `伪代码第8行`：从HBM（显存）上读取 $Q_{i}, O_{i}, l_{i}, m_{i}$ 。**记住我们之前说的trick，下标带 $i$ 的都表示截止到前一个分块的计算结果。**虽然我们前面没介绍过 $O_{i}$ （在后文会细说），但按这个trick你应该也能猜到， $O_{i}$ 也是随着分块的移动而逐步更新的。等移动到最后一个分块时，我们就能得到和标准场景下一模一样的输出结果 $O_{i}$ 。在之前的图例中，为了方便大家对分块的整体流程有快速理解，我们画了很多个 $O$ 出来，**现在你应该能猜到，对每个 $i$ ，我们只维持并不断更新一个** $O_{i}$ **，直至遍历完毕（例如之前的图例中，我们画了6个 $O$ ，但实际我们要维护更新的，只有3个： $O_{0}, O_{1}, O_{2}$ ）**

-   `伪代码第9行`：正常计算 $S_{ij} = Q_{i}K^{T}_{j}$

-   `伪代码第10行`：基于当前分块 $S_{ij}$ 计算 $\widetilde{m}_{ij}, \widetilde{P}_{ij}, \widetilde{l_{ij}}$ 。

-   `伪代码第11行`：引入当前分块，计算截止目前为止的rowmax和rowsum，分别用 $m^{new}_{i}, l^{new}_{i}$ 表示。

-   `伪代码第12行`：更新 $O_{i}$ ，后文会详细解析这部分公式

-   `伪代码第13行`：用 $m^{new}_{i}, l^{new}_{i}$ 去更新 $m_{i}, l_{i}$

讲完了分块safe softmax的伪代码，这时你可能发现一个问题了：**之前你是否一直以为，在这一顿操作后，分块计算得出的 $S, \widetilde{P}$ 应该要和标准场景下的完全一致（比如应该是我们步骤（1）~（5）介绍的那样）？但是现在看来，每个分块** $S_{ij}, \widetilde{P}_{ij}$ **依然是用自己局部的rowmax和rowsum做计算的，并没有达到我们理想中的效果呀！**

别急，还记得伪代码第12行我们说的更新 $O_{i}$ 的公式么？**分块计算的真正意义不在于得到正确的 $S, \widetilde{P}$ ，而在于得到正确的 $O$ 。**

然后，你再来看伪代码5-13行，你会发现，在整个计算过程中，只有 $m_{i}, l_{i}, O_{i}$ 被从on-chip的SRAM中写回到显存（HBM）中。**把 $i$ 都遍历完后，读写量也不过是 $m, l, O$ 。相比于标准场景下，我们要读写的是 $S, P, O$ ，读写量是不是一下就少很多，这不就能解决memory-bound的问题了吗。**

所以，**分块计算safe softmax的意义，就是抹去对** $S,P$ **的读写。**

### 4.5 分块计算中的输出O

终于到翘首以盼的输出$O$的分析部分了，当你第一次看到伪代码12行更新$O\_{i}$的公式，是不是觉得两眼一黑？不要紧，这里我们依然通过图解的方式，帮助大家理解并推导这个公式。

![](https://pic1.zhimg.com/v2-494b855fe59891bf264c31ef7dae6100_1440w.jpg)

之前我们说过，**上图中画的6个 $O$ 并不是我们最终想要的结果。**我们期望维护并更新 $O_{i}$ ，当该 $i$ 下的所有 $j$ 遍历完毕后，我们的 $O_{i}$ 就应该和标准场景下的 $O_{i}$ 完全相等。

回到图例中，图中的 $O_{i}$ 就应该等于被红框圈起来的 $S,P$ 部分和 $V$ 部分的乘积。但是别忘记之前说过，**这里各块** $S, P$ **都是局部rowmax，rowsum计算出来的结果。** 所以我们必须对各块 $S, P$ 再做一些处理，才能让它们和V相乘，更新 $O_{i}$ 。

那么要处理到什么程度为止呢？**第一想法可能是，只要让每块 $S,P$ 结果和标准场景下的结果完全一致，不就行了吗？但是别忘了，你不计算到最后一块 $S,P$ ，你是拿不到全局的rowmax和rowsum的。** 而由于为了解决memory-bound的问题，我们只保留 $m,l,O$ 而不存各块 $S,P$ 。因此等你遍历到最后一块时，虽然有了全局的rowmax和rowsum，但没有 $S, P$ ，你根本算不出最终的 $O_{i}$ 。

所以这里我们**换个思路**： $O_{i}$ 不是每遍历一块就更新一次吗？那有没有一种办法，**不断用当前最新的rowmax和rowsum去更新 $O_{i}$ ， 直到遍历完最后一块，这时的 $O_{i}$ 不就和标准场景下的结果完全一致了吗？也就是我们想构造形如下面这样的更新等式：**

$O_{i} = O_{i} + 当前最新结果$

沿着这个思路，我们来看伪代码第12行公式的诞生过程：

$\begin{aligned}  O^{(j+1)}_{i}&= P_{i,:j+1}V_{:j+1}\\  &= softmax(S_{i,:j+1})V_{:j+1}\\ &= diag(l^{(j+1)})^{-1}[exp([S_{i,:j}, S_{i(j+1)}]-m^{(j+1)})]\begin{bmatrix}  V_{:j}\\V_{j+1} \end{bmatrix}\\ &= diag(l^{(j+1)})^{-1}[exp(S_{i,:j}-m^{(j+1)})V_{:j} + exp(S_{i(j+1)}-m^{(j+1)})V_{j+1})]\\ &= diag(l^{(j+1)})^{-1}[e^{-m^{(j+1)}}exp(S_{i,:j})V_{:j} + e^{-m^{(j+1)}}exp(S_{i(j+1)})V_{j+1})]\\ &= diag(l^{(j+1)})^{-1}[diag(l^{(j)})e^{m^{(j)}-m^{(j+1)}}diag(l^{(j)})^{-1}exp(S_{i,:j}-m^{(j)})V_{:j} + e^{-m^{(j+1)}}exp(S_{i(j+1)})V_{j+1})]\\ &= diag(l^{(j+1)})^{-1}[diag(l^{(j)})e^{m^{(j)}-m^{(j+1)}}{P}_{i,:j}V_{:j} + e^{-m^{(j+1)}}exp(S_{i(j+1)})V_{j+1})]\\ &= diag(l^{(j+1)})^{-1}[diag(l^{(j)})e^{m^{(j)}-m^{(j+1)}}O^{(j)}_{i} + e^{\widetilde{m}-m^{(j+1)}}exp(S_{i(j+1)}-\widetilde{m})V_{j+1}]\\ &= diag(l^{(j+1)})^{-1}[diag(l^{(j)})e^{m^{(j)}-m^{(j+1)}}O^{(j)}_{i} + e^{\widetilde{m}-m^{(j+1)}}\widetilde{P}_{i(j+1)}V_{j+1}] \end{aligned}$

初次看到这个推导过程，你可能有些懵圈，不要紧，我们一行一行来看。在讲解之前，我们先明确以上推导过程中符号**上下标**的含义：

-   $i$ ：这个大家应该很熟悉了。例如图例中， $i=0,1,2$ 分别对应着深浅绿、深浅蓝、深浅黄块。
-   $i(j+1)$ ：表示**当前分块**的相关结果
-   $i,:j+1$ ：表示**截止到当前分块（包含当前分块）**的相关结果。 $i, :j$ 表示**截止到前一分块（包含前一分块）**的相关结果。

（1）第一行：**首先，我们期望的结果是，每遍历一个分块，就更新一次** $O_{i}$ **，遍历完全部的分块后，我们就能得到和标准场景下完全一致的** $O_{i}$ 。基于此我们有 $O^{(j+1)}_{i} = P_{i,:j+1}V_{:j+1}$ 。其中， $P_{i,:j+1}$ 表示从第0个分块到当前分块，我们用**当前最新**的rowmax，rowsum更新一次**所有分块**的 $P$ 结果（因为做过归一化了，所以是不带波浪号的 $P$ ）。 $V_{:j+1}$ 则表示当前分块及之前所有分块所对应着的 $V$ 部分（例如图例中，若当前分块是浅绿色块，则其对应着浅灰色 $V$ ；若当前分块是深绿色块，则其对应着浅灰色+深灰色 $V$ ）。

（2）第二行：将 $P_{i,:j+1}$ 改写成 $softmax(S_{i, :j+1})$ 的形式。**特别注意，这里 $S_{i,:j+1}$ 所代表的各个分块间都是相互独立的**，你可以理解为，只有在做 $softmax$ 这个操作时，才考虑对这些独立的 $S$ 用最新的rowmax，rowsum去更新 $P$ 。

（3）第三行：就是把(2)当中的 $softmax$ 展开写了。即用当前最新的rowmax和rowsum去计算 $P$ 。这里将 $S_{i,:j+1}$ 拆成 $[S_{i,:j}, S_{i(j+1)}]$ 两部分（**\[之前所有的分块，当前分块\]**）。同理拆 $V$ 。

（4）～（5）第四～五行：做简单的变式，不再赘述。

（6）第六行：我们观察到，中括号式子里的前半部分，和之前所有分块的结果密切相关。**联想到我们最终的目标是不断更新** $O_{i}$ **，也就是在上一个** $O_{i}$ **的基础上，引入当前分块的信息做更新。**因此，能不能把上一个 $O_{i}$ （对应到我们的式子里就是 $O^{(j)}_{i}$ ）表达出来呢？

基于这个思想做递推， $O^{(j)}_{i}$ 当然就是**之前的所有分块**，用**上一分块**的rowmax、rowsum做更新后求得 $P$ ，再乘上对应的 $V$ 得到的结果呀，所以根据此我们攒出了 $diag(l^{(j)})^{-1}exp(S_{i,:j}-m^{(j)})V_{:j}$ 这一项（就是 $O^{(j)}_{i}$ ），然后再用 $diag(l^{(j)})e^{m^{(j)}-m^{(j+1)}}$ 去抵消我们在攒它的过程中引入的项。

(7)~(9)：第七～九行：明确了（6）以后，剩下的部分就很好理解啦。这里额外说下，为什么要把 $\widetilde{m}$ 放进去呢（毕竟有了 $S_{i(j+1)}, m^{(j+1)}$ 都是已知的，已经可以算了）。因为我们在求解rowsum相关的数据时，还是要把数据从 $S$ 转为 $\widetilde{P}$ 才能求，因此避不开算 $\widetilde{P}$ 。另外也是为了让表达起来更统一，因此这里引入 $\widetilde{m}$ ，进而引入 $\widetilde{P}$ 进行计算。

现在再回头看伪代码的第12行，是不是就很清楚了呢？**建议大家可以自行画图，动手推导，加深理解。**

## 五、Backward运作流程

### 5.1 softmax求导

在后文对分块计算backward中，我们会频繁接触到和softmax求导相关的知识，繁杂的数学符号可能会使很多朋友看得蒙圈，所以这里我们做个快速复习。

设

$\left\{\begin{matrix} \begin{aligned}  y &= softmax(z)\\  L &= f(y) \end{aligned} \end{matrix}\right.$

其中， $L$ 表示Loss， $f(.)$ 表示Loss函数， $y = \begin{bmatrix} y_{1}&y_{2}&y_{3} \end{bmatrix}$ ， $z = \begin{bmatrix} z_{1}&z_{2}&z_{3} \end{bmatrix}$ ，若现在我们想求 $\frac{\partial L}{\partial z_{j}}$ ，要怎么算呢？

根据链式法则，我们有 $\frac{\partial L}{\partial z_{j}} = \frac{\partial L}{\partial y}\frac{\partial y}{\partial z_{j}}$ ，所以我们分别来看这两项。

（1） $\frac{\partial L}{\partial y}$

我们现在不考虑具体的Loss函数，直接假设这一项的结果为 $\begin{bmatrix}   m_{1}&m_{2}&m_{3} \end{bmatrix}$

（2） $\frac{\partial y}{\partial z_{j}}$

我们知道，对于某个 $z_{j}$ 来说，在softmax的操作下，它参与了 $y_{1}, y_{2}, y_{3}$ 三者的计算，因此它的偏导也和这三者密切相关，这里我们分成两种情况：

$\left\{\begin{matrix} \begin{aligned}  \frac{\partial y_{i}}{\partial z_{j}} &= y_{i}(1-y_{i})，当i=j\\ \frac{\partial y_{i}}{\partial z_{j}} &= -y_{i}y_{j}，\qquad当i\neq j \end{aligned} \end{matrix}\right.$

这里不再赘述详细的推动过程，有需要的朋友可以参考[这篇文章](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/wuliytTaotao/p/10787510.html)。

有了这个理解，我们再来谈谈基于 $y= softmax(z)$ 的Jacobian矩阵 $diag(y) - y^{T}y$ :

$\begin{aligned}  diag(y) - y^{T}y &= \begin{bmatrix}   y_{1}&0&0 \\   0&y_{2}&0 \\   0&0&y_{3} \end{bmatrix}-\begin{bmatrix}  y_{1}\\y_{2}\\y_{3} \end{bmatrix}*\begin{bmatrix}   y_{1}&y_{2}&y_{3} \end{bmatrix}\\ &=\begin{bmatrix}   y_{1}-y_{1}^{2}&-y_{1}y_{2}&-y_{1}y_{3} \\   -y_{2}y_{1}&y_{2}-y_{2}^{2}&-y_{2}y_{3} \\   -y_{3}y_{1}&-y_{3}y_{2}&y_{3}-y_{3}^{2} \end{bmatrix} \end{aligned}$

很容易发现只要把每行/每列相加，就能得到对应 $z$ 的偏导。别着急求和，我们继续往下看。

（3） $\frac{\partial L}{\partial z_{j}} = \frac{\partial L}{\partial y}\frac{\partial y}{\partial z_{j}}$

有了（1）（2）的结果，现在就可以来推导 $\frac{\partial L}{\partial z_{j}}$ ，我们有：

$\frac{\partial L}{\partial z_{j}} = \frac{\partial L}{\partial y}\frac{\partial y}{\partial z_{j}} =\sum_{i=1}^{l}\frac{\partial L}{\partial y_{i}}\frac{\partial y_{i}}{\partial z_{j}} = y_{j}(\mathrm{d}y_{j}-\sum_{j=1}^{l}y_{j}dy_{j})$

举个例子，若我们现在想求 $\frac{\partial L}{\partial z_{1}}$ ，我们将 $\frac{\partial L}{\partial y} = \begin{bmatrix}   m_{1}&m_{2}&m_{3} \end{bmatrix}$ 代入上面公式，则有：

$\frac{\partial L}{\partial z_{1}} = m_{1}(y_{1}-y_{1}^{2}) - m_{2}y_{1}y_{2} - m_{3}y_{1}y_{3}$

现在，针对所有的 $z$ ，我们将 $\frac{\partial L}{\partial z}$ 写成矩阵表达式有：

$\begin{aligned} \frac{\partial L}{\partial z} &=\frac{\partial L}{\partial y}\frac{\partial y}{\partial z} =\mathrm{d}y(diag(y) - y^{T}y)\\ &=\begin{bmatrix}   m_{1}&m_{2}&m_{3} \end{bmatrix}(\begin{bmatrix}   y_{1}&0&0 \\   0&y_{2}&0\\   0&0&y_{3} \end{bmatrix} - \begin{bmatrix}   y_{1}\\y_{2}\\y_{3} \end{bmatrix}\begin{bmatrix}   y_{1}&y_{2}&y_{3} \end{bmatrix}))\\ &=\begin{bmatrix}   m_{1}&m_{2}&m_{3} \end{bmatrix}\begin{bmatrix}   y_{1}-y_{1}^{2}&-y_{1}y_{2}&-y_{1}y_{3} \\   -y_{2}y_{1}&y_{2}-y_{2}^{2}&-y_{2}y_{3} \\   -y_{3}y_{1}&-y_{3}y_{2}&y_{3}-y_{3}^{2} \end{bmatrix} \end{aligned}$

**至此，大家记住这两个重要的结论：**

$\begin{aligned} \frac{\partial L}{\partial z} &=\frac{\partial L}{\partial y}\frac{\partial y}{\partial z} =\mathrm{d}y(diag(y) - y^{T}y)\\ \frac{\partial L}{\partial z_{j}} &= y_{j}(\mathrm{d}y_{j}-\sum_{j=1}^{l}y_{j}dy_{j})  \end{aligned}$

### 5.2 标准backward计算

![](https://pica.zhimg.com/v2-36eadc31074e8040db6e53da6c8eecb4_1440w.jpg)

我们先来总结下forward中做的操作，为了表达简便，这里将mask、dropout等零碎操作省去，同时假设$f(.)$是损失函数：

$\begin{matrix} \begin{aligned}  S &= QK^{T}\\  P &= softmax(S)\\  O &= PV \\  L & = f(O) \end{aligned} \end{matrix}$

对于标准backward来说，在计算开始时，显存（HBM）上已经存放有 $Q, K, V, O, S, P$ 这些数据。论文中的伪代码已经介绍得非常清楚，大家可以自行阅读，这里就不赘述了。对伪代码第3行求 $dS_{ij}$ 有困惑的朋友，可见上文“softamx求导”部分。

### 5.3 分块backward计算

在讲解backward计算前，我们先来看看经过分块Forward计算后，显存（HBM）上都存了哪些数据：

-   $m$ ：全局rowmax
-   $l$ ：全局rowsum
-   $Q, K, V$ ：等同于标准attention场景下的结果
-   $O$ ：等同于标准attention场景下的输出结果 $O$
-   $\mathrm{d}O$ ：有了完整的 $O$ ，我们就可以按正常的backward步骤先求出它的梯度，也存放在显存上。然后我们就能按照链式法则，分块地去求别的矩阵的梯度了。

既然有了全局的 $m,l$ ，那么现在对于任意一块 $S_{ij}$ ，我们就能基于$m,l$算出和标准场景下完全一致的 $P_{ij}$ 了。因此，在backward的过程中，flash attention将采用**重计算**的方式，**重新算出 $S_{ij}, P_{ij}$ ，并将它们运用到backward的计算中去，所以在接下来的讲解中，大家就可以把 $S, P$ 理解成完全等同于标准场景下的结果，而不是像分块计算forward中那样的 $S, P$ 。**

**另外需要注意的是，为了简化表达，在接下来的分析中，关于mask、dropout之类的步骤，我们在表述上都略去。**现在让我们来看分块计算backward的伪代码：

![](https://picx.zhimg.com/v2-a8593fdc6bbadc744d4f0571bc0acddb_1440w.jpg)

**（1）求 $V_{j}$ 梯度**

由Forward过程我们知： $O = PV$ ，因此有了 $dO$ 后，我们就可以先来求 $dP$ 和 $dV$ 了。**观察下方的图，我们会发现此时所有的** $P$ **都是不带波浪号的，再强调一下，这是因为经过了重计算，此处** $S, P$ **的结果都等同于标准场景下的结果，而不是forward中所代表的含义。**

![](https://pic3.zhimg.com/v2-baebd05e8ddc5652f0df6cbc1f41096a_1440w.jpg)

假设现在 $j=0$ ，那我们要怎么求 $dV_{0}$ 呢？

**我们先来看** $V_{0}$ **都参与了** $O$ **哪些部分的计算，以及是怎么参与的**：由图可知， $P_{00}$ 和 $V_{00}$ 参与了 $O_{0}$ 的计算， $P_{10}$ 和 $V_{00}$ 参与了 $O_{1}$ 的计算， $P_{20}$ 和 $V_{0}$ 参与了 $O_{2}$ 的计算。所以我们有：

$dV_{0} = (P_{00})^{T}dO_{0} + (P_{10})^{T}dO_{1} + (P_{20})^{T}dO_{2}$

进而推知：

$dV_{j} = \sum_{i}(P_{ij})^{T}dO_{i}$

在伪代码11～15行中，做的都是 $S, P$ **重计算**的过程，伪代码的第16行，就是在按这个方法分块计算并累积 $dV_{j}$ 。

**（2）求 $P_{ij}$ 梯度**

![](https://pica.zhimg.com/v2-f4cebe5b186bdd8d0d6e2a9dd543c39a_1440w.jpg)

观察上图，可以发现 $P_{ij}$ 只与 $V_{j}, O_{i}$ 相关，例如 $P_{10}$ 只与 $V_{0}, O_{1}$ 相关。因此我们有：

$dP_{ij} = dO_{i}V_{j}^{T}$

这就是伪代码第17行做的事情。

**（3）求 $S_{ij}$ 梯度**

这一块是令许多人感到迷惑的，我们先来**回顾下“softmax求导”部分让大家记住的一个重要结论：**

$\frac{\partial L}{\partial z} =\frac{\partial L}{\partial y}\frac{\partial y}{\partial z} =\mathrm{d}y(diag(y) - y^{T}y)$

我们假设 $s_{i}, p_{i}, o_{i}$ 分别为矩阵 $S,P,O$ 的某一行（注意这里 $i$ 不是表示第 $i$ 块的意思，是表示第 $i$ 行，所以我们用小写的 $s, p, o$ 表示），那么根据这个结论，我们有:

$\begin{aligned} ds_{i} &= dp_{i}(diag(p_{i}) - p_{i}^{T}p_{i})\\        &= dp_{i}diag(p_{i})-dp_{i}p_{i}^{T}p_{i}\\        &=dp_{i}diag(p_{i})-do_{i}V^{T}p_{i}^{T}p_{i}\\        &= dp_{i}diag(p_{i}) - do_{i}o_{i}^{T}p_{i}\\        &= p_{i}\circ [dp_{i} - rowsum(do_{i}\circ o_{i})] \end{aligned}$

**你可能对这个推导的最后一步有疑惑：为什么要大费周章，将** $ds_{i}$ **改写成这么复杂的形式呢？因为在最后一步之前，我们都是针对“某一行”来求导，而引入最后一步的目的，是为了延展至对“某一块（多行）”的求导，**也就是说针对某一块 $dS_{i}$ （注意这里是大写的 $S$ ， $i$ 的含义也回归至“第几块”），我们有：

$dS_{i} = P_{i}\circ[dP_{i} - rowsum(dO_{i}\circ O_{i})]$

如果实在难以理解推导过程，建议大家可以带一些具体的值进去，就能理解我们为什么要写成这种形式了。进而，我们可以推知：

$dS_{ij} = P_{ij}\circ[dP_{ij} - rowsum(dO_{i}\circ O_{i})]$

这就是伪代码第19～20行做的事情。

**（4）求 $Q_{i}$ 梯度**

![](https://pic4.zhimg.com/v2-0a4fb34b242e3c304df306aced63a0bf_1440w.jpg)

到目前为止，我们已经知道 $dS_{ij}$ ，那么现在就可以根据链式法则继续求 $dQ_{i}$ 了。

对照上图，我们把目光聚焦在 $Q_{0}$ 身上，由forward过程可知：

$\begin{matrix} S_{00}&= Q_{0}K_{0}^{T}\\ S_{01} &= Q_{0}K_{1}^{T} \end{matrix}$

因此，针对 $Q_{0}$ ，我们有： $dQ_{0} = dS_{00}K_{0} + dS_{01}K_{1}$

推广到任意 $Q_{i}$ ，我们有： $dQ_{i} = \sum_{j}dS_{ij}K_{j}$

这就是伪代码第21行做的事情。

**（5）求 $K_{j}$ 梯度**

![](https://picx.zhimg.com/v2-8def82318d4c8e162a62bc9e86647765_1440w.jpg)

这一步就很简单啦，**如果你被复杂的分块推导弄懵了脑袋，那不妨再复习一下我们前面提过的trick**：对照上图，取出某一块 $K_{j}$ 。由于我们是从 $dS_{ij}$ 链式推向 $K_{j}$ ，所以这里只要搞明白这块 $K_{j}$ 和哪些 $Q$ 一起计算出了哪些 $S$ 再把相关结果相加即可。

只要看了流程图，就不难得知：某块 $K_{j}$ 和对应的 $Q_{i}$ 共同计算出了对应的 $S_{ij}$ ，因此有：

$dK_{j} = \sum_{i}dS_{ij}^{T}Q_{i}$

**这就是伪代码第22行做的事情。**

好！现在我们就把分块backward的细节讲完了，**当大家感到迷茫时，一定记得画图**；在碰到需要做累加才能计算出梯度的步骤中，画图也可以帮助我们快速理解是按 $i$ 维度还是按 $j$ 维度进行累加。

## 六、计算量和显存需求

### 6.1 矩阵相乘的计算量

我们先来看一个前置知识：**两个矩阵相乘，要怎么统计它们的计算量？**

我们一般用**FLOPs（floating point operations，浮点运算次数）**来表示运算量的大小。对于“两矩阵相乘”这个操作而言，其**运算量 = 乘法运算的次数 + 加法运算的次数。**

来看一个具体例子：

![](https://pic3.zhimg.com/v2-3e8b79111e61b85c39fe5d5fb8367e50_1440w.jpg)

两矩阵相乘，**为了获取图中深橘色部分的元素，我们一共需要进行n次乘法运算和n-1次加法运算**。

那么现在结果矩阵中，一共有m*p个橘色方块，则意味着我们需要进行：`m*p*(n + n - 1)`次浮点计算。

再进一步，假设此时在蓝色和绿色的矩阵外，我们还有一个**bias矩阵**，意味着计算单个橘色方块时我们需要进行n次乘法和n-1+1次加法运算，那么此时总计算量为：`m*p*(n+n) = 2mnp`。当然，即使不加这个bias，我们也可以把-1项给忽略，得到相同的结果。

**所以这里我们总结下，假设有两个矩阵A和B，它们的维度分别为(m, n)和(n, p)，则这两矩阵相乘的运算量为`2mnp`。**

一般在矩阵运算中，**乘法运算的时间要高于加法运算的时间，因此有时在统计运算量时，我们只考虑乘法运算的次数，则此时两矩阵相乘的运算量可近似为mnp**

### 6.2 Flash Attention的计算量

有了前置知识，我们就能分析flash attention的计算量了，我们以forward过程为例（为了大家阅读方便，我们再把forward的伪代码放一遍）：

![](https://picx.zhimg.com/v2-7d576eafb486e7561058e596bb5b79c5_1440w.jpg)

我们知道矩阵相乘运算占据了运算量的大头，因此我们把分析目光集中到所有的矩阵运算上来。

（1）在代码第9行，我们有 $S_{ij} = Q_{i}K_{j}^{T}$ ，其中 $Q_{i}\in\mathbb{R}^{B_{r}*d}, K_{j}^{T} \in \mathbb{R}^{d*B_{c}}$ 。根据前置知识，求 $S_{ij}$ 的计算量为 $O(B_{r}B_{c}d)$ 。

（2）在代码第12行，我们有 $\widetilde{P}_{ij}V_{j}$ ，其中 $\widetilde{P}_{ij} \in \mathbb{R}^{B_{r}*B_{c}}, V_{j} \in \mathbb{R}^{B_{c}*d}$ 。则这里的计算量同样为 $O(B_{r}B_{c}d)$

（3）接下来我们看一共计算了多少次（1）和（2），也就是执行了多少次内循环： $T_{c}T_{r} = \frac{N}{B_{c}}\frac{N}{B_{r}}$

（4）**综合以上三点，flash attention的forward计算量为：** $O(\frac{N^{2}}{B_{c}B_{r}}B_{r}B_{c}d) = O(N^{2}d)$ ，注意，因为计算量是用大O阶表示的，所以这里我们把常数项都省略了。

同理大家可以自行推一下backward中的计算量，在论文里给出的结论是 $O(N^{2})$ ，d远小于N，因此 $d$ 也可以略去不表达。

### 6.3 Flash Attention的显存需求

和标准attention相比，如果不考虑 $O$ 的话，Flash Attention只需要存储 $m,l$ ，其显存需求为 $O(N)$ 。

而标准attention需要存储 $S,P$ ，其显存需求为 $O(N^{2})$ 。

**可以发现相比于标准attention，flash attention明显降低了对显存的需求。**

## 七、IO复杂度

之前我们强调过，flash attention相比于标准attention的最大优势，就是其减少了对显存（HBM）的访问次数，一定程度上解决了memory bound的问题。所以这一节我们就来具体分析这两者对显存的访问次数（同样都是以forward为例，backward部分论文中也有给出相关推导过程，大家可以类比forward自行阅读）。

### 7.1 标准attention的IO复杂度

![](https://pic2.zhimg.com/v2-7f0acab1a283b0b250010b7ebde2d211_1440w.jpg)

（1）从HBM中读取 $Q, K \in \mathbb{R}^{N * d}$ ，计算 $S = QK^{T}, S \in \mathbb{R}^{N*N}$ 并将 $S$ 写回HBM。一读一写的IO复杂度为： $O(Nd + N^{2})$ ，在表示大O阶时我们忽略常数项。

（2）从HBM中读取 $S \in \mathbb{R}^{N*N}$ ，同时计算 $P \in \mathbb{R}^{N*N}$ 并将其写回HBM。一读一写的IO复杂度为： $O(N^{2})$

（3）从HBM中读取 $P \in \mathbb{R}^{N*N}, V \in \mathbb{R}^{N*d}$ ，计算 $O=PV, O \in \mathbb{R}^{N*d}$ 并将 $O$ 写回HBM。一读一写的IO复杂度为： $O(Nd + N^{2})$

所以，**总体来说标准attention的IO复杂度为：** $O(Nd + N^{2})$

### 7.2 Flash attention的IO复杂度

（1）我们来看伪代码的第6行，在每个外循环中，我们都会加载 $K, V$ 的block。所有外循环结束后，相当于我们加载了完整的 $K, V \in \mathbb{R}^{N*d}$ ，因此这里的IO复杂度为： $O(Nd)$

（2）再看伪代码第8行，在每个内循环中，我们都加载了部分 $Q, O, m, l$ block，由于 $m, l$ 本身比较小(IO复杂度是 $O(N)$ )，因此我们暂时忽略它们，只考虑 $Q, O$ （原论文也是这么分析的）。固定某个外循环，所有内循环结束后，我们相当于完整遍历了 $Q, O \in \mathbb{R}^{N*d}$ 。同时我们会经历 $T_{c}$ 次外循环。因此这里最终的IO复杂度为： $O(T_{c}Nd)$ 。

（3）将 $O, m, l$ 写回HBM，这里近似后IO复杂度为： $O(Nd)$ 。不过在原论文的分析中并没有考虑写回的复杂度，不过省略一些常数项不会影响我们最终的分析。

所以，**总体来说flash attention的IO复杂度为：**

$O(T_{c}Nd) = O(\frac{N}{B_{c}}Nd) = O(\frac{4Nd}{M}Nd) = O(\frac{N^{2}d^{2}}{M})$

论文中提过，一般d的取值在64～128，M的取值在100KB左右，因此有 $\frac{d^{2}}{M} << 1$ 。**因此可以看出，Flash attention的IO复杂度是要显著小于标准attention的IO复杂度的。**

## 八、实验效果

![](https://pic2.zhimg.com/v2-c0b8916e3c66f6d1bdf6cfb375bfecb9_1440w.jpg)

Flash attention的作者将 $N=1024, d = 64, B = 64$ 的GPT2-medium部署在A100 GPU上，来观测采用flash attention前后的模型的计算性能。

我们先看最左侧图表，标准attention下，计算强度 $I = \frac{66.6}{40.3} = 1.6 < 201$ ，说明GPT2在A100上的训练是受到内存限制的。而在采用flash attention后得到了明显改善，runtime也呈现了显著下降。

我们再来看中间的图表，它表示在使用flash attention的前提下，以forward过程为例，每个数据块的大小对HBM读写次数（绿色）和耗时（蓝色）的影响。可以发现，数据块越大，读写次数越少，而随着读写次数的减少，runtime也整体下降了（复习一下，读写复杂度为 $O(T_{c}Nd)$ ，数据块越大意味着 $T_{c}$ 越小）。**但有意思的是，当数据块大小>256后，runtime的下降不明显了，这是因为随着矩阵的变大，计算耗时也更大了，会抹平读写节省下来的时间。**

## 九、参考

1、[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2205.14135)

2、[https://leimao.github.io/blog/Math-Bound-VS-Memory-Bound-Operations/](https://link.zhihu.com/?target=https%3A//leimao.github.io/blog/Math-Bound-VS-Memory-Bound-Operations/)

3、[回旋托马斯x：FlashAttention:加速计算,节省显存, IO感知的精确注意力](https://zhuanlan.zhihu.com/p/639228219)

4、[紫气东来：NLP（十七）：从 FlashAttention 到 PagedAttention, 如何进一步优化 Attention 性能](https://zhuanlan.zhihu.com/p/638468472)

5、[暧暧内含光：GPU 内存概念浅析](https://zhuanlan.zhihu.com/p/651179378)

6、[kaiyuan：GPU内存(显存)的理解与基本使用](https://zhuanlan.zhihu.com/p/462191421)

7、[小小将：CUDA编程入门极简教程](https://zhuanlan.zhihu.com/p/34587739)

8、[Michael Yuan：Roofline Model与深度学习模型的性能分析](https://zhuanlan.zhihu.com/p/34204282)
