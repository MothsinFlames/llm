---
created: 2025-08-20T14:17:38 (UTC +08:00)
tags: [LoRa,算法,大模型]
source: https://zhuanlan.zhihu.com/p/688361341
author: 北京大学 计算机科学与技术硕士
---
### 揭秘：LoRA快在哪？

**LoRA的显存节省在于梯度和优化器状态部分**。被冻结住的参数不用更新，自然也就不需要相应的梯度，以及Adam一阶和二阶动量。设模型参数的显存占用为**x**，原本全量训练时显存占用为**4x**，LoRA冻结住主干参数，增加了**m**%可训练的LoRA权重，则LoRA训练时，显存占用为：

-   参数部分：**(1+m%)x**；
-   梯度部分：**m%x**；
-   优化器状态部分：**2m%x.**

加起来就是 **(1+4m%)x。** 如 **m =1%** 时，最终的显存占用就从**4x**降低到了**1.04x**。

所以，“LoRA为什么能对训练加速”这个问题的最终答案是：

-   **因为被冻结住的主干模型参数不用存储相应的梯度和优化器状态，显存占用降低，可以用更多显存装输入数据和hidden states，相同硬件下batch size可以开大；**
-   **从计算量来看，[反向传播](https://zhida.zhihu.com/search?content_id=241099546&content_type=Article&match_order=1&q=%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD&zhida_source=entity)时只需要求LoRA权重对应的梯度，优化器只需要计算LoRA权重对应的一小部分权重的更新量并加到原参数上，运算次数降低。**
