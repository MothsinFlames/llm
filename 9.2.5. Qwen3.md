---
created: 2025-09-03T10:28:55 (UTC +08:00)
tags: [LLM,RLHF,人工智能]
source: https://zhuanlan.zhihu.com/p/1907098454011942106
author: 关于作者sanmuyang强化学习，大模型，多智能体wx1997 也关注了他回答0文章105关注者2,713关注他发私信
---
Qwen3 Technical Report，原文[https://github.com/QwenLM/Qwen3/blob/main/Qwen3\_Technical\_Report.pdf](https://link.zhihu.com/?target=https%3A//github.com/QwenLM/Qwen3/blob/main/Qwen3_Technical_Report.pdf)。

本文介绍了Qwen3，Qwen3系列包括dense和[MoE架构](https://zhida.zhihu.com/search?content_id=257855211&content_type=Article&match_order=1&q=MoE%E6%9E%B6%E6%9E%84&zhida_source=entity)的模型，参数规模从0.6B到235B不等。Qwen3的一个关键创新是将思考模式和非思考集成到一个统一的框架中，并能够根据用户query或chat template进行动态切换。同时，Qwen3引入了[思考预算](https://zhida.zhihu.com/search?content_id=257855211&content_type=Article&match_order=1&q=%E6%80%9D%E8%80%83%E9%A2%84%E7%AE%97&zhida_source=entity)机制，允许用户在推理过程中动态分配计算资源，从而平衡延迟和性能。

Qwen3系列模型的模型架构如下：

![](https://picx.zhimg.com/v2-d2cbeed928de587af065fbf38f9aa117_1440w.jpg)

Qwen3的训练可以分为预训练和后训练两个部分。

### 预训练

**[预训练数据](https://zhida.zhihu.com/search?content_id=257855211&content_type=Article&match_order=1&q=%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE&zhida_source=entity)：**与Qwen2.5相比，Qwen3显著扩展了训练数据的规模和多样性。Qwen3训练数据集由119种语言和方言组成，总共有**36T** **token**。该数据集包括各种领域的高质量内容，如代码、STEM（科学、技术、工程和数学）、推理任务、书籍、多语言文本和合成数据。

为了进一步扩展预训练数据语料库，会首先采用Qwen2.5-VL模型对大量类PDF的文档进行文本识别。然后，使用Qwen2.5模型对识别出的文本进行refine，提高其质量。通过两步过程，能够额外获得一组高质量的文本token，总计达数个T。此外，采用Qwen2.5、Qwen2.5-Math和Qwen2.5-Coder模型以不同格式合成数T个文本token，包括教材、问答、指令和代码片段，覆盖了众多领域。最后，会引入更多多语言的数据来进一步扩展预训练语料库。与Qwen2.5中使用的预训练数据相比，支持的语言数量从29种显著增加到119种。Qwen开发了一个多语言数据标注系统，来提高训练数据的质量和多样性，在多个维度上注释了超过30 T个token。

**预训练阶段：**Qwen3的预训练分为3个阶段：

1.  **通用阶段**：使用超过30T个token进行训练，序列长度为4096个token，训练数据涵盖了119种语言和方言。
2.  **推理阶段**：为了进一步提高推理能力，增加STEM、代码、推理和合成数据的比例来优化此阶段的预训练数据。使用大约5T个更高质量的token进行预训练，序列长度为4,096个token。**会加速此阶段的学习率衰减**。
3.  **[长上下文阶段](https://zhida.zhihu.com/search?content_id=257855211&content_type=Article&match_order=1&q=%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E9%98%B6%E6%AE%B5&zhida_source=entity)**：在最后一个预训练阶段，会收集高质量的长上下文语料库，将Qwen3模型的上下文长度扩展到32768个token。长度在16384到32768个token之间的文本占比75%，长度在4096到16384个token的文本占比25%。A使用BF技术将RoPE的基本频率从10000提高到1000000。同时，会引入[YARN](https://zhida.zhihu.com/search?content_id=257855211&content_type=Article&match_order=1&q=YARN&zhida_source=entity)和[Dual Chunk Attention](https://zhida.zhihu.com/search?content_id=257855211&content_type=Article&match_order=1&q=Dual+Chunk+Attention&zhida_source=entity)，以实现推理过程中序列长度容量的四倍增长。

**预训练评估：**预训练模型的评估主要集中在常识、推理、数学、科学、编程和多语言能力方面的表现。评估结论如下：

1.  与开源的SOTA模型（如DeepSeek-V3 base、Llama-4-Maverick base和Qwen2.5-72B base）相比，Qwen3-235B-A22B base在大多数任务中表现更优，且总参数或激活参数显著更少。

-   使用相同的预训练数据，Qwen3 MoE base模型可以实现与Qwen3 dense base模型相近的性能，且激活参数仅为1/5。
-   由于Qwen3 MoE架构的改进、训练token的增加以及更先进的训练策略，Qwen3 MoE base模型在激活参数不到1/2且总参数更少的情况下，可以超越Qwen2.5 MoE base模型。
-   即使激活参数仅为Qwen2.5 dense基础模型的1/10，Qwen3 MoE基础模型也能实现相当的性能。

2\. Qwen3 dense base模型的整体性能在更大的参数规模上与Qwen2.5基础模型相当。例如，Qwen3-1.7B/4B/8B/14B/32B base模型分别实现了与Qwen2.5-3B/7B/14B/32B/72B base模型相当的性能。

![](https://pic3.zhimg.com/v2-c772a6004f7d7696435fcac2f300ef3c_1440w.jpg)

### 后训练

Qwen3的后训练具有两个核心目标：

1.  **思考控制**：整合“非思考”和“思考”模式，让用户灵活地选择模型是否进行推理，并通过为思考过程指定token预算来控制思考的深度。
2.  **[强到弱蒸馏](https://zhida.zhihu.com/search?content_id=257855211&content_type=Article&match_order=1&q=%E5%BC%BA%E5%88%B0%E5%BC%B1%E8%92%B8%E9%A6%8F&zhida_source=entity)**：简化轻量级模型的后训练过程。通过利用来自大模型的知识，大幅降低构建更小规模模型所需的计算成本和开发工作量。

Qwen3系列中的旗舰模型遵循复杂的四阶段训练过程：

![](https://pic3.zhimg.com/v2-dad03471486724a226f43995aaebd8ea_1440w.jpg)

前两个阶段专注于开发模型的“思考”能力。接下来的两个阶段旨在将“非思考”功能整合到模型中。

**Long-CoT冷启动：**策划了一个全面的数据集，包括数学、代码、逻辑推理和一般的STEM问题。数据集中的每个问题都与经过验证的参考答案或者代码的测试用例相匹配。该数据集作为长思维链训练冷启动阶段的基础。

数据集的构建涉及严格的两阶段过滤过程：query过滤和response过滤。在query过滤阶段，使用Qwen2.5-72B-Instruct来识别和去除不容易验证的query。这包括排除包含多个子问题或仅要求生成一般文本的query。此外，**还去除了Qwen2.5-72B-Instruct在不使用CoT推理的情况下就可以正确回答的query，来防止模型不使用深层次推理只依赖表面猜测。** 此外，会使用Qwen2.5-72B-Instruct为每个query标注其领域，以在整个数据集中各个领域的平衡。

验证query后，会使用QwenQ-32B为每个剩余query生成N个候选response。当QwenQ-32B始终无法为query生成正确解决方案时，会人工评估这些response的准确性。

对于Pass@N为正数的query，会应用更严格的过滤标准，以去除以下response：(1)答案不正确的response (2)包含大量重复的response (3)在没有足够推理的情况下明显表现出猜测行为的response (4)在思考内容和总结内容之间表现出不一致的response，(5) 涉及不恰当语言混合或风格转变的response (6)被怀疑与潜在验证集过于相似的response。

之后，从数据集中精心选择的子集会用于冷启动训练。此阶段的目标是让模型学会推理模式，而不过分强调推理性能，确保模型在后续RL阶段具有更大的灵活性和改进潜力。为了有效地实现这一目标，在此准备阶段最小化训练样本的数量和训练步数。

**Reasoning** **RL：** 在推理RL阶段使用的query-必须满足以下四个标准：（1）未在冷启动阶段使用过。（2）对于冷启动模型来说是可学习的。（3）尽可能具有挑战性。（4）涵盖广泛的子领域。最终总共收集3995个query-verifier pair，使用GRPO来更新模型参数。**实验中观察到，使用大batch** **size和每个query大量rollout进行off-policy训练，有助于提高采样效率，对训练过程有益。** Qwen还解决了如何通过控制模型的熵来平衡探索和利用的问题，以使其效果稳步增加。在总共170个RL训练步骤中，Qwen3-235B-A22B模型的AIME'24分数从70.1提高到85.1。

**思考模式融合：** 思维模式融合阶段的目标是将“非思考”能力整合到先前开发的“思考”模型中，降低同时部署思考和非思考模型的成本和复杂性。因此，在推理模型上进行SFT，并设计模板来融合这两种模式。

[SFT数据集](https://zhida.zhihu.com/search?content_id=257855211&content_type=Article&match_order=1&q=SFT%E6%95%B0%E6%8D%AE%E9%9B%86&zhida_source=entity)结合了思考和非思考数据。为了确保模型的性能不会因额外的SFT而受到影响，非思考数据是使用第二阶段模型本身对第一阶段query进行拒绝采样生成的。另一方面，非思考数据经过精心策划，涵盖了各种任务，包括代码、数学、指令跟随、多语言任务、创意写作、问答和角色扮演。此外，采用自动生成的checklist来评估非思考数据的response质量。为了增强低资源的语言任务的性能，会特别增加翻译任务的比例。

SFT数据格式如下：

![](https://pic1.zhimg.com/v2-ddb74f9306bee85f0171483506831a46_1440w.jpg)

会在用户query或system设定中分别引入/think和/no\_think标志，允许模型通过模板中的空思维块来遵循用户的输入并相应地选择适当的模式 **。对于非思考模式样本，会保留一个空思维链，从而确保模型内部的格式一致性**，**并允许开发人员通过在模板中连接加入空思维链来防止模型思考。** 默认情况下，模型以思考模式运行；因此，会添加一些思考模式训练样本，其中query中不包括/think标志。对于更复杂的多轮对话，会在query中随机插入多个/think和/no\_think标志，模型response遵循遇到的最后一个标志。

**思考预算**：思考融合模式的另一个优点是，**一旦模型学会在非思考和思考模式下回复，会自然地开发出处理中间情况的能力——即基于不完整思维链生成response，这种能力使模得模型可以控制思考预算。具体来说，当模型的思考长度达到用户定义的阈值时，可以手动停止思考过程并插入停止思考指令。** 在插入指令后，模型继续基于积累的推理生成最终response。这种能力不是通过显式训练获得的，而是通过思考模式融合后自然形成的。

**[通用RL](https://zhida.zhihu.com/search?content_id=257855211&content_type=Article&match_order=1&q=%E9%80%9A%E7%94%A8RL&zhida_source=entity):** 通用RL阶段旨在广泛提升模型在各种场景下的能力和稳定性。Qwen建立了一套复杂的奖励系统，涵盖超过20个不同的任务，每个任务都有定制的评分标准。这些任务专门针对以下核心能力的提升：

-   **指令跟随**：确保模型能够准确解释和遵循用户指令，包括与内容、格式、输出结构化使用相关的要求，提供符合用户期望的response。
-   **格式遵循**：期望模型遵守特定的格式约定，在思考和非思考模式之间适当切换，并始终使用指定的token来分离最终输出中的思考和response部分。
-   **偏好对齐**：对于开放式query，偏好对齐侧重于提高模型回复的有用性、参与度和风格，最终提供更自然且令用户满意的体验。
-   **agent能力**：这涉及训练模型通过指定接口正确调用工具。在RL训练阶段，模型被允许执行完整的多轮交互循环，并接收真实的环境反馈，从而改进其在长期决策任务中的性能和稳定性。
-   **专业场景能力**：针对特定上下文定制的任务。例如RAG任务纳入奖励信号，以引导模型生成准确且上下文合适的response，从而最大限度地降低幻觉风险。

为了为上述任务提供反馈，Qwen使用了三种不同类型的奖励：

-   **基于规则的奖励**：基于规则的奖励在推理RL阶段被使用，对于一般任务（如指令跟随）和格式遵循也很有用。精心设计的基于规则的奖励可以高精度地评估模型输出的正确性，防止奖励hacking行为。
-   **基于模型的奖励（带参考答案）**：为每个query提供参考答案，并使用Qwen2.5-72B-Instruct根据此参考答案对模型的输出进行评分。
-   **基于模型的奖励（无参考答案）**：利用人类偏好数据，训练一个奖励模型来为模型response分配标量的分数。这种方法不依赖于参考答案，可以处理更广泛的query范围，同时有效地提高模型的参与度和有用性。

**强到弱蒸馏**：强到弱蒸馏专门用于优化轻量级模型，包括5个dense模型（Qwen3-0.6B、1.7B、4B、8B和14B）和一个MoE模型（Qwen3-30B-A3B）。蒸馏过程分为两个主要阶段：

-   **off-policy蒸馏**：在初始阶段，结合教师模型在思考和非思考模式下生成的输出进行response蒸馏。
-   **on-policy蒸馏**：在此阶段，学生模型为微调生成on-policy序列。具体来说，采样prompt，学生模型以思考或非思考生成response。然后，通过将学生模型的logits与教师模型（Qwen3-32B或Qwen3-235B-A22B）的logits对齐来微调学生模型，最小化KL散度。

**后训练评估：** 对于所有处于思考模式的Qwen3模型，采样temperature为0.6、top-p为0.95、top-k为20。对于Creative Writing v3和WritingBench，设置惩罚值1.5，以鼓励生成更多样化的内容。对于处于非思考模式的Qwen3模型，配置采样temperature = 0.7、top-p=0.8、top-k = 20，惩罚值 = 1.5。对于思考和非思考两种模式，最大输出长度设置为32768个token，AIME24和AIME25会将长度扩展到38912个token以提供足够的思考空间。实验结论如下：

（1）旗舰模型Qwen3-235B-A22B在思考模式和非思考模式下，在开源模型中均展现出顶尖的整体性能，超越了DeepSeek-R1和DeepSeek-V3等强大的基线模型。Qwen3-235B-A22B在与OpenAI-o1、Gemini2.5-Pro和GPT-4o等闭源领先模型的竞争中同样极具竞争力。

（2）旗舰dense模型Qwen3-32B在大多数基准测试中表现优于之前的推理模型QwQ-32B，与闭源的OpenAI-o3-mini表现相当，显示出其强大的推理能力。Qwen3-32B在非思考模式下同样表现出色，超越了之前的非推理dense模型Qwen2.5-72B-Instruct。

（3）轻量级模型，包括Qwen3-30B-A3B、Qwen3-14B以及其他更小的dense模型，在参数数量相近或更多的开源模型中始终表现出色，证明了从强到弱的蒸馏方法的成功。

235B-A22B的评估结果如下：

![](https://pic1.zhimg.com/v2-6987bc5ec4eb8c4b57316664fc74d046_1440w.jpg)

![](https://pic3.zhimg.com/v2-80dcc1d03ee1022e298f2f3ee07f53a2_1440w.jpg)

**思考预算的有效性**:为了验证Qwen3能够通过增加思考预算来提升其智能水平，在数学、代码和STEM领域的四个基准测试上调整了分配的思考预算。由此得出的扩展曲线如下图所示，Qwen3展示了思考预算的可扩展，随预算增加性能平稳的性能提升。此外还观察到，如果将输出长度进一步扩展到32K以上，预计模型性能在未来会进一步提升。

![](https://pic1.zhimg.com/v2-08d5e7c5a5850b7ca0b7054e3390d90e_1440w.jpg)

**on-policy蒸馏的有效性**: 蒸馏出的模型在性能上明显优于强化学习。

![](https://pica.zhimg.com/v2-23b9face0a1a16db4cc9c1d473234a8c_1440w.jpg)

**思考模式融合的效果**:（1）第三阶段还增强了模型在思考模式下的通用、指令跟随和智能体能力。

（2）第四阶段进一步加强了模型在思考和非思考模式下的通用、指令跟随和智能体能力。ThinkFollow得分提高到98.9，并且模式切换准确。

（3）对于知识、STEM、数学和编码任务，思考模式融合与通用强化学习并未带来显著提升。相反，在AIME24和LiveCodeBench等具有挑战性的任务中，思考模式下的性能在经过这两个训练阶段后实际上有所下降。

![](https://pic1.zhimg.com/v2-cda223c31f3e5f60861ecdb3a4f829fe_1440w.jpg)
