### 1.1 从价值近似到策略近似
强化学习算法可以分为两大类：基于值函数的强化学习和基于策略的强化学习。
**基于值函数的强化学习** 通过递归地求解贝尔曼方程来维护Q值函数（可以是离散的列表，也可以是神经网络），每次选择动作时会选择该状态下对应Q值最大的动作，使得未来积累的期望奖励值最大。经典的基于值函数的强化学习算法有Q-Learning、SARSA、DQN算法等。这些算法在学习后的Q值函数不再发生变化，每次做出的策略也是一定的，可以理解为确定性策略。
**基于策略的强化学习** 不再通过价值函数来确定选择动作的策略，而是直接学习策略本身，通过一组参数 $\theta$ 对策略进行参数化，并通过神经网络方法优化 $\theta$ 。
基于策略的强化学习用参数化概率分布$\pi_\theta(a|s)=P(a|s;\theta)$代替了基于值函数的强化学习中的确定性策略$\pi:s \to a$，在返回的动作概率列表中对不同的动作进行抽样选择。
### 1.2 定义目标函数
目标：最大化
$$
\begin{equation}
   E(R(\tau))_{\tau \sim P_\theta(\tau)}=\sum_\tau R(\tau) P_\theta(\tau)
\end{equation}
$$

基于参数化策略的思想，目标就是找到那些可能获得更多奖励的动作，使它们对应的概率更大，从而策略就更有可能选择这些动作。
为此，我们定义的最大化目标函数 $J(\theta)$ 如下：
$$\max_\theta J(\theta)=\max_\theta E_{\tau \sim \pi_{\theta}} R(\tau)=\max_\theta \sum_{\tau} P(\tau ; \theta) R(\tau)$$
其中 $\tau$ 是agent与环境交互产生的状态-动作轨迹 $\tau=(s_1,a_1,\dots,s_T,a_T)$ ，对 $\tau$ 求和代表的是与环境交互可能产生的所有情况。我们的目标是通过调整 $\theta$ ，使得获得更大奖励的轨迹出现的概率更高。
其中，轨迹 $\tau$在策略 $\pi_\theta(a|s)$ 下发生的概率定义为： $P(\tau ; \theta)=\left[\prod_{t=0}^{T} P\left(s_{t+1} \mid s_{t}, a_{t}\right) \cdot \pi_{\theta}\left(a_{t} \mid s_{t}\right)\right]$
表示为状态转移概率和动作选择概率的乘积，因为状态和动作决定轨迹。
### 1.3 导出策略梯度
与神经网络的优化思路相同，为了通过参数 $\theta$ 优化目标函数，我们需要计算目标函数 $J(\theta)$ 对 $\theta$ 的导数：$$\begin{aligned}\nabla_{\theta} J(\theta)&=\sum_{\tau} \nabla_{\theta} P(\tau ; \theta) R(\tau)\\&=\sum_{\tau} P(\tau ; \theta) \frac{\nabla_{\theta} P(\tau ; \theta)}{P(\tau ; \theta)} R(\tau)\\&=\sum_\tau P(\tau;\theta)\nabla_\theta\log P(\tau;\theta)R(\tau)\\&=\mathbb{E}_{\tau\sim\pi_{\theta}}\nabla_{\theta}\log P(\tau;\theta)R(\tau)\end{aligned}$$

梯度上升：
   $$\begin{align}
       \nabla E(R(\tau))_{\tau \sim P_\theta(\tau)}  
       & =\nabla \sum_\tau R(\tau) P_\theta(\tau) \\
       & =\sum_\tau R(\tau) \nabla P_\theta(\tau) \\
       & =\sum_\tau R(\tau) \nabla P_\theta(\tau) \frac{P_\theta(\tau)}{P_\theta(\tau)} \\
       & =\sum_\tau P_\theta(\tau) R(\tau) \frac{\nabla P_\theta(\tau)}{P_\theta(\tau)} \\
       & =\sum_\tau P_\theta(\tau) R(\tau) \frac{\nabla P_\theta(\tau)}{P_\theta(\tau)} \\
       & \approx \frac{1}{N} \sum_{n=1}^N R\left(\tau^n\right) \frac{\nabla P_\theta\left(\tau^n\right)}{P_\theta\left(\tau^n\right)} \\
       & =\frac{1}{N} \sum_{n=1}^N R\left(\tau^n\right) \nabla \log P_\theta\left(\tau^n\right) \\
       & =\frac{1}{N} \sum_{n=1}^N R\left(\tau^n\right) \nabla \log \prod_{t=1}^{T_n} P_\theta\left(a_n^t \mid s_n^t\right) \\
       & =\frac{1}{N} \sum_{n=1}^N R\left(\tau^n\right) \sum_{t=1}^{T_n} \nabla \log P_\theta\left (a_n^t \mid s_n^t\right) \\
       & =\frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n} R\left(\tau^n\right) \nabla \log P_\theta\left (a_n^t \mid s_n^t\right)
   \end{align}$$

推导过程中使用了对数导数技巧，将 $P(\tau ; \theta)$表示为对数的形式，从而可以方便的重写为：
$$\begin{aligned}\nabla_{\theta} \log P(\tau ; \theta)&=\nabla_{\theta}\left[\sum_{t=0}^{T} \log P\left(s_{t+1} \mid s_{t}, a_{t}\right)+\sum_{t=0}^{T} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right)\right]\\&=\sum_{t=0}^{T} \nabla_{\theta}\log \pi_{\theta}(a_{t} \mid s_{t})\end{aligned}$$其中第一步中第一项的状态转移概率只与环境有关，而与 $\theta$ 无关。
然而，到目前为止的计算还需要对所有可能的轨迹进行求和，这在实际的RL问题中是难以处理的，我们需要利用轨迹样本进行梯度近似，表达式如下：
$$\begin{aligned}\nabla_{\theta}J(\theta)&\approx\frac{1}{m}\sum_{i=1}^{m}\nabla_{\theta}\log P(\tau^{(i)};\theta)R(\tau^{(i)})\\&=\frac{1}{m}\sum_{i=1}^{m}(\sum_{t^{(i)}=0}^{T^{(i)}} \nabla_{\theta}\log \pi_{\theta}(a_{t^{(i)}} \mid s_{t^{(i)}}))R(\tau^{(i)})\\&\approx\frac{1}{n}\sum_{i=1}^{n}( \nabla_{\theta}\log \pi_{\theta}(a_{t^{(i)}} \mid s_{t^{(i)}}))R(t^{(i)})\end{aligned}$$现在的表达式是完全可计算的，现在只需要给出策略 $\pi_\theta$ 的明确定义，就可以计算得到 $\nabla_{\theta}J(\theta)$ ，从而用策略梯度更新规则： $\begin{aligned}\theta\leftarrow\theta+\alpha\text{}\nabla_\theta J(\theta)\end{aligned}$来对 \theta 进行调整优化，从而不断迭代至最优策略。
这里给出两种常见的策略示例及其对应梯度。
-   Softmax策略
对于**离散动作空间**，多使用Softmax策略。其定义如下：
$$\pi_{\theta}(s,a)=\frac{e^{\phi(s,a)^\top\theta}}{\sum_{a'\in A}e^{\phi(s,a')^\top}\theta}$$
对应的策略梯度为：
$$\nabla_\theta\log\pi_\theta(a\mid s)=\phi(s,a)-\sum\limits_{a'\in A}\phi(s,a')\pi_\theta(a\mid s)$$
梯度的结果可以解释为观察到的特征向量减去所有动作的平均特征向量。因此，如果奖励信号很高并且观察到的向量与平均向量相差很大，就会有增加该动作概率的强烈趋势。
-   高斯策略
对于**连续动作空间**，多使用高斯策略。其定义如下
$$\quad\pi_{\theta}(a\mid s)=\dfrac{1}{\sqrt{2\pi}\sigma_{\theta}}e^{-\frac{a-\mu_{\theta}}{2\sigma_{\theta}^{2}}}$$
其中正态分布的均值 $\mu_\theta=\phi(s,\mathrm{a})^\top\theta$ 。
对应的策略梯度为：
$$\nabla_\theta\log(\pi_\theta(a\mid s))=\dfrac{(a-\mu_\theta)\phi(s)}{\sigma_\theta^2}$$
同样地，在高回报的情况下，远离均值的动作会触发强烈的更新信号。由于概率总和必须为 1，因此增加某些轨迹的概率也意味着减少其他轨迹的概率。
在实际任务中，我们没有必要手动计算偏导数，使用深度学习框架的自动求导就好了。为了实现自动求导，我们只需要根据之前策略梯度的推导结果，定义损失函数如下：
$$\begin{aligned}\mathcal{L}(a,s,r)=-\log\left(\pi_\theta(a\mid s)\right)r\end{aligned}$$就可以让计算机解决所有的推导。
### 1.4 算法实现
Williams提出的REINFORCE算法[[1]](#ref_1)是经典的策略梯度算法之一。其伪代码如下所示：
![](https://picx.zhimg.com/v2-ee61e233cb70e8f6ff725b40d88e31b9_1440w.jpg)
图2. REINFORCE算法
算法思路简介明了，是最简单的基于似然比的策略梯度强化学习算法。接下来我们将介绍如何对REINFORCE算法进行一步步的优化，直到发展为强大的PPO算法的过程。