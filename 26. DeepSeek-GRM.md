---
created: 2025-08-28T16:28:29 (UTC +08:00)
tags: [DeepSeek,推理大模型,LLM（大型语言模型）]
source: https://zhuanlan.zhihu.com/p/1891510512497446981
author: 关于作者北方的郎专注模型与代码，公众号：AI方法与实践吕阿华、郭达森、Sam聊算法也关注了他回答1,200文章1,588关注者13,741关注他发私信
---
这是DeepSeek-R2使用的新技术吗？

2025年4月3日，来自DeepSeek-AI和清华大学的研究团队提出了一种革命性的方法——"[推理时扩展](https://zhida.zhihu.com/search?content_id=255993419&content_type=Article&match_order=1&q=%E6%8E%A8%E7%90%86%E6%97%B6%E6%89%A9%E5%B1%95&zhida_source=entity)的通用奖励建模"(Inference-Time Scaling for Generalist Reward Modeling)，通过创新的"[自原则批判调优](https://zhida.zhihu.com/search?content_id=255993419&content_type=Article&match_order=1&q=%E8%87%AA%E5%8E%9F%E5%88%99%E6%89%B9%E5%88%A4%E8%B0%83%E4%BC%98&zhida_source=entity)"(Self-Principled Critique Tuning: SPCT)技术，使奖励模型在推理阶段能够动态扩展计算资源，显著提升评判质量。

这项研究最引人注目的发现是：一个经过SPCT训练的270亿参数模型[DeepSeek-GRM](https://zhida.zhihu.com/search?content_id=255993419&content_type=Article&match_order=1&q=DeepSeek-GRM&zhida_source=entity)\-27B，通过并行采样32次并配合[元奖励模型](https://zhida.zhihu.com/search?content_id=255993419&content_type=Article&match_order=1&q=%E5%85%83%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B&zhida_source=entity)引导的投票机制，其性能甚至可以超越未经SPCT训练的671B参数模型。这一突破不仅展示了推理时扩展的巨大潜力，也为AI对齐研究开辟了新方向。

论文地址：[Inference-Time Scaling for Generalist Reward Modeling](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2504.02495)

## **1\. 引言**

![](https://pica.zhimg.com/v2-d03b27caab482b052a774e778a6087b2_1440w.jpg)

近年来，大型语言模型在理解和生成任务上取得了显著进展，而强化学习作为一种后训练方法，进一步提升了模型的推理能力和价值观对齐。然而，传统奖励模型面临两大挑战：

1.  **通用性不足**：多数奖励模型依赖人工规则或特定领域的数据，难以适应开放领域的复杂需求。
2.  **推理时扩展效率低**：增加计算资源时，模型性能提升有限。

针对这些问题，DeepSeek-AI团队提出了**生成式奖励模型（Generative Reward Model, GRM）**，并结合**SPCT方法**，通过动态生成原则和批判性反馈，实现了模型在推理时的灵活扩展。实验表明，DeepSeek-GRM在多个基准测试中表现优异，甚至优于更大规模的模型。

![](https://pic2.zhimg.com/v2-63e2ce25f343de956ebc91f0c71d0129_1440w.jpg)

## 2 背景与相关工作

奖励模型作为强化学习中的关键组件，其质量直接影响着语言模型的最终表现。传统奖励模型主要分为三类：标量型、半标量型和生成型，每种类型在输入灵活性和推理时扩展性方面各有优劣。

**标量奖励模型(**Scalar Reward Models**)**如Bradley-Terry模型，直接输出数值评分，虽然简单高效但缺乏灵活性。研究表明，这类模型在特定领域(如数学验证)表现良好，但在通用场景下容易产生偏差。例如，在实验中，标量模型DeepSeek-BTRM-27B在某些子测试集上准确率可达86.2%，但在其他领域可能骤降至52.3%，显示出明显的领域偏差。

**半标量模型(**Semi-Scalar Reward Models**)**如CLoud-[Gemma-2-27B](https://zhida.zhihu.com/search?content_id=255993419&content_type=Article&match_order=1&q=Gemma-2-27B&zhida_source=entity)，尝试通过同时生成文本评判和标量分数来平衡灵活性与效率。然而，这类模型的推理时扩展能力仍然受限，实验显示其性能提升幅度仅为0.3%，远低于生成型方法。

**[生成型奖励模型](https://zhida.zhihu.com/search?content_id=255993419&content_type=Article&match_order=1&q=%E7%94%9F%E6%88%90%E5%9E%8B%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B&zhida_source=entity)(Generalist Reward Modeling: GRM)**代表最新方向，完全通过自然语言生成评判内容。[LLM-as-a-Judge](https://zhida.zhihu.com/search?content_id=255993419&content_type=Article&match_order=1&q=LLM-as-a-Judge&zhida_source=entity)等方法展示了这一范式的潜力，但在准确性和一致性方面仍有不足。研究团队发现，现有方法在推理时扩展方面存在明显局限——要么无法有效利用增加的计算资源，要么缺乏系统性的质量提升机制。

一个关键洞见是：高质量评判往往依赖于恰当的"原则"(principles)指导。初步实验显示，当使用经过筛选的合适原则时，GPT-4o在Chat Hard测试集上的准确率可从76.1%提升至77.8%，而Gemma-2-27B-it模型则从59.1%跃升至68.0%。这激发了研究者探索如何让模型自主生成和运用这些原则，从而在通用领域实现更可靠的奖励建模。

![](https://pic2.zhimg.com/v2-ad29a535b5d5a9381cc217cbe05944bf_1440w.jpg)

## 3 自原则批判调优(Self-Principled Critique Tuning：SPCT)方法

### 从理解到生成的原则转变

![](https://pic4.zhimg.com/v2-bb2137d30a15975819b34618ed04107f_1440w.jpg)

传统方法将原则视为预定义的固定规则，而SPCT创新性地将原则生成作为奖励建模过程的一部分。这种转变带来了三个关键优势：

1.  **动态适应性**：原则可以根据具体查询和待评判响应动态生成，而非僵化套用。公式表示为：
    

![](https://picx.zhimg.com/v2-914ace8833a5c3c8b7b3a73fe1849961_1440w.jpg)

其中 $p_θ$ 是原则生成函数，与奖励生成函数 $r_θ$ 共享模型参数。

1.  **质量可提升性**：通过后续训练，模型生成的原则和相关批判的质量可以持续改进。实验数据显示，加入原则生成使模型在Reward Bench上的整体性能从67.5%提升至69.9%。
2.  **评判细粒度**：更多原则意味着更丰富的评判视角。在推理时扩展中，模型能生成多样化的原则集，从而使最终奖励具有更精细的区分度。

### 基于规则的强化学习

SPCT采用两阶段训练策略：拒绝式微调(RFT)作为冷启动，随后是基于规则的在线强化学习。

**拒绝式微调阶段**精心构建训练数据：

-   使用预训练GRM对单响应、配对响应和多响应三种格式进行采样(N\_RFT=3次)
-   采用双重拒绝策略：排除预测奖励与真实标签不符的轨迹，以及所有N\_RFT次评判都正确的"过于简单"样本
-   引入提示采样(hinted sampling)技术，将argmax{r\_i}直接加入提示，引导模型对齐真实标签

![](https://pic1.zhimg.com/v2-35bb703b3fbadbbacd5217286157a694_1440w.jpg)

**基于规则的RL阶段**采用GRPO框架，创新之处在于：

1.  完全依赖基于结果的规则奖励(准确=+1，错误=-1)，不使用格式奖励
2.  设置较大的KL惩罚系数(β=0.08)以防止模式崩溃和严重偏差
3.  组大小G=4，平衡效率与性能

实验数据显示，这一阶段使模型在Reward Bench上的性能从RFT后的68.8%进一步提升至69.9%。值得注意的是，即使没有冷启动，仅使用在线RL也能将仅经一般指令调优的模型性能从66.1%提升至68.7%，证明了该方法的有效性。

![](https://picx.zhimg.com/v2-a03caa1d851a62c9eb121d7c510e6c03_1440w.jpg)

## 4 推理时扩展（Inference-Time Scaling with SPCT）

### 基于投票的奖励生成

DeepSeek-GRM通过并行采样实现推理时计算扩展，其投票过程数学表示为：

![](https://pic2.zhimg.com/v2-9e0349abe1a5c2fb58e412ee74d12625_1440w.jpg)

关键优势在于：

-   将离散奖励空间(通常1-10)扩展k倍，使细微差异可分辨
-   每个原则可视为评判视角的代理，更多原则能更准确反映真实分布
-   实验显示，从1样本扩展到8样本时，DeepSeek-GRM-27B性能提升2.7%，远超LLM-as-a-Judge的0.6%和CLoud-Gemma-2-27B的0.3%

### 元奖励模型引导的投票（Meta Reward Modeling Guided Voting）

为解决部分采样质量低的问题，研究团队训练了一个专门的元奖励模型：

-   架构：基于Gemma-2-27B的点wise标量RM
-   训练数据：来自RFT阶段的拒绝采样数据+DeepSeek-GRM的新采样轨迹
-   目标函数：二元交叉熵损失，标签由公式(4)确定
-   应用方式：对k个采样奖励输出元评分，仅使用top k\_meta(默认k/2)进行最终投票

这一设计带来了显著提升：当k=32时，纯投票达到71.0%，而元RM引导投票达到72.8%，超过了未使用SPCT的671B模型的表现(88.4% vs 90.4%)。消融实验显示，k\_meta=16时效果最佳，过小(k\_meta=1)或过大(k\_meta=32)都会导致性能下降。

![](https://pic1.zhimg.com/v2-032f3855ce4b7c9c94fc6097cec54168_1440w.jpg)

## 5 实验结果与分析

### 基准测试表现

研究团队在四个权威基准上进行了全面评估：

1.  **Reward Bench**：包含聊天、推理和安全三个子领域。DeepSeek-GRM-27B在推理时扩展后达到90.4%，超越GPT-4o的86.7%和Nemotron-4-340B-Reward的92.0%。
2.  **PPE**：重点关注可验证任务的正确性。虽然标量模型在此类任务上传统占优(如DeepSeek-BTRM-27B达66.7%)，但加入参考信息后，DeepSeek-GRM-27B可达到惊人的91.6%准确率。
3.  **RMB**：评估帮助性和无害性。在"多选一"(BoN)任务中，DeepSeek-GRM-27B直接评估多个响应的方式(62.3%)与传统的成对比较(62.1%)效果相当，证明了其输入灵活性。
4.  **Real.Mistake**：诊断单响应中的错误。DeepSeek-GRM-27B达到72.2%的ROC-AUC，优于同规模标量模型(69.3%)，扩展后进一步提升至74.4%。

![](https://pic3.zhimg.com/v2-1e1c81513b61d2a27adaa495e2cb1914_1440w.jpg)

### 扩展效率比较

最引人注目的发现是推理时扩展与训练时扩展的对比：

-   纯计算扩展：DeepSeek-GRM-27B(27B参数)通过32样本投票达到88.5%，接近DeepSeek-GRM-671B(671B参数)的88.4%
-   元RM引导扩展：相同条件下达到90.4%，实际超越671B模型
-   计算成本：训练671B模型需要数千GPU小时，而推理时扩展仅增加有限延迟

这一结果验证了研究核心假设：**通过适当的训练方法，推理时计算扩展可以成为模型规模扩展的有效替代**。

![](https://pic1.zhimg.com/v2-cbd157a4bb09179d959f5ee393a7b402_1440w.jpg)

### 失败模式分析

![](https://pic1.zhimg.com/v2-2b89f98ae724da6037bcc82b2412402a_1440w.jpg)

通过对错误案例的抽样分析(图8)，研究发现主要失败原因包括：

1.  复杂模式识别不足(38%)
2.  特定领域知识缺乏(29%)
3.  原则权重分配失衡(21%)
4.  基准标注自身矛盾(12%)

这些发现为未来改进指明了方向，特别是增强模型的专家知识和复杂推理能力。

### 未来方向

论文提出多个有前景的发展路径：

1.  **工具增强**：集成代码解释器和搜索引擎，提升数值计算等特定任务的准确性
2.  **流程解耦**：将原则生成与批判生成分离，提升效率
3.  **离线评估**：利用生成原则作为解释性协议，诊断LLM弱点
4.  **长链推理**：探索更复杂的推理过程，可能进一步提升性能

## 结论

DeepSeek-AI和清华大学团队的这项工作，通过创新的自原则批判调优方法，重新定义了通用奖励模型的训练范式。SPCT技术使生成型奖励模型能够：

-   自主产生适应性原则
-   通过在线RL优化评判行为
-   有效利用推理时计算扩展

实验结果令人信服地证明，适当的方法创新可以超越单纯的规模扩展，为AI对齐研究提供了新的技术路径。随着DeepSeek-GRM模型的开源，这一技术有望在语言模型训练、评估和应用等多个环节产生深远影响，推动人工智能向更安全、更可靠的方向发展。

——完——

[@北方的郎](https://www.zhihu.com/people/7af62e4119791a452e88718cb5ccc0be) · 专注模型与代码

喜欢的朋友，欢迎赞同、关注、分享三连 ^O^
