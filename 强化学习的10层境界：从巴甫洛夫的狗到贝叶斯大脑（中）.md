---
created: 2025-11-05T11:32:04 (UTC +08:00)
tags: [强化学习 (Reinforcement Learning)]
source: https://zhuanlan.zhihu.com/p/1933659071204038283
author: 关于作者gwave我思故我在，生命的意义在于负熵复旦大学 电子与信息博士郭达森也关注了他回答985文章193关注者37,956已关注发私信
---

# 强化学习的10层境界：从巴甫洛夫的狗到贝叶斯大脑（中）

> ## Excerpt
> 在 上一篇中，我们探讨了行为心理学的根基（第1–3层境界）以及强化学习的基石（第4–6层境界）。本文将继续迈向更高层次，讨论强化学习的进阶话题，我们先开启第七层境界：Actor-Critic 与 PPO。第七境界：双轮驱…

---
在[上一篇](https://zhuanlan.zhihu.com/p/1932009376987717993)中，我们探讨了行为心理学的根基（第1–3层境界）以及强化学习的基石（第4–6层境界）。本文将继续迈向更高层次，讨论强化学习的进阶话题，我们先开启第七层境界：[Actor-Critic](https://zhida.zhihu.com/search?content_id=260990906&content_type=Article&match_order=1&q=Actor-Critic&zhida_source=entity) 与 PPO。

## **第七境界：双轮驱动 —— Actor-Critic 与 PPO**

7.1 **引言**

在基于策略的方法中，我们见识了策略梯度的“真传”——[REINFORCE](https://zhida.zhihu.com/search?content_id=260990906&content_type=Article&match_order=1&q=REINFORCE&zhida_source=entity) 用一条公式揭示了“好行为应被强化”的朴素逻辑。然而，这把利剑虽锋利，却也颇为难驾驭。问题出在哪里？高方差。REINFORCE 每次都要等一整个轨迹跑完，用回报 $G_t$ 来估计梯度，这种全局评估带来了巨大的不确定性。一次好运或厄运，可能让策略更新方向偏离应有的轨迹。就像一个学生考试只看总分，不管过程细节，很难知道哪道题做对了，哪里需要改进。

我们需要一种更细腻、更稳定的评估方式：**能不能在每一步都告诉策略“这个动作值不值”？”**答案就是——引入一个**Critic（评论家）**，为每一个状态—动作组合打分。这样，策略（Actor）不再孤军奋战，而是在价值函数的引导下，沿着更合理的方向前进。这就是**Actor-Critic**方法诞生的契机。它融合了策略优化与价值评估的优点，开启了强化学习“双轮驱动”的新时代。

![|500](https://pic1.zhimg.com/v2-26ec04a34e03b4244d4742a15d3ca692_1440w.jpg)

https://huggingface.co/blog/deep-rl-a2c

值得一提的是，Actor-Critic 架构在结构上与生成对抗网络（GAN）有着有趣的相似性。虽然它们分别诞生于强化学习和生成建模两个领域，但都体现出“两个网络协同博弈、彼此驱动学习”的核心思想。在 Actor-Critic 中，**Actor** 是策略网络，负责生成动作；**Critic** 是评估器，判断当前动作是否“值得鼓励”。Actor 根据 Critic 的反馈调整自己的参数，以便未来能做出更优的决策。而在 GAN 中，**Generator** 生成伪造样本，**Discriminator** 判别样本真伪。Generator 的目标是不断改进生成策略，欺骗Discriminator；而Discriminator则努力分辨真伪，从而促使**Generator**学习出更“真实”的分布。换句话说，**Actor 相当于 Generator，它们都负责“动手”；Critic 类似于 Discriminator，负责“动嘴“--打分与反馈。** 不同的是，GAN 中的两个智能体相互对抗，是**零和博弈（Zero-sum Game；**而 Actor-Critic中，二者目标一致，**共同最大化整体回报**，本质上是一种 **合作性博弈**（Cooperative Game）。

然而，Actor-Critic 虽然解决了高方差问题，却也引入了新的挑战：价值评估的不稳定可能反过来误导策略更新，尤其是在使用深度神经网络时。这时候，人们发现，仅仅有“轮子”还不够，还要有**减震器**。这便是 **PPO（Proximal Policy Optimization）** 的出场时机。它可以被看作是带“安全带”的策略梯度法，对每一次策略更新都加以限制，防止跳得太远、走得太快。PPO 采用了一种“剪切（Clip）目标函数”的策略优化方式，确保更新过程保持在一个合理范围内，就像在悬崖边为学生装上了护栏，既能大胆尝试，又不至于跌落深渊。PPO 的成功，不仅在于它优化效果好、泛化能力强，更重要的是它让强化学习真正走进了工业界。今天，无论是游戏、机器人控制，还是大模型对齐训练，PPO 都是主流中的主流。它与 Actor-Critic 的融合，为策略优化奠定了坚实而优雅的基石。

最后，我们也会看下PPO的变形--DeepSeek的[GRPO](https://zhida.zhihu.com/search?content_id=260990906&content_type=Article&match_order=1&q=GRPO&zhida_source=entity)。

### **7.1 Actor-Critic：双轮协作的智慧进化**

策略梯度方法（如 REINFORCE）虽然直观易懂，能够直接从回报中学习策略，但其训练过程常常面临**高方差问题**。原因在于它采用 [Monte Carlo](https://zhida.zhihu.com/search?content_id=260990906&content_type=Article&match_order=1&q=Monte+Carlo&zhida_source=entity) 方式估计回报，即整条轨迹跑完后，用总回报 $G_t$ 来更新策略。这种“轨迹级”的更新方法对每一步动作的反馈过于粗糙（因为，在这个动作之后，还有很多未来随机因素影响最终总回报），训练效率低下，稳定性也较差。

![|500](https://pic4.zhimg.com/v2-bad050fe2fb91c1d577be0e03d3f4e9f_1440w.jpg)

https://www.mdpi.com/2079-9292/12/24/4939

为此，我们引入了一位新的“评审”——Critic，它与策略网络 Actor 形成了协同工作的一对网络：Actor 决定采取什么动作，而 Critic 则评估这个动作的价值，并指导 Actor 如何调整策略。Critic 学习的是值函数（如 $V(s)$ 或 $Q(s,a)$ ），而 Actor 则根据策略梯度公式更新策略，整体思路如下：

> **“Actor 基于策略梯度学习策略，Critic 通过估计值函数来评估 Actor 的行为好坏。”**

这种结构称为 **Actor-Critic**，实质上是将 REINFORCE 的全轨迹回报替换为使用 TD 学习（Temporal Difference）的 bootstrapping 值，降低了方差（critic使用的值估计平滑了随机性），提高了训练效率（无需等到Episode结束）。从形式上看，原始 REINFORCE 的优势函数为：

$A(s_t, a_t) = \sum_{t’=t}^{T-1} r_{t’} - b(s_t)$

其中 $b(s_t)$ 是某种 baseline，通常选为当前状态的值函数估计： $b(s_t) = V_{\pi_\theta}(s_t)$ 。

![|500](https://pic1.zhimg.com/v2-f878b00488f77c12a7202590cd78cec8_1440w.jpg)

https://huggingface.co/blog/deep-rl-a2c

而在 Actor-Critic 框架下，优势函数变为： $A_{\pi_\theta}(s_t, a_t) = r(s_t, a_t) + \gamma V_{\pi_\theta}(s_{t+1}) - V_{\pi_\theta}(s_t)$ 。打个比方：REINFORCE 就像只根据一篇作文的总分来给学生打分——你根本不知道哪部分写得好，哪部分有问题。而 Actor-Critic 则像是在写作过程中提供逐句点评——更稳定、更有指导意义，也更容易帮助学生改进。

值得一提的是，这种结构与 GAN（生成对抗网络）颇有几分相似：Actor 像生成器，Critic 像判别器。然而不同于 GAN 中的零和博弈，Actor-Critic 中双方目标一致，彼此协作，共同提升策略性能。二者都属于“双网络博弈”结构，却一个对抗一个共生，呈现出不同的学习动态：GAN 追求两者博弈下的纳什均衡，而 Actor-Critic 并不追求收敛于均衡，而是由 Critic 引导 Actor 攀登策略性能的“回报山峰”。Actor 与 Critic 是一个合作性博弈系统中的两个子智能体，互为师徒、彼此成就。**Actor 借 Critic 的眼，看清未来回报的方向；Critic 借 Actor 的脚，踏遍环境状态的山河。

从博弈论的角度来看，Actor-Critic 结构可形式化为一种**协作型的 [Stackelberg 博弈](https://zhida.zhihu.com/search?content_id=260990906&content_type=Article&match_order=1&q=Stackelberg+%E5%8D%9A%E5%BC%88&zhida_source=entity)（Stackelberg Game）**。在这种博弈中，参与方分为“领导者（Leader）”与“跟随者（Follower）”，两者在策略制定上存在先后顺序，而目标并非相互对抗，而是共同最大化某个全局效用函数。在 Actor-Critic 框架中：

-   **Critic 相当于领导者**：其职责是根据当前策略 $\pi_\theta$ 估计值函数 $V_w(s)$ 或优势函数 $A_w(s,a)$ ，从而定义出一个策略优化的“局部梯度方向”；
-   **Actor 则是跟随者**：它接受 Critic 的评估，更新策略，以最大化长期回报。

整个优化过程可以被表述为一个双层优化问题（Bi-level Optimization Problem）：

$\begin{aligned} & \max_\theta \; J(\theta) = \mathbb{E}_{s \sim d^{\pi_\theta}, a \sim \pi_\theta} \left[ A_w(s, a) \right] \\ & \text{s.t.} \quad w = \arg\min_{w’} \; \mathcal{L}_{\text{Critic}}(w’) = \mathbb{E} \left[ \left( r + \gamma V{w’}(s’) - V_{w’}(s) \right)^2 \right] \end{aligned}$ ，其中， $d^{\pi_\theta}$ 它表示在遵循策略 $\pi$ 时，智能体在各个状态上出现的概率分布。

在这个结构中，**Actor 优化的是上层问题**，其梯度方向依赖于 **Critic 对值函数的估计质量**；而 Critic 本身是下层问题的求解者，基于 Actor 当前策略下的数据分布不断自我迭代逼近。换句话说：

-   Critic 决定“登山的方向”；
-   Actor 决定“登山的速度与路径”；
-   二者通过梯度与采样相互作用，形成一个动态协同系统。
    

![](https://pic1.zhimg.com/v2-5386bd3290f5366c1fa10cd6d8a1a388_1440w.jpg)

https://www.researchgate.net/publication/342286407\_MEC-Assisted\_Immersive\_VR\_Video\_Streaming\_Over\_Terahertz\_Wireless\_Networks\_A\_Deep\_Reinforcement\_Learning\_Approach/figures?lo=1

> 在 Actor-Critic 中，Critic 并不是直接估计整条轨迹的回报，而是估算 Advantage（优势函数），表示“这个动作相较于平均水平好多少”。因此，Actor-Critic 方法中其实有两个 “A”：一个是 Actor，一个是 Advantage，这也让它得名 **[A2C](https://zhida.zhihu.com/search?content_id=260990906&content_type=Article&match_order=1&q=A2C&zhida_source=entity)**（Advantage Actor-Critic）。再加上一点工程技巧——并行多个智能体进行异步更新，那么就演化为著名的 **[A3C](https://zhida.zhihu.com/search?content_id=260990906&content_type=Article&match_order=1&q=A3C&zhida_source=entity)**（Asynchronous Advantage Actor-Critic），进一步提升了训练效率和稳定性。

### **7.2 PPO：策略优化的“安全带”**

在理解了 Actor-Critic（A2C）架构之后，我们看到了策略梯度和价值估计“双轮驱动”的威力：策略在价值函数的引导下，学习得更快更稳。然而，A2C 存在一个关键问题是：策略更新可能太激进。由于策略是直接优化的，如果每一步更新幅度太大，模型可能会“遗忘”旧策略，陷入策略崩塌（policy collapse）或训练不稳定的境地。这就像一个演员在评论家的指点下改进表演，但每次改动都太剧烈，反而失去了原本的风格。

![](https://pic1.zhimg.com/v2-43cc852ca03dca46631b5d1068401446_1440w.jpg)

https://vitalab.github.io/article/2019/05/09/PPO.html

这时，**Proximal Policy Optimization（PPO）** 应运而生。它在 A2C 的基础上，引入了“克制更新”的思想，确保每次策略更新都“不过火”，稳步推进。这种“靠近优化”（Proximal Optimization）策略，主要通过两种方式实现：

1.  **截断重要性采样比值（Clipped Objective）**：约束新旧策略的差异在一个范围内（比如 1.0±0.2），避免过大跳跃；
2.  **提前终止更新（Early Stopping）**：一旦策略偏离旧策略太远，就提前中止梯度更新。

![](https://pica.zhimg.com/v2-8fd88485584527c62cd2fa7755b3e278_1440w.jpg)

PPO https://www.mdpi.com/2076-3417/12/18/9376

简单来说，PPO 就是“给 Actor 上了安全带”，在 Critic 的指导下小步快跑，成为当前强化学习中最稳定、最实用的策略优化算法之一。

![](https://picx.zhimg.com/v2-243400bb20b19706923e2d68ea7efbdf_1440w.jpg)

https://huggingface.co/blog/deep-rl-ppo

其中$r_t(\theta)$ 表示的是当前策略与旧策略之间的**概率比值**(ratio)，它表示的是当前策略在状态 $s_t$ 下采取动作 $a_t$ 的概率，与旧策略下采取同一动作的概率之比 ,在状态 $s_t$ 下：

-   如果 $r_t(\theta) >1$ ，说明当前策略比旧策略，更可能倾向于选择动作 $a_t$ ;
    
-   如果 $r_t(\theta)\in[0,1]$ ，说明当前策略相比旧策略，在选择动作 a\_t 的可能性降低了。

因此，这个**概率比值**提供了一个直观而有效的方式，用于估计新旧策略之间的差异程度（即策略之间的“偏移”）。这个Ratio起到了REINFORCE中 $\log$ 项（ $\nabla_\theta \log \pi_\theta(a_t｜ s_t)$ ）的类似作用，且降低了variance，因而更稳定。PPO使用了Clip机制，限制该比值偏离 1 的幅度，从而实现了**稳定而受控的策略更新**。

![](https://pica.zhimg.com/v2-5a50af32ad1e586dc8fadc096995b066_1440w.jpg)

https://huggingface.co/blog/deep-rl-ppo

**小结**：相比 REINFORCE 高方差、Actor-Critic 不稳定等问题，PPO 实现了策略优化的“三性统一”：**稳定性、效率、收敛性**，因此成为深度强化学习中最主流的算法之一，广泛用于游戏智能体、机器人控制和语言模型微调等任务。其关键改进主要有三点：1）**概率比值 (probability ratio)作为策略梯度的调整因子； 2）Clip机制防止策略的激进更新；3）优势函数（Advantage）引导。**

### 7.3 GRPO: **无 Critic 的群体相对策略优化**

![](https://pic2.zhimg.com/v2-bc7b54db762f9580a06df6847768c227_1440w.jpg)

https://www.oxen.ai/blog/why-grpo-is-important-and-how-it-works

传统的 PPO 依赖 Critic 网络估算 $V(s)$ ，但对于参数量巨大的 LLMs，这带来了高昂的显存开销和训练复杂度。DeepSeek提出GRPO，其突破在于：**弃用价值函数**，只用当前模型生成多个候选输出（group sampling），并通过组内平均得分作为 baseline：

-   给定一个Prompt $q$ ，**策略模型**（Policy Model）生成 $G$ 个候选输出 $\{o_1, o_2, \dots, o_G\}$ ，形成一个Group；
-   **Reward Mode**l对每个候选输出评分，得到对应的奖励： $\{r_1, r_2, \dots, r_G\}$ ；同时，通过一个 **Reference Model** 计算出策略漂移的 KL 惩罚；
-   **组内归一化比较（Group Computation），** 变成组内的相对 advantage 分数： $\{A_1, A_2, \dots, A_G\}$ ；
-   利用组内的优势函数 $A_i$ 和 KL 约束下的损失函数，对策略模型进行优化更新；
    

在 GRPO 中，**优势估计（Advantage Estimation）不再依赖 Critic 网络**。相反，它通过 **组内输出的相对表现** 来构建优势函数，从而彻底省去了价值函数的学习。这一设计不仅减少了模型参数和训练成本，还规避了 Critic 带来的高方差估计问题。具体而言，GRPO 沿用了 PPO 的核心结构和 **Clipping 更新机制**，但替换了 Advantage 的计算方式： $\max_\theta \frac{1}{G} \sum_{i=1}^{G} \min\left( r_i A_i, \ \text{clip}(r_i, 1 - \epsilon, 1 + \epsilon) \cdot A_i \right)$ ，其中：

-   $r_i = \frac{\pi_\theta(a_i \mid s)}{\pi_{\theta_{\text{old}}}(a_i \mid s)}$ ：是新旧策略的概率比；
-   $A_i = r(s, a_i) - \mu$ ：是 group 内第 i 个输出的 **群体优势**，用个体奖励减去组内平均奖励 $\mu$ 。
    

这种设计使得策略更新更倾向于组内表现 **优于平均（positive advantage）** 的样本，而对那些落后于组平均的样本进行压制，从而提高整体学习质量。与PPO相比，优点在于：

-   **完全去除 Critic 模块**：无需估计状态值或动作值，模型更轻量。
-   **Variance 更低**：组内排序、归一化等方式天然降低了梯度估计的方差。
-   **训练收敛更快**：适用于大语言模型的自然语言生成任务，尤其在 RLHF 场景中表现良好。

但也存在一定局限性：

1.  **优势估计精度有限**：群体优势只能基于当前 prompt 的 group 表现，**缺乏对未来回报的建模**，估计可能偏离最优。如果某组（通常容量为 6–8 个回答）的整体质量较差，也就是所有候选答案的奖励都处于一个较低水平，那么 GRPO 的“群体优势”计算就会失真，策略会向着“局部最优”（其实也比较差）的结果收敛，强化劣质输出，只看眼前，看不见长远，不利于多步决策，个人认为这是GRPO的核心短板。
2.  **对奖励模型高度依赖**：如果 Reward Model 本身质量不高或打分不稳定，容易导致策略学习偏移。
3.  **抗噪性弱**：PPO 中 Critic 提供了对 noisy reward 的平滑能力，GRPO 则直接受奖励影响，鲁棒性下降。
4.  **缺乏 Bootstrapping 能力**：无法通过值函数将长期收益回传，**难以建模长期依赖任务**。
5.  **策略更新缺乏理论保障**：虽然GRPO保留了类似 PPO 的 clipping 结构，其核心仍基于 group 内部排序，**缺乏严格理论边界约束**，可能出现训练不稳定或过拟合 batch 的问题（特别在Group分布不平衡时）。

小结：GRPO 用 group 内比较替代了价值估计，提升了效率但降低了泛化与稳定性，适合 reward 足够精确、偏重局部反馈的任务，例如大模型文本生成。

## **第八境界：动机觉醒** —— 内部奖励与自主探索

从前面的境界可以看到，策略优化的核心是“如何更好地学习外部奖励信号”，不管是通过值函数（Actor-Critic）、策略比值（PPO），还是通过相对排序（GRPO），其本质都属于**外部驱动（extrinsic motivation）** —— 代理被动地追随环境给予的奖励信号。然而，人类的学习远不止如此。

我们并非总是依赖外部反馈，有时，我们会因为好奇、兴趣、挑战自我、保持秩序感，甚至是“完成一件事的快感”而持续学习。**这类无需外部奖励的驱动，我们称之为“内在动机”（intrinsic motivation）**。在强化学习中，动机觉醒的到来，是对智能体“自主性”的一次觉醒：

> **“智能体**不仅在寻找最大回报的策略，**更在寻找探索世界的理由。”**

内部奖励有几种常见形式，我们可以将内在动机视为一种自我构造的奖励信号，并注入到策略学习过程中，主要形式包括：

-   **好奇心（Curiosity）：**激励智能体探索模型尚不熟悉的状态（例如基于 prediction error 的 Novelty bonus）。常见方法如 [ICM](https://zhida.zhihu.com/search?content_id=260990906&content_type=Article&match_order=1&q=ICM&zhida_source=entity)（Intrinsic Curiosity Module）、[RND](https://zhida.zhihu.com/search?content_id=260990906&content_type=Article&match_order=1&q=RND&zhida_source=entity)（Random Network Distillation）等。
-   **状态熵（State Entropy）：**激励智能体进入高度不确定的状态，鼓励广泛探索，而非陷入局部策略。代表方法如 MaxEnt RL（最大熵强化学习）。
-   **进展/掌控感（Competence / Empowerment）：**让智能体学习如何更有效地影响环境、达到目标，而非仅获取即时奖励。
    

![](https://pic2.zhimg.com/v2-a7c804c1d827557807de1c6dc7a5a02b_1440w.jpg)

Intrinsic Motivation and Reinforcement Learning https://link.springer.com/chapter/10.1007/978-3-642-32375-1\_2

### 8.1 **好奇心驱动 —— 预测误差与新奇探索**

在现实世界中，人类并不会只因外部奖励而行动。我们常常会因为“想知道接下来会发生什么”而主动去尝试、去探索，这种源自未知与不确定性的内在冲动，被称为**好奇心（Curiosity）**。在强化学习中，**好奇心驱动的探索**是一类典型的内在动机机制，其核心思想是：

> **“如果模型当前无法很好预测某个状态的后续变化，那么这个状态就值得探索。”**

这种机制为强化学习智能体引入了一种**自我监督**的方式：即便外部环境没有明确给出奖励，智能体也可以根据自身的预测误差生成“伪奖励”，从而主动进入那些尚未掌握的状态空间。

**原理：基于预测误差的内部奖励**

具体而言，好奇心机制通常基于以下两个核心组件：

-   一个 **前向预测器**（Forward Model），预测当前状态 $s_t$ 和动作 $a_t$ 所导致的下一个状态 $s_{t+1}$
-   一个 **误差函数**，衡量实际观察到的 $s_{t+1}$ 与预测值之间的差异
    

然后，将这个差异作为内部奖励： $r^{\text{int}}_t = \| f(s_t, a_t) - s_{t+1} \|^2$ ，该信号可以与环境奖励叠加： $r^{\text{total}}_t = r^{\text{ext}}_t + \beta \cdot r^{\text{int}}_t$ ，其中 $\beta$ 是内外部奖励的权重平衡因子。

**代表方法**

-   Pathak et al. (2017)提出**ICM（Intrinsic Curiosity Module），**其核心思想：使用编码器将原始图像状态压缩为特征向量，基于特征空间中的预测误差计算内部奖励，优点是在像素级状态下仍能稳定工作，适用于视觉任务；
-   Burda et al. (2018) 提出的RND（Random Network Distillation）随机初始化一个 target 网络，然后训练一个 predictor 网络去逼近它。预测误差越大，表示状态越“新颖”，与 ICM 不同点：RND 不需要前向模型，只基于状态本身的难以预测性，稳定性更强，训练更简单；
-   DeepMind 提出的结合 episodic memory + lifelong novelty 的方法NGU（Never Give Up），兼顾短期新颖和长期好奇探索，适合复杂游戏环境（如 Atari、DM Lab）。
    

在近两年的研究中，关于**好奇心驱动（curiosity-driven）探索机制**的工作仍在持续演进，并展现出更高的稳定性与环境适应性。2025年，一项系统性研究(The impact of intrinsic rewards on exploration in Reinforcement Learning<sup data-text="" data-url="https://arxiv.org/html/2501.11533v1?utm_source=chatgpt.com" data-numero="1" data-draft-node="inline" data-draft-type="reference" data-tooltip="&lt;a href=&quot;https://arxiv.org/html/2501.11533v1?utm_source=chatgpt.com&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/html/2501.11533v1?utm_source=chatgpt.com&lt;/a&gt;" data-tooltip-richtext="1" data-tooltip-preset="white" data-tooltip-classname="ztext-reference-tooltip"><a id="ref_1_0" href="https://zhuanlan.zhihu.com/p/1933659071204038283#ref_1" data-reference-link="true" aria-labelledby="ref_1">[1]</a></sup>)对包括 ICM、RND、状态计数等主流好奇心算法进行了统一实验比较，结果表明：这些方法在不同状态空间维度（如图像空间 vs. 低维特征空间）下的性能差异显著，其中基于特征空间预测误差的机制（如 ICM）在高维视觉任务中更具优势，而基于状态新颖度的估计方法（如 RND）在训练稳定性方面表现更优。

进一步地，研究者对好奇心算法中的预测误差来源进行了更细致的刻画。例如，2024 年的新方法<sup data-text="" data-url="https://arxiv.org/abs/2412.04775" data-numero="2" data-draft-node="inline" data-draft-type="reference" data-tooltip="&lt;a href=&quot;https://arxiv.org/abs/2412.04775&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2412.04775&lt;/a&gt;" data-tooltip-richtext="1" data-tooltip-preset="white" data-tooltip-classname="ztext-reference-tooltip"><a id="ref_2_0" href="https://zhuanlan.zhihu.com/p/1933659071204038283#ref_2" data-reference-link="true" aria-labelledby="ref_2">[2]</a></sup>引入了**重建误差与未来预测误差的对比分析机制**，以区分“暂时不可预测状态”与“结构性新颖状态”，有效避免了传统好奇心方法陷入环境噪声或混乱状态的误导。此外，还有工作将好奇心信号用于指导注意力机制，引导智能体更聚焦于“结构性信息缺失区域”，提高策略更新的样本效率。还有研究<sup data-text="" data-url="https://arxiv.org/abs/2412.04775" data-numero="3" data-draft-node="inline" data-draft-type="reference" data-tooltip="&lt;a href=&quot;https://arxiv.org/abs/2412.04775&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2412.04775&lt;/a&gt;" data-tooltip-richtext="1" data-tooltip-preset="white" data-tooltip-classname="ztext-reference-tooltip"><a id="ref_3_0" href="https://zhuanlan.zhihu.com/p/1933659071204038283#ref_3" data-reference-link="true" aria-labelledby="ref_3">[3]</a></sup>尝试将好奇心动态调节为时间折扣因子的函数，提出**时间调度式内在动机（Temporal-Scaled Curiosity）**，使探索行为更加阶段性、任务感知。

![](https://pica.zhimg.com/v2-2efdd8cc57e5b0d3495424b26c3c6376_1440w.jpg)

Agent不停的从电视机接受exploration bonuses，停滞不前 https://arxiv.org/pdf/2203.02298

总体而言，近两年的好奇心相关研究重点已从“是否探索”向“何时探索、探索何处”过渡，重心从粗糙的预测误差信号，逐步迁移到更稳健、可解释的好奇心建模机制。这些进展不仅提升了训练效率与策略泛化能力，也让好奇心在更复杂的实际任务（如多阶段视觉导航、策略泛化、多模态输入）中展现出更广泛的适用性。虽然好奇心机制增强了探索性，但也存在一些典型问题：

-   **无意义探索**（Noisy TV problem）：预测误差并不总能代表“有价值”的状态，例如电视随机播放的画面预测误差高，但不一定对任务有帮助
-   **陷入局部误差陷阱**：模型可能会一直尝试在某些“难预测但无意义”的区域刷分
-   **与真实奖励冲突**：有时好奇心驱动的探索可能干扰主任务的收敛（特别是在 reward 很稀疏的环境下）
    

**应用场景与意义**

好奇心驱动的探索尤其适用于以下场景：

-   外部奖励极其稀疏（Sparse Reward）的任务
-   无监督预训练（如世界模型、自监督强化学习）
-   Open-ended exploration，例如游戏、机器人导航

更进一步，好奇心机制也被视为迈向“自主智能体”的关键一步 —— 它赋予了智能体一种“自我生成任务”的能力，从而不再完全依赖外部信号。

### **8.2 状态熵与最大熵强化学习：从最优行为到信息量**最大**的行为**

如果说好奇心方法旨在发现“新奇”的状态，那么最大熵强化学习（Maximum Entropy Reinforcement Learning, MaxEnt RL）则是在策略层面鼓励“保持不确定性”——即让智能体在执行策略时，不仅追求回报最大化，还希望保有足够的行为多样性与探索性。

下图中，我们定义 Q 函数 $Q(s, a)$ ，表示在状态 $s$ 下执行动作 $a$ 后的期望累积奖励。 Q 函数可能如图 3a 中的灰色曲线所示，具有两个明显的峰值，分别对应于两个“通道”（passages）。传统的强化学习方法通常采用单峰的策略分布，该分布以最大 Q 值为中心，并在其周围添加噪声以进行探索（红色分布）。由于这种探索偏向于 Q 值较高的上通道，智能体最终会将策略收敛于此，而完全忽略了右方的通道。

一种更高层次的解决方案是：确保智能体能够探索所有潜在有前景的状态，同时优先考虑更有希望的选择。其中一种方法是，直接根据 Q 值的指数函数来定义策略分布（图 3b 中的绿色分布）： $\pi(a|s) \propto \exp Q(s, a)$

![](https://pica.zhimg.com/v2-48f7c9b4d556da50dd9a0f2c081c0268_1440w.jpg)

https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/

传统 RL 中，目标函数是最大化期望累计奖励： $\pi^* = \arg\max_\pi \mathbb{E}\left[\sum_t r(s_t, a_t)\right]$ ，而最大熵 RL 则在此基础上加入了策略熵项： $\pi^* = \arg\max_\pi \mathbb{E}\left[\sum_t r(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot|s_t))\right]$ ，其中 $\mathcal{H}(\pi(\cdot|s_t)) = - \sum_a \pi(a|s_t) \log \pi(a|s_t)$ 表示状态下的策略熵， $\alpha$ 控制熵项的权重。直观而言，这一机制让智能体偏好“高奖励且高熵”的策略，避免过早收敛至确定性策略，有效缓解探索-利用困境。

最大熵框架下，一些关键算法被提出并广泛使用，例如：

-   **Soft Actor-Critic (SAC)**：结合 actor-critic 结构与最大熵目标，在离线/连续控制任务中表现优异；
-   **REDQ (Randomized Ensemble Double Q-learning)**：强化 Q 函数的估计鲁棒性，在最大熵目标下展现更高样本效率；
-   **AWAC (Advantage Weighted Actor-Critic)**：将离线数据用于加权策略改进，引入最大熵目标以提升探索多样性。

此外，最大熵思想也被用于联合奖励与好奇心信号的融合场景，例如将外部奖励与内部奖励共同建模为最大熵下的目标分布。

最大熵方法从策略空间角度出发，不再关注某一个“最优行为”，而是学习一组“高信息量”的行为分布，让智能体具备更多选择的自由度。这种自由度恰恰是智能“自主性”的体现：当任务不确定、奖励稀疏时，熵项鼓励智能体大胆尝试不同行为；当策略逐渐收敛时，智能体则能自然减少探索，向目标聚焦。

总之，最大熵强化学习为智能体赋予了更强的“行动开放性”，在动机觉醒的过程中，不再只是为了奖励而行，而是为了学习和保留更丰富的行为策略。

### **8.3 进展 / 掌控感（Competence / Empowerment）**

如果说“好奇心”驱动智能体探索未知的世界，那么**进展感（Competence）与掌控感（Empowerment）**，则体现了智能体在环境中逐步建立**主观能动性**的过程。它们标志着智能体不再只是“被动感知”，而是在主动尝试“影响世界”。与传统的内在动机机制（如新颖性、预测误差）不同，Empowerment 和 Competence 不再仅关注“不确定”与“意外”，而是转向一种更深层的思考：**我能对世界产生多大的影响？我能实现什么样的成长？**

### **8.3.1 掌控感（Empowerment）：信息论视角下的“影响力渴望”**

Empowerment 起源于信息论，其核心思想是：**智能体倾向于选择那些能带来更多未来可能性的状态**，也即最大化“自身的潜在影响力”。具体来说，它被形式化为： $\text{Empowerment}(s) = \max_{p(a)} I(a; s’)$ ，即：在当前状态 $s$ 下，智能体通过优化其动作分布 $p(a)$ ，最大化动作 $a$ 与未来状态 $s’$ 之间的互信息。

-   $I(a; s’)$ 表示当前动作与未来状态之间的信息增益；
-   最大化这个互信息，意味着智能体倾向于选择那些**能保持未来控制权、避免走入死胡同**的路径。

换句话说：**越是“掌控未来”的状态，越值得前往。**

### **8.3.2 进展感（Competence Progress）：目标导向的“能力递增”**

相比之下，Competence 更强调“目标导向性学习”：智能体是否能**完成一个个逐步升级的任务**，是否在这个过程中感受到自己的成长。这一理念常体现在 **Curriculum Learning**（课程式学习）中：

-   智能体先完成简单任务；
-   然后被引导去挑战更高难度；
-   每一次完成，都会强化其“我能做到”的信念；

很多 Competence 机制会将目标可达性（goal achievability）纳入奖励函数——智能体不是被鼓励去“尝试”，而是被鼓励去“做到”。

至此，智能体已从外部奖励的追随者，逐步成长为具备**内在动机、自主探索意图**的学习者。然而，真正的智能并不孤立存在。它需要在多智能体世界中，通过**协作、冲突与博弈**建立规则，构建身份，形成秩序。而在人类社会中，那些最深层的规则与目标，往往都隐藏在**语言之中**。未完待续，下一篇将迎来终章：

-   **第九境界：社会智能** —— 多智能体与博弈
    
    在合作与竞争中涌现规则、身份与语言
    
-   **第十境界：语言即激励** —— LLM 引导的策略生成
    
    语言不只是观察，更是策略、目标、动机的统一表达

## 参考

1.  [^](https://zhuanlan.zhihu.com/p/1933659071204038283#ref_1_0)[https://arxiv.org/html/2501.11533v1?utm\_source=chatgpt.com](https://arxiv.org/html/2501.11533v1?utm_source=chatgpt.com)
2.  [^](https://zhuanlan.zhihu.com/p/1933659071204038283#ref_2_0)[https://arxiv.org/abs/2412.04775](https://arxiv.org/abs/2412.04775)
3.  [^](https://zhuanlan.zhihu.com/p/1933659071204038283#ref_3_0)[https://arxiv.org/abs/2412.04775](https://arxiv.org/abs/2412.04775)
