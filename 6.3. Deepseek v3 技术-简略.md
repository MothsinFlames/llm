---
---


## 1\. 模型基本性能  

![|750](https://picx.zhimg.com/v2-0c01247d8fd40689c8036a86852833a5_1440w.jpg)

## 2\. 模型架构层

![|600](https://pic4.zhimg.com/v2-2e6762a3c88374c1df6b704bc040fecd_1440w.jpg)

### 2.1 Config

```
 "aux_loss_alpha": 0.001,
  "ep_size": 1,
  "hidden_act": "silu",#激活函数
  "hidden_size": 7168,# hidden size
  "initializer_range": 0.02,
  "intermediate_size": 18432,#这个是mlp中间层大小
  "kv_lora_rank": 512, # mla的kv低秩维度
  "max_position_embeddings": 163840,#最大长度
  "moe_intermediate_size": 2048,#专家的中间维度
  "n_group": 8,
  "n_routed_experts": 256, # 非共享专家数
  "n_shared_experts": 1,# 共享专家书
  "num_attention_heads": 128,#注意力头数
  "num_experts_per_tok": 8, # 每个token激活抓夹数
  "num_hidden_layers": 61,#transformer层数
  "num_key_value_heads": 128, # k，v头数
  "pretraining_tp": 1,# 预训练tp数
  "q_lora_rank": 1536,#query的lora维度
  "qk_nope_head_dim": 128,#qk的无rope维度
  "qk_rope_head_dim": 64,#qk的rope维度
  "quantization_config": {
    "activation_scheme": "dynamic",#activation动态量化
    "fmt": "e4m3",# 指数4位尾数三位
    "quant_method": "fp8",# 量化数据结构
    "weight_block_size": [
      128,# 量化分组数，即一个128*128的矩阵共享一组scaling和bias
      128
    ]
  },
  "rms_norm_eps": 1e-06,#rms 的参数
  "rope_scaling": {# yarn的上下文长度拓展参数
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 40,
    "scale": 1.0,
    "mscale_all_dim": 1.0,
    "original_max_position_embeddings": 4096,
    "type": "yarn"
  },
  "rope_theta": 10000, # theta的大小
  "routed_scaling_factor": 2.5, #router的放大系数
  "scoring_fun": "sigmoid",
  "seq_aux": true,
  "tie_word_embeddings": false, #lm_head和embeding不共享
  "topk_group": 4,#激活专家group
  "v_head_dim": 128,#v的维度
  "vocab_size": 129280 #词表数
```

### 2.2 分词
采用 **BBPE 技术**  
好处：从二进制的角度进行分词，训练的新年会更鲁邦，不会出现😠这种无法编码的问题
### 2.3 MLA

### 2.4 Share MOE

1.  专家头包括 **Share 专家** 和 **Router 专家**。
2.  **Share 专家** 是一直激活的，即输入的 token 都会被 Share 专家头计算。
3.  **Router 专家头** 会先和上图中的 expert 计算亲和度（代码中直接用一个 Linear 层进行投影），选择 top-k 各专家进行推理。（代码中推理计算 top-k 时，会先将 N 各专家进行分组为 `n_groups`，将每个组中 top-2 各专家的亲和力加起来，算出亲和力最高的 `top_k_group` 个组，然后在这些组里选 top-k 个专家）。
4.  **最终将所有的 Share 输出和 Router 专家进行亲和度加权相加**，得到 MoE 层的输出。

![|500](https://picx.zhimg.com/v2-cd800b7812f8c84b96902c6f6885fd4d_1440w.jpg)



## 3\. 训练方法创新

### 3.1 MTP （muti-token predict）
不同于next token predict

![|850](https://pic3.zhimg.com/v2-e8c405c7e27b0ada53f6f7e98bcaed58_1440w.jpg)

多token预测
1. 绿色的部分是共享的
2. 最后一个transformer block用于预测多个next token

双重优势：

首先，MTP 目标通过增加训练信号的密度可能提高数据利用效率；其次，它使模型能够提前规划表征，从而更准确地预测后续 token。

如图3所示，该实现方案与先前研究的方法有所不同：前者使用独立输出头并行预测个额外 token，而 DeepSeek-V3 采用顺序预测方式，并在每个预测层级保持完整的因果关系链。
对于输入序列中的第个 ，在第层预测时，模型首先将两个向量进行组合：该 token 在第 (k-1) 层的特征表示 和第 () 个 token 的向量，通过线性变换进行融合：

![](https://i-blog.csdnimg.cn/img_convert/4e816a517226412aa23a36399a208c10.png)

其中\[·;·\]表示向量拼接操作。需要特别说明的是，在 时，代表主模型输出的特征表示。值得注意的是，每个 MTP 模块都与主模型共享同一个向量层。经过组合的特征向量 随后输入到第 层的 Transformer 处理单元，生成该层的输出特征表示：

![](https://i-blog.csdnimg.cn/img_convert/777d2bab764de5e9d27dcce47fa587ec.png)

其中 代表输入序列的长度， 表示包含两端的切片操作。接着，系统将输入到共享输出层，计算第 k 个预测 token 的概率分布 （V 为词表大小）：

![](https://i-blog.csdnimg.cn/img_convert/85bad7e32d9115057df2009224694534.png)

输出层 **OutHead(·)** 首先通过线性变换将特征表示转换为 logits，然后使用 Softmax(·) 函数计算第 k 个预测 token 的概率分布。与向量层类似，每个 MTP 模块的输出层也与主模型共享。这种保持预测因果链的设计思路与 **EAGLE** 相近，但两者目标不同：EAGLE 主要用于推测解码，而本研究中的 MTP 主要用于优化训练效果。

**MTP 训练目标优化**：系统为每个预测层级计算交叉熵损失 ：

![](https://i-blog.csdnimg.cn/img_convert/9c375983851e39aa23b0f4e919b8c913.png)

其中， 表示输入序列长度，代表第 个位置的目标 token， 表示第 k 个 MTP 模块对 的预测概率。最终，通过计算所有层级 MTP 损失的平均值并乘以权重系数 ，得到总体 MTP 损失 ，作为 DeepSeek-V3 的补充训练目标：

![](https://i-blog.csdnimg.cn/img_convert/49c574a1c4551e5971f512bdb689dd97.png)

**推理阶段的MTP**：应用 MTP 机制的主要目的是提升基础模型的性能，因此在实际推理阶段可以不使用 MTP 模块，基础模型能够独立完成正常推理。此外，这些 MTP 模块也可以被重新配置用于推测解码，从而降低模型生成的时间延迟。


### 3.2 专家负载均衡

对于 MoE 模型，不平衡的专家负载将导致路由崩溃，并在专家并行场景中降低计算效率。传统解决方案通常依赖辅助损失来避免不平衡负载。然而，过大的辅助损失会损害模型性能。为了在负载平衡和模型性能之间实现更好的权衡，研究团队开创了一种无辅助损失负载均衡策略来确保负载平衡。

1.  对于每一个专家的亲和度（值越高越容易激活）加上一个偏置 b，如下图所示：
$$
g_{i, t}^{\prime}= \begin{cases}s_{i, t}, & s_{i, t}+b_i \in \operatorname{Topk}\left(\left\{s_{j, t}+b_j \mid 1 \leqslant j \leqslant N_r\right\}, K_r\right), \\ 0, & \text { otherwise. }\end{cases}
$$
亲和度计算

2.  训练过程中，记录每个专家的负载率，通过调整偏置 b 进行循环，最终保证专家负载率平衡。

```
#以下 a、b、c 为专家，第一个数字为原始亲和度 Sit，第二个数字为偏置 bi，第三个数字为该专家的输出值。
topk 为 2，output 为 MoE 层最终输出：

a: 0.4 - 0.1  100 
b: 0.35 + 0   50 
c: 0.25 + 0.1 120 
moe_output = 50 * 0.35 + 0.25 * 120
```

在这种设计中，偏置项仅用于路由选择，而门控值（用于与 FFN 输出相乘）仍基于原始相关度分数 计算。训练过程中，系统会实时监控每个训练步骤中所有批次的专家负载分布。在每个步骤结束时，对于负载过高的专家，其偏置项会减少 ；对于负载不足的专家，其偏置项会增加 ，其中 是控制偏置更新速率的超参数。

通过这种动态调整机制，DeepSeek-V3 在训练过程中实现了专家负载的均衡分布，其性能优于传统仅依靠辅助损失来实现负载均衡的模型。
**序列级辅助损失补充机制**：虽然 DeepSeek-V3 主要采用无辅助损失策略来实现负载均衡，但为了防止单个序列中出现显著的负载不均衡现象，模型还引入了补充性的序列级平衡损失：

![](https://i-blog.csdnimg.cn/img_convert/377da40aa7c1af04c0fdd09d1b2afacb.jpeg)

其中平衡因子 是一个超参数，在 DeepSeek-V3 中被设置为极小值； 𝟙表示指示函数；T 代表序列中的 token 总数。这种序列级平衡损失机制有助于保持单个序列内专家负载的均衡性。

**节点约束路由机制**：类似于 DeepSeek-V2 的设备限制路由策略，DeepSeek-V3 采用了受控路由机制来优化训练过程中的通信开销。具体而言，系统限制每个 token 最多只能分配给 M 个计算节点，这些节点的选择基于每个节点上专家的最高 相关度分数总和。

在这种约束下，**MoE 训练框架能够实现计算与通信的近乎完全并行处理**。

**完整的 Token 保留机制**：得益于高效的负载均衡策略，DeepSeek-V3 在整个训练过程中都保持着良好的负载平衡状态。因此，训练过程中不存在 token 丢弃现象。同时，通过特定的推理部署策略，DeepSeek-V3 在推理阶段同样实现了完整的 token 保留。


### 3.3. 训练成本

在同等模型总参数量级下，训练成本降低一个量级。主要来源于以下几点：

**3.1 混合精度**：  

- FP8 训练与 BF16 相比，理论计算量与通信量降低一倍。但是由于反量化和解决精度累积的分块计算，提升不会超过百分之50
- optimizer: adamw（主权重fp32、一阶段动量fp16、二阶段动量fp16）
- Bias fp32
- fp8主要在矩阵乘法的activation：Output=activation（fp8） x weight(fp8) + Bias （fp32）

**3.2 MoE 层**：

以下对同参数量的MLP和MOE进行计算量和通信量的对比（按照两层MLP计算）

**3.2.1原始 MLP 的计算量**

-   **MLP 结构**：  
	-   两个矩阵：  
		-   第一个矩阵：`[h, 2.5h]`。
		-   第二个矩阵：`[2.5h, h]`。

-   每个 token 向量的计算量为：$h \times 2.5h+2.5h×h=5h^2$

**3.2.2 MoE 的计算量**

-   **MoE 结构**：  
	-   假设有 `n` 各专家，每次选用 `k` 个专家。
	-   每个专家的两个矩阵：  
		-   第一个矩阵： $[h/n^{1/2},2.5h/n^{1/2}]$
		-   第二个矩阵： $[2.5h/n^{1/2},h/n^{1/2}]$
-   每个 token 每个专家的计算量=$5h^2/n$
-   `k` 个专家的平均每 token 计算量=$k×5h^2/n=5kh^2/n$

**3.2.3 MoE 与 MLP 的计算量对比**

-   **MLP 计算量**：$$MLP计算量=5h^2$$
-   **MoE 计算量**：$$MoE 计算量=5kh^2/n$$
-   **计算量比例**：

$$比例=MoE 计算量/MLP 计算量=5kh^2/5h^2/n=k/n$$

-   由于 $k≪n$，MoE 的计算量大大下降。

**3.2.4** **分布式训练的通信量**

-   **MLP 通信量**：  
	-   MLP 需要传输所有参数，通信量为 1（假设为单位通信量）。
-   **MoE 通信量**：  
	-   MoE 每次仅选择 `k` 个专家，因此通信量下降到：
	-   通信量=k/n

**3.2.5** **总结**
-   **MLP**：  
	-   计算量： $5h^2$
	-   通信量：$1$
-   **MoE**：
	-   计算量： $5kh^2/n$
	-   通信量：$k/n$
-   **优势**：
	-   MoE 通过选择部分专家（`k` 个）进行计算，显著降低了计算量和通信量。
	-   由于 $k≪n$，MoE 的计算量和通信量都大大下降。

**3.3.Dual 训练框架**：

-   精心设计的框架可以在通信与计算并行，减少了等待时间，并且降低了气泡占比。

### **3.4 YARN 长文本拓展两阶段课程学习（非原创）**：

#需要搜索的内容
-   YARN 是目前最常见的长文本拓展技术，仅需 0.2% 的预训练量可以很好拓展文本长度。
-   YARN 是 NTK-aware 的一种进阶形式。
-   具体是第一阶段将文本长度从 4k 拓展到 32k，第二阶段从 32k 拓展到 128k。

### **3.5. 强化学习**

1. Reward Model
	1.  如果是代码、数学、agent等等，每一步都是**强逻辑**的，使用**规则和结果验证**的方法获取Reward
	2.  如果是**自由形式**但是**有明确答案的任务**，**Reward Model 提供与正式答案是否相同的反馈**
	3.  如果是例如**写作等没有明确结果的任务**，我们使用**Reward model 进行输入和结果的打分**，  
	4.  **奖励值应该分步给予，而不是只对FInal answer**
	5.  从**deep seek v3的检查点初始化训练**

2. 使用GRPO算法进行在线-离线强化学习，**丢弃了critc模型，减少显存并且加速**
	-   关于LLM的Critic一直困扰笔者，原因是Critic是用来估计Value的，也就是训练Critic的loss一直是用Critic的预测值和Value的MSE$$\text{Critic}\_\text{loss}=\text{MSE}(V^{\pi}(\Theta)-Value)$$
	-   然而Value是由reward累加获得的，公式如下$$V^\pi(s)=\mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t R_{t+1} \mid S_0 = s \right]$$
	-   R在LLM的PPO中又是由Reward model计算出来的$$R_{t+1}=Rw(t+1)$$$$\text{Critic}\_\text{loss} = \text{MSE}(V^{\pi}(\Theta) - \text{Value}) = \mathbb{E}_\pi \left[ \text{MSE}\big(V^{\pi}(\Theta) - \sum_{t=0}^\infty \gamma^t Rw(t+1) \mid S_0 = s \big) \right]$$
	-   因此Critic即在拟合Reward Model的期望值，笔者理解使用Critic Model的唯一作用，Reward Model是用来模拟环境反馈，一般难以得到很好的泛化，所以使用Critic进行拟合能够减少方差。


3. 以下是GRPO使用组内相对优势进行优化的loss公式
$$
\begin{aligned}
\mathcal{J}_{G R P O}(\theta) =\mathbb{E}_{\left[q \sim P(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_{\theta_{o l d}}(O \mid q)\right] }
 \frac{1}{G} \sum_{i=1}^G\left(\min \left(\frac{\pi_\theta\left(o_i \mid q\right)}{\pi_{\theta_{o l d}}\left(o_i \mid q\right)} A_i, \operatorname{clip}\left(\frac{\pi_\theta\left(o_i \mid q\right)}{\pi_{\theta_{\text {old }}}\left(o_i \mid q\right)}, 1-\varepsilon, 1+\varepsilon\right) A_i\right)-\beta \mathbb{D}_{K L}\left(\pi_\theta \| \pi_{r e f}\right)\right),
\end{aligned}
$$

$$
\mathbb{D}_{K L}\left(\pi_\theta \| \pi_{r e f}\right)=\frac{\pi_{r e f}\left(o_i \mid q\right)}{\pi_\theta\left(o_i \mid q\right)}-\log \frac{\pi_{r e f}\left(o_i \mid q\right)}{\pi_\theta\left(o_i \mid q\right)}-1
$$
类似 PPO的经典loss，关键不同点在于优势函数$A$的计算


4. 以上GRPO不再计算Critic估计的优势函数，而是优化每个组（每一个组代表ACTOR同输入下，不同的输出采样)内的reward
	1.  以下每次采样获得的优势，是该次采样的奖励减去组内奖励均值并且处以方差，即标准化过程
	2.  标准化过程的目的是：不同组内标准化有一个统一量纲:
	-   举例如下第一组（代码任务）分数，5，4，3。第二组（作文任务）分数1，2，3。如果不做标准化，所有优势为正，更加重要的是，由于两组的整体均值为3，此时将导致第一组所有样本被奖励，第二组所有样本被惩罚。这种情况下完全不会使得模型变好。
$$
A_i=\frac{r_i-\operatorname{mean}\left(\left\{r_1, r_2, \cdots, r_G\right\}\right)}{\operatorname{std}\left(\left\{r_1, r_2, \cdots, r_G\right\}\right)} 
$$
5. 使用了多种提示词进行强化学习，减少了对SFT数据的依赖。（个人认为需要一个泛化能力优秀的reward model）


6. 为什么不使用[dpo](https://zhida.zhihu.com/search?content_id=253325529&content_type=Article&match_order=1&q=dpo&zhida_source=entity)和[ppo](https://zhida.zhihu.com/search?content_id=253325529&content_type=Article&match_order=1&q=ppo&zhida_source=entity)：
	1. dpo需要对每个输入，标注至少两个相对优劣的answer，标注成本高，将注定难以泛化。
	2. 由于$\pi_{ref}$的正则项，不进行任何long-cot的SFT直接dpo将会导致模型的正则项非常大，难以收敛
$$L^{\text{DPO}}(\theta) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \left( \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right) \right) \right]$$
7. ppo算法的优势依赖于Critic模型对状态价值进行估计，Value的的含义是当前状态未来所有可能的奖励（reward）期望总和，reward本身就是reward model进行估计的，使用Critic model估计value将更加难以泛化。  
$$L^{\text{CLIP}}(\theta) = E_t \left[ \min \left( r_t(\theta) A_t,\text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right]$$
$$A_t = r_t + \gamma V(s_{t+1}) - V(s_t)$$
	时序差分的$A_t$优势，这里的V都是用Critic估计的。
## 4\. 分布式训练

### 4.1 FP8 [混合精度训练](https://zhida.zhihu.com/search?content_id=252286914&content_type=Article&match_order=1&q=%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83&zhida_source=entity)

![|850](https://picx.zhimg.com/v2-c5f0da22ae04cc1a7b7086cd0473359f_1440w.jpg)

前、后向传播时的流程及其混合精度

FP8 的表示精度都要优于 INT8

### **4.1.1 细粒度量化方法**

本文中 BF16、FP32 向 FP8 转换采用分组量化，Activation 采用 per-token per-channel `[1, 128]`，Weight 采用 per-tile `[128, 128]`。

-   被量化向量：Weight: `[7168, 4/3 * 7168]`
-   Per-tensor: `[7168, 4/3 * 7168] ->共享一个 SCALE, BIAS`（最粗粒度）
-   Per-token: `[1, 7168 * 4/3] ->` 共享一个`SCALE, BIAS`（粒度与 [per-channel](https://zhida.zhihu.com/search?content_id=252286914&content_type=Article&match_order=2&q=per-channel&zhida_source=entity) 类似，适合weight）
-   Per-channel: `[7168，1] ->` 共享一个`SCALE, BIAS`（粒度与 per-token 类似，适合activation）
-   Per-tile: `[128, 128] ->` 共享一个`SCALE, BIAS`（粒度较细，精度较高）
-   Per-token per-channel: -> 共享一个`[1, 128]`（粒度最细，128 个值量化一次）

![|650](https://pic4.zhimg.com/v2-2f59a7af08575525bf0a8fb85f89aecf_1440w.jpg)

deepseek 量化分组示意图

![|500](https://pic4.zhimg.com/v2-da0fb7931b36df5af3f239129f870a31_1440w.jpg)

原文量化图

-   fp8的优势在于计算量与显存降低一倍

### 4.1.2 fp8精度下溢问题：

比如A（10000，4000）\*B（4000，10000），会得到一个C矩阵（10000，10000），每个值都要进行4000次乘法与加法，这种情况很容易造成**精度下溢**，因为要对齐指数位，所以尾数位要后移。举例如下：

```
假设有两个 FP8 数：
数 A：1.125（二进制表示为 0 1000 001）
数 B：0.0625（二进制表示为 0 0111 000）
1. 解码
数 A：
符号位：0（正数）
指数位：1000=8，实际指数 EA=8−7=1
尾数位：001，实际尾数 MA=1.001
数值：1.125
数 B：
符号位：0（正数）
指数位：0111=7，实际指数 EB=7−7=0
尾数位：000，实际尾数 MB=1.000
数值：0.0625
2. 对齐指数
EA=1，EB=0
将数 B 的尾数右移 1 位：
MB=0.1000
EB=1
3. 尾数相加
MA=1.001
MB=0.1000
相加结果：Msum=1.1010
4. 规范化
尾数 Msum=1.1010 已经是规范化形式，无需调整。
5. 舍入
FP8 的尾数只有 3 位，因此需要对 Msum=1.1010 进行舍入。
使用向最近偶数舍入（Round to Nearest, Ties to Even）：
舍入后的尾数：Msum=1.101
6. 编码
符号位：0（正数）
指数位：1+7=8=1000
尾数为：101
最终 FP8 表示：0 1000 101
数值：1.625
```

### 4.1.3 fp8大矩阵MMA算子精度下溢解决方案：

-   报告原文：

为了解决这一问题，我们采取了向[CUDA](https://zhida.zhihu.com/search?content_id=252286914&content_type=Article&match_order=1&q=CUDA&zhida_source=entity) Core升级以提高精度的策略 （Thakkar et al.，[2023年](https://link.zhihu.com/?target=https%3A//arxiv.org/html/2412.19437v1%23bib.bib89)）。 该过程如图[7](https://link.zhihu.com/?target=https%3A//arxiv.org/html/2412.19437v1%23S3.F7)（B）所示。 具体地说，在张量核上执行MMA（矩阵乘法累加）期间，使用有限的位宽累加中间结果。 一旦达到间隔 NC ，这些部分结果将被复制到CUDA内核上的FP32寄存器，在那里执行全精度FP32累加。 如前所述，我们的细粒度量化沿内部维度K沿着应用每组[缩放因子](https://zhida.zhihu.com/search?content_id=252286914&content_type=Article&match_order=1&q=%E7%BC%A9%E6%94%BE%E5%9B%A0%E5%AD%90&zhida_source=entity)。 这些缩放因子可以在CUDA内核上高效地相乘，作为具有最小附加计算成本的反量化过程。过程如下所示：
![|282](https://pica.zhimg.com/v2-c556efbb88e1f5d85c60c7fa47714f98_1440w.jpg)

deepseek v3原图

-   个人理解：

![|800](https://picx.zhimg.com/v2-4ee73d295fdede3f202a369b1f4cb38d_1440w.jpg)

fp8MMA防下溢算子及反量化过程

上图中右下角解读：  
黄色代表Tensor core 算子：速度快，并行计算，但是fp8累加存在精度下溢  
绿色代表Cuda core 算子：速度较慢，但是fp32的精度高，反量化和累加在这里做。  
总的来说：量化后的分块的activation和weight在tensor core中进行MMA计算，得到的小块放到cuda core里面做累加以及反量化到fp32

### 4.1.4 FP8 大矩阵 MMA防止精度下溢 操作步骤

1. **做 MMA 操作时**：
	-   例如，`fp8_A[1000, 2560] * fp_W[2560, 200]`，NC（量化数）为 128。
2. **做矩阵分解，遍例**：  
	-   每个 `A` 中的 `[1, 128]` 和 `W` 中的 `[128, 128]`，先使用 FP8 在 Tensor 核上算出结果 `C[1, 128]`。
	-   因为fp8大矩阵累加精度丢失，此时的分块后的矩阵仅仅累计 128 次，精度丢失较少。
3. **存储中间结果**：
	-   将 `C[1, 128]` 放到寄存器。
	-   将 `fp8_A` 和 `fp8_W` 量化是对应的 `scalingA` 和 `scalingW` 也放到寄存器中。
4. 在 CUDA 核中进行反量化：
	-   计算：  $$DFP32​=scalingA​×scalingB​×CFP8​$$
5. 循环重复步骤 2-4，直到矩阵分块完：
	-   通过矩阵分解，重复前面的步骤 2-4。
	-   最终将获得高精度的：  $finalanswer_{[i,j]}=\sum DFP32$

```
## MMA防止精度下溢伪代码
def MMA_Mixed_Precision(A_fp8, W_fp8, scal_A, scal_W, NC=128):
    """
    使用混合精度（FP8 + FP32）进行矩阵乘法累加（MMA）操作。

    参数：
    - A_fp8: 形状为 [M, K] 的 FP8 激活值矩阵。
    - W_fp8: 形状为 [K, N] 的 FP8 权重矩阵。
    - scal_A: A_fp8 的量化缩放因子，形状为 [M, K // NC]。
    - scal_W: W_fp8 的量化缩放因子，形状为 [K // NC, N]。
    - NC: 量化分组大小，默认为 128。

    返回：
    - final_answer: 高精度的 FP32 结果矩阵，形状为 [M, N]。
    """
    import numpy as np

    # 获取输入矩阵的形状
    M, K = A_fp8.shape
    K, N = W_fp8.shape

    # 初始化最终结果矩阵
    final_answer = np.zeros((M, N), dtype=np.float32)

    # 遍历 A 的行
    for i in range(M):  # 遍历 A 的每一行
        # 遍历 W 的列，以 NC 为步长
        for j in range(0, N, NC):  # 遍历 W 的每一列，每次处理 NC 列
            # 初始化累加器，形状为 [1, NC]
            D_fp32 = np.zeros((1, NC), dtype=np.float32)

            # 遍历内部维度 K，以 NC 为步长
            for k in range(0, K, NC):
                # 提取 A 的当前块 [1, NC]
                A_block = A_fp8[i, k:k+NC]  # 形状为 [1, NC]

                # 提取 W 的当前块 [NC, NC]
                W_block = W_fp8[k:k+NC, j:j+NC]  # 形状为 [NC, NC]

                # 在 Tensor 核上使用 FP8 计算矩阵乘法
                C_fp8 = np.dot(A_block, W_block)  # 形状为 [1, NC]

                # 提取对应的缩放因子
                scale_A = scal_A[i, k // NC]  # A 的缩放因子
                scale_W = scal_W[k // NC, j:j+NC]  # W 的缩放因子，形状为 [1, NC]

                # 在 CUDA 核中进行反量化
                D_fp32 += scale_A * scale_W * C_fp8  # 累加到 D_fp32

            # 将累加结果存储到最终答案矩阵
            final_answer[i, j:j+NC] = D_fp32

    # 返回最终结果
    return final_answer
```

### 4.2 DualPipe 框架

本部分参考了[deepseek预训练](https://zhuanlan.zhihu.com/p/15073492309)

分布式训练的目的其实就一个：节省更多的资源，资源包括计算时间、显存、机器数量。总的来说应该是节省总的GPUhours。

![](https://pic4.zhimg.com/v2-2e2df3a5a283c86a05d4c8ef590c2d03_1440w.jpg)

通讯与计算同步示意图

上下图中的F（前向计算）、B（梯度计算）、W（跟新权重）含意一致

![|500](https://pic3.zhimg.com/v2-4da572520e714ce31708523e95000200_1440w.jpg)

模型前向后向图

在模型训练中，主要计算量来源于 **ATTENTION-O(L²)** 和 **MLP-O(H²)**。

由于分布式训练，前向和后向计算均需要通信，通信包括 **dispatch**（将输入分到各个 weight、expert）和 **combine**（将各个 weight、expert 的输出结果聚合）两部分。

在同一个 batch 中，通信和计算是交替进行的，这会导致效率低下。为此，DeepSeek V3 提出了双管路的方法，使得通信与计算能够并行

1.  模型训练中主要计算量来源于ATTENTION-O(L2)，MLP-O(H2)。
2.  由于分布式训练，导致前向后向计算均需要通信，通信包括dispatch（将输入分到各个weight、expert）、combine（将各个weight、expert的输出结果聚合）两部分。
3.  由于在同一个batch中，通信和计算是交替进行的，这将导致效率低下。为此deepseek v3 提出双管路的方法，使得通信与计算能够并行。

前向图如下：

![](https://pic2.zhimg.com/v2-ba00768d396e64990d847ae22341b901_1440w.jpg)

前向传播

反向传播如下：

![](https://pic1.zhimg.com/v2-42d086b5e5a8b585fc963b82335bee00_1440w.jpg)

反向传播

1.  总体来说：
![[Pasted image 20250131170842.png]]
2.  双路的流水线并行
	-   DeepSeek V3 的双向 PP 调度中，还是 8 PP 为例：
	-   device 0 上有 Layer 0 以及 Layer 7 的权重
	-   device 1 上有 Layer 1 以及 Layer 6 的权重
	-   device 7 上有 Layer 7 以及 Layer0 的权重
	-   相当于有 2 份相同的模型副本切分在不同的层，Forward 的顺序可以从 device 0 到 7，也可以从 device 7 到 0。
	-   这么设计的目的在于，下图中间部分存在极大范围的overlap backward&forward，由上面前向图和后向图可知，这样做可以减少时间。本质就是在每个device都有同时进行的第i层的前向和n-i层的反向传播，比如图中蓝色框中是4号数据在device4上的layer4进行前向传播，同时10号数据在device4的layer3上进行反向传播。

![](https://pica.zhimg.com/v2-18ce073b30454c42a220cd04a56d8e86_1440w.jpg)

dual pipe解读

### 4.3 节省显存

### 4.3.1 Gradient Checkpoint

通过梯度检查点技术，减少显存占用。

### 4.3.2 将模型参数的指数移动平均值保存到 CPU

将模型参数的指数移动平均值保存到 CPU，减少 GPU 显存占用。

### 4.4 MTP 的 Share 模块

将 embedding、lmhead 等共享模块放在一起 PP 的 GPU 上，减少通信和显存占用。


## 6\. 适配推理引擎

### 6.1 Prefilling（Compute Bound）

### 并行策略

-   **Attention 模块**：TP4，8DP，SP（4000 的训练长度需要 SP？）
-   **MoE 模块**：

-   32路ep并行，专家总数256/专家并行数32=8，即每个gpu有8+1（冗余专家）个专家：
-   这样配置的原因是专家数一定时，专家并行数小，每张卡上专家数就大，那每张卡上获得的token计算就多，由于同一个卡内的专家计算过程中相互独立，此时可以做并行计算，从而达到充分利用算力的目的。
-   另外需要注意的是Moe是特殊的MLP，在prefilling阶段是可以并行的，输入向量\[B,L,H\],可以看成一次型输入B\*L个token，每个token占了\[H\]的向量，然后向Moe层的专家去分配这些token。

-   **节点间通信**：infinite band
-   **节点内通信**：[NVLink](https://zhida.zhihu.com/search?content_id=252286914&content_type=Article&match_order=1&q=NVLink&zhida_source=entity)
-   **浅层的 MLP**：仅使用 1TP 并行，因为 TP 的通信量最大，TP1 为最少通信量
-   **两个微批次同时进行**：一个微批次进行计算，一个微批次进行通信，从而覆盖通信时间
