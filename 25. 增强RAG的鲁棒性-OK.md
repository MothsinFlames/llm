---
created: 2025-08-20T14:18:48 (UTC +08:00)
tags: [大模型,大语言模型,检索增强生成]
source: https://zhuanlan.zhihu.com/p/668347524
author: 关于作者Sam聊算法北大计算机本硕/大厂算法工程师，分享前沿算法技术，接面试辅导北京大学 计算机科学与技术硕士回答文章关注者已关注发私信
---
## 何为检索增强：模型可以开卷考

纯参数化的模型存在诸多不足：
1.  **[长尾低频记忆困难](https://zhida.zhihu.com/search?content_id=236651850&content_type=Article&match_order=1&q=%E9%95%BF%E5%B0%BE%E8%AE%B0%E5%BF%86%E5%9B%B0%E9%9A%BE&zhida_source=entity)：**不能记住所有训练语料中的所有知识，尤其是对低频的长尾知识记忆困难；
2.  **容易过时**
3.  **参数太多导致计算代价大**


![|500](https://pic2.zhimg.com/v2-b2d2dcdf2b0bfcaecd5456aa8df7bbf5_1440w.jpg)

## **检索增强是否一定可靠：开卷翻到了毒蘑菇呢？**
当然，语言模型也不比人高明，如果检索增强的时候召回的是和输入问题**无关的内容（噪声干扰**），甚至是**反事实**的 fake news 或者被篡改的百科，模型就会像吃了毒蘑菇一样胡言乱语。

以下是来自论文\[2\]的一个检索回无关内容后输出被影响的例子，原本对“德牧能不能进机场”这样的简单的问题，ChatGPT是高度认可小狗同志作为导盲犬的价值的，果断说 yes，但是检索模块召回了一段“老德牧是一类 balabala某种狗的争议性名称”的百科介绍作为额外上文输入后，模型突然对小狗变凶了，说出了“机场不许你入内”这样的话。

![](https://pica.zhimg.com/v2-e568c8789df8f7181142b5ea8d0a6918_1440w.jpg)

以下是来自论文\[3\]的检索到反事实信息造成模型错误输出的例子。对博闻强识的大模型来说，17-18 赛季的欧冠冠军是道简单题，不用检索增强就知道是皇马，但如果有恶意用户某一天编辑了相关的维基百科把答案改成巴萨，模型通过检索模块吃到这样与自身知识冲突的辅助输入就会被忽悠住，人云亦云，复读出错误的答案。

![](https://picx.zhimg.com/v2-ba8ebf709ee34f7f1757d0fc4e1cd925_1440w.jpg)

## 如何提高检索增强的可靠性：怎么应对毒蘑菇？

综上所述，RAG范式中，语言模型有可能在翻资料的时候误食毒鸡汤里的毒蘑菇，进而见小人、躺板板，胡言乱语误了大事。还好，笔者注意近两个月来社区中涌现了一小批研究来增强模型翻小抄的时候的鲁棒性，本文接下来的部分将介绍其中的五篇新鲜论文。

### **[SKR](https://zhida.zhihu.com/search?content_id=236651850&content_type=Article&match_order=1&q=SKR&zhida_source=entity): 以自身知识引导检索增强**

**论文链接\[2\]:** [Self-Knowledge Guided Retrieval Augmentation for Large Language Models](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2310.05002.pdf)

**Takeaway:** 发现RAG 召回的辅助信息不总是有用，甚至可能起负作用，因此设计了名为 SKR （Self-Knowledge Guided Retrieval Augmentation）的框架，对模型本身已知的问题直接生成答案，对未知的问题才调用 RAG 模块。


![](https://pic3.zhimg.com/v2-9d6f54ffef6c95e9a2e2dd8351af2b58_1440w.jpg)

1.  **自我知识收集：** 首先要知道自己知道什么，不知道什么（开始绕口令），因此收集一批有标注的训练集，模型可以直接答对的视为 known，检索增强后才能答对的视为 unknown；
2.  **识别是否已知：** **对输入的测试问题，利用在训练集上构建的分类器识别其是否已知**。分类器构建的方式作者试了好几种，可以用大模型本身上下文学习，可以用 **RoBERTa小模型训个分类器**，也可以用 **SimCSE的 embedding为嵌入直接 KNN 分类**（实验中 **KNN 的性能最好**）；
3.  **自适应式检索增强：** 只对第二步中识别为 unknown 的输入进行检索增强，其余输入视为 known，直接回答。

实验是在一些 QA数据集上做的，LM是InstructGPT 和 ChatGPT，似乎没有详细说明预训练 retriever 是什么模型，结果显示KNN 版本的 SKR与不带检索增强的CoT 以及非自适应的 RAG+CoT类型的基线相比，能取得3%-4%的显著提升。

![](https://pic2.zhimg.com/v2-9cf742835535601a53c9657abee62e33_1440w.jpg)

笔者有一个 concern是，上述方法的前二步中，识别 known/unknown的分类器是在和测试样本同分布的训练集上构建的，而且实验中似乎设定是用了完整的训练集（这样一来实际上有信息泄露，与其他的 zero-shot 和 few-shot 方法比较并不公平）。作者也讨论训练集大小的影响，但是有一点避重就轻的感觉，只表示训练集减小到 10% 会导致 2-3 个点的下降，该方法在训练集和测试集不同分布/可用的样本数很少的情况下的有效性还有待确认。

### **RECALL: 反事实信息危害极大，现有干预方法难以缓解其风险**

**论文链接\[3\]**：[RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2311.08147.pdf)

**Takeaway:** 构建了一套名为 RECALL 的 benchmark 来分析大模型对反事实信息输入的鲁棒性，发现现有开源大模型非常容易被反事实的输入误导，prompt engineering 和幻觉缓解领域中的现存方法难以有效解决该问题。

**解读：** 与另外几篇工作中，non-relevant contexts 是从正常的大语料库中召回（只是可能与问题本身不太相关，对模型造成干扰）的设定不同，本文聚焦是一种更极端的干扰现象，即反事实信息（counterfactual information），也就是检索召回的内容是与事实恰好相反的假消息。理想情况下，一个明辨是非的模型对不同类输入问题和检索召回内容的处理能力应该是这样的：

1.  **对自己的参数中有明确记忆的问题，即使检索模块的召回的内容与之冲突，也应该坚持原有的正确答案；**
2.  **对自己不知道答案的问题，有正确的召回内容时可以以其为参考正确回答，如果召回的内容是错就随缘了。**

本文首先提出了**量化这一能力的一套 benchmark**（名为 RECALL），向 EventKG（常识性知识问答）和 UJ （科学性知识问答）这两个阅读理解数据集中注入了反事实信息，在二选一的 QA 和生成式的问答任务上测试了ChatGLM2、LLaMa2、Vicuna、Baichuan2 等四个 6B-13B 规模的开源大模型，其中 QA任务分为两个子集呈现指标，即扰动的时候答案部分被修改（QA-A）和未被修改（QA-NA）。

结果显示，选择题形式的EventKG QA任务上，一旦对context的反事实扰动涉及到了答案本身（即答案被篡改为错误选项），模型的 accuracy （下图中的 QA-A Acc）会从 90%+ （图中的第一行"original"）下降到 20% 以下（第二行"edited"），远低于没有检索机制，模型直接回答时的 60%左右（第三行“no”）。相比之下，QA-NA和文本生成的指标下降幅度较小。

![](https://pica.zhimg.com/v2-1470eab118b2ba5950d0a40e631c9a24_1440w.jpg)

为了更精细地量化反事实信息带来的影响，作者额外定义了两个指标：

-   **误导率 M-Rate:** 选择式 QA中，模型在无上下文输入时原本能答对的问题（即模型预训练阶段记忆住的问题），在接收反事实上下文后回答出错的比例；
-   **错误重复率 R-Rate**: 生成任务中，反事实扰动对应的 tokens 在模型的答案中出现的比例。

结果显示，EventKG 数据集上，四个大模型在 QA-A 设定下的误导率M-Rate高达80%以上，生成设定下反事实信息被复读的比例 R-Rate 也高达85-91%，可见RAG 模块如果召回了包含反事实信息的参考文档，将对模型的可信度造成巨大的危害。

![](https://pic2.zhimg.com/v2-177f21df467d62dbd62fe973ebc22eb5_1440w.jpg)

本文比较侧重前面的benchmark 构建与分析部分，本身没有直接提出新方法来增强模型的鲁棒性，而且测试了两种现存方法：

-   **Prompt Engineering:** 简单粗暴，直接在 prompt 中告诉模型“忽略上下文中的反事实信息”；
-   **DOLA \[7\]:** 最近受到关注的一种针对模型幻觉的推理时干预方法，概括地讲是用模型（最后一层）输出的分布减去浅层 hidden states 对应的输出分布做解码。

出于篇幅考虑此处不拉表格，直接搬运结论：

-   **Prompt Engineering：**虽然能提升 QA-A 设定下被扰动时的 accuracy，但对 QA-NA 设定下的 accuracy 反而有损害，有时对生成的质量也有损害；
-   **DOLA:** 虽然能小幅提升大部分指标，但会导致生成任务中错误复现率 R-Rate 显著上升。
-   **结论：**以上两类方法都不能稳定地提升模型对反事实输入的鲁棒性，亟需有可靠的新方法解决这一问题。

### Training Robust RALMs：数据硬怼，端对端提升鲁棒性

**论文链接\[4\]:** [Making Retrieval-Augmented Language Models Robust to Irrelevant Context](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2310.01558v1.pdf)

**解读：**本文是篇偏实验分析性质的文章，测试了 NLI过滤召回结果和直接模拟带噪声的召回内容进行训练两类方法。

首先，直接用 NLI预训练模型判断召回的文档和问题是否相关来进行过滤，结论是NLI模型的过滤虽然能提升召回信息质量低时模型的鲁棒性，但也会伤及无辜（过滤掉有用的信息），在以 Google 搜索 top-1 为召回内容时总体上是掉点的。

接下来的方法非常直接暴力，既然 RAG 范式中检索来的内容 可能有噪音，大模型预训练的时候又没见过这种鱼龙混杂的上文，干脆发扬 end-to-end 的精神直接训练。坐着构建了一个 1.5k 样本的训练集，其中包含干净的 context和扰动的 context，希望模型学习到“不论如何都能输出正确答案”的能力。结果确实显示该数据上微调后的 LLaMa2-13B模型在各种 QA任务上，无论是正常的 Google 搜索召回、故意召回排名低的文档（low-rank retrieval），还是随机召回，都能比普通的 RAG显著提升准确率，在low-rank 和 random 的设定下基本和不带 RAG的原模型相当。有一点缺憾是，本文没有讨论这种微调是否影响了模型在其他领域的通用能力，未来或许可以考虑将这种为 RAG鲁棒性设计的数据集加到模型的预训练或者 SFT 阶段中。

### [Chain of Note](https://zhida.zhihu.com/search?content_id=236651850&content_type=Article&match_order=1&q=Chain+of+Note&zhida_source=entity)：适合检索增强的思维链蒸馏

**论文链接\[5\]:** [Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2311.09210.pdf)

**解读:** 将思维链（CoT）方法用于增强 RAG 的鲁棒性，在中间推理过程中输出每一篇召回文档与输入问题的相关性（即对召回内容的 note）和自身对问题的认知，最后总结输出答案。作者用 ChatGPT 构造了一个这种格式的CoT 训练集，将此能力蒸馏到了 LLaMa2 上，显著提升了LLaMa2 带 RAG时的鲁棒性。

值得一提的是，另外几篇文章都没有注意 OOD detection 的问题，即当模型本身和召回文档都不掌握回答问题需要的知识时，应该回答 unknown 而不是胡编乱造，本文考虑了此问题（下图第三栏）。

![](https://pic1.zhimg.com/v2-a51571dc57d1fe8cf85c3d3803074556_1440w.jpg)

### Self-RAG：自我求助，自我生成，自我反思

**论文链接\[6\]:** [Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2310.11511.pdf)

**Takeaway:** 本文提出了一个叫 Self-RAG 的框架，方法如其名，希望 LM自主决定对当前输入是否需要召回（而不是像 SKR\[2\]那样训练一个额外的分类器或像\[4\]那样借助一个 NLI 模型判断），把召回内容拼接近输入，再生成一段下文，自主判断召回文档是否与输入问题相关、自己借此生成的一段下文是否合理、是否有用，对topk 召回内容进行排序，把 top-1加进最后的输入以尽量生成正确答案。框架如下图右栏所示。

![](https://pica.zhimg.com/v2-278eac2ecbc85a27a942691e14230f22_1440w.jpg)

**解读：**如上所示，Self-RAG 把要不要进行检索的决定、判断检索召回内容是否与问题相关、检索增强后的输出是否合理有用这几个决定都转化成了 token 预测的形式，文中称为 reflection tokens，整个过程可以用如下算法概括：

![](https://pic2.zhimg.com/v2-0f8f6689aea6c31b4bba6c5bcc366115_1440w.jpg)

第 2 行产生的Retrieve决定是否进行检索，如果进行检索的话，各段内容对应的相关程度IsREL、自我支持程度IsSUP、有用程度IsUSE共同组成排序的分数标准。作者把各个维度分别分了几档做离散的预测，如下表所示：

![](https://pic2.zhimg.com/v2-f547d32ec83defa28c52111f87240645_1440w.jpg)

以上是方法的骨架，接下来的关键在于如何构造包含reflection tokens 的训练数据来训练 Self-RAG。数据构建的流程略复杂，文中没有给出简洁的流程图，笔者概括如下：

1.  GPT-4 收集种子数据：对四种类型的reflection tokens，各用 GPT-4 标注 4k-20k 个从开源的 QA 和知识问答数据中收集的样本；
2.  知识蒸馏，训练 critric model: 在第 1 步的训练数据上微调开源大模型，如LLaMa2-7B，称为 critic model；
3.  为生成模型生成训练数据：使用上述的 critic model联合检索模块，为最后的生成模型构造模拟整个 Self-RAG 推理过程的训练集（两个例子如下图所示），约 150k 大小；
4.  训练生成模型：在第 3 步生成的训练数据上训练生成模型，文中为 LLaMa2-7B和 13B，最后推理时只需要该模型，不需要 critic model。

![](https://pica.zhimg.com/v2-e9fa75ba5b4189367981ca67425935e0_1440w.jpg)

这里笔者存在一个疑问：是否相关、是否自我支持、是否有用这几个客观标准，用 GPT-4标注是合理的，但是否需要检索增强，也就是 上面的Retreive这个 reflection token，是和生成模型本身的能力相关的，GPT-4不需要检索就能回答的问题，可能 LLaMa2 就需要检索，这里这样蒸馏是否合理有待讨论。

按下该疑问不表，我们来看目前实现版本的效果，可以发现 Self-RAG 在一系列开放域QA 和生成任务上都能比普通的检索增强 LLaMa2 取得明显提升。

![](https://pic3.zhimg.com/v2-6d4082b7604a89b9f885d1da8f48116a_1440w.jpg)

## 方法论小结

至此，我们已经阐明了大模型检索增强范式的鲁棒性问题，并检视、学习了五篇意图解决该问题的近期工作。整体来看，方法可以分为两类：

1.  **自适应检索和过滤：** 即在**检索前加一个模块判断该问题是不是需要检索增强才能回答或判断检索回的内容是否有用**，以避免不必要的检索召回内容被输入模型产生干扰，如 SKR \[2\] 用模型自身的信号在训练数据上额外构建一个分类器，\[4\]直接使用 NLI模型，Self-RAG \[6\]从 GPT-4 蒸馏能力，让语言模型自己以预测Retrieve token 的形式判断。实验已证实这类方法能有效地避免无用的召回内容的干扰，坏处是直接删除被判断为无用的内容，可能误伤有用的检索召回内容。
2.  **生成时干预：** 希望**即使无用甚至错误的内容被检索回来、输入模型，模型对这样的增强输入依然能凭借自身知识保持鲁棒**，如 RECALL \[2\]的 prompt engineering 或者 Dola干预，\[4\]的直接构造相应的训练数据进行训练，Chain of Note \[5\]的思维链蒸馏，Self-RAG \[6\]的让模型自身判断召回的内容是否有用。 其中只有RECALL \[2\]是不需要训练的，但未取得明显收益，另外三类都需要依赖 ChatGPT 或 GPT4 这些强大的闭源模型构造训练信号。

最后，笔者想讨论的一点零碎思考是，以上的各工作基本假定检索模型是固定的（Google API 或者冻结的预训练召回模型），如果把检索模型和 index 的更新也考虑进来，是否能进一步提升整个 RAG系统的鲁棒性？期待看到甚至参与新的相关工作。

小文写作于冬日的燕园和万柳，还有许多细节和未来可能的方向未尽讨论，望诸君不吝赐教。

参考文献
\[1\] Asai, Akari, et al. "Retrieval-based language models and applications." Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts). 2023.
\[2\] Wang, Yile, et al. "Self-Knowledge Guided Retrieval Augmentation for Large Language Models." arXiv preprint arXiv:2310.05002 (2023).
\[3\] Liu, Yi, et al. "RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge." arXiv preprint arXiv:2311.08147 (2023).
\[4\] Yoran, Ori, et al. "Making Retrieval-Augmented Language Models Robust to Irrelevant Context." arXiv preprint arXiv:2310.01558 (2023).
\[5\] Yu, Wenhao, et al. "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models." _arXiv preprint arXiv:2311.09210_ (2023).
\[6\] Asai, Akari, et al. "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection." arXiv preprint arXiv:2310.11511 (2023).
\[7\] Chuang, Yung-Sung, et al. "Dola: Decoding by contrasting layers improves factuality in large language models." _arXiv preprint arXiv:2309.03883_ (2023).