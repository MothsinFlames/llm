---
created: 2025-09-29T12:00:08 (UTC +08:00)
tags: [LLM,大规模预训练模型,llama2]
source: https://zhuanlan.zhihu.com/p/648094197
author: 关于作者紫气东来吾生也有涯，而知也无涯上海交通大学 工学硕士JOYWIN、Whisper、Nasusu 也关注了他回答32文章112关注者22,412已关注发私信
---
篇旨在研究对比目前主流预训练方案在单机多卡的情况下性能情况，主要研究的方法包括：[HuggingFace](https://zhida.zhihu.com/search?content_id=232150991&content_type=Article&match_order=1&q=HuggingFace&zhida_source=entity) 的 [Base](https://zhida.zhihu.com/search?content_id=232150991&content_type=Article&match_order=1&q=Base&zhida_source=entity) 、FSDP、DeepSpeed、[Flash- Attn-2](https://zhida.zhihu.com/search?content_id=232150991&content_type=Article&match_order=1&q=Flash-+Attn-2&zhida_source=entity)、ColossalAI、Sophia Optimizer等。由于笔者计算资源有限，在此仅讨论单机多卡的情况。另外，本篇主要关注于性能相关的问题，精度问题及loss等问题将不会特别说明，还需要说明的情况包括：

-   模型规模：[Llama-7b](https://zhida.zhihu.com/search?content_id=232150991&content_type=Article&match_order=1&q=Llama-7b&zhida_source=entity) (参数见 config/config.json)
-   硬件设备：[A6000](https://zhida.zhihu.com/search?content_id=232150991&content_type=Article&match_order=1&q=A6000&zhida_source=entity)\*8
-   测试数据：[Wikipedia/20220301.simple](https://link.zhihu.com/?target=https%3A//huggingface.co/datasets/wikipedia)
-   测试代码：[ifromeast/LLMTrainer: A comparison of pretraining framework for LLM (github.com)](https://link.zhihu.com/?target=https%3A//github.com/ifromeast/LLMTrainer)

### 01\. Base 版

此处采用最基础 [Torch](https://zhida.zhihu.com/search?content_id=232150991&content_type=Article&match_order=1&q=Torch&zhida_source=entity) 的 [DDP](https://zhida.zhihu.com/search?content_id=232150991&content_type=Article&match_order=1&q=DDP&zhida_source=entity) 算法，作为基础的对比版本，运行的脚本如下：

```text
torchrun --nnodes 1 --nproc_per_node 8 pretrain_hf.py \
    --model_config_path ../config/config.json \
    --tokenizer_name_or_path ../ckpt/Llama-2-13b-hf \
    --per_device_train_batch_size 8 \
    --do_train \
    --seed 1234 \
    --fp16 \
    --num_train_epochs 1 \
    --lr_scheduler_type cosine \
    --learning_rate 2e-5 \
    --warmup_ratio 0.05 \
    --weight_decay 0.01 \
    --logging_strategy steps \
    --logging_steps 10 \
    --save_strategy steps \
    --save_total_limit 1 \
    --save_steps 100 \
    --gradient_accumulation_steps 8 \
    --model_max_length 2048 \
    --output_dir './hf_logs' \
    --overwrite_output_dir \
    --gradient_checkpointing \
    --ddp_find_unused_parameters False
```

运行结果如下所示：

```text
***** train metrics *****
  epoch                    =       0.99
  train_loss               =     7.1505
  train_runtime            = 0:19:08.39
  train_samples_per_second =     27.127
  train_steps_per_second   =      0.052
```

同时可以查看GPU的使用情况：

![](https://pic4.zhimg.com/v2-ee7f3fdc6f83c1e5f801cc1611762ba7_1440w.jpg)

GPU 利用率

![](https://pic1.zhimg.com/v2-770b7768b5319b10655a6f4610c881ac_1440w.jpg)

GPU 显存占用

### 02\. FSDP 版

FSDP是从DeepSpeed [ZeRO](https://zhida.zhihu.com/search?content_id=232150991&content_type=Article&match_order=1&q=ZeRO&zhida_source=entity)以及[FairScale](https://zhida.zhihu.com/search?content_id=232150991&content_type=Article&match_order=1&q=FairScale&zhida_source=entity)的FSDP中获取灵感的一种完全数据分片并行的方法，其基本原理可参考[这里](https://link.zhihu.com/?target=https%3A//pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)。

![](https://pic3.zhimg.com/v2-cf1d706571abca543117f453e9289d20_1440w.jpg)

FSDP 2路并行

HuggingFace 的 transformers 已经原生支持了 FSDP，可以在原生代码的运行命令上增加2行配置来使用：

```text
    --fsdp "full_shard auto_wrap" \
    --fsdp_config 'fsdp.json' \
```

此时可见 GPU 显存占用明显下降：

![](https://pic4.zhimg.com/v2-360e5cfe5b70e6c5e51580003014cbb1_1440w.jpg)

由此可进一步增大 batch\_size，以提高设备利用率

```text
--per_device_train_batch_size 12 \
```

运行结果如下所示，与原版DDP (即 Base 版)相比性能提升约 8.3% ：

```text
***** train metrics *****
  epoch                    =       0.98
  train_loss               =      9.634
  train_runtime            = 0:17:40.04
  train_samples_per_second =     29.387
  train_steps_per_second   =      0.038
```

### 03 DeepSpeed 版

DeepSpeed 的原理在前几期中多次介绍，感兴趣的可以回看前几期内容，HuggingFace 的 transformers 已经原生支持了 DeepSpeed，其用法也非常简单，只需要添加一个配置文件和一个参数即可：

以下是zero-2的配置文件

```text
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 100,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1e-10
    },

    "zero_optimization": {
        "stage": 2,
        "allgather_partitions": true,
        "allgather_bucket_size": 1e8,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 1e8,
        "contiguous_gradients": true
    },

    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "steps_per_print": 2000,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false
}
```

使用该配置文件如下：

```text
--deepspeed 'ds_zero2_no_offload.json' \
```

运行结果如下所示，与原版 DDP 相比性能提升约 15.4% ：

```text
***** train metrics *****
  epoch                    =       0.98
  train_loss               =     7.6718
  train_runtime            = 0:16:35.35
  train_samples_per_second =     31.297
  train_steps_per_second   =       0.04
```

如果使用 zero3 算法，结果如下，zero2 略有下降 ：

```text
***** train metrics *****
  epoch                    =       0.98
  train_loss               =     9.5239
  train_runtime            = 0:16:55.82
  train_samples_per_second =     30.667
  train_steps_per_second   =       0.01
```

如果使用 zero3 算法同时使用 offload，性能略有下降：

```text
***** train metrics *****
  epoch                    =       0.98
  train_loss               =     9.3981
  train_runtime            = 0:17:21.20
  train_samples_per_second =     29.919
  train_steps_per_second   =       0.01
```

### 04 FlashAttention-2

FlashAttention 的基本原理可参考笔者之前的文章，FlashAttention-2 在此基础上又做了并行和分块的优化，性能进一步提高

在此处通过补丁的方式使用，实现方式见 [LLMTrainer/utils/flash\_attn\_patch.py](https://link.zhihu.com/?target=https%3A//github.com/ifromeast/LLMTrainer/blob/main/utils/flash_attn_patch.py)，使用方式为：

```text
    if training_args.flash_attn:
        from utils.flash_attn_patch import replace_llama_attn_with_flash_attn
        replace_llama_attn_with_flash_attn()
```

只需要在运行文件中增加一句使能即可：

```text
    --flash_attn \
```

运行结果如下所示，与原版 DDP 相比性能提升约 52.7% ：

```text
***** train metrics *****
  epoch                    =       0.98
  train_loss               =     8.0186
  train_runtime            = 0:12:32.22
  train_samples_per_second =     41.413
  train_steps_per_second   =       0.04
```

FlashAttention + FSDP 性能如下，略有下降：

```text
***** train metrics *****
  epoch                    =       0.98
  train_loss               =     9.9216
  train_runtime            = 0:12:55.09
  train_samples_per_second =     40.191
  train_steps_per_second   =      0.039
```

FlashAttention + DeepSpeed 性能如下，略有提升：

```text
***** train metrics *****
  epoch                    =       0.98
  train_loss               =     8.0758
  train_runtime            = 0:12:17.55
  train_samples_per_second =     42.237
  train_steps_per_second   =      0.041
```

以上方法均可在 transformers 的 Trainer 框架下实现，比较方便，下面的方法则需要更多训练过程理解与开发工作。

### 05 ColossalAI

在之前 ColossalChat 中曾经使用过部分 ColossalAI 的算法，详情可参考

在此尝试使用 ColossalAI 的优化算法实现 llama 模型的预训练。

首先尝试 Gemini 算法，这是 ColossalAI 提出的异构内存空间管理器。它通过在 CPU 和 GPU 中容纳模型数据，并仅在必要时将数据移动到当前设备，可以同时利用 GPU 内存、CPU 内存（由 CPU DRAM 或 NVMe SSD内存组成）来突破单GPU内存墙的限制。

目前 DeepSpeed采用的 <u><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2101.06840" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">Zero-offload</a></u> 在CPU和GPU内存之间静态划分模型数据，并且它们的内存布局对于不同的训练配置是恒定的。如下图左边所示，当 GPU 内存不足以满足其相应的模型数据要求时，即使当时CPU上仍有可用内存，系统也会崩溃。而 ColossalAI 可以通过将一部分模型数据换出到CPU上来完成训练，这就是Gemini。它管理CPU和GPU二者内存空间。它的内存管理器由两部分组成，分别是MemStatsCollector(MSC)和StatefulTensorMgr(STM)，可以让张量在训练过程中动态分布在CPU-GPU的存储空间内。

![](https://pic3.zhimg.com/v2-9b91c07a7a0e866a748ce38d9610fc4a_1440w.jpg)

Zero-Offload和Gemini的内存管理方案比较

Gemini 还利用了深度学习网络训练过程的迭代特性，将迭代分为warmup和non-warmup两个阶段，开始时的一个或若干迭代步属于预热阶段，其余的迭代步属于正式阶段。在warmup阶段为MSC收集信息，而在non-warmup阶段STM入去MSC收集的信息来移动tensor，以达到最小化CPU-GPU数据移动volume的目的。

![](https://pic1.zhimg.com/v2-0d6187239440b5a52d16d9338454a7b4_1440w.jpg)

Gemini在不同训练阶段的运行流程

下面是该算法的用法：

```text
colossalai run --nproc_per_node 8 pretrain_colossalai.py \
    --config ../config/config.json \
    --plugin gemini_cuda \
    --batch_size 28 \
    --lr 2e-5 \
    --weigth_decay 0.01 \
    --warmup_steps 2 \
    --grad_checkpoint \
    --max_length 2048 \
    --mixed_precision fp16 \
    --flash_attention
```

从日志可以看到该算法对GPU及CPU的内存分配情况：

```text
Booster init max CUDA memory: 1523.89 MB
Booster init max CPU memory: 6737.21 MB
Max CUDA memory usage: 36608.19 MB
```

此时可观察到 GPU 及 CPU 的使用情况，可以清楚看到该算法在二者间动态使用的过程

![](https://pic4.zhimg.com/v2-1916d0b1f4a77702e661204f8f3046e9_1440w.jpg)

GPU 的使用情况

![](https://pic1.zhimg.com/v2-331d3b18a1e6167872f8e8b97c5961da_1440w.jpg)

CPU 的使用情况

性能如下所示，与原版 DDP 相比性能提升约 50.7%，其中主要的性能提升来源于 FlashAttention：

```text
train_runtime            = 0:12:42.53
train_samples_per_second =     40.882
```

如果不使能 FlashAttention，性能如下，与原版 DDP 相比性能提升约 8.1 %：

```text
train_runtime            = 0:17:42.36
train_samples_per_second =     29.333
```

由此可比较以上5种方法的性能，可以看出性能提升明显的都是由于 FlashAttention-2 的加成

![](https://pic3.zhimg.com/v2-60145ed7ecd6460e6b4f49884f2fdf1c_1440w.jpg)

### 06 Sophia Optimizer

[Sophia](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2305.14342.pdf) 是斯坦福大学研究者提出的二阶优化器。关于优化方法的原理，可参考笔者之前的文章

Sophia 优化器使用随机估计作为 Hessian 矩阵对角线的 pre-conditioner，并采用剪切（clipping）机制来控制最坏情况下的参数大小更新。在像 GPT-2 这样的预训练语言模型上，Sophia 与 Adam 相比，在减少了 50% step 数量的情况下实现了相同的验证预训练损失，这相当于总计算量减少了 50%，wall-clock 时间减少了 50%。

![](https://pic2.zhimg.com/v2-bd14e1e1370881c227c893ac0bb2e933_1440w.jpg)

下图为实测的 sophia 与 adamw loss曲线的对比，与论文结论一致

![](https://pic4.zhimg.com/v2-935ad9b9e3fa621c6b0a17ea15a7384b_1440w.jpg)

sophia v.s. adamw

## 参考资料

\[1\] [Dao-AILab/flash-attention: Fast and memory-efficient exact attention (github.com)](https://link.zhihu.com/?target=https%3A//github.com/Dao-AILab/flash-attention)

\[2\] [lm-sys/FastChat: An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and Chatbot Arena. (github.com)](https://link.zhihu.com/?target=https%3A//github.com/lm-sys/FastChat)

\[3\] [mk322/pretrain\_llama at 78b09efff7120346338d018a0b14dc0a941574b4 (github.com)](https://link.zhihu.com/?target=https%3A//github.com/mk322/pretrain_llama/tree/78b09efff7120346338d018a0b14dc0a941574b4)

\[4\] [Neutralzz/BiLLa: BiLLa: A Bilingual LLaMA with Enhanced Reasoning Ability (github.com)](https://link.zhihu.com/?target=https%3A//github.com/Neutralzz/BiLLa)

\[5\] [ymcui/Chinese-LLaMA-Alpaca-2: 中文 LLaMA-2 & Alpaca-2 大模型二期项目 (Chinese LLaMA-2 & Alpaca-2 LLMs) (github.com)](https://link.zhihu.com/?target=https%3A//github.com/ymcui/Chinese-LLaMA-Alpaca-2/tree/main)

\[6\] [ColossalAI/examples/language/llama/README.md at example/llama · hpcaitech/ColossalAI (github.com)](https://link.zhihu.com/?target=https%3A//github.com/hpcaitech/ColossalAI/blob/example/llama/examples/language/llama/README.md)

\[7\] [认识Gemini：ColossalAI的异构内存空间管理器 | Colossal-AI](https://link.zhihu.com/?target=https%3A//colossalai.org/zh-Hans/docs/advanced_tutorials/meet_gemini/)

\[8\] [https://github.com/kyegomez/Sophia](https://link.zhihu.com/?target=https%3A//github.com/kyegomez/Sophia)

\[9\] [Sophia Optimizer](https://link.zhihu.com/?target=https%3A//nn.labml.ai/optimizers/sophia.html)

> 万事到头都是梦，休休。明日黄花蝶也愁。 ——苏轼《南乡子·重九涵辉楼呈徐君猷》
