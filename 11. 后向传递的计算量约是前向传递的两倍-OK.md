---
title: 浅谈后向传递的计算量大约是前向传递的两倍
source: https://zhuanlan.zhihu.com/p/675517271
author: []
published:
created: 2025-03-06
tags:
---
## 1\. 前言
训练神经网络的一次迭代分为三步：
**（1）前向传递计算损失函数；**
**（2）后向传递计算梯度；**
**（3）优化器更新模型参数。**
在实验观察到一个现象：**后向传递的耗时几乎是前向传递的两倍，相比之下，优化器更新的耗时几乎可以忽略**。要解释这个现象，要从前向传递、后向传递和优化器参数更新的浮点数计算次数入手。
![|600](https://pica.zhimg.com/v2-6ea07753a3ff66e4c7f2bf19f52289b4_1440w.jpg)
上图表示一次训练迭代中各个环节（前向传递、后向传递、通信环节、优化器更新）的耗时占比，来自于《[PyTorch Distributed](https://zhida.zhihu.com/search?content_id=238245056&content_type=Article&match_order=1&q=PyTorch+Distributed&zhida_source=entity): Experiences on Accelerating Data Parallel Training》。
上图中，纵轴表示耗时占比
**FWD**表示一次训练迭代中**前向传递的耗时**占比
**BWD**则表示一次训练迭代中**后向传递的耗时占比**
**OPT**表示一次训练**迭代中优化器更新模型参数的耗时占比**。从上图中可以看到，一次训练迭代中，后向传递的耗时几乎是前向传递的两倍，相比之下，优化器更新的耗时占比很小，几乎可以忽略。

![|1025](https://pica.zhimg.com/v2-14df6abe27304fd17c006be5140aa10a_1440w.jpg)
上图表示GPipe流水线并行的调度策略，来自于《Efficient large-scale language model training on gpu clusters using megatron-lm》。
上图中，横轴表示耗时，一个蓝色小块表示一个微批次的前向传递，一个绿色小块表示一个微批次的后向传递，**黑色竖线表示一次流水线刷新**，也就是优化器更新模型参数。从上图中可以看到，**一个绿色小块的宽度大约是蓝色小块的二倍，一次训练迭代中，后向传递的耗时几乎是前向传递的两倍**，相比之下，优化器更新的耗时占比很小，几乎可以忽略。

## 2\. 反向传播算法是怎么工作的
梯度下降更新模型参数（包含权重weights和偏置bias），用反向传播（backpropagation）算法计算损失函数关于模型参数的梯度。
反向传播算法的核心是计算损失函数C关于神经网络权重 w 或偏置 b 的偏微分 $\frac{\partial{C}}{\partial{w}}$ 或 $\frac{\partial{C}}{\partial{b}}$ ，这两个偏微分表达式就是梯度。梯度的物理意义是：我们改变网络权重和偏置可以在多大程度上影响损失函数 C 。

### 2.1 前向传递：计算神经网络的输出
在讨论反向传播算法之前，我们先以多层前馈神经网络为例，用基于矩阵的方法来计算神经网络的输出。在此过程中，先定义一些数学符号。
首先明确地定义神经网络中的权重参数，用$w^l_{jk}$ 表示第$l-1$层第 k 个神经元到第l层第 j 个神经元的连接。例如，下图中的权重 $w^3_{24}$ 表示第2层第4个神经元到第3层第2个神经元的连接。这个定义看起来有些麻烦，一个迷惑之处是 jk 的顺序，应该用j表示输入神经元，用k表示输出神经元，而不是相反。但之后的计算中，你会看到这个定义是自然而然的。
![](https://pic1.zhimg.com/v2-03625c0fff6c8fbf765de169344ec970_1440w.jpg)
类似地，我们定义网络中的偏置和激活。用 $b^l_j$ 表示网络中第l层第j个神经元的偏置，用 $a_j^l$ 表示网络中第l层第j个神经元的激活。下图给出了示例。
![](https://pic2.zhimg.com/v2-508dac2b1e4074afe7e06407c6e4d9c5_1440w.jpg)
有了以上定义，我们就可以把网络第l层第j个神经元的激活 $a_j^l$ 与第 $l-1$ 层的激活联系起来。**单个神经元的计算公式(逐点形式)：**
$$a_{j}^{l}=\sigma(\sum_{k}w^{l}_{jk}a^{l-1}_{k}+b^{l}_{j})$$
其中，求和是对第l-1层上所有神经元 k 的求和， $\sigma ()$ 是激活函数。
为了以矩阵形式重写上式，我们为第l层定义一个权重矩阵（weight matrix） $w^l$ ，表示连接到第l层的权重，矩阵第j行第k列的元素为 $w^l_{jk}$ 。设第l层的输入维度为 M ，输出维度为 N 。则该权重矩阵 $w^l$ 的形状为 $N \times M$ 。
同样地，我们定义一个偏置向量（bias vector） $b^l$ ，它的元素是 $b_j^l$ ，形状是 N 。定义一个激活向量（activation vector） $a^l$ ，它的元素是 $a^l_j$，形状是 N 。
重写上式需要的最后一个要素是向量化（vectorization）函数。核心是函数逐点应用到向量中的每个元素。
有了以上定义，我们就可以**将上式改写为矩阵形式**：
$$a^{l}_{N}=\sigma(w^{l}_{N\times M}a^{l-1}_{M}+b^{l}_{N})$$
下标表示了矩阵或者向量的形状。这个公式给出一个更全局的视角来观察一层神经元的激活是如何与上一层神经元的激活联系起来的：先将权重矩阵应用到上一层的激活，再加上偏置向量，最后过激活函数。
我们**引入中间变量** $z^{l} = w^{l}a^{l-1}+b^{l}$ ，我们将 z^l 称为第 l 层神经元的加权输入（weighted input）。其元素为：
$$z^{l}_{jk}=\sum_{k}w^{l}_{jk}a^{l-1}_{k}+b^{l}_{j}$$
### 2.2 反向传播的四个基本等式
反向传播是要理解改变网络中的权重和偏置多大程度上影响损失函数。这意味着计算偏微分 $\frac{\partial C}{\partial w^{l}_{jk}}$ 和 $\frac{\partial C}{\partial b^{l}_{j}}$ 。为了计算这两个偏微分，我们引入中间变量 $\delta^{l}_{j}$ ，称为第l层第j个神经元的误差（error）。反向传播会给出计算误差 $\delta^{l}_{j}$ 的步骤，再将 $\delta^{l}_{j}$ 与 $\frac{\partial C}{\partial w^{l}_{jk}}$ 和 $\frac{\partial C}{\partial b^{l}_{j}}$ 联系起来。
将误差 $\delta^{l}_{j}$ 定义为：
$$\delta^{l}_{j}=\frac{\partial C}{\partial z^{l}_{j}}$$
### 2.2.1 输出层的误差
$$\delta^{L}_{j}=\frac{\partial C}{\partial a^{L}_{j}}\sigma^{'}(z^{L}_{j}) \qquad\qquad\qquad \cdots\cdots(BP1)$$
右边的第一项表示第j个输出激活多大程度上影响了损失函数。如果损失函数不太依赖于第j个神经元激活，那么 \delta^{L}_{j} 会是一个比较小的值。第二项表示激活函数在 z^{L}_{j} 上的改变程度。
将上式改写为矩阵形式： $$\delta^{L} = \nabla_{a}C \odot \sigma^{'}(z^{L})$$
**证明**：
从定义出发，应用多元微积分的链式法则：$$ \delta^{L}_{j} =\frac{\partial C}{\partial z^{L}_{j}} = \sum_{k}\frac{\partial C}{\partial a^{L}_{k}}\frac{\partial a^L_k}{z^L_j} =\frac{\partial C}{\partial a^{L}_{j}}\frac{\partial a^L_j}{z^L_j} =\frac{\partial C}{\partial a^{L}_{j}}\sigma^{'}(z^L_j)$$
### 2.2.2 误差与下一层误差的关联
$$\delta^{l}=((w^{l+1})^{T}\delta^{l+1})\odot\sigma^{'}(z^l) \qquad\qquad\qquad \cdots\cdots(BP2)$$
上式中， $(w^{l+1})^{T}$ 是第 $l+1$ 层权重矩阵 $w^{l+1}$ 的转置。这个式子乍一看是很复杂的，但每一项都有优美的解释。假设我们已经知道第 $l+1$ 层的误差 $\delta^{l+1}$ ，乘以转置的权重矩阵 $(w^{l+1})^{T}$ ，可以理解为将误差通过网络反向传播，这衡量了第l层输出激活的误差；再乘以阿达玛乘积 $\odot\sigma^{'}(z^l_j)$ ，将误差通过第l层的激活函数反向传播，这衡量了第l层加权输入的误差。
这个式子是很优雅的。有了公式(BP1)和(BP2)，就可以计算出所有层对加权输入的误差了。 $\delta^{L}\rightarrow\delta^{L-1}\rightarrow \cdots\rightarrow \delta^{l}\rightarrow \cdots\rightarrow\delta^{2}\rightarrow \delta^{1}$ 。这也就是称之为梯度反向传播算法的原因。

**证明**：
从定义出发，应用多元微积分的链式法则：有 $\delta^{l}_j=\frac{\partial C}{\partial z^l_j}$ ，以及 $\delta^{l+1}_j=\frac{\partial C}{\partial z^{l+1}_j}$ 。
$$\delta^{l}_j=\frac{\partial C}{\partial z^l_j}=\sum_{k}\frac{\partial C}{\partial z^{l+1}_k}\frac{\partial z^{l+1}_{k}}{\partial z^l_j}=\sum_{k}\frac{\partial z^{l+1}_{k}}{\partial z^l_j}\delta^{l+1}_{j}$$
要评估等式右边的第一项，注意到：
$$z^{l+1}_{k} =\sum_{j}w^{l+1}_{kj}a^{l}_{j}+b^{l+1}_k =\sum_{j}w^{l+1}_{kj}\sigma(z^{l}_{j})+b^{l+1}_k$$
取微分，得到：
$$\frac{\partial z^{l+1}_k}{\partial z^l_j} =w^{l+1}_{kj}\sigma^{'}(z^l_j)$$
带入上上式中，得到：
$$\delta^l_j=\sum_{k}w^{l+1}_{kj}\delta^{l+1}_k\sigma^{'}(z^l_j)$$
重写为矩阵形式，即：
$$\delta^{l}=((w^{l+1})^{T}\delta^{l+1})\odot\sigma^{'}(z^l)$$
### 2.2.3 偏置的梯度
$$\frac{\partial C}{\partial b^l_j}=\delta^{l}_{j}\qquad\qquad\qquad \cdots\cdots(BP3)$$
第l层偏置的梯度就等于第l层加权输入的误差 $\delta^{l}$ 。改写为矩阵形式：
$$\frac{\partial C}{\partial b^l}=\delta^{l}$$
**证明**：
由定义出发，用多元微积分的链式法则：
$$\frac{\partial C}{\partial b^{l}_j} = \frac{\partial C}{\partial z^{l}_j}\frac{\partial z^{l}_j}{\partial b^l_j} =\delta^l_j\frac{\partial z^{l}_j}{\partial b^l_j}$$
注意到： $z^{l}_{j}=\sum_{k}w^{l}_{jk}a^{l}_{k}+b^{l}_j$ ，故 $\frac{\partial z^l_j}{\partial b^l_j}=1$ 。带入上式中，得到：
$$\frac{\partial C}{\partial b^l_j}=\delta^{l}_{j}$$
### 2.2.4 权重的梯度
$$\frac{\partial C}{\partial w^l_{jk}} =a^{l-1}_{k}\delta^l_j\qquad\qquad\qquad \cdots\cdots(BP4)$$
第l层权重的梯度就等于第l层加权输入的误差 $\delta^{l}_{j}$ 与 上一层神经元输出激活 $a^{l-1}_{k}$ 的乘积。将上式改写为：
$$\frac{\partial C}{\partial w} = a_{in}\delta_{out}$$
假设权重 $w$ 连接了两个神经元， $a_{in}$ 是神经元输入给权重的激活， $\delta_{out}$ 是权重输出给神经元的误差。
![](https://pica.zhimg.com/v2-7c17b2488f2a0281ef6ba9506c3a3a8c_1440w.jpg)
**证明**：
由定义出发，应用多元微积分的链式法则：
$$\frac{\partial C}{\partial w^{l}_{jk}} = \frac{\partial C}{\partial z^{l}_j}\frac{\partial z^{l}_j}{\partial w^l_{jk}} =\delta^l_j\frac{\partial z^{l}_j}{\partial w^l_{jk}}$$
注意到： $z^{l}_{j}=\sum_{k}w^{l}_{jk}a^{l}_{k}+b^{l}_j$ ，故 $\frac{\partial z^l_j}{\partial w^l_{jk}}=a^{l-1}_k$ 。带入上式中，得到：
$$\frac{\partial C}{\partial w^l_{jk}} =a^{l-1}_{k}\delta^l_j$$
### 2.2.5 四个基本等式
综上，梯度反向传播的四个基本等式是：

$$\begin{align}
\delta^{L} &= \nabla_{a}C\odot \sigma^{'}(z^{L)} \\
\delta^{l} &= (w^{l+1})^{T}\delta^{l+1} \odot \sigma^{'}(z^{l}) \\
\frac{\partial C}{\partial b^l_j} &= \delta^l_j \\
\frac{\partial C}{\partial w^l_{jk}} &= a^{l-1}_k\delta^l_j
\end{align}$$
有了这四个基本等式，我们就基本理解了梯度的计算过程。

## 3\. 后向传递与前向传递的FLOPs比率

### 3.1 定义
**FLOPS**：全大写，floating point operations per second，每秒钟浮点数计算次数，理解为运算速度，是衡量硬件性能的指标。例如：A100的float32运算速度是9.7TFLOPS，V100的float32运算速度是7.8TFLOPS。

**FLOPs**：s为小写，floating point operations，表示浮点数运算次数，理解为计算量，衡量模型或算法的复杂度。
> 如何计算矩阵乘法的FLOPs呢？ 对于 $A\in R^{1\times n},B\in R^{n\times 1}$ ，**计算 AB 需要进行 n 次乘法运算和 n 次加法运算，共计 2n 次浮点数运算，需要 2n FLOPs。** 对于 $A\in R^{m\times n},B\in R^{n\times p}$ ，浮点数运算次数为 2mnp 。

**backward-forward FLOP ratio**：后向传递与前向传递的FLOPs比率，表示一次后向传递的浮点数计算次数与一次前向传递的浮点数计算次数的比率。衡量了一次后向传递要比一次前向传递多进行的浮点数运算。

### 3.2 前向传递与后向传递的浮点数操作次数的理论分析
为了理解后向传递的浮点数运算为什么要比前向传递多？我们就要从反向传播算法的四个基本等式入手。
我们从一个有2层隐藏层的神经网络入手：
![|550](https://pic1.zhimg.com/v2-68d069ebdce611f8af8a79696ff7c656_1440w.jpg)
我们假设线性层采用[ReLU激活函数](https://zhida.zhihu.com/search?content_id=238245056&content_type=Article&match_order=1&q=ReLU%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0&zhida_source=entity)，采用[随机梯度下降](https://zhida.zhihu.com/search?content_id=238245056&content_type=Article&match_order=1&q=%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D&zhida_source=entity)优化器。下表中h1和h2分别表示第1层和第2层隐藏层的神经元个数。有2层隐藏层的神经网络一次训练迭代的计算过程如下表所示：
![|1075](https://pic3.zhimg.com/v2-91c71e6df7809d9b094887a206095f1e_1440w.jpg)

从上表中，我们可以观察到，对于多层前馈神经网络模型，有以下结论：
1\. **相比于线性层，激活函数ReLU和损失函数的浮点数运算量可以忽略**。
2\. 对于**第一层，后向传递-前向传递的FLOPs比率是1:1**。
3\. 对于**其他层，后向传递-前向传递的FLOPs比率是2:1**。
4\. 采用**随机梯度下降作为优化器，权重更新的FLOPs是模型参数规模的2倍。**

### 3.2.1 第一层与其他层的区别
对于多层前馈神经网络模型，采用随机梯度下降作为优化器。
第一层的后向传递-前向传递的FLOPs比率是1:1，其他层的后向传递-前向传递的FLOPs比率是2:1。模型参数更新的FLOPs是模型参数规模的2倍。
### 3.2.2 batch_size的影响
前向传递和后向传递的计算量FLOPs与batch_size成正比，即随着batch_size增大而线性增长。
优化器更新模型参数的FLOPs与batch_size无关，只与模型参数规模和优化器类型有关。
随着batch_size增大，前向传递和后向传递的FLOPs线性增长，而权重更新的FLOPs保持不变，这使得权重更新的FLOPs变得逐渐可以忽略不计了。
### 3.2.3 网络深度的影响
神经网络层数对后向传递-前向传递FLOPs比率有着间接的影响。由于第一层的后向-前向FLOPs比率是1:1，而其他层后向-前向FLOPs比率是2:1。层数的影响实际上是第一层与其他层的比较。
定义一个CNN神经网络，中间层的数量由0到100，可以看到后向-前向FLOPs比率由1.5提高到了接近2的水平。当层数逐渐变深的时候，第一层对模型整体的FLOPs影响就变小了，模型整体的后向-前向FLOPs比率就很接近2。
![|725](https://pic4.zhimg.com/v2-3acadebf85a3907ce358cfe804be63df_1440w.jpg)
## 4\. 总结
随着batch_size的增大，前向传递和后向传递的FLOPs线性增长，而权重更新的FLOPs保持不变，参数更新的FLOPs变得可以忽略不计了。这体现为：**当batch_size足够大时，在训练神经网络的一次迭代中，前向传递和后向传递是主要的耗时环节，而参数更新的耗时变得几乎可以忽略不计**。
由于第一层的后向-前向FLOPs比率是1:1，而其他层后向-前向FLOPs比率是2:1。随着网络层数的加深，第一层对整体FLOPs的影响变得可以忽略不计了。这体现为：**当网络层数足够深时，后向传递的耗时几乎是前向传递耗时的2倍**。
总的来说，对于**用大batch_size的深层神经网络**来说，**后向传递-前向传递的FLOPs比率接近于2:1**，换句话说，**后向传递的计算量大约是前向传递的两倍**。
## 5\. 参考链接
1\. Li S, Zhao Y, Varma R, et al. Pytorch distributed: Experiences on accelerating data parallel training\[J\]. arXiv preprint arXiv:2006.15704, 2020.
2\. Narayanan D, Shoeybi M, Casper J, et al. Efficient large-scale language model training on gpu clusters using megatron-lm\[C\]//Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 2021: 1-15.
3\. [How the backpropagation algorithm works--Michael Nielsen](https://link.zhihu.com/?target=http%3A//neuralnetworksanddeeplearning.com/chap2.html)
4\. [What’s the backward-forward FLOP ratio for Neural Networks?](https://link.zhihu.com/?target=https%3A//www.alignmentforum.org/posts/fnjKpBoWJXcSDwhZk/what-s-the-backward-forward-flop-ratio-for-neural-networks)