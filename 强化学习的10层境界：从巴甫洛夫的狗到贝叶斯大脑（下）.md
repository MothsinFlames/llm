---
created: 2025-11-05T11:32:11 (UTC +08:00)
tags: [强化学习 (Reinforcement Learning)]
source: https://zhuanlan.zhihu.com/p/1934577410013115265
author: 关于作者gwave我思故我在，生命的意义在于负熵复旦大学 电子与信息博士郭达森也关注了他回答985文章193关注者37,956已关注发私信
---

# 强化学习的10层境界：从巴甫洛夫的狗到贝叶斯大脑（下）

> ## Excerpt
> 在 上一篇文章中，我们探讨了双轮驱动的 A2C、PPO 以及由内在动机驱动的强化学习方法。本篇作为收官之作，将从多智能体（Multi-Agent Reinforcement Learning, MARL）出发，聚焦智能体的社会性演化，并进一步探讨…

---
在[上一篇文章](https://zhuanlan.zhihu.com/p/1933659071204038283)中，我们探讨了双轮驱动的 A2C、PPO 以及由内在动机驱动的强化学习方法。本篇作为收官之作，将从多智能体（Multi-Agent Reinforcement Learning, MARL）出发，聚焦智能体的社会性演化，并进一步探讨[大语言模型](https://zhida.zhihu.com/search?content_id=261102297&content_type=Article&match_order=1&q=%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B&zhida_source=entity)（Large Language Models, LLMs）如何与强化学习（RL）相互促进，共塑智能体的认知与行为能力。

> **第九境界：社会智能 —— 多智能体与博弈**
> 
> 在合作与竞争的动态中，智能体自发涌现出规则、身份与语言，演化出社会结构与博弈策略。
> 
> **第十境界：语言即激励 —— LLM 引导的策略生成**
> 
> 语言不再只是观察的媒介，它同时承载策略、目标与动机，成为认知架构中的统一表达层，驱动行为生成与决策调整。

![](https://pic4.zhimg.com/v2-7584f097cde1ada76bcb6dc4b98fa965_1440w.jpg)

自然界中的自发性任务行为包括：(a) 鱼群为躲避捕食者而表现出的群游行为，(b) 蚂蚁在合作过程中展现出的集体行为，(c) 蜜蜂的蜂群行为，(d) 大雁在迁徙过程中展现出的有序行为模式。https://www.ejece.org/index.php/ejece/article/view/619

## 第九境界：社会智能 —— 多智能体与博弈

### **9.1 从个体到群体：多智能体的诞生与挑战**

传统的强化学习聚焦于单一智能体与环境之间的交互。然而，真实世界往往是由多个智能体组成的复杂系统，无论是蚂蚁群体觅食，还是人类社会协作决策。这促使多智能体强化学习（Multi-Agent Reinforcement Learning, MARL）应运而生。

![](https://pic1.zhimg.com/v2-e80b407c05b9cc976fee7d9323ce1948_1440w.jpg)

层级的MARL问题，Agent级别之和要大于等于苹果的级别，才能吃到苹果，这要求不同Agent之间展开合作。/https://www.marl-book.com/download/marl-book.pdf

在 MARL 框架下，环境不再静态，其他智能体的行为也构成环境的一部分。这一转变不仅带来了非平稳性（Non-stationarity）的问题，也为智能体提供了更丰富的交互可能性。合作、竞争与协同策略成为智能体学习的核心课题。与单智能体相比，MARL面临以下挑战：

![](https://pic4.zhimg.com/v2-d22b0b1b1ee548abbf5b63dac0fa015b_1440w.jpg)

MARL的挑战 (Ning and Xie)

-   **非平稳性（Non-Stationarity）**

在多智能体场景中，由于其他智能体的存在，每个智能体所处的环境变得动态化。随着智能体不断更新其策略，从任意一个智能体的视角来看，环境的动态特性也随之发生变化。这违反了马尔可夫性质（Markov Property），因为状态转移概率和奖励函数不再是固定不变的。每个智能体的最优策略会随着他人行为的调整而改变，导致学习过程出现不稳定性。

-   **部分可观测性（Partial Observability）**

在大多数多智能体环境中，智能体无法获得对环境状态或其他智能体动作的完整观察。每个智能体只能观测到环境的一部分，从而在决策过程中引入了不确定性。此时问题变为一个**部分可观测马尔可夫决策过程（POMDP）**，智能体需要在信息不完整的情况下进行推理和决策。这增加了策略学习的难度，因为智能体不仅要应对环境的不确定性，还需要预测其他智能体的行为。

-   **可扩展性与联合动作空间（Scalability and Joint Action Space）**

随着智能体数量的增加，联合动作空间呈指数级增长。对于 $n$ 个智能体，其动作集合分别为 $A_1, A_2, \dots, A_n$ ，那么联合动作空间为 $A_1 \times A_2 \times \dots \times A_n$ 。这一状态-动作空间的扩展显著增加了计算复杂度，使得传统强化学习方法效率大大降低。当智能体数量增加时，寻找最优策略将变得不可计算，从而需要更具可扩展性的学习算法。

-   **归因分配问题（Credit Assignment）**

在多智能体场景中，归因分配问题指的是如何确定每个智能体的动作对整个团队目标的贡献程度。该问题在合作场景中尤为复杂，因为多个智能体必须共同努力来最大化一个共享的全局奖励。传统方法往往难以准确评估个体贡献，通常只是将全局奖励粗略地分解为个体效用，而未考虑每个智能体的行为是否从全局角度来看是最优的。

### **9.2 MARL的问题设定与分类**

**9.2.1 问题设定**

为了将单智能体强化学习（RL）扩展到多智能体场景，并考虑环境中其他智能体的行为，我们需要对系统进行不同于单智能体马尔可夫决策过程（MDP）的建模。这种情况下，通常使用多智能体[马尔可夫博弈](https://zhida.zhihu.com/search?content_id=261102297&content_type=Article&match_order=1&q=%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%8D%9A%E5%BC%88&zhida_source=entity)（Markov Game）进行建模。在该框架下，环境中存在多个智能体同时进行交互，彼此的行为会共同影响状态转移与奖励结果。一个马尔可夫博弈由六元组 ( $N, S, A, P, R, \gamma$ ) 定义，其中：

-   $N$ ：智能体的数量；
-   $S$ ：状态空间；
-   $A = A_1 \times A_2 \times \dots \times A_n$ ：联合动作空间，每个智能体拥有自己的动作集合 A\_i；
-   $P$ ：状态转移概率函数，即在某一状态和联合动作下，环境转移到下一个状态的概率分布；
-   $R = (R_1, R_2, \dots, R_n)$ ：每个智能体各自的奖励函数；
-   $\gamma$ ：折扣因子，用于衡量未来奖励的权重。
    
    这种博弈式建模框架为多智能体强化学习提供了理论基础，能够涵盖合作、对抗以及混合策略等多种情形。

### **9.2.2 分类**

**1\. 合作型（Cooperative）**

合作型 MARL 强调多个智能体协同学习，共同实现一个共享目标，并最大化全体的总奖励。在这种设置中，每个智能体的行为都对整体任务的成功起到关键作用，奖励机制通常被设计为强化群体表现。这种类型的 MARL 特别适用于诸如多机器人系统、协同导航、无人机编队等场景。

![](https://pic3.zhimg.com/v2-d9bd3ad28fa05b9bc133f92b94e25934_1440w.jpg)

在《星际争霸 II》中实现的去中心化单位微操管理（Decentralised unit micromanagement），即每个学习智能体分别控制一个单位，其目标是通过智能体间的协调配合，击败所有敌方单位。https://proceedings.mlr.press/v80/rashid18a/rashid18a.pdf

**2\. 对抗型（Competitive）**

对抗型 MARL 通常用于博弈或敌对环境中，智能体之间存在明确的竞争关系，彼此试图最大化自身收益的同时最小化对手的得分，常见于零和博弈（Zero-Sum Game）设置中。典型案例包括棋类游戏（如围棋、国际象棋）或其他敌对性的策略博弈场景。

**3\. 混合型（Mixed-Interest）**

混合型 MARL 描述的是一种既有合作又有冲突的动态关系，其中智能体之间的目标部分一致、部分对立。在这类环境中，智能体可能需要在“结盟”与“对抗”之间权衡。常见于诸如金融交易（Trading）、城市交通调度（Traffic Systems）、以及多玩家电子游戏等复杂交互场景。

### **9.2.3 学习范式**

![](https://pic2.zhimg.com/v2-3b6f8a7d097bf9d57bc51a3b3d8896b9_1440w.jpg)

https://www.sciencedirect.com/science/article/pii/S2949855424000042

上图展示了多智能体强化学习（MARL）的三种典型学习范式：

**(a) 集中训练、集中执行（Centralized Training and Centralized Execution, CTCE）：**在该范式中，所有智能体共享一个统一的策略网络，该策略适用于所有个体的行为决策。可以将 $N$ 个智能体整体视作一个“虚拟的大智能体”进行建模与训练。尽管实现简单，但其适用性有限，通常仅适用于完全可观测、同质智能体的静态环境中。

**(b) 集中训练、分散执行（Centralized Training with Decentralized Execution, CTDE）**

这是当前最常见且最流行的学习范式。在训练阶段，智能体可以访问全局信息，如其他智能体的观察、动作或隐藏状态，从而提升训练稳定性和策略优化效率；而在执行阶段，系统回归到分布式结构，每个智能体只能基于自身的局部观察独立做出决策，无法访问他人信息。CTDE 在保证可扩展性和泛化能力的同时，兼顾训练效率与执行灵活性，广泛应用于协作、多机器人控制、竞技博弈等场景中。

**(c) 分散训练、分散执行（Decentralized Training and Decentralized Execution, DTDE）**

在完全分散式学习（Fully Decentralized Learning）中，每个智能体在训练和执行过程中都无法获取任何其他智能体的信息，完全独立地更新自身策略。其训练目标仍是最大化全局团队奖励，因此需在缺乏协作信号的情况下学习出可协调的策略。该范式适用于高可扩展性要求的复杂环境，尤其是包含**合作-竞争混合动机（mixed-interest）或智能体数量极大的场景（如 Swarm 系统）。然而，DTDE 也带来了显著挑战，尤其是环境非平稳性**问题：由于其他智能体在不断学习和更新策略，对于任意一个个体而言，环境动态是不断变化的，这使得学习过程更加不稳定、收敛更困难。

### 9.3 相关算法

### 9.3.1 [价值分解网络](https://zhida.zhihu.com/search?content_id=261102297&content_type=Article&match_order=1&q=%E4%BB%B7%E5%80%BC%E5%88%86%E8%A7%A3%E7%BD%91%E7%BB%9C&zhida_source=entity)（VDN）

**价值分解网络（Value Decomposition Networks）** 是一种应用于多智能体场景的算法，适用于集中训练、分散执行（CTDE）的范式。其核心思想是：将每个智能体各自估计的 $Q$ 值或 $V$ 值进行求和，从而得到一个全局的总价值函数（全局 $Q$ 值或 $V$ 值）。这种分解方式简化了多智能体间的协同策略学习，有助于在保持去中心化执行的前提下，实现有效的集中训练。

![](https://picx.zhimg.com/v2-5288010a09245d79c7f2531483fb6597_1440w.jpg)

https://arxiv.org/pdf/1706.05296.pdf

**IQL（Independent Q-Learning）** 是一种简单的“各自为战”式多智能体强化学习（MARL）方法。其核心思想是将每个智能体**视为独立个体分别训练**，并将其他智能体的行为**视作环境动态的一部分**来建模。在 IQL 中，每个智能体仅关注最大化自身的奖励，并根据自身的目标函数独立地进行策略优化。这样自然会遇到前文提到的平稳性问题。

**VDN（Value Decomposition Networks）** 在结构上与 IQL 或 IDQL（Independent DQN）类似：系统中包含多个 DQN 智能体，每个智能体拥有自己独立的神经网络和状态输入表示。但关键区别在于——VDN 引入了联合价值函数（Joint Value Function）来进行集中训练。在 VDN 中，全局的团队奖励信号会通过联合价值函数反向传播至每个智能体的网络，从而使各智能体能够学习到**有利于整体协作的策略**。这种方式有效弥合了个体最优与全局最优之间的差距，显著提升了协作任务中的表现能力。

![](https://pica.zhimg.com/v2-cef1b8274c289f5342e87de610c38e24_1440w.jpg)

VDN

上面公式展示了在双智能体场景下，使用 VDN 进行价值函数分解。其中，全局奖励可以基于各个智能体的观测信息**加性地分解**为局部奖励。因此，该架构在建模时**鼓励将全局价值函数拆解为更简单的子函数**，从而降低学习难度。实践中也确实观察到了这类**自然分解结构**的出现，验证了其有效性。

为了进一步减少可学习参数的数量，一种常见策略是**在多个智能体之间共享部分网络权重**。这种权重共享不仅提升了训练效率，还引出了一个重要的设计原则：**智能体不变性（Agent Invariance）**，即模型对智能体顺序的排列保持不敏感。这一特性在避免常见的“懒惰智能体问题”（Lazy Agent Problem）中具有重要作用，可促使所有智能体积极参与协作任务的学习过程。

不足之处：简单地对各个智能体的 Q 值进行求和，可能会导致策略的多样性降低，尤其在各智能体**共享同一个 Q 网络**的情况下，容易陷入**局部最优解**，限制了整体策略的表达能力与探索空间。

### **9.3.2 QMIX**

**QMIX** 是在 **VDN（Value Decomposition Networks）** 提出的价值函数分解方法基础上发展而来的。两者在基本思想上保持一致：均采用**联合动作价值函数（Joint Action-Value Function）因式分解**策略，将全局 Q 值拆解为多个智能体的个体 Q 值，从而支持**去中心化执行（Decentralized Execution）**。这种设计使得每个智能体在执行阶段仅需依赖自身的局部观测和个体价值函数即可独立决策，无需访问其他智能体的信息或全局状态。

为了克服 VDN 在表达能力上的限制，QMIX 引入了一个**混合网络（Mixing Network）**，用于将各个智能体的个体 Q 值灵活组合为一个全局联合 Q 值。与 VDN 的线性求和不同，QMIX 的混合网络支持**非线性组合**，从而能够捕捉更复杂的智能体间协作关系。同时，该网络结构遵循一个关键约束：**全局 Q 值需对每个个体 Q 值保持单调性（**即 $\frac{\partial Q_{tot}}{\partial Q_{a}} \geq 0， \forall a \in A$ **）**，确保在集中训练后仍能实现一致性分散执行。

为了实现**集中训练、分散执行（CTDE）**，我们需要确保：在全局联合动作空间上执行的一次 $\arg\max Q_{\text{tot}}$ 操作，其结果应与在每个智能体上分别执行的 $\arg\max Q_a$ 操作相一致。换言之，联合最优动作应可以由每个智能体根据自身的个体 $Q$ 值独立选出，从而确保集中训练时的最优策略能够在执行阶段无缝地映射为分散决策。$\begin{split}\arg \max _{\boldsymbol{u}} Q_{\mathrm{tot}}(\boldsymbol{\tau}, \boldsymbol{u})=\left(\begin{array}{c}\arg \max _{u_{1}} Q_{1}\left(\tau_{1}, u_{1}\right) \\ \vdots \\ \arg \max _{u_{N}} Q_{N}\left(\tau_{n}, u_{N}\right)\end{array}\right)\end{split}$

QMIX 遵循标准的 Q-learning 学习范式，旨在学习出能够最大化期望累计奖励的最优联合策略。在训练过程中，智能体与环境交互，通过基于 **Bellman 方程** 的时序差分误差（TD Error）来更新全局 Q 值，并反向传播至每个个体网络，从而引导策略优化。

![](https://pica.zhimg.com/v2-01131658d41161c599da9218f0e92572_1440w.jpg)

(a) 混合网络结构：红色部分表示用于生成混合网络中蓝色层的权重和偏置的超网络（Hypernetworks）。(b) QMIX 的整体架构图。(c) 智能体网络结构。https://proceedings.mlr.press/v80/rashid18a/rashid18a.pdf

### 9.4 与Swarm Intelligence集成

[Swarm RL](https://zhida.zhihu.com/search?content_id=261102297&content_type=Article&match_order=1&q=Swarm+RL&zhida_source=entity) 体现了一种高度去中心化的组织形式，其中每个智能体仅依赖自身的局部观测，并与邻近的智能体进行信息交互，无需全局控制器或集中式通信机制而进行自组织。这种范式源于自然界中常见的群体行为，例如鱼群游动、鸟群飞行和蚂蚁觅食，个体间通过简单的局部规则即可涌现出复杂的群体行为。Swarm RL 以其高度自组织、自适应的特性，为模拟与构建多智能体社会提供了理想的平台。

在强化学习框架下，Swarm 通常不依赖全局奖励，而采用局部观察和局部反馈进行策略更新。训练过程中，每个智能体独立地与环境交互，通过局部经验进行学习，同时其行为会间接影响邻域智能体，形成一种通过局部协同达成全局协调的机制。这种方式不仅具备出色的可扩展性和鲁棒性，也使得智能体系统具备更强的迁移能力和泛化能力，适用于机器人编队、交通调度、群体仿真等大规模协作任务。

下图展示了两个相邻智能体 $i$ 和 $j$ ，它们面朝各自的速度方向 ${v}^i$ 和 ${v}^j$ 。图中以智能体 $i$ 为参考，标出了其观测到的若干物理量：

-   **方位角** $\phi_{i,j}$ ：指从智能体 $i$ 指向智能体 $j$ 的方向；
-   **相对朝向** $\theta_{i,j}$ ：表示智能体 $j$ 相对于智能体 $i$ 的朝向角度；
-   **距离** $d_{i,j}$ ：两者之间的欧几里得距离；
-   **相对速度向量** $\Delta v^{i,j} = {v}^i - {v}^j$ ：反映了两个智能体的速度差异。

在这个简单的示例中，智能体 $i$ 所观测到的邻居数量为 $|\mathcal{N}(i)| = 1$ ，而智能体 $j$ 所通报的邻居数量也是 $|\mathcal{N}(j)| = 1$ 。

![](https://pic1.zhimg.com/v2-bb3499ded999862c2a24c489ff663e76_1440w.jpg)

https://www.jmlr.org/papers/v20/18-476.html

然而，Swarm RL 也面临诸多挑战，包括局部奖励难以有效引导全局最优行为、系统收敛性缺乏理论保障，以及智能体之间信息不足导致学习效率低下等问题。此外，在缺乏中心监管的情况下，如何保障系统的安全性与稳定性仍是一个亟待解决的关键难题。尽管如此，Swarm RL 作为研究自下而上涌现智能与复杂社会行为的核心路径，正在成为多智能体学习领域的重要前沿方向。

在多智能体系统中，个体行为之间的相互作用常常会产生超出设计者预期的全局模式，这种现象被称为涌现（Emergence）。涌现并非源于某个中央控制器的明确指令，而是由大量个体基于简单规则、在局部交互中逐渐演化而来的复杂行为。正如鸟群的迁徙队形、蚂蚁的路径形成、或人类社会中的语言规范和角色分化，涌现是智能体系统从“动作集合”迈向“社会结构”的关键跃迁。

![](https://pic4.zhimg.com/v2-95f428f2786f47af349feffb424fd21b_1440w.jpg)

https://link.springer.com/chapter/10.1007/978-3-540-69913-2\_6#Fig5\_6

在强化学习框架中，涌现主要体现在以下几个层面：一是**策略模式的涌现**，即多个智能体在无需显式协调的情况下，自发形成协同策略或攻守结构；二是**角色与功能的涌现**，如在异质性任务中，一些智能体逐渐演化为领导者、传感者或中介者；三是**通信协议与社会规范的涌现**，即智能体通过经验积累，形成一致的行为习惯或决策规则。这些现象往往无法通过单智能体建模捕捉，却是理解智能群体行为不可或缺的线索。

涌现是多智能体系统中最具魅力、也最具挑战性的研究课题之一。它不仅需要精心设计个体规则与奖励机制，还涉及动态系统建模、信息论、博弈论等多学科交叉知识。更重要的是，涌现的产生往往伴随着不可预测性与多样性，这为如何**评估、调控和利用**涌现行为带来了全新问题。因此，理解涌现、塑造涌现、甚至设计涌现，正在成为多智能体社会智能迈向更高层级的重要研究方向。

## 第十境界 语言即世界-- **LLMs × RL 的闭环共舞**

![](https://pic2.zhimg.com/v2-4bc8cb4543aeb6f4df303445281f31d5_1440w.jpg)

https://www.goodreads.com/book/show/913171

> **语言的边界，就是我们所能思考与理解的世界的边界**。
> 
> \--维特根斯坦《逻辑哲学论》（_Tractatus Logico-Philosophicus_）

大语言模型（LLMs）已将人类几乎全部的知识压缩进万亿级参数的神经网络中，并能在多数场景中给出迅捷而相对准确的回应，某些领域的表现已逼近甚至超过博士水平。从维特根斯坦的视角来看，**“语言的界限即是世界的界限”**。而当语言模型能够理解、生成乃至操纵语言本身时，我们或许已站在**通往通用智能的门槛**。解决语言的问题，也许正意味着解决智能的问题。但若转向诺奖得主丹尼尔卡尼曼的“双系统理论”，今天的 LLMs 更像是**系统1**——直觉、快速、擅长模式识别，却缺乏系统2所代表的**缓慢、推理、逻辑演绎与计划**。智能的另一半——深度认知与目标导向行为——仍有待补全。

![](https://pica.zhimg.com/v2-b1491d18e9151330fabc8a9557680ef6_1440w.jpg)

https://www.mind-your-business.net/de-biasing/system1-system2-background-to-cognitive-biases.html

这一终极境界真正的跃迁在于：当语言不再仅传递意义，而开始驱动选择、塑造偏好、生成奖励时，语言本身便成为了智能的核心能量——**语言即激励，语言即行动**。

### **10.1 LLM4RL**

“世界是事实的总和，而非事物的总和。”维特根斯坦在《逻辑哲学论》中这句话，开启了语言哲学的一个高峰：**语言不仅是沟通工具，更是我们构建世界的方式。**

在过去的 AI 发展历程中，语言始终被当作“输入”或“输出”处理——我们用语言提问，机器给出回答，仿佛语言只是一种接口。而现在，这种认知正在被颠覆。大语言模型（LLMs）不仅能够理解语言，还能**生成复杂的思维路径、指令结构、推理链条，甚至激发人类行动**。当我们开始用语言让 AI 设计奖励函数、调整偏好结构、规划未来行为，语言就不再是“内容”，而是“动因”。语言从世界的描述者，变成了**世界的塑造者**。这正是“语言即激励”的第一重意义：**语言不再是智能系统的终点，而是它的起点。**

**认知注入的智能体范式**

**LLM-enhanced Reinforcement Learning** 指的是将预训练大语言模型（LLMs）所具备的多模态理解、推理生成、高阶认知能力，引入强化学习（RL）框架中，作为**智能体认知能力的增强模块**。这一方法与传统的基于模型的强化学习（Model-based RL）最大的区别在于，它**引入了知识密集型的通用模型作为认知先验**。这一范式带来三大核心优势：

**1\. 起点即高点：认知能力预注入:** 得益于 LLM 的预训练能力，RL 智能体在“出生”时便具备了高阶认知，比如推理、规划、总结与抽象等，而不是从零开始在环境中试错学习。

**2\. 强泛化：跨域迁移的能力:** 传统 RL 模型大多是“数据驱动”，而 LLM 是“知识驱动”。它们经过大规模多领域语料训练，具有良好的迁移能力，能够帮助智能体**在新任务、新环境中迅速适应**，而无需从头训练。

**3\. 交互补强：克服语言模型的“静态性”:** 预训练语言模型的最大限制在于：它们**无法主动与环境互动**，无法持续学习、校正知识偏差。而 RL 框架恰好弥补了这一点——智能体通过与环境交互生成任务相关的数据，在上下文中“喂给” LLM，从而实现“in-context grounding”；LLM 在任务环境中被**动态塑造和微调**，从静态生成器转化为**自我优化的认知体**。

![](https://pica.zhimg.com/v2-47baa59696aa3dfd5422cbc3c477f1c8_1440w.jpg)

Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods

在RL中，上图显示了LLM 4种功能：

![](https://picx.zhimg.com/v2-c0b828df1da9b3f1755cf16326f7db7f_1440w.jpg)

LLM作为特征表示提取器 Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods

1）充当**信息处理器**，帮助智能体理解复杂的自然语言指令或多模态感知信息。面对涉及语言和视觉的任务描述，传统智能体往往难以同时完成感知理解与策略优化。LLM 能提取具有语义意义的特征表示，从而加快网络训练，也能将开放式的自然语言任务描述转化为标准化、结构化的任务语言，降低学习难度。例如在机器人指令执行任务中，LLM 能将用户多样化的自然语言表达映射为统一的机器可识别指令，有效提升策略鲁棒性。

![](https://pica.zhimg.com/v2-87b23c66c0f6d0100e8f9615ccd837fa_1440w.jpg)

LLM作为奖励设计器 Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods

2）**奖励设计器**，在稀疏奖励或复杂场景下，生成有意义的奖励信号。基于其丰富的知识、推理能力和代码生成能力，LLM 可以充当“隐式奖励模型”，通过对环境状态和语言目标的理解直接给出奖励值；也可以作为“显式奖励模型”，生成逻辑清晰、可执行的奖励函数代码。在如灵巧操作等高复杂度任务中，LLM 不仅能提出初始奖励定义，还能通过试错与反馈不断迭代优化奖励逻辑。

![](https://pic4.zhimg.com/v2-77d4e9d4de1d93af6b4e654d8773bacf_1440w.jpg)

LLM作为决策制定器 Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods

**3）决策制定**，LLM 可作为策略生成者或策略指导者参与决策过程。本质上，LLM 可以将离线强化学习建模为序列建模问题，利用其强大的语义理解能力条件生成最优动作。此外，LLM 也可通过专家引导的方式，输出候选动作集或专家动作，限制原始动作空间，从而提升探索效率和样本利用率。尤其在具身智能任务中，LLM 基于语言输入生成潜在动作，供 RL 策略进一步筛选与学习。

![](https://pica.zhimg.com/v2-72a5dea1d6ad958235cd7995b2ef0880_1440w.jpg)

4）LLM 也能在**生成建模**中发挥关键作用，辅助模型驱动的 RL 学习真实世界，成为**世界模型**模拟器和提供策略可解释性。借助其常识推理和多模态理解能力，LLM 可生成模拟的世界模型或高保真轨迹预测，指导 RL 智能体进行更高效的策略学习。同时，在可解释 RL 中，LLM 可通过提示生成行为决策背后的逻辑解释。比如在 Minecraft 的复杂任务中，LLM 所生成的“子目标序列”不仅帮助智能体规划路径，还能通过反馈修正这些世界模型，从而融合知识驱动与经验驱动，实现显著的样本效率提升。

当然还有RL4LLM，但不是本文重点，文章已经太长了，故而就不赘述了。

___

至此，十个层级的探索暂告一段落。从巴甫洛夫的条件反射到“LLM4RL”的闭环智能，我们追溯了从生物本能、心理实验，到数理建模与多智能体交互的演化之路。每一层境界既是认知的跃升，也代表了当前人工智能的一种实现范式。

然而，还有一个尚未被强化学习社区广泛接纳的重要思想没有在本系列中展开——那就是“[贝叶斯大脑](https://zhida.zhihu.com/search?content_id=261102297&content_type=Article&match_order=1&q=%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%A4%A7%E8%84%91&zhida_source=entity)”（Bayesian Brain）与“自由能原理”（Free Energy Principle）。它试图将智能的本质建构为一种最小化预测误差的主动推理机制，在哲学、神经科学和机器学习之间搭起了一座桥梁。但由于其尚不成熟（如马尔可夫毯遭挑战）、工程路径不清晰，目前仍处于理论研究的前沿。

因此，我计划将其单独作为“番外篇”呈现，待有空再与大家分享。正如语言是思维的边界，探索本身也是认知的边界。在AI尚未完成闭环推理、自我建模与持续更新之前，这一系列的终点，也许只是下一个问题的起点。

再会，各位晚安！
