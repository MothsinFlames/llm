---
created: 2025-08-20T14:16:43 (UTC +08:00)
tags: [算法,大模型,深度学习（Deep Learning）]
source: https://zhuanlan.zhihu.com/p/3690546009
author: 北京大学 计算机科学与技术硕士
---
今天Sam和大家聊聊一个不是特别起眼，但在实践中随处可见的问题：

**如何往一个预训练得很好的模型中添加新结构/新特征？**

我们从搜推广模型的”**老汤**“现象谈起，向大家安利一个添加新特征时非常实用的 **[新权重零初始化](https://zhida.zhihu.com/search?content_id=249760630&content_type=Article&match_order=1&q=%E6%96%B0%E6%9D%83%E9%87%8D%E9%9B%B6%E5%88%9D%E5%A7%8B%E5%8C%96&zhida_source=entity)** 技巧，再展示几个CV/NLP领域的类似例子，最后介绍上面这个问题的学术化topic——**[神经网络态射](https://zhida.zhihu.com/search?content_id=249760630&content_type=Article&match_order=1&q=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%80%81%E5%B0%84&zhida_source=entity)Network Morphism**。

## 老汤添新味：如何往预训练模型终添加新特征

**何为老汤**：大厂的各种搜推广场景中，往往都运行着祖传的[老汤模型](https://zhida.zhihu.com/search?content_id=249760630&content_type=Article&match_order=1&q=%E8%80%81%E6%B1%A4%E6%A8%A1%E5%9E%8B&zhida_source=entity)，这些模型经过长年累月的训练而见多识广，向传承多年的老汤添加新料（比如新的特征字段，或新结构抽取的特征）时，如果训练数据窗口不是很长的话，往往性能还没原来的base模型好。

**怎么才能优雅添加新特征呢？** 换言之，怎么保证添加新特征后，模型初始时的功能和原来的老汤模型相同，保住老汤的浓香，又能吸收新料的味道，逐渐学习新特征而涨点？

 
## 温和改变：新权重初始化为零

Sam给大家介绍一个很实用的**网络初始化**技巧，给老汤模型的更新保驾护航：

如下图，假设我们要添加新特征到某个全连接层中，橙色的是该层的神经元，左边红色的是老特征和对应的原有模型权重，我们添加新特征（绿色圆点）到该层中时，只需将**新权重（绿色斜线）【初始化为0】**，就能保证新模型初始时的输出和老汤模型相等，不损失性能。

![](https://pic3.zhimg.com/v2-b028f6812f579fe33a6d30ff41111996_1440w.jpg)

## 搜推之外的应用例子

除了缓解搜推广领域的”老汤“问题外，上述的**新权重零初始化**trick在各种需要修改预训练模型的场景中都可以使用，Sam给大家介绍两个多模态领域的例子。

### 例一：图像模型添加时序Attention，转为视频模型

**场景：** 在图像上预训练的ViT只需要添加时序Attention（在不同帧的同一位置的tokens之间做self attention）就能转为[TimesFormer](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2102.05095)、[ViViT](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2103.15691)这样的视频模型。当然，如果原本的ViT后面接了个文本encoder/decoder形成了图文预训练模型（如[CLIP](https://zhida.zhihu.com/search?content_id=249760630&content_type=Article&match_order=1&q=CLIP&zhida_source=entity)和[BLIP](https://zhida.zhihu.com/search?content_id=249760630&content_type=Article&match_order=1&q=BLIP&zhida_source=entity)），添加时序Attention就能得到视频-语言模型。

![](https://pic2.zhimg.com/v2-b5ec571e36281bfe5e7285531d06a1d7_1440w.jpg)

**方案：** 为了尽量保持原有权重的作用、加快模型在视频数据上继续预训练的过程，可以用原有的空间Attention权重作为时序Attention权重的初始化，再对时序Attention抽取的特征过一个初始化为0的fc层，再加到原来的特征上。TimesFormer的相关初始化代码如下：

```python
        ## initialization of temporal attention weights
        if self.attention_type == 'divided_space_time':
            i = 0
            for m in self.blocks.modules():
                m_str = str(m)
                if 'Block' in m_str:
                    if i > 0:
                      nn.init.constant_(m.temporal_fc.weight, 0)
                      nn.init.constant_(m.temporal_fc.bias, 0)
                    i += 1
```

前向过程：

```python
            ## Temporal
            xt = x[:,1:,:]
            xt = rearrange(xt, 'b (h w t) m -> (b h w) t m',b=B,h=H,w=W,t=T)
            res_temporal = self.drop_path(self.temporal_attn(self.temporal_norm1(xt)))
            res_temporal = rearrange(res_temporal, '(b h w) t m -> b (h w t) m',b=B,h=H,w=W,t=T)
            res_temporal = self.temporal_fc(res_temporal)
            xt = x[:,1:,:] + res_temporal
```

**例二：SD添加条件模块，转为ControlNet**

无独有偶，文生图领域的[ControlNet](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2302.05543)在往[StableDiffusion](https://zhida.zhihu.com/search?content_id=249760630&content_type=Article&match_order=1&q=StableDiffusion&zhida_source=entity)预训练模型添加condition时，也对条件模型编码的特征过了初始化为0的卷积层zero convolution：

![](https://pic2.zhimg.com/v2-666514d4fe33767c08b62d0fe6258333_1440w.jpg)

![](https://picx.zhimg.com/v2-886b4c0c5d2e480348445157ca9a126d_1440w.jpg)

## 学术化：神经网络态射

这个”如何在魔改时保持预训练模型功能“的问题，在学术界叫**【神经网络态射 Network Morphism】**，感兴趣的同学可以参阅ICML 2016这篇老文章，内含更多类型的网络结构改动后的初始化技巧。

![](https://picx.zhimg.com/v2-113a38a79d9f0cdedcc83668898e2445_1440w.jpg)
