---
created: 2025-08-20T14:16:49 (UTC +08:00)
tags: [大模型,算法岗,面经]
source: https://zhuanlan.zhihu.com/p/701147795
author: 北京大学 计算机科学与技术硕士
---
想必大多数同学都知道是将[BN](https://zhida.zhihu.com/search?content_id=243940940&content_type=Article&match_order=1&q=BN&zhida_source=entity)和dropout这些训练和测试时行为不同的层切换到test mode。但是继续细问呢？

1.  **dropout训练和测试有什么不同？**
2.  **BN测试的时候统计量从哪来？**
3.  **大模型也有这些操作嘛？**

### Dropout训练和推理时的差异

众所周知，dropout是训练的时候将某层的神经元输出以**p**（如10%）的概率随机置为零，以减缓过拟合。那么测试时呢？

为了保证测试时每次推理的一致性，肯定是不能再随机让神经失活了，那什么都不做，让所有神经元都按正常输出值原样输出可以嘛？

从训练和测试时的输出值尺度一致性来看，答案显然是否定的。设每个神经元原始输出的绝对值均值为**x**，则该层训练时dropout后每个神经元输出的绝对值均值为 **(1-p)x** ，如果测试时简单地让所有神经元都按正常输出值原样输出，则输出的绝对值均值为**x**，导致训练和测试的不一致。

因此，带dropout的层，测试时的行为是让所有神经元都输出，但输出值要从原始的**x**变成**(1-p)x**。

当然，也可以在训练的时候就除以(1-p)，这样训练时dropout后每个神经元输出的绝对值均值就是x，测试的时候直接简单地做恒等映射输出就行，速度更快。torch官方使用的是这种实现：

![](https://pic4.zhimg.com/v2-01c9b323fa16795f4555662e7e6ccf27_1440w.jpg)

torch官方说明的dropout原理

### BatchNorm测试时统计量怎么来

[Batch Normalization](https://zhida.zhihu.com/search?content_id=243940940&content_type=Article&match_order=1&q=Batch+Normalization&zhida_source=entity)训练时，使用的均值和方差统计量是从同batch数据的同一特征维度统计而来。

而测试时，为了保证每个测试点的预测输出不受batch内其他数据影响，肯定得采用固定的统计量，这个统计量从何而来呢？

直观的想法是再过一遍整个训练集，在整个训练集上估计均值和方差，但在数据多的时候，每次在测试之前都要过一遍整个训练集超级费时：


因此，我们需要一个近似的在训练时估计全局统计量分布的办法。这个统计量要满足两个要求：

1.  尽量反映整个训练集的分布；
2.  尽量使用模型训练结束或者即将结束时的权重来计算，以便和测试时行为一致。

考虑到这两个要求，是不是想起了[SGD](https://zhida.zhihu.com/search?content_id=243940940&content_type=Article&match_order=1&q=SGD&zhida_source=entity)里的[momentum](https://zhida.zhihu.com/search?content_id=243940940&content_type=Article&match_order=1&q=momentum&zhida_source=entity)？采用训练时**指数滑动平均**的方式就可以得到测试时用的统计量，参考torch的官方文档：

![](https://pic4.zhimg.com/v2-b942b0b09d62fa405d0426e504aa0083_1440w.jpg)

### **model.eval()影响大模型吗**

我们已经厘清了model.eval()对dropout层和batch norm层的影响，那么**model.eval()影响大模型吗**这个问题就等价于：大模型用batch norm和dropout吗？

首先，大模型用的都是[layer norm](https://zhida.zhihu.com/search?content_id=243940940&content_type=Article&match_order=1&q=layer+norm&zhida_source=entity), batch norm显然是不存在的。那么大模型开dropout吗？

先明确一下dropout的位置，大多数[transformer](https://zhida.zhihu.com/search?content_id=243940940&content_type=Article&match_order=1&q=transformer&zhida_source=entity)的实现（尤其是类gpt的decoder模型中）只在[self-attention](https://zhida.zhihu.com/search?content_id=243940940&content_type=Article&match_order=1&q=self-attention&zhida_source=entity)的softmax概率之后使用dropout，相应的失活概率p为超参数attention\_dropout。

而在LLaMA、Mistral这些常见的开源大模型中，attention\_dropout都被设为0，也就是**实际上不使用dropout，model.eval()对这些不开dropout的大模型实际上没有影响。**

![](https://pic4.zhimg.com/v2-22d2d5ad3923803c56dad2db8f32357d_1440w.jpg)
