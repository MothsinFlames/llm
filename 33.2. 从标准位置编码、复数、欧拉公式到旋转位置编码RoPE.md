---
created: 2025-09-17T12:26:28 (UTC +08:00)
tags: [旋转位置编码]
source: https://blog.csdn.net/v_JULY_v/article/details/134085503
author: 成就一亿技术人!
---

# 一文通透位置编码：从标准位置编码、复数、欧拉公式到旋转位置编码RoPE

> ## Excerpt
> 文章浏览阅读6.7w次，点赞516次，收藏950次。本文深入解析位置编码在大模型中的应用，从标准位置编码到RoPE、ALiBi，涵盖LLaMA2 Long的位置编码改进，助您全面理解位置信息如何增强模型性能。

---
### 前言

关于位置编码和RoPE 

1.  应用广泛，是很多大模型使用的一种位置编码方式，包括且不限于LLaMA、baichuan、ChatGLM等等
2.  我之前在本博客中的另外两篇文章中有阐述过(一篇是关于LLaMA解读的，一篇是关于transformer从零实现的)，但自觉写的不是特别透彻好懂
    
    再后来在我参与主讲的类ChatGPT微调实战课中也有讲过，但有些学员依然反馈RoPE不是特别好理解

考虑到只要花足够多的时间 心思 投入，没有写不清楚的，讲课更是如此，故为彻底解决这个位置编码/RoPE的问题，我把另外两篇文章中关于位置编码的内容抽取出来，并不断深入、扩展、深入，比如其中最关键的改进是两轮改进，一个12.16那天，一个12.21那天

1.  12.16那天
    
    小的改进是把“1.1 标准位置编码的起源”中，关于i、2i、2i+1的一系列计算结果用表格规整了下
    
    如此，相比之前把一堆数字一堆，表格更加清晰、一目了然
    
    大的改进是把“3.1.1 第一种形式的推导(通俗易懂版)”的细节重新梳理了以下，以更加一目了然、一看即懂，可能是全网关于RoPE最通俗细致的推导
2.  12.21那天
    
    把RoPE的本质给强调出来

最终成为本文

### 第一部分 transformer原始论文中的标准位置编码

如此篇文章《[Transformer通俗笔记：从Word2Vec、Seq2Seq逐步理解到GPT、BERT](https://blog.csdn.net/v_JULY_v/article/details/127411638 "Transformer通俗笔记：从Word2Vec、Seq2Seq逐步理解到GPT、BERT")》所述，RNN的结构包含了序列的时序信息，而Transformer却完全把时序信息给丢掉了，比如“他欠我100万”，和“我欠他100万”，两者的意思千差万别，故为了解决时序的问题，Transformer的作者用了一个绝妙的办法：位置编码(Positional Encoding)

#### 1.1 标准位置编码的起源

即将每个位置编号，从而每个编号对应一个向量，最终通过结合位置向量和词向量，作为输入embedding，就给每个词都引入了一定的位置信息，这样Attention就可以分辨出不同位置的词了，具体怎么做呢？

1.  如果简单粗暴的话，直接给每个向量分配一个数字，比如1到1000之间
2.  也可以用one-hot编码表示位置
    
    ![|500](https://i-blog.csdnimg.cn/blog_migrate/31f4ddc25d27c73eb7957e5c23627540.png)
    
3.  transformer论文中作者通过sin函数和cos函数交替来创建 positional encoding，其计算positional encoding的公式如下
    
    $PE_{(pos,2i+1)} = cos\left ( \frac{pos}{10000^{\frac{2i}{d_{model}}}} \right )$
    
    ![PE_{(pos,2i)} = sin\left ( \frac{pos}{10000^{\frac{2i}{d_{model}}}} \right )$PE_%7B%28pos%2C2i%29%7D%20%3D%20sin%5Cleft%20%28%20%5Cfrac%7Bpos%7D%7B10000%5E%7B%5Cfrac%7B2i%7D%7Bd_%7Bmodel%7D%7D%7D%7D%20%5Cright%20%29)
    
    其中，pos相当于是每个token在整个序列中的位置，相当于是0, 1, 2, 3...(看序列长度是多大，比如10，比如100)，![d_{model}$d_%7Bmodel%7D)代表位置向量的维度(也是词embedding的维度，transformer论文中设置的512维) 
    
    至于![i$i)是embedding向量的位置下标对2求商并取整(可用双斜杠![//$//)表示整数除法，即求商并取整)，它的取值范围是![[0,...,\frac{d_{model}}{2}]$%5B0%2C...%2C%5Cfrac%7Bd_%7Bmodel%7D%7D%7B2%7D%5D)，比如
    
    |**位置向量的第多少维**
    |---|
    (0 2 4等偶数维用sin函数计算)|
    |0|
    |1|
    |2|
    |3|
    |4|
    |5|
    |6|
    |....|
    |510|
    |511|
    
    相当于
    
    ![2i$2i)是指向量维度中的偶数维，即第0维、第2维、第4维...，第510维，用sin函数计算
    
    ![2i+1$2i&plus;1) 是向量维度中的奇数维，即第1维、第3维、第5维..，第511维，用cos函数计算

不要小看transformer的这个位置编码，不少做NLP多年的人也不一定对其中的细节有多深入，而网上大部分文章谈到这个位置编码时基本都是千篇一律、泛泛而谈，很少有深入，故本文还是细致探讨下

#### 1.2 标准位置编码的示例：多图多举例

考虑到一图胜千言 一例胜万语，举个例子，当我们要编码「我 爱 你」的位置向量，假定每个token都具备512维，如果位置下标从0开始时，则根据位置编码的计算公式可得**『**_且为让每个读者阅读本文时一目了然，我计算了<u>每个单词对应的位置编码示例</u>(__在此之前，这些示例在其他地方基本没有__)_**』**

最终得到的可视化效果如下图所示

![](https://i-blog.csdnimg.cn/blog_migrate/46dd04702db4727492d78979e23722eb.png)

#### 1.3 标准位置编码的coding实现

代码实现如下

```cobol
“”“位置编码的实现，调用父类nn.Module的构造函数”“”class PositionalEncoding(nn.Module):    def __init__(self, d_model, dropout, max_len=5000):super(PositionalEncoding, self).__init__()  self.dropout = nn.Dropout(p=dropout)  # 初始化dropout层        # 计算位置编码并将其存储在pe张量中        pe = torch.zeros(max_len, d_model)                # 创建一个max_len x d_model的全零张量        position = torch.arange(0, max_len).unsqueeze(1)  # 生成0到max_len-1的整数序列，并添加一个维度        # 计算div_term，用于缩放不同位置的正弦和余弦函数        div_term = torch.exp(torch.arange(0, d_model, 2) *                             -(math.log(10000.0) / d_model))        # 使用正弦和余弦函数生成位置编码，对于d_model的偶数索引，使用正弦函数；对于奇数索引，使用余弦函数。        pe[:, 0::2] = torch.sin(position * div_term)        pe[:, 1::2] = torch.cos(position * div_term)        pe = pe.unsqueeze(0)                  # 在第一个维度添加一个维度，以便进行批处理self.register_buffer('pe', pe)        # 将位置编码张量注册为缓冲区，以便在不同设备之间传输模型时保持其状态    # 定义前向传播函数    def forward(self, x):        # 将输入x与对应的位置编码相加        x = x + Variable(self.pe[:, :x.size(1)],                          requires_grad=False)        # 应用dropout层并返回结果return self.dropout(x)
```

本文发布之后，有同学留言问，上面中的第11行、12行代码

```cobol
div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))
```

为什么先转换为了等价的指数+对数运算，而不是直接幂运算？是效率、精度方面有差异吗？

这里使用指数和对数运算的原因是为了确保数值稳定性和计算效率

-   一方面，直接使用幂运算可能会导致数值上溢或下溢。当d\_model较大时，10000.0 \*\* (-i / d\_model)中的幂可能会变得非常小，以至于在数值计算中产生下溢。通过将其转换为指数和对数运算，可以避免这种情况，因为这样可以在计算过程中保持更好的数值范围
-   二方面，在许多计算设备和库中，指数和对数运算的实现通常比幂运算更快。这主要是因为指数和对数运算在底层硬件和软件中有特定的优化实现，而幂运算通常需要计算更多的中间值

所以，使用指数和对数运算可以在保持数值稳定性的同时提高计算效率。

既然提到了这行代码，我们干脆就再讲更细致些，上面那行代码对应的公式为

![](https://i-blog.csdnimg.cn/blog_migrate/75e7ae395d7aaf62581d1f65f08e83bc.png)

其中的中括号对应的是一个从 0 到 ![d_{\text{model}} - 1$d_%7B%5Ctext%7Bmodel%7D%7D%20-%201) 的等差数列(步长为 2)，设为![i$i)

且上述公式与这个公式是等价的

![](https://i-blog.csdnimg.cn/blog_migrate/0e8b55fabfae6a5d9f996cc390626040.png)

为何，原因在于![a^x=e^{(x\cdot ln(a))}$a%5Ex%3De%5E%7B%28x%5Ccdot%20ln%28a%29%29%7D)，从而有![10000^{-\frac{i}{d_{model}}}=e^{(-\frac{i}{d_{model}}\cdot log(10000))}$10000%5E%7B-%5Cfrac%7Bi%7D%7Bd_%7Bmodel%7D%7D%7D%3De%5E%7B%28-%5Cfrac%7Bi%7D%7Bd_%7Bmodel%7D%7D%5Ccdot%20log%2810000%29%29%7D)

 最终，再通过下面这两行代码完美实现位置编码

```cobol
        # 使用正弦和余弦函数生成位置编码，对于d_model的偶数索引，使用正弦函数；对于奇数索引，使用余弦函数。        pe[:, 0::2] = torch.sin(position * div_term)        pe[:, 1::2] = torch.cos(position * div_term)
```

### 第二部分 从复数到欧拉公式

先复习下复数的一些关键概念

1.  我们一般用![a + bi$a%20&plus;%20bi)表示[复数](https://zh.wikipedia.org/zh-hans/%E5%A4%8D%E6%95%B0_%28%E6%95%B0%E5%AD%A6%29 "复数")，实数![a$a) 叫做复数的实部，实数![b$b) 叫做复数的虚部
    
    ![](https://i-blog.csdnimg.cn/blog_migrate/ead674dbb091f36b7016488c67a3f8ba.png)
    
2.  复数的辐角是指复数在复平面上对应的向量和正向实数轴所成的有向角
    
    ![](https://i-blog.csdnimg.cn/blog_migrate/76ffc0d6582ab593689b7acc32fd0274.png)
    
3.  ![z = a + ib$z%20%3D%20a%20&plus;%20ib)的共轭复数定义为：![z^* = a - ib$z%5E*%20%3D%20a%20-%20ib)，也可记作![\bar{z}$%5Cbar%7Bz%7D)，复数与其共轭的乘积等于它的模的平方，即![z \times z^* = a^2 + b^2 = |z|^2$z%20%5Ctimes%20z%5E*%20%3D%20a%5E2%20&plus;%20b%5E2%20%3D%20%7Cz%7C%5E2)，这是一个实数

#### 2.1 如何通俗易懂的理解复数

在我们的日常生活中，经常会遇到各种平移运动，为了描述这些平移运动，数学上定义了加减乘除，然还有一类运动是旋转运动，而加减乘除无法去描述旋转运动，而有了复数之后，便不一样了，此话怎讲？

根据复数的定义：![i=\sqrt{-1}$i%3D%5Csqrt%7B-1%7D)，可以看出来：![i^{2}=1 \times i \times i=-1$i%5E%7B2%7D%3D1%20%5Ctimes%20i%20%5Ctimes%20i%3D-1)，而这个展开过程就揭示了虚数 ![i$i) 背后的本质，因为这个展开过程中的两次乘法可以看成连续的操作

-   即把 1 经过2次完全一样的操作：![\times i$%5Ctimes%20i)，变成了 −1 ，那什么样的操作能得到这个效果呢？
-   你两眼一亮，直呼：旋转啊，先旋转 90度，再旋转 90 度就可以了啊，如下图所示
    
    ![](https://i-blog.csdnimg.cn/blog_migrate/ea037fabe5318351ac822e5da71d0254.png)
    

so，![i$i) 就代表了旋转(_至此，可能你已经隐隐约约意识到，为何我们在解释旋转位置编码时，为何要扯上复数了_)，为形象说明，再举两个例子

#### 2.2 如何快速理解欧拉公式

##### 2.2.1 什么是欧拉公式

当![x$x) 表示任意实数，![e$e) 是自然对数的底数，![i$i) 是复数中的虚数单位，则根据欧拉公式有

![e^{i x}=\cos x+i \sin x$e%5E%7Bi%20x%7D%3D%5Ccos%20x&plus;i%20%5Csin%20x)

表达的含义在于该指数函数可以表示为实部为![cos x$cos%20x)，虚部为![sinx$sinx)的一个复数

> 该欧拉公式相当于建立了指数函数、三角函数和复数之间的桥梁，但怎么推导出来的呢，其实很简单
> 
> 1.  由于有
>     
>     ![e^{x}=1+x+\frac{1}{2 !} x^{2}+\frac{1}{3 !} x^{3}+\cdots$e%5E%7Bx%7D%3D1&plus;x&plus;%5Cfrac%7B1%7D%7B2%20%21%7D%20x%5E%7B2%7D&plus;%5Cfrac%7B1%7D%7B3%20%21%7D%20x%5E%7B3%7D&plus;%5Ccdots)
>     
>     ![\sin (x)=x-\frac{1}{3 !} x^{3}+\frac{1}{5 !} x^{5}+\cdots$%5Csin%20%28x%29%3Dx-%5Cfrac%7B1%7D%7B3%20%21%7D%20x%5E%7B3%7D&plus;%5Cfrac%7B1%7D%7B5%20%21%7D%20x%5E%7B5%7D&plus;%5Ccdots)
>     
>     ![\cos (x)=1-\frac{1}{2 !} x^{2}+\frac{1}{4 !} x^{4}+\cdots$%5Ccos%20%28x%29%3D1-%5Cfrac%7B1%7D%7B2%20%21%7D%20x%5E%7B2%7D&plus;%5Cfrac%7B1%7D%7B4%20%21%7D%20x%5E%7B4%7D&plus;%5Ccdots)
>     
> 2.  所以，如果![x = i\theta$x%20%3D%20i%5Ctheta) ，则有
> 
> ![\begin{aligned} e^{i \theta} & =1+i \theta+\frac{(i \theta)^{2}}{2 !}+\frac{(i \theta)^{3}}{3 !}+\frac{(i \theta)^{4}}{4 !}+\frac{(i \theta)^{5}}{5 !}+\frac{(i \theta)^{6}}{6 !}+\frac{(i \theta)^{7}}{7 !}+\frac{(i \theta)^{8}}{8 !}+\cdots \\ & =1+i \theta-\frac{\theta^{2}}{2 !}-\frac{i \theta^{3}}{3 !}+\frac{\theta^{4}}{4 !}+\frac{i \theta^{5}}{5 !}-\frac{\theta^{6}}{6 !}-\frac{i \theta^{7}}{7 !}+\frac{\theta^{8}}{8 !}+\cdots \\ & =\left(1-\frac{\theta^{2}}{2 !}+\frac{\theta^{4}}{4 !}-\frac{\theta^{6}}{6 !}+\frac{\theta^{8}}{8 !}-\cdots\right)+i\left(\theta-\frac{\theta^{3}}{3 !}+\frac{\theta^{5}}{5 !}-\frac{\theta^{7}}{7 !}+\cdots\right) \\ & =\cos \theta+i \sin \theta \end{aligned}$%5Cbegin%7Baligned%7D%20e%5E%7Bi%20%5Ctheta%7D%20%26%20%3D1&plus;i%20%5Ctheta&plus;%5Cfrac%7B%28i%20%5Ctheta%29%5E%7B2%7D%7D%7B2%20%21%7D&plus;%5Cfrac%7B%28i%20%5Ctheta%29%5E%7B3%7D%7D%7B3%20%21%7D&plus;%5Cfrac%7B%28i%20%5Ctheta%29%5E%7B4%7D%7D%7B4%20%21%7D&plus;%5Cfrac%7B%28i%20%5Ctheta%29%5E%7B5%7D%7D%7B5%20%21%7D&plus;%5Cfrac%7B%28i%20%5Ctheta%29%5E%7B6%7D%7D%7B6%20%21%7D&plus;%5Cfrac%7B%28i%20%5Ctheta%29%5E%7B7%7D%7D%7B7%20%21%7D&plus;%5Cfrac%7B%28i%20%5Ctheta%29%5E%7B8%7D%7D%7B8%20%21%7D&plus;%5Ccdots%20%5C%5C%20%26%20%3D1&plus;i%20%5Ctheta-%5Cfrac%7B%5Ctheta%5E%7B2%7D%7D%7B2%20%21%7D-%5Cfrac%7Bi%20%5Ctheta%5E%7B3%7D%7D%7B3%20%21%7D&plus;%5Cfrac%7B%5Ctheta%5E%7B4%7D%7D%7B4%20%21%7D&plus;%5Cfrac%7Bi%20%5Ctheta%5E%7B5%7D%7D%7B5%20%21%7D-%5Cfrac%7B%5Ctheta%5E%7B6%7D%7D%7B6%20%21%7D-%5Cfrac%7Bi%20%5Ctheta%5E%7B7%7D%7D%7B7%20%21%7D&plus;%5Cfrac%7B%5Ctheta%5E%7B8%7D%7D%7B8%20%21%7D&plus;%5Ccdots%20%5C%5C%20%26%20%3D%5Cleft%281-%5Cfrac%7B%5Ctheta%5E%7B2%7D%7D%7B2%20%21%7D&plus;%5Cfrac%7B%5Ctheta%5E%7B4%7D%7D%7B4%20%21%7D-%5Cfrac%7B%5Ctheta%5E%7B6%7D%7D%7B6%20%21%7D&plus;%5Cfrac%7B%5Ctheta%5E%7B8%7D%7D%7B8%20%21%7D-%5Ccdots%5Cright%29&plus;i%5Cleft%28%5Ctheta-%5Cfrac%7B%5Ctheta%5E%7B3%7D%7D%7B3%20%21%7D&plus;%5Cfrac%7B%5Ctheta%5E%7B5%7D%7D%7B5%20%21%7D-%5Cfrac%7B%5Ctheta%5E%7B7%7D%7D%7B7%20%21%7D&plus;%5Ccdots%5Cright%29%20%5C%5C%20%26%20%3D%5Ccos%20%5Ctheta&plus;i%20%5Csin%20%5Ctheta%20%5Cend%7Baligned%7D)

##### 2.2.2 欧拉公式与三角函数

如何直观的理解这个欧拉公式呢？

其实，可以把![e^{i \theta}$e%5E%7Bi%20%5Ctheta%7D)看作通过单位圆的圆周运动来描述单位圆上的点，![\cos \theta+i \sin \theta$%5Ccos%20%5Ctheta&plus;i%20%5Csin%20%5Ctheta)通过复平面的坐标来描述单位圆上的点，是同一个点不同的描述方式，所以有![e^{i \theta}=\cos \theta+i \sin \theta$e%5E%7Bi%20%5Ctheta%7D%3D%5Ccos%20%5Ctheta&plus;i%20%5Csin%20%5Ctheta)，如下图所示

![](https://i-blog.csdnimg.cn/blog_migrate/b152d35e3de220cb5c978b39b25d47a2.png)

根据欧拉公式![e^{i \theta}=\cos \theta+i \sin \theta$e%5E%7Bi%20%5Ctheta%7D%3D%5Ccos%20%5Ctheta&plus;i%20%5Csin%20%5Ctheta)，可以轻易推出：

![\sin \theta=\frac{e^{i \theta}-e^{-i \theta}}{2 i}$%5Csin%20%5Ctheta%3D%5Cfrac%7Be%5E%7Bi%20%5Ctheta%7D-e%5E%7B-i%20%5Ctheta%7D%7D%7B2%20i%7D)

![\cos \theta=\frac{e^{i \theta}+e^{-i \theta}}{2}$%5Ccos%20%5Ctheta%3D%5Cfrac%7Be%5E%7Bi%20%5Ctheta%7D&plus;e%5E%7B-i%20%5Ctheta%7D%7D%7B2%7D)

我们把复数当作向量来看待，复数的实部是![x$x)方向，虚部是![y$y)方向，很容易观察出其几何意义，如下图所示

![](https://i-blog.csdnimg.cn/blog_migrate/a8c9e58dab75ef83b5f773133de3a8d1.png)![](https://i-blog.csdnimg.cn/blog_migrate/df35e904650257e336e3cd6776a467b9.png)

> 还在思考怎么得来的？很简单哦，还记得向量的加减法么？
> 
> ![](https://i-blog.csdnimg.cn/blog_migrate/cc2a6df4325282ea83b9c2b45dffd873.png)![](https://i-blog.csdnimg.cn/blog_migrate/2f671eb8b4dc7517fde16ffa18cdcbd4.png)

### 第三部分 旋转位置编码(RoPE)的推导与实现

#### 3.1 旋转位置编码的原理与推导

所谓旋转位置编码，其在位置编码上删除了绝对位置嵌入，而在网络的每一层增加了苏剑林等人(2021)提出的[旋转位置嵌入(RoPE)](https://kexue.fm/archives/8265 "旋转位置嵌入(RoPE)")，其思想是采用绝对位置编码的形式 实现相对位置编码，且RoPE主要借助了复数的思想

具体来说，当咱们给self-attention中的![q,k,v$q%2Ck%2Cv)向量都加入了位置信息后，便可以表示为

![\begin{aligned} \boldsymbol{q}_{m} & =f_{q}\left(\boldsymbol{x}_{m}, m\right) \\ \boldsymbol{k}_{n} & =f_{k}\left(\boldsymbol{x}_{n}, n\right) \\ \boldsymbol{v}_{n} & =f_{v}\left(\boldsymbol{x}_{n}, n\right) \end{aligned}$%5Cbegin%7Baligned%7D%20%5Cboldsymbol%7Bq%7D_%7Bm%7D%20%26%20%3Df_%7Bq%7D%5Cleft%28%5Cboldsymbol%7Bx%7D_%7Bm%7D%2C%20m%5Cright%29%20%5C%5C%20%5Cboldsymbol%7Bk%7D_%7Bn%7D%20%26%20%3Df_%7Bk%7D%5Cleft%28%5Cboldsymbol%7Bx%7D_%7Bn%7D%2C%20n%5Cright%29%20%5C%5C%20%5Cboldsymbol%7Bv%7D_%7Bn%7D%20%26%20%3Df_%7Bv%7D%5Cleft%28%5Cboldsymbol%7Bx%7D_%7Bn%7D%2C%20n%5Cright%29%20%5Cend%7Baligned%7D)

其中

##### 3.1.1 第一种形式的推导(可能是全网最通俗易懂版)

接着论文中提出为了能利用上 token 之间的相对位置信息，假定 query 向量 ![q_m$q_m) 和 key 向量 ![k_n$k_n) 之间的内积操作可以被一个函数 ![g$g) 表示，该函数 ![g$g) 的输入是词嵌入向量 ![x_m$x_m)、![x_n$x_n) ，和它们之间的相对位置 ![m - n$m%20-%20n)：

![<f_{q}\left(x_{m}, m\right), f_{k}\left(x_{n}, n\right)>=g\left(x_{m}, x_{n}, m-n\right)$%3Cf_%7Bq%7D%5Cleft%28x_%7Bm%7D%2C%20m%5Cright%29%2C%20f_%7Bk%7D%5Cleft%28x_%7Bn%7D%2C%20n%5Cright%29%3E%3Dg%5Cleft%28x_%7Bm%7D%2C%20x_%7Bn%7D%2C%20m-n%5Cright%29)

> 这里面其实有很大的一个关键，但大部分资料甚至RoPE原始论文都不会给你特别强调出来，即为何要构造这么一个等式呢？
> 
> -   原因在于左边算是q和k向量的内积，而这恰好是transformer计算自注意力机制的核心一步，右边等式则意味着m与n的相对位置
>     
>     如此一来，该等式便把“q和k的内积”与“它们的相对位置”给串起来了
> -   也如阿荀所说，左边是含有各自绝对位置信息的q向量和k向量，而这个等式就是RoPE追求的目标，物理含义就是**通过显式传入绝对位置信息实现与传入相对位置信息对等**的情况

假定现在词嵌入向量的维度是两维 ![d = 2$d%20%3D%202) ，然后RoPE利用2维度平面上的向量的几何性质，再结合复数的性质，神奇般的找到了满足上述等式的 ![f$f) 和 ![g$g) ，其形式如下：

![\begin{array}{l} f_{q}\left(\boldsymbol{x}_{m}, m\right)=\left(\boldsymbol{W}_{q} \boldsymbol{x}_{m}\right) e^{i m \theta} \\ f_{k}\left(\boldsymbol{x}_{n}, n\right)=\left(\boldsymbol{W}_{k} \boldsymbol{x}_{n}\right) e^{i n \theta} \\ g\left(\boldsymbol{x}_{m}, \boldsymbol{x}_{n}, m-n\right)=\operatorname{Re}\left[\left(\boldsymbol{W}_{q} \boldsymbol{x}_{m}\right)\left(\boldsymbol{W}_{k} \boldsymbol{x}_{n}\right)^{*} e^{i(m-n) \theta}\right] \end{array}$%5Cbegin%7Barray%7D%7Bl%7D%20f_%7Bq%7D%5Cleft%28%5Cboldsymbol%7Bx%7D_%7Bm%7D%2C%20m%5Cright%29%3D%5Cleft%28%5Cboldsymbol%7BW%7D_%7Bq%7D%20%5Cboldsymbol%7Bx%7D_%7Bm%7D%5Cright%29%20e%5E%7Bi%20m%20%5Ctheta%7D%20%5C%5C%20f_%7Bk%7D%5Cleft%28%5Cboldsymbol%7Bx%7D_%7Bn%7D%2C%20n%5Cright%29%3D%5Cleft%28%5Cboldsymbol%7BW%7D_%7Bk%7D%20%5Cboldsymbol%7Bx%7D_%7Bn%7D%5Cright%29%20e%5E%7Bi%20n%20%5Ctheta%7D%20%5C%5C%20g%5Cleft%28%5Cboldsymbol%7Bx%7D_%7Bm%7D%2C%20%5Cboldsymbol%7Bx%7D_%7Bn%7D%2C%20m-n%5Cright%29%3D%5Coperatorname%7BRe%7D%5Cleft%5B%5Cleft%28%5Cboldsymbol%7BW%7D_%7Bq%7D%20%5Cboldsymbol%7Bx%7D_%7Bm%7D%5Cright%29%5Cleft%28%5Cboldsymbol%7BW%7D_%7Bk%7D%20%5Cboldsymbol%7Bx%7D_%7Bn%7D%5Cright%29%5E%7B*%7D%20e%5E%7Bi%28m-n%29%20%5Ctheta%7D%5Cright%5D%20%5Cend%7Barray%7D)

这里面的 Re 表示复数的实部

![\begin{aligned} f_{k}\left(\boldsymbol{x}_{m}, m\right) & =\left(\begin{array}{cc} \cos m \theta & -\sin m \theta) \\ \sin m \theta & \cos m \theta \end{array}\right)\left(\begin{array}{ll} W_{k}^{(1,1)} & W_{k}^{(1,2)} \\ W_{k}^{(2,1)} & W_{k}^{(2,2)} \end{array}\right)\left(\begin{array}{c} x_{m}^{(1)} \\ x_{m}^{(2)} \end{array}\right) \\ & =\left(\begin{array}{cc} \cos m \theta & -\sin m \theta) \\ \sin m \theta & \cos m \theta \end{array}\right)\left(\begin{array}{l} k_{m}^{(1)} \\ k_{m}^{(2)} \end{array}\right) \end{aligned}$%5Cbegin%7Baligned%7D%20f_%7Bk%7D%5Cleft%28%5Cboldsymbol%7Bx%7D_%7Bm%7D%2C%20m%5Cright%29%20%26%20%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%20%5Ccos%20m%20%5Ctheta%20%26%20-%5Csin%20m%20%5Ctheta%29%20%5C%5C%20%5Csin%20m%20%5Ctheta%20%26%20%5Ccos%20m%20%5Ctheta%20%5Cend%7Barray%7D%5Cright%29%5Cleft%28%5Cbegin%7Barray%7D%7Bll%7D%20W_%7Bk%7D%5E%7B%281%2C1%29%7D%20%26%20W_%7Bk%7D%5E%7B%281%2C2%29%7D%20%5C%5C%20W_%7Bk%7D%5E%7B%282%2C1%29%7D%20%26%20W_%7Bk%7D%5E%7B%282%2C2%29%7D%20%5Cend%7Barray%7D%5Cright%29%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D%20x_%7Bm%7D%5E%7B%281%29%7D%20%5C%5C%20x_%7Bm%7D%5E%7B%282%29%7D%20%5Cend%7Barray%7D%5Cright%29%20%5C%5C%20%26%20%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%20%5Ccos%20m%20%5Ctheta%20%26%20-%5Csin%20m%20%5Ctheta%29%20%5C%5C%20%5Csin%20m%20%5Ctheta%20%26%20%5Ccos%20m%20%5Ctheta%20%5Cend%7Barray%7D%5Cright%29%5Cleft%28%5Cbegin%7Barray%7D%7Bl%7D%20k_%7Bm%7D%5E%7B%281%29%7D%20%5C%5C%20k_%7Bm%7D%5E%7B%282%29%7D%20%5Cend%7Barray%7D%5Cright%29%20%5Cend%7Baligned%7D)

-   最终![g\left(\boldsymbol{x}_{m}, \boldsymbol{x}_{n}, m-n\right)$g%5Cleft%28%5Cboldsymbol%7Bx%7D_%7Bm%7D%2C%20%5Cboldsymbol%7Bx%7D_%7Bn%7D%2C%20m-n%5Cright%29)可以表示如下：

![g\left(\boldsymbol{x}_{m}, \boldsymbol{x}_{n}, m-n\right)=\left(\begin{array}{ll} \boldsymbol{q}_{m}^{(1)} & \boldsymbol{q}_{m}^{(2)} \end{array}\right)\left(\begin{array}{cc} \cos ((m-n) \theta) & -\sin ((m-n) \theta) \\ \sin ((m-n) \theta) & \cos ((m-n) \theta) \end{array}\right)\left(\begin{array}{c} k_{n}^{(1)} \\ k_{n}^{(2)} \end{array}\right)$g%5Cleft%28%5Cboldsymbol%7Bx%7D_%7Bm%7D%2C%20%5Cboldsymbol%7Bx%7D_%7Bn%7D%2C%20m-n%5Cright%29%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bll%7D%20%5Cboldsymbol%7Bq%7D_%7Bm%7D%5E%7B%281%29%7D%20%26%20%5Cboldsymbol%7Bq%7D_%7Bm%7D%5E%7B%282%29%7D%20%5Cend%7Barray%7D%5Cright%29%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%20%5Ccos%20%28%28m-n%29%20%5Ctheta%29%20%26%20-%5Csin%20%28%28m-n%29%20%5Ctheta%29%20%5C%5C%20%5Csin%20%28%28m-n%29%20%5Ctheta%29%20%26%20%5Ccos%20%28%28m-n%29%20%5Ctheta%29%20%5Cend%7Barray%7D%5Cright%29%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D%20k_%7Bn%7D%5E%7B%281%29%7D%20%5C%5C%20k_%7Bn%7D%5E%7B%282%29%7D%20%5Cend%7Barray%7D%5Cright%29)

> 然上述分别关于![f_q$f_q)、![f_k$f_k)、![g\left(\boldsymbol{x}_{m}, \boldsymbol{x}_{n}, m-n\right)$g%5Cleft%28%5Cboldsymbol%7Bx%7D_%7Bm%7D%2C%20%5Cboldsymbol%7Bx%7D_%7Bn%7D%2C%20m-n%5Cright%29)的三个式子，咋一步一步推导来的？为做细致说明，特参考[此文](https://zhuanlan.zhihu.com/p/642884818 "此文")一步一步解释下
> 
> ___
> 
> 首先看第一个式子，对于![f_{q}\left(x_{m}, m\right)=\left(W_{q} x_{m}\right) e^{i m \theta}$f_%7Bq%7D%5Cleft%28x_%7Bm%7D%2C%20m%5Cright%29%3D%5Cleft%28W_%7Bq%7D%20x_%7Bm%7D%5Cright%29%20e%5E%7Bi%20m%20%5Ctheta%7D)，这个式子的右边项有两部分，一部分是![W_{q} x_{m}$W_%7Bq%7D%20x_%7Bm%7D)、一部分是![e^{i m \theta}$e%5E%7Bi%20m%20%5Ctheta%7D)
> 
> 1.  对于前者![W_{q} x_{m}$W_%7Bq%7D%20x_%7Bm%7D)，可知其中的![W_q$W_q)是个二维矩阵，![x_m$x_m)是个二维向量，自然相乘的结果也必然是一个二维向量，用![q_m$q_m)表示
>     
>     ![q_{m}=\left(\begin{array}{c} q_{m}^{(1)} \\ q_{m}^{(2)} \end{array}\right)=W_{q} x_{m}=\left(\begin{array}{ll} W_{q}^{(11)} & W_{q}^{(12)} \\ W_{q}^{(21)} & W_{q}^{(22)} \end{array}\right)\left(\begin{array}{c} x_{m}^{(1)} \\ x_{m}^{(2)} \end{array}\right)$q_%7Bm%7D%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D%20q_%7Bm%7D%5E%7B%281%29%7D%20%5C%5C%20q_%7Bm%7D%5E%7B%282%29%7D%20%5Cend%7Barray%7D%5Cright%29%3DW_%7Bq%7D%20x_%7Bm%7D%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bll%7D%20W_%7Bq%7D%5E%7B%2811%29%7D%20%26%20W_%7Bq%7D%5E%7B%2812%29%7D%20%5C%5C%20W_%7Bq%7D%5E%7B%2821%29%7D%20%26%20W_%7Bq%7D%5E%7B%2822%29%7D%20%5Cend%7Barray%7D%5Cright%29%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D%20x_%7Bm%7D%5E%7B%281%29%7D%20%5C%5C%20x_%7Bm%7D%5E%7B%282%29%7D%20%5Cend%7Barray%7D%5Cright%29)
> 2.  对于后者![e^{i m \theta}$e%5E%7Bi%20m%20%5Ctheta%7D)，根据欧拉公式![e^{i x}=\cos x+i \sin x$e%5E%7Bi%20x%7D%3D%5Ccos%20x&plus;i%20%5Csin%20x)，可得
>     
>     ![\begin{array}{c} e^{i m \theta}=\cos (m \theta)+i \sin (m \theta) \\ e^{i n \theta}=\cos (n \theta)+i \sin (n \theta) \\ e^{i(m-n) \theta}=\cos ((m-n) \theta)+i \sin ((m-n) \theta) \end{array}$%5Cbegin%7Barray%7D%7Bc%7D%20e%5E%7Bi%20m%20%5Ctheta%7D%3D%5Ccos%20%28m%20%5Ctheta%29&plus;i%20%5Csin%20%28m%20%5Ctheta%29%20%5C%5C%20e%5E%7Bi%20n%20%5Ctheta%7D%3D%5Ccos%20%28n%20%5Ctheta%29&plus;i%20%5Csin%20%28n%20%5Ctheta%29%20%5C%5C%20e%5E%7Bi%28m-n%29%20%5Ctheta%7D%3D%5Ccos%20%28%28m-n%29%20%5Ctheta%29&plus;i%20%5Csin%20%28%28m-n%29%20%5Ctheta%29%20%5Cend%7Barray%7D)
>     
>      
> 3.  基于上面第1点结论，可知
>     
>     ![f_{q}\left(x_{m}, m\right)=\left(W_{q} x_{m}\right) e^{i m \theta}=q_{m} e^{i m \theta}$f_%7Bq%7D%5Cleft%28x_%7Bm%7D%2C%20m%5Cright%29%3D%5Cleft%28W_%7Bq%7D%20x_%7Bm%7D%5Cright%29%20e%5E%7Bi%20m%20%5Ctheta%7D%3Dq_%7Bm%7D%20e%5E%7Bi%20m%20%5Ctheta%7D)
>     
>     然后将![q_m$q_m)表示成复数形式，可得
>     
>     ![q_{m}=\left[q_{m}^{(1)}, q_{m}^{(2)}\right]=\left[q_{m}^{(1)}+i q_{m}^{(2)}\right]$q_%7Bm%7D%3D%5Cleft%5Bq_%7Bm%7D%5E%7B%281%29%7D%2C%20q_%7Bm%7D%5E%7B%282%29%7D%5Cright%5D%3D%5Cleft%5Bq_%7Bm%7D%5E%7B%281%29%7D&plus;i%20q_%7Bm%7D%5E%7B%282%29%7D%5Cright%5D)
>     
>     从而有
>     
>     ![f_{q}\left(x_{m}, m\right)= q_{m} e^{i m \theta} = \left[q_{m}^{(1)}+i q_{m}^{(2)}\right] e^{i m \theta}$f_%7Bq%7D%5Cleft%28x_%7Bm%7D%2C%20m%5Cright%29%3D%20q_%7Bm%7D%20e%5E%7Bi%20m%20%5Ctheta%7D%20%3D%20%5Cleft%5Bq_%7Bm%7D%5E%7B%281%29%7D&plus;i%20q_%7Bm%7D%5E%7B%282%29%7D%5Cright%5D%20e%5E%7Bi%20m%20%5Ctheta%7D)
>     
>     基于上面第2点结论，可知![f_{q}\left(x_{m}, m\right)$f_%7Bq%7D%5Cleft%28x_%7Bm%7D%2C%20m%5Cright%29)即是两个复数相乘
>     
>     ![f_{q}\left(x_{m}, m\right) = q_{m} e^{i m \theta}=\left(q_{m}^{(1)}+i q_{m}^{(2)}\right) *(\cos (m \theta)+i \sin (m \theta))$f_%7Bq%7D%5Cleft%28x_%7Bm%7D%2C%20m%5Cright%29%20%3D%20q_%7Bm%7D%20e%5E%7Bi%20m%20%5Ctheta%7D%3D%5Cleft%28q_%7Bm%7D%5E%7B%281%29%7D&plus;i%20q_%7Bm%7D%5E%7B%282%29%7D%5Cright%29%20*%28%5Ccos%20%28m%20%5Ctheta%29&plus;i%20%5Csin%20%28m%20%5Ctheta%29%29)
>     
> 4.  考虑到以下两个关于复数的背景知识
>     
>     ![(a+i b) \cdot(c+i d)=a c+i b c+i a d+i^{2} b d=(a c-b d)+i(b c+a d)$%28a&plus;i%20b%29%20%5Ccdot%28c&plus;i%20d%29%3Da%20c&plus;i%20b%20c&plus;i%20a%20d&plus;i%5E%7B2%7D%20b%20d%3D%28a%20c-b%20d%29&plus;i%28b%20c&plus;a%20d%29)
>     
>     ![i^{2}=-1$i%5E%7B2%7D%3D-1)
>     
>     可得
>     
>     ![\begin{aligned} q_{m} e^{i m \theta} & =\left(q_{m}^{(1)}+i q_{m}^{(2)}\right) *(\cos (m \theta)+i \sin (m \theta)) \\ =\left(q_{m}^{(1)} \cos (m \theta)\right. & \left.-q_{m}^{(2)} \sin (m \theta)\right)+i\left(q_{m}^{(2)} \cos (m \theta)+q_{m}^{(1)} \sin (m \theta)\right) \end{aligned}$%5Cbegin%7Baligned%7D%20q_%7Bm%7D%20e%5E%7Bi%20m%20%5Ctheta%7D%20%26%20%3D%5Cleft%28q_%7Bm%7D%5E%7B%281%29%7D&plus;i%20q_%7Bm%7D%5E%7B%282%29%7D%5Cright%29%20*%28%5Ccos%20%28m%20%5Ctheta%29&plus;i%20%5Csin%20%28m%20%5Ctheta%29%29%20%5C%5C%20%3D%5Cleft%28q_%7Bm%7D%5E%7B%281%29%7D%20%5Ccos%20%28m%20%5Ctheta%29%5Cright.%20%26%20%5Cleft.-q_%7Bm%7D%5E%7B%282%29%7D%20%5Csin%20%28m%20%5Ctheta%29%5Cright%29&plus;i%5Cleft%28q_%7Bm%7D%5E%7B%282%29%7D%20%5Ccos%20%28m%20%5Ctheta%29&plus;q_%7Bm%7D%5E%7B%281%29%7D%20%5Csin%20%28m%20%5Ctheta%29%5Cright%29%20%5Cend%7Baligned%7D)
>     
>     将这个结果表达成实数向量形式，即是
>     
>     ![q_{m} e^{i m \theta}=\left[q_{m}^{(1)} \cos (m \theta)-q_{m}^{(2)} \sin (m \theta), q_{m}^{(2)} \cos (m \theta)+q_{m}^{(1)} \sin (m \theta)\right]$q_%7Bm%7D%20e%5E%7Bi%20m%20%5Ctheta%7D%3D%5Cleft%5Bq_%7Bm%7D%5E%7B%281%29%7D%20%5Ccos%20%28m%20%5Ctheta%29-q_%7Bm%7D%5E%7B%282%29%7D%20%5Csin%20%28m%20%5Ctheta%29%2C%20q_%7Bm%7D%5E%7B%282%29%7D%20%5Ccos%20%28m%20%5Ctheta%29&plus;q_%7Bm%7D%5E%7B%281%29%7D%20%5Csin%20%28m%20%5Ctheta%29%5Cright%5D)
>     
>     至此，你也就不难发现，这不就是**query向量乘以了一个旋转矩阵**么
>     
>     ![\begin{array}{c} f_{q}\left(x_{m}, m\right)=\left(W_{q} x_{m}\right) e^{i m \theta}=q_{m} e^{i m \theta} \\ =\left[q_{m}^{(1)} \cos (m \theta)-q_{m}^{(2)} \sin (m \theta), q_{m}^{(2)} \cos (m \theta)+q_{m}^{(1)} \sin (m \theta)\right] \\ =\left(\begin{array}{cc} \cos (m \theta) & -\sin (m \theta) \\ \sin (m \theta) & \cos (m \theta) \end{array}\right)\left(\begin{array}{c} q_{m}^{(1)} \\ q_{m}^{(2)} \end{array}\right) \end{array}$%5Cbegin%7Barray%7D%7Bc%7D%20f_%7Bq%7D%5Cleft%28x_%7Bm%7D%2C%20m%5Cright%29%3D%5Cleft%28W_%7Bq%7D%20x_%7Bm%7D%5Cright%29%20e%5E%7Bi%20m%20%5Ctheta%7D%3Dq_%7Bm%7D%20e%5E%7Bi%20m%20%5Ctheta%7D%20%5C%5C%20%3D%5Cleft%5Bq_%7Bm%7D%5E%7B%281%29%7D%20%5Ccos%20%28m%20%5Ctheta%29-q_%7Bm%7D%5E%7B%282%29%7D%20%5Csin%20%28m%20%5Ctheta%29%2C%20q_%7Bm%7D%5E%7B%282%29%7D%20%5Ccos%20%28m%20%5Ctheta%29&plus;q_%7Bm%7D%5E%7B%281%29%7D%20%5Csin%20%28m%20%5Ctheta%29%5Cright%5D%20%5C%5C%20%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%20%5Ccos%20%28m%20%5Ctheta%29%20%26%20-%5Csin%20%28m%20%5Ctheta%29%20%5C%5C%20%5Csin%20%28m%20%5Ctheta%29%20%26%20%5Ccos%20%28m%20%5Ctheta%29%20%5Cend%7Barray%7D%5Cright%29%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D%20q_%7Bm%7D%5E%7B%281%29%7D%20%5C%5C%20q_%7Bm%7D%5E%7B%282%29%7D%20%5Cend%7Barray%7D%5Cright%29%20%5Cend%7Barray%7D)
>     
> 
> 至于第二个式子，根据上述过程同理，可得key向量![k_n$k_n)
> 
> -   ![\begin{array}{c} f_{k}\left(x_{n}, n\right)=\left(W_{k} x_{n}\right) e^{i n \theta}=k_{n} e^{i n \theta} \\ =\left[k_{n}^{(1)} \cos (n \theta)-k_{n}^{(2)} \sin (n \theta), k_{n}^{(2)} \cos (n \theta)+k_{n}^{(1)} \sin (n \theta)\right] \\ =\left(\begin{array}{cc} \cos (n \theta) & -\sin (n \theta) \\ \sin (n \theta) & \cos (n \theta) \end{array}\right)\left(\begin{array}{c} k_{n}^{(1)} \\ k_{n}^{(2)} \end{array}\right) \end{array}$%5Cbegin%7Barray%7D%7Bc%7D%20f_%7Bk%7D%5Cleft%28x_%7Bn%7D%2C%20n%5Cright%29%3D%5Cleft%28W_%7Bk%7D%20x_%7Bn%7D%5Cright%29%20e%5E%7Bi%20n%20%5Ctheta%7D%3Dk_%7Bn%7D%20e%5E%7Bi%20n%20%5Ctheta%7D%20%5C%5C%20%3D%5Cleft%5Bk_%7Bn%7D%5E%7B%281%29%7D%20%5Ccos%20%28n%20%5Ctheta%29-k_%7Bn%7D%5E%7B%282%29%7D%20%5Csin%20%28n%20%5Ctheta%29%2C%20k_%7Bn%7D%5E%7B%282%29%7D%20%5Ccos%20%28n%20%5Ctheta%29&plus;k_%7Bn%7D%5E%7B%281%29%7D%20%5Csin%20%28n%20%5Ctheta%29%5Cright%5D%20%5C%5C%20%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%20%5Ccos%20%28n%20%5Ctheta%29%20%26%20-%5Csin%20%28n%20%5Ctheta%29%20%5C%5C%20%5Csin%20%28n%20%5Ctheta%29%20%26%20%5Ccos%20%28n%20%5Ctheta%29%20%5Cend%7Barray%7D%5Cright%29%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D%20k_%7Bn%7D%5E%7B%281%29%7D%20%5C%5C%20k_%7Bn%7D%5E%7B%282%29%7D%20%5Cend%7Barray%7D%5Cright%29%20%5Cend%7Barray%7D)
> 
> 最后第三个式子，函数g，则可得
> 
> -   ![g\left(x_{m}, x_{n}, m-n\right)=\operatorname{Re}\left[\left(W_{q} x_{m}\right)\left(W_{k} x_{n}\right)^{*} e^{i(m-n) \theta}\right]$g%5Cleft%28x_%7Bm%7D%2C%20x_%7Bn%7D%2C%20m-n%5Cright%29%3D%5Coperatorname%7BRe%7D%5Cleft%5B%5Cleft%28W_%7Bq%7D%20x_%7Bm%7D%5Cright%29%5Cleft%28W_%7Bk%7D%20x_%7Bn%7D%5Cright%29%5E%7B*%7D%20e%5E%7Bi%28m-n%29%20%5Ctheta%7D%5Cright%5D)
> 
> 其中，![Re[x]$Re%5Bx%5D)表示一个复数![x$x)的实数部分，而![\left(W_{k} x_{n}\right)^{*}$%5Cleft%28W_%7Bk%7D%20x_%7Bn%7D%5Cright%29%5E%7B*%7D)则表示复数![W_{k} x_{n}$W_%7Bk%7D%20x_%7Bn%7D)的共轭
> 
> 1.  考虑到
>     
>     ![\begin{array}{c} z=a+i b \\ z^{*}=a-i b \end{array}$%5Cbegin%7Barray%7D%7Bc%7D%20z%3Da&plus;i%20b%20%5C%5C%20z%5E%7B*%7D%3Da-i%20b%20%5Cend%7Barray%7D)
>     
>     再结合上面第一个式子中的推导，可得
>     
>     ![\begin{array}{c} W_{q} x_{m}=q_{m}=q_{m}^{(1)}+i q_{m}^{(2)} \\ W_{k} x_{n}=k_{n}=k_{n}^{(1)}+i k_{n}^{(2)} \\ \left(W_{k} x_{n}\right)^{*}=k_{n}^{*}=k_{n}^{(1)}-i k_{n}^{(2)} \\ e^{i(m-n) \theta}=\cos ((m-n) \theta)+i \sin ((m-n) \theta) \end{array}$%5Cbegin%7Barray%7D%7Bc%7D%20W_%7Bq%7D%20x_%7Bm%7D%3Dq_%7Bm%7D%3Dq_%7Bm%7D%5E%7B%281%29%7D&plus;i%20q_%7Bm%7D%5E%7B%282%29%7D%20%5C%5C%20W_%7Bk%7D%20x_%7Bn%7D%3Dk_%7Bn%7D%3Dk_%7Bn%7D%5E%7B%281%29%7D&plus;i%20k_%7Bn%7D%5E%7B%282%29%7D%20%5C%5C%20%5Cleft%28W_%7Bk%7D%20x_%7Bn%7D%5Cright%29%5E%7B*%7D%3Dk_%7Bn%7D%5E%7B*%7D%3Dk_%7Bn%7D%5E%7B%281%29%7D-i%20k_%7Bn%7D%5E%7B%282%29%7D%20%5C%5C%20e%5E%7Bi%28m-n%29%20%5Ctheta%7D%3D%5Ccos%20%28%28m-n%29%20%5Ctheta%29&plus;i%20%5Csin%20%28%28m-n%29%20%5Ctheta%29%20%5Cend%7Barray%7D)
>     
>     继续结合上面第一个式子中的推导(_比如![(a+i b) \cdot(c+i d)=a c+i b c+i a d+i^{2} b d=(a c-b d)+i(b c+a d)$%28a&plus;i%20b%29%20%5Ccdot%28c&plus;i%20d%29%3Da%20c&plus;i%20b%20c&plus;i%20a%20d&plus;i%5E%7B2%7D%20b%20d%3D%28a%20c-b%20d%29&plus;i%28b%20c&plus;a%20d%29)，及_![i^{2}=-1$i%5E%7B2%7D%3D-1))，继续可知，我们现在要证明的是存在
>     
>     ![\begin{array}{c} g\left(x_{m}, x_{n}, m-n\right)=\operatorname{Re}\left[\left(W_{q} x_{m}\right)\left(W_{k} x_{n}\right)^{*} e^{i(m-n) \theta}\right] \\ =\operatorname{Re}\left[\left(q_{m}^{(1)}+i q_{m}^{(2)}\right)\left(k_{n}^{(1)}-i k_{n}^{(2)}\right)(\cos ((m-n) \theta)+i \sin ((m-n) \theta))\right] \\ =\operatorname{Re}\left[\left(\left(q_{m}^{(1)} k_{n}^{(1)}+q_{m}^{(2)} k_{n}^{(2)}\right)+i\left(q_{m}^{(2)} k_{n}^{(1)}-q_{m}^{(1)} k_{n}^{(2)}\right)\right)(\cos ((m-n) \theta)+i \sin ((m-n) \theta))\right] \\ =\left(q_{m}^{(1)} k_{n}^{(1)}+q_{m}^{(2)} k_{n}^{(2)}\right) \cos ((m-n) \theta)-\left(q_{m}^{(2)} k_{n}^{(1)}-q_{m}^{(1)} k_{n}^{(2)}\right) \sin ((m-n) \theta) \end{array}$%5Cbegin%7Barray%7D%7Bc%7D%20g%5Cleft%28x_%7Bm%7D%2C%20x_%7Bn%7D%2C%20m-n%5Cright%29%3D%5Coperatorname%7BRe%7D%5Cleft%5B%5Cleft%28W_%7Bq%7D%20x_%7Bm%7D%5Cright%29%5Cleft%28W_%7Bk%7D%20x_%7Bn%7D%5Cright%29%5E%7B*%7D%20e%5E%7Bi%28m-n%29%20%5Ctheta%7D%5Cright%5D%20%5C%5C%20%3D%5Coperatorname%7BRe%7D%5Cleft%5B%5Cleft%28q_%7Bm%7D%5E%7B%281%29%7D&plus;i%20q_%7Bm%7D%5E%7B%282%29%7D%5Cright%29%5Cleft%28k_%7Bn%7D%5E%7B%281%29%7D-i%20k_%7Bn%7D%5E%7B%282%29%7D%5Cright%29%28%5Ccos%20%28%28m-n%29%20%5Ctheta%29&plus;i%20%5Csin%20%28%28m-n%29%20%5Ctheta%29%29%5Cright%5D%20%5C%5C%20%3D%5Coperatorname%7BRe%7D%5Cleft%5B%5Cleft%28%5Cleft%28q_%7Bm%7D%5E%7B%281%29%7D%20k_%7Bn%7D%5E%7B%281%29%7D&plus;q_%7Bm%7D%5E%7B%282%29%7D%20k_%7Bn%7D%5E%7B%282%29%7D%5Cright%29&plus;i%5Cleft%28q_%7Bm%7D%5E%7B%282%29%7D%20k_%7Bn%7D%5E%7B%281%29%7D-q_%7Bm%7D%5E%7B%281%29%7D%20k_%7Bn%7D%5E%7B%282%29%7D%5Cright%29%5Cright%29%28%5Ccos%20%28%28m-n%29%20%5Ctheta%29&plus;i%20%5Csin%20%28%28m-n%29%20%5Ctheta%29%29%5Cright%5D%20%5C%5C%20%3D%5Cleft%28q_%7Bm%7D%5E%7B%281%29%7D%20k_%7Bn%7D%5E%7B%281%29%7D&plus;q_%7Bm%7D%5E%7B%282%29%7D%20k_%7Bn%7D%5E%7B%282%29%7D%5Cright%29%20%5Ccos%20%28%28m-n%29%20%5Ctheta%29-%5Cleft%28q_%7Bm%7D%5E%7B%282%29%7D%20k_%7Bn%7D%5E%7B%281%29%7D-q_%7Bm%7D%5E%7B%281%29%7D%20k_%7Bn%7D%5E%7B%282%29%7D%5Cright%29%20%5Csin%20%28%28m-n%29%20%5Ctheta%29%20%5Cend%7Barray%7D)
> 2.  总之，接下来我们就要证明上述函数 g 的计算公式是成立的
>     
>     首先，回顾一下attention操作，位置m的query和位置n的key会做一个内积操作
>     
>     即由
>     
>     ![\begin{array}{c} f_{q}\left(x_{m}, m\right)=\left[q_{m}^{(1)} \cos (m \theta)-q_{m}^{(2)} \sin (m \theta), q_{m}^{(2)} \cos (m \theta)+q_{m}^{(1)} \sin (m \theta)\right] \\ f_{k}\left(x_{n}, n\right)=\left[k_{n}^{(1)} \cos (n \theta)-k_{n}^{(2)} \sin (n \theta), k_{n}^{(2)} \cos (n \theta)+k_{n}^{(1)} \sin (n \theta)\right] \end{array}$%5Cbegin%7Barray%7D%7Bc%7D%20f_%7Bq%7D%5Cleft%28x_%7Bm%7D%2C%20m%5Cright%29%3D%5Cleft%5Bq_%7Bm%7D%5E%7B%281%29%7D%20%5Ccos%20%28m%20%5Ctheta%29-q_%7Bm%7D%5E%7B%282%29%7D%20%5Csin%20%28m%20%5Ctheta%29%2C%20q_%7Bm%7D%5E%7B%282%29%7D%20%5Ccos%20%28m%20%5Ctheta%29&plus;q_%7Bm%7D%5E%7B%281%29%7D%20%5Csin%20%28m%20%5Ctheta%29%5Cright%5D%20%5C%5C%20f_%7Bk%7D%5Cleft%28x_%7Bn%7D%2C%20n%5Cright%29%3D%5Cleft%5Bk_%7Bn%7D%5E%7B%281%29%7D%20%5Ccos%20%28n%20%5Ctheta%29-k_%7Bn%7D%5E%7B%282%29%7D%20%5Csin%20%28n%20%5Ctheta%29%2C%20k_%7Bn%7D%5E%7B%282%29%7D%20%5Ccos%20%28n%20%5Ctheta%29&plus;k_%7Bn%7D%5E%7B%281%29%7D%20%5Csin%20%28n%20%5Ctheta%29%5Cright%5D%20%5Cend%7Barray%7D)
>     
>     可得
>     
>     ![\begin{array}{c} <f_{q}\left(x_{m}, m\right), f_{k}\left(x_{n}, n\right)> \\ = \left(q_{m}^{(1)} \cos (m \theta)-q_{m}^{(2)} \sin (m \theta)\right)\left(k_{n}^{(1)} \cos (n \theta)-k_{n}^{(2)} \sin (n \theta)\right) \\ +\left(q_{m}^{(2)} \cos (m \theta)+q_{m}^{(1)} \sin (m \theta)\right)\left(k_{n}^{(2)} \cos (n \theta)+k_{n}^{(1)} \sin (n \theta)\right) \\ =q_{m}^{(1)} \cos (m \theta) k_{n}^{(1)} \cos (n \theta)-q_{m}^{(1)} \cos (m \theta) k_{n}^{(2)} \sin (n \theta) \\ -q_{m}^{(2)} \sin (m \theta) k_{n}^{(1)} \cos (n \theta)+q_{m}^{(2)} \sin (m \theta) k_{n}^{(2)} \sin (n \theta) \\ +q_{m}^{(2)} \cos (m \theta) k_{n}^{(2)} \cos (n \theta)+q_{m}^{(2)} \cos (m \theta) k_{n}^{(1)} \sin (n \theta) \\ +q_{m}^{(1)} \sin (m \theta) k_{n}^{(2)} \cos (n \theta)+q_{m}^{(1)} \sin (m \theta) k_{n}^{(1)} \sin (n \theta) \end{array}$%5Cbegin%7Barray%7D%7Bc%7D%20%3Cf_%7Bq%7D%5Cleft%28x_%7Bm%7D%2C%20m%5Cright%29%2C%20f_%7Bk%7D%5Cleft%28x_%7Bn%7D%2C%20n%5Cright%29%3E%20%5C%5C%20%3D%20%5Cleft%28q_%7Bm%7D%5E%7B%281%29%7D%20%5Ccos%20%28m%20%5Ctheta%29-q_%7Bm%7D%5E%7B%282%29%7D%20%5Csin%20%28m%20%5Ctheta%29%5Cright%29%5Cleft%28k_%7Bn%7D%5E%7B%281%29%7D%20%5Ccos%20%28n%20%5Ctheta%29-k_%7Bn%7D%5E%7B%282%29%7D%20%5Csin%20%28n%20%5Ctheta%29%5Cright%29%20%5C%5C%20&plus;%5Cleft%28q_%7Bm%7D%5E%7B%282%29%7D%20%5Ccos%20%28m%20%5Ctheta%29&plus;q_%7Bm%7D%5E%7B%281%29%7D%20%5Csin%20%28m%20%5Ctheta%29%5Cright%29%5Cleft%28k_%7Bn%7D%5E%7B%282%29%7D%20%5Ccos%20%28n%20%5Ctheta%29&plus;k_%7Bn%7D%5E%7B%281%29%7D%20%5Csin%20%28n%20%5Ctheta%29%5Cright%29%20%5C%5C%20%3Dq_%7Bm%7D%5E%7B%281%29%7D%20%5Ccos%20%28m%20%5Ctheta%29%20k_%7Bn%7D%5E%7B%281%29%7D%20%5Ccos%20%28n%20%5Ctheta%29-q_%7Bm%7D%5E%7B%281%29%7D%20%5Ccos%20%28m%20%5Ctheta%29%20k_%7Bn%7D%5E%7B%282%29%7D%20%5Csin%20%28n%20%5Ctheta%29%20%5C%5C%20-q_%7Bm%7D%5E%7B%282%29%7D%20%5Csin%20%28m%20%5Ctheta%29%20k_%7Bn%7D%5E%7B%281%29%7D%20%5Ccos%20%28n%20%5Ctheta%29&plus;q_%7Bm%7D%5E%7B%282%29%7D%20%5Csin%20%28m%20%5Ctheta%29%20k_%7Bn%7D%5E%7B%282%29%7D%20%5Csin%20%28n%20%5Ctheta%29%20%5C%5C%20&plus;q_%7Bm%7D%5E%7B%282%29%7D%20%5Ccos%20%28m%20%5Ctheta%29%20k_%7Bn%7D%5E%7B%282%29%7D%20%5Ccos%20%28n%20%5Ctheta%29&plus;q_%7Bm%7D%5E%7B%282%29%7D%20%5Ccos%20%28m%20%5Ctheta%29%20k_%7Bn%7D%5E%7B%281%29%7D%20%5Csin%20%28n%20%5Ctheta%29%20%5C%5C%20&plus;q_%7Bm%7D%5E%7B%281%29%7D%20%5Csin%20%28m%20%5Ctheta%29%20k_%7Bn%7D%5E%7B%282%29%7D%20%5Ccos%20%28n%20%5Ctheta%29&plus;q_%7Bm%7D%5E%7B%281%29%7D%20%5Csin%20%28m%20%5Ctheta%29%20k_%7Bn%7D%5E%7B%281%29%7D%20%5Csin%20%28n%20%5Ctheta%29%20%5Cend%7Barray%7D)
>     
>     「_相当于\[A,B\]与\[C,D\]做内积，则相当于A B横着，C D竖着，最终结果为AC BD，最后再把括号里的项全部对应相乘、展开_」
> 3.  首先，把上面第二点的式子整理一下，总计8项，为了把![qk$qk)相关的项提取出来，第1项 8项合并处理、第2项 7项合并处理、第3项 6项合并处理、第4项 5项合并处理
>     
>     其次，考虑到
>     
>     ![\begin{array}{l} \sin (a+b)=\sin a \cos b+\cos a \sin b \\ \sin (a-b)=\sin a \cos b-\cos a \sin b \\ \cos (a+b)=\cos a \cos b-\sin a \sin b \\ \cos (a-b)=\cos a \cos b+\sin a \sin b \end{array}$%5Cbegin%7Barray%7D%7Bl%7D%20%5Csin%20%28a&plus;b%29%3D%5Csin%20a%20%5Ccos%20b&plus;%5Ccos%20a%20%5Csin%20b%20%5C%5C%20%5Csin%20%28a-b%29%3D%5Csin%20a%20%5Ccos%20b-%5Ccos%20a%20%5Csin%20b%20%5C%5C%20%5Ccos%20%28a&plus;b%29%3D%5Ccos%20a%20%5Ccos%20b-%5Csin%20a%20%5Csin%20b%20%5C%5C%20%5Ccos%20%28a-b%29%3D%5Ccos%20a%20%5Ccos%20b&plus;%5Csin%20a%20%5Csin%20b%20%5Cend%7Barray%7D)
>     
>     最后，再把相关项的特点，两次调整下顺序即可
>     
>     依据以上三点，从而有
>     
>     ![\begin{array}{c} <f_{q}\left(x_{m}, m\right), f_{k}\left(x_{n}, n\right)>\\ = q_{m}^{(1)} k_{n}^{(1)}(\cos (m \theta) \cos (n \theta)+\sin (m \theta) \sin (n \theta)) \\ +q_{m}^{(1)} k_{n}^{(2)}(-\cos (m \theta) \sin (n \theta)+\sin (m \theta) \cos (n \theta)) \\ +q_{m}^{(2)} k_{n}^{(1)}(-\sin (m \theta) \cos (n \theta)+\cos (m \theta) \sin (n \theta)) \\ +q_{m}^{(2)} k_{n}^{(2)}(\sin (m \theta) \sin (n \theta)+\cos (m \theta) \cos (n \theta)) \\ =q_{m}^{(1)} k_{n}^{(1)} \cos ((m-n) \theta) \\ +q_{m}^{(1)} k_{n}^{(2)} \sin ((m-n) \theta) \\ -q_{m}^{(2)} k_{n}^{(1)} \sin ((m-n) \theta) \\ +q_{m}^{(2)} k_{n}^{(2)} \cos ((m-n) \theta) \\ =\left(q_{m}^{(1)} k_{n}^{(1)}+q_{m}^{(2)} k_{n}^{(2)}\right) \cos ((m-n) \theta)+\left(q_{m}^{(1)} k_{n}^{(2)}-q_{m}^{(2)} k_{n}^{(1)}\right) \sin ((m-n) \theta) \\ =\left(q_{m}^{(1)} k_{n}^{(1)}+q_{m}^{(2)} k_{n}^{(2)}\right) \cos ((m-n) \theta)-\left(q_{m}^{(2)} k_{n}^{(1)}-q_{m}^{(1)} k_{n}^{(2)}\right) \sin ((m-n) \theta) \\ =g\left(x_{m}, x_{n}, m-n\right) \end{array}$%5Cbegin%7Barray%7D%7Bc%7D%20%3Cf_%7Bq%7D%5Cleft%28x_%7Bm%7D%2C%20m%5Cright%29%2C%20f_%7Bk%7D%5Cleft%28x_%7Bn%7D%2C%20n%5Cright%29%3E%5C%5C%20%3D%20q_%7Bm%7D%5E%7B%281%29%7D%20k_%7Bn%7D%5E%7B%281%29%7D%28%5Ccos%20%28m%20%5Ctheta%29%20%5Ccos%20%28n%20%5Ctheta%29&plus;%5Csin%20%28m%20%5Ctheta%29%20%5Csin%20%28n%20%5Ctheta%29%29%20%5C%5C%20&plus;q_%7Bm%7D%5E%7B%281%29%7D%20k_%7Bn%7D%5E%7B%282%29%7D%28-%5Ccos%20%28m%20%5Ctheta%29%20%5Csin%20%28n%20%5Ctheta%29&plus;%5Csin%20%28m%20%5Ctheta%29%20%5Ccos%20%28n%20%5Ctheta%29%29%20%5C%5C%20&plus;q_%7Bm%7D%5E%7B%282%29%7D%20k_%7Bn%7D%5E%7B%281%29%7D%28-%5Csin%20%28m%20%5Ctheta%29%20%5Ccos%20%28n%20%5Ctheta%29&plus;%5Ccos%20%28m%20%5Ctheta%29%20%5Csin%20%28n%20%5Ctheta%29%29%20%5C%5C%20&plus;q_%7Bm%7D%5E%7B%282%29%7D%20k_%7Bn%7D%5E%7B%282%29%7D%28%5Csin%20%28m%20%5Ctheta%29%20%5Csin%20%28n%20%5Ctheta%29&plus;%5Ccos%20%28m%20%5Ctheta%29%20%5Ccos%20%28n%20%5Ctheta%29%29%20%5C%5C%20%3Dq_%7Bm%7D%5E%7B%281%29%7D%20k_%7Bn%7D%5E%7B%281%29%7D%20%5Ccos%20%28%28m-n%29%20%5Ctheta%29%20%5C%5C%20&plus;q_%7Bm%7D%5E%7B%281%29%7D%20k_%7Bn%7D%5E%7B%282%29%7D%20%5Csin%20%28%28m-n%29%20%5Ctheta%29%20%5C%5C%20-q_%7Bm%7D%5E%7B%282%29%7D%20k_%7Bn%7D%5E%7B%281%29%7D%20%5Csin%20%28%28m-n%29%20%5Ctheta%29%20%5C%5C%20&plus;q_%7Bm%7D%5E%7B%282%29%7D%20k_%7Bn%7D%5E%7B%282%29%7D%20%5Ccos%20%28%28m-n%29%20%5Ctheta%29%20%5C%5C%20%3D%5Cleft%28q_%7Bm%7D%5E%7B%281%29%7D%20k_%7Bn%7D%5E%7B%281%29%7D&plus;q_%7Bm%7D%5E%7B%282%29%7D%20k_%7Bn%7D%5E%7B%282%29%7D%5Cright%29%20%5Ccos%20%28%28m-n%29%20%5Ctheta%29&plus;%5Cleft%28q_%7Bm%7D%5E%7B%281%29%7D%20k_%7Bn%7D%5E%7B%282%29%7D-q_%7Bm%7D%5E%7B%282%29%7D%20k_%7Bn%7D%5E%7B%281%29%7D%5Cright%29%20%5Csin%20%28%28m-n%29%20%5Ctheta%29%20%5C%5C%20%3D%5Cleft%28q_%7Bm%7D%5E%7B%281%29%7D%20k_%7Bn%7D%5E%7B%281%29%7D&plus;q_%7Bm%7D%5E%7B%282%29%7D%20k_%7Bn%7D%5E%7B%282%29%7D%5Cright%29%20%5Ccos%20%28%28m-n%29%20%5Ctheta%29-%5Cleft%28q_%7Bm%7D%5E%7B%282%29%7D%20k_%7Bn%7D%5E%7B%281%29%7D-q_%7Bm%7D%5E%7B%281%29%7D%20k_%7Bn%7D%5E%7B%282%29%7D%5Cright%29%20%5Csin%20%28%28m-n%29%20%5Ctheta%29%20%5C%5C%20%3Dg%5Cleft%28x_%7Bm%7D%2C%20x_%7Bn%7D%2C%20m-n%5Cright%29%20%5Cend%7Barray%7D)
>     
>     完美! 如此，也就证明了，位置 m 的 query 和位置 n 的 key 的内积就是函数 g
>     
>     ___
>     
>     最后，把上面的式子一、式子二的最终结果都分别用矩阵向量乘的形式来表达就是：
>     
>     ![\begin{array}{c} <f_{q}\left(x_{m}, m\right), f_{k}\left(x_{n}, n\right)> \\ =\left(\left(\begin{array}{cc} \cos (m \theta) & -\sin (m \theta) \\ \sin (m \theta) & \cos (m \theta) \end{array}\right)\left(\begin{array}{c} q_{m}^{(1)} \\ q_{m}^{(2)} \end{array}\right)\right)^{T}\left(\left(\begin{array}{cc} \cos (n \theta) & -\sin (n \theta) \\ \sin (n \theta) & \cos (n \theta) \end{array}\right)\left(\begin{array}{c} k_{n}^{(1)} \\ k_{n}^{(2)} \end{array}\right)\right) \\ =\left(\begin{array}{ll} q_{m}^{(1)} & q_{m}^{(2)} \end{array}\right)\left(\begin{array}{cc} \cos (m \theta) & \sin (m \theta) \\ -\sin (m \theta) & \cos (m \theta) \end{array}\right)\left(\begin{array}{cc} \cos (n \theta) & -\sin (n \theta) \\ \sin (n \theta) & \cos (n \theta) \end{array}\right)\left(\begin{array}{l} k_{n}^{(1)} \\ k_{n}^{(2)} \end{array}\right) \end{array}$%5Cbegin%7Barray%7D%7Bc%7D%20%3Cf_%7Bq%7D%5Cleft%28x_%7Bm%7D%2C%20m%5Cright%29%2C%20f_%7Bk%7D%5Cleft%28x_%7Bn%7D%2C%20n%5Cright%29%3E%20%5C%5C%20%3D%5Cleft%28%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%20%5Ccos%20%28m%20%5Ctheta%29%20%26%20-%5Csin%20%28m%20%5Ctheta%29%20%5C%5C%20%5Csin%20%28m%20%5Ctheta%29%20%26%20%5Ccos%20%28m%20%5Ctheta%29%20%5Cend%7Barray%7D%5Cright%29%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D%20q_%7Bm%7D%5E%7B%281%29%7D%20%5C%5C%20q_%7Bm%7D%5E%7B%282%29%7D%20%5Cend%7Barray%7D%5Cright%29%5Cright%29%5E%7BT%7D%5Cleft%28%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%20%5Ccos%20%28n%20%5Ctheta%29%20%26%20-%5Csin%20%28n%20%5Ctheta%29%20%5C%5C%20%5Csin%20%28n%20%5Ctheta%29%20%26%20%5Ccos%20%28n%20%5Ctheta%29%20%5Cend%7Barray%7D%5Cright%29%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D%20k_%7Bn%7D%5E%7B%281%29%7D%20%5C%5C%20k_%7Bn%7D%5E%7B%282%29%7D%20%5Cend%7Barray%7D%5Cright%29%5Cright%29%20%5C%5C%20%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bll%7D%20q_%7Bm%7D%5E%7B%281%29%7D%20%26%20q_%7Bm%7D%5E%7B%282%29%7D%20%5Cend%7Barray%7D%5Cright%29%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%20%5Ccos%20%28m%20%5Ctheta%29%20%26%20%5Csin%20%28m%20%5Ctheta%29%20%5C%5C%20-%5Csin%20%28m%20%5Ctheta%29%20%26%20%5Ccos%20%28m%20%5Ctheta%29%20%5Cend%7Barray%7D%5Cright%29%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%20%5Ccos%20%28n%20%5Ctheta%29%20%26%20-%5Csin%20%28n%20%5Ctheta%29%20%5C%5C%20%5Csin%20%28n%20%5Ctheta%29%20%26%20%5Ccos%20%28n%20%5Ctheta%29%20%5Cend%7Barray%7D%5Cright%29%5Cleft%28%5Cbegin%7Barray%7D%7Bl%7D%20k_%7Bn%7D%5E%7B%281%29%7D%20%5C%5C%20k_%7Bn%7D%5E%7B%282%29%7D%20%5Cend%7Barray%7D%5Cright%29%20%5Cend%7Barray%7D)
>     
>     接下来，我们要计算两个旋转矩阵的乘积，即中间部分的这个式子
>     
>     ![\left(\begin{array}{cc} \cos (m \theta) & \sin (m \theta) \\ -\sin (m \theta) & \cos (m \theta) \end{array}\right)\left(\begin{array}{cc} \cos (n \theta) & -\sin (n \theta) \\ \sin (n \theta) & \cos (n \theta) \end{array}\right)$%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%20%5Ccos%20%28m%20%5Ctheta%29%20%26%20%5Csin%20%28m%20%5Ctheta%29%20%5C%5C%20-%5Csin%20%28m%20%5Ctheta%29%20%26%20%5Ccos%20%28m%20%5Ctheta%29%20%5Cend%7Barray%7D%5Cright%29%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%20%5Ccos%20%28n%20%5Ctheta%29%20%26%20-%5Csin%20%28n%20%5Ctheta%29%20%5C%5C%20%5Csin%20%28n%20%5Ctheta%29%20%26%20%5Ccos%20%28n%20%5Ctheta%29%20%5Cend%7Barray%7D%5Cright%29)
>     
>     展开之后，可得
>     
>     ![\left(\begin{array}{cc} \cos (m \theta) \cos (n \theta)+\sin (m \theta) \sin (n \theta) & -\cos (m \theta) \sin (n \theta)+\sin (m \theta) \cos (n \theta) \\ -\sin (m \theta) \cos (n \theta)+\cos (m \theta) \sin (n \theta) & \sin (m \theta) \sin (n \theta)+\cos (m \theta) \cos (n \theta) \end{array}\right)$%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%20%5Ccos%20%28m%20%5Ctheta%29%20%5Ccos%20%28n%20%5Ctheta%29&plus;%5Csin%20%28m%20%5Ctheta%29%20%5Csin%20%28n%20%5Ctheta%29%20%26%20-%5Ccos%20%28m%20%5Ctheta%29%20%5Csin%20%28n%20%5Ctheta%29&plus;%5Csin%20%28m%20%5Ctheta%29%20%5Ccos%20%28n%20%5Ctheta%29%20%5C%5C%20-%5Csin%20%28m%20%5Ctheta%29%20%5Ccos%20%28n%20%5Ctheta%29&plus;%5Ccos%20%28m%20%5Ctheta%29%20%5Csin%20%28n%20%5Ctheta%29%20%26%20%5Csin%20%28m%20%5Ctheta%29%20%5Csin%20%28n%20%5Ctheta%29&plus;%5Ccos%20%28m%20%5Ctheta%29%20%5Ccos%20%28n%20%5Ctheta%29%20%5Cend%7Barray%7D%5Cright%29)
>     
>     从而有
>     
>     ![\begin{array}{l} <f_{q}\left(x_{m}, m\right), {f_{k}\left(x_{n}, n\right)>} \\ =\left(\begin{array}{ll} q_{m}^{(1)} & q_{m}^{(2)} \end{array}\right)\left(\begin{array}{cc} \cos ((m-n) \theta) & -\sin ((n-m) \theta) \\ \sin ((n-m) \theta) & \cos ((m-n) \theta) \end{array}\right)\left(\begin{array}{c} k_{n}^{(1)} \\ k_{n}^{(2)} \end{array}\right) \end{array}$%5Cbegin%7Barray%7D%7Bl%7D%20%3Cf_%7Bq%7D%5Cleft%28x_%7Bm%7D%2C%20m%5Cright%29%2C%20%7Bf_%7Bk%7D%5Cleft%28x_%7Bn%7D%2C%20n%5Cright%29%3E%7D%20%5C%5C%20%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bll%7D%20q_%7Bm%7D%5E%7B%281%29%7D%20%26%20q_%7Bm%7D%5E%7B%282%29%7D%20%5Cend%7Barray%7D%5Cright%29%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%20%5Ccos%20%28%28m-n%29%20%5Ctheta%29%20%26%20-%5Csin%20%28%28n-m%29%20%5Ctheta%29%20%5C%5C%20%5Csin%20%28%28n-m%29%20%5Ctheta%29%20%26%20%5Ccos%20%28%28m-n%29%20%5Ctheta%29%20%5Cend%7Barray%7D%5Cright%29%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D%20k_%7Bn%7D%5E%7B%281%29%7D%20%5C%5C%20k_%7Bn%7D%5E%7B%282%29%7D%20%5Cend%7Barray%7D%5Cright%29%20%5Cend%7Barray%7D)
>     
> 
> ___
> 
> 之前上图中第三个大括号里的「两个旋转矩阵相乘」是下面这么写的，~**是不对的**~
> 
> ![\begin{array}{l} <f_{q}\left(x_{m}, m\right), {f_{k}\left(x_{n}, n\right)>} \\ =\left(\begin{array}{ll} q_{m}^{(1)} & q_{m}^{(2)} \end{array}\right)\left(\begin{array}{cc} \cos ((m-n) \theta) & -\sin ((m-n) \theta) \\ \sin ((m-n) \theta) & \cos ((m-n) \theta) \end{array}\right)\left(\begin{array}{c} k_{n}^{(1)} \\ k_{n}^{(2)} \end{array}\right) \end{array}$%5Cbegin%7Barray%7D%7Bl%7D%20%3Cf_%7Bq%7D%5Cleft%28x_%7Bm%7D%2C%20m%5Cright%29%2C%20%7Bf_%7Bk%7D%5Cleft%28x_%7Bn%7D%2C%20n%5Cright%29%3E%7D%20%5C%5C%20%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bll%7D%20q_%7Bm%7D%5E%7B%281%29%7D%20%26%20q_%7Bm%7D%5E%7B%282%29%7D%20%5Cend%7Barray%7D%5Cright%29%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%20%5Ccos%20%28%28m-n%29%20%5Ctheta%29%20%26%20-%5Csin%20%28%28m-n%29%20%5Ctheta%29%20%5C%5C%20%5Csin%20%28%28m-n%29%20%5Ctheta%29%20%26%20%5Ccos%20%28%28m-n%29%20%5Ctheta%29%20%5Cend%7Barray%7D%5Cright%29%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D%20k_%7Bn%7D%5E%7B%281%29%7D%20%5C%5C%20k_%7Bn%7D%5E%7B%282%29%7D%20%5Cend%7Barray%7D%5Cright%29%20%5Cend%7Barray%7D)
> 
> -   后经本文的两位读者指出，实际应该是
>     
>     \[cos(m - n)θ，\-sin(n-m)θ\]
>     
>     \[sin(n-m)θ, cos(m - n)θ\]
>     
>     也就是原论文里的R(n - m)，**即下面才是对的**
> 
> ![\begin{array}{l} <f_{q}\left(x_{m}, m\right), {f_{k}\left(x_{n}, n\right)>} \\ =\left(\begin{array}{ll} q_{m}^{(1)} & q_{m}^{(2)} \end{array}\right)\left(\begin{array}{cc} \cos ((m-n) \theta) & -\sin ((n-m) \theta) \\ \sin ((n-m) \theta) & \cos ((m-n) \theta) \end{array}\right)\left(\begin{array}{c} k_{n}^{(1)} \\ k_{n}^{(2)} \end{array}\right) \end{array}$%5Cbegin%7Barray%7D%7Bl%7D%20%3Cf_%7Bq%7D%5Cleft%28x_%7Bm%7D%2C%20m%5Cright%29%2C%20%7Bf_%7Bk%7D%5Cleft%28x_%7Bn%7D%2C%20n%5Cright%29%3E%7D%20%5C%5C%20%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bll%7D%20q_%7Bm%7D%5E%7B%281%29%7D%20%26%20q_%7Bm%7D%5E%7B%282%29%7D%20%5Cend%7Barray%7D%5Cright%29%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%20%5Ccos%20%28%28m-n%29%20%5Ctheta%29%20%26%20-%5Csin%20%28%28n-m%29%20%5Ctheta%29%20%5C%5C%20%5Csin%20%28%28n-m%29%20%5Ctheta%29%20%26%20%5Ccos%20%28%28m-n%29%20%5Ctheta%29%20%5Cend%7Barray%7D%5Cright%29%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D%20k_%7Bn%7D%5E%7B%281%29%7D%20%5C%5C%20k_%7Bn%7D%5E%7B%282%29%7D%20%5Cend%7Barray%7D%5Cright%29%20%5Cend%7Barray%7D)
> 
> -   原因在于
>     
>     右上：-cos(mθ)sin(nθ) + sin(mθ)cos(nθ) = sin((m-n)θ) = \-sin(n-m)θ_——正弦差公式_
>     
>     左下：-sin(mθ)cos(nθ) + cos(mθ)sin(nθ) = \-sin((m-n)θ) = sin(n-m)θ_——正弦差公式的负值_

上面都还只是针对词嵌入维度为2的情况，那对于![d>=2$d%3E%3D2)的通用情况呢，将2维推广到任意维度，可以表示如下：

![f_{\{q, k\}}\left(\boldsymbol{x}_{m}, m\right)=\boldsymbol{R}_{\Theta, m}^{d} \boldsymbol{W}_{\{q, k\}} \boldsymbol{x}_{m}$f_%7B%5C%7Bq%2C%20k%5C%7D%7D%5Cleft%28%5Cboldsymbol%7Bx%7D_%7Bm%7D%2C%20m%5Cright%29%3D%5Cboldsymbol%7BR%7D_%7B%5CTheta%2C%20m%7D%5E%7Bd%7D%20%5Cboldsymbol%7BW%7D_%7B%5C%7Bq%2C%20k%5C%7D%7D%20%5Cboldsymbol%7Bx%7D_%7Bm%7D)

内积满足线性叠加性，因此任意偶数维的RoPE，我们都可以表示为二维情形的拼接，即将词嵌入向量元素按照两两一组分组

![\boldsymbol{R}_{\Theta, m}^{d}=\underbrace{\left(\begin{array}{ccccccc} \cos m \theta_{0} & -\sin m \theta_{0} & 0 & 0 & \cdots & 0 & 0 \\ \sin m \theta_{0} & \cos m \theta_{0} & 0 & 0 & \cdots & 0 & 0 \\ 0 & 0 & \cos m \theta_{1} & -\sin m \theta_{1} & \cdots & 0 & 0 \\ 0 & 0 & \sin m \theta_{1} & \cos m \theta_{1} & \cdots & 0 & 0 \\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & 0 & 0 & \cdots & \cos m \theta_{d / 2-1} & -\sin m \theta_{d / 2-1} \\ 0 & 0 & 0 & 0 & \cdots & \sin m \theta_{d / 2-1} & \cos m \theta_{d / 2-1} \end{array}\right)}_{\boldsymbol{W}_{m}}$%5Cboldsymbol%7BR%7D_%7B%5CTheta%2C%20m%7D%5E%7Bd%7D%3D%5Cunderbrace%7B%5Cleft%28%5Cbegin%7Barray%7D%7Bccccccc%7D%20%5Ccos%20m%20%5Ctheta_%7B0%7D%20%26%20-%5Csin%20m%20%5Ctheta_%7B0%7D%20%26%200%20%26%200%20%26%20%5Ccdots%20%26%200%20%26%200%20%5C%5C%20%5Csin%20m%20%5Ctheta_%7B0%7D%20%26%20%5Ccos%20m%20%5Ctheta_%7B0%7D%20%26%200%20%26%200%20%26%20%5Ccdots%20%26%200%20%26%200%20%5C%5C%200%20%26%200%20%26%20%5Ccos%20m%20%5Ctheta_%7B1%7D%20%26%20-%5Csin%20m%20%5Ctheta_%7B1%7D%20%26%20%5Ccdots%20%26%200%20%26%200%20%5C%5C%200%20%26%200%20%26%20%5Csin%20m%20%5Ctheta_%7B1%7D%20%26%20%5Ccos%20m%20%5Ctheta_%7B1%7D%20%26%20%5Ccdots%20%26%200%20%26%200%20%5C%5C%20%5Cvdots%20%26%20%5Cvdots%20%26%20%5Cvdots%20%26%20%5Cvdots%20%26%20%5Cddots%20%26%20%5Cvdots%20%26%20%5Cvdots%20%5C%5C%200%20%26%200%20%26%200%20%26%200%20%26%20%5Ccdots%20%26%20%5Ccos%20m%20%5Ctheta_%7Bd%20/%202-1%7D%20%26%20-%5Csin%20m%20%5Ctheta_%7Bd%20/%202-1%7D%20%5C%5C%200%20%26%200%20%26%200%20%26%200%20%26%20%5Ccdots%20%26%20%5Csin%20m%20%5Ctheta_%7Bd%20/%202-1%7D%20%26%20%5Ccos%20m%20%5Ctheta_%7Bd%20/%202-1%7D%20%5Cend%7Barray%7D%5Cright%29%7D_%7B%5Cboldsymbol%7BW%7D_%7Bm%7D%7D)

每组应用同样的旋转操作且每组的旋转角度计算方式如下：

![\Theta=\left\{\theta_{i}=10000^{-2(i-1) / d}, i \in[1,2, \ldots, d / 2]\right\}$%5CTheta%3D%5Cleft%5C%7B%5Ctheta_%7Bi%7D%3D10000%5E%7B-2%28i-1%29%20/%20d%7D%2C%20i%20%5Cin%5B1%2C2%2C%20%5Cldots%2C%20d%20/%202%5D%5Cright%5C%7D)

所以简单来说 RoPE 的 self-attention 操作的流程是

![](https://i-blog.csdnimg.cn/blog_migrate/1edcbd5076bb815851068f56d3a42b36.png)

1.  对于 token 序列中的每个词嵌入向量，首先计算其对应的 query 和 key 向量
2.  然后对每个 token 位置都计算对应的旋转位置编码
3.  接着对每个 token 位置的 query 和 key 向量的元素按照 两两一组 应用旋转变换
4.  最后再计算 query 和 key 之间的内积得到 self-attention 的计算结果

##### 3.1.2 第二种形式的推导(苏剑林版)

与上面第一种形式的推导类似，为了引入复数，首先假设了在加入位置信息之前，原有的编码向量是二维行向量![q_m$q_m)和![k_n$k_n)，其中![m$m)和![n$n)是绝对位置，现在需要构造一个变换，将![m$m)和![n$n)引入到![q_m$q_m)和![k_n$k_n)中，即寻找变换： 

![\tilde {q_m} = f(q, m), \tilde{k_n} = f(k, n)$%5Ctilde%20%7Bq_m%7D%20%3D%20f%28q%2C%20m%29%2C%20%5Ctilde%7Bk_n%7D%20%3D%20f%28k%2C%20n%29)

也就是说，我们分别为![q$q)、![k$k)设计操作![f(\cdot ,m)$f%28%5Ccdot%20%2Cm%29)、![f(\cdot ,n)$f%28%5Ccdot%20%2Cn%29)，使得经过该操作后，![\tilde {q_m}$%5Ctilde%20%7Bq_m%7D)、![\tilde{k_n}$%5Ctilde%7Bk_n%7D)就带有了位置![m$m)、![n$n)的绝对位置信息

考虑到Attention的核心计算是内积：![Re[]$Re%5B%5D)

![Attention(Q, K,V) = softmax(\frac {QK^T} {\sqrt{d_k}})V$Attention%28Q%2C%20K%2CV%29%20%3D%20softmax%28%5Cfrac%20%7BQK%5ET%7D%20%7B%5Csqrt%7Bd_k%7D%7D%29V)

故我们希望的内积的结果带有相对位置信息，即寻求的这个![f(*)$f%28*%29)变换，应该具有特性：

![\langle f(q, m), f(k, n) \rangle = g(q, k, m-n)$%5Clangle%20f%28q%2C%20m%29%2C%20f%28k%2C%20n%29%20%5Crangle%20%3D%20g%28q%2C%20k%2C%20m-n%29)

「**怎么理解？很简单，当m和n表示了绝对位置之后，m与n在句子中的距离即位置差m-n，就可以表示为相对位置了，且对于复数，内积通常定义为一个复数与另一个复数的共轭的乘积」**

1.  为合理的求出该恒等式的一个尽可能简单的解，可以设定一些初始条件，比如![f(q,0)=q$f%28q%2C0%29%3Dq)、![f(k,0)=k$f%28k%2C0%29%3Dk)，然后可以先考虑二维情形，然后借助复数来求解
    
    在复数中有![\langle\boldsymbol{q}, \boldsymbol{k}\rangle=\operatorname{Re}\left[\boldsymbol{q} \boldsymbol{k}^{*}\right]$%5Clangle%5Cboldsymbol%7Bq%7D%2C%20%5Cboldsymbol%7Bk%7D%5Crangle%3D%5Coperatorname%7BRe%7D%5Cleft%5B%5Cboldsymbol%7Bq%7D%20%5Cboldsymbol%7Bk%7D%5E%7B*%7D%5Cright%5D)，![Re[]$Re%5B%5D)表示取实部的操作(复数 ![q$q) 和“ 复数 ![k$k) 的共轭即![k^*$k%5E*) ”之积仍是一个复数)
    
    _因论文100课的群里有学员对该点存在疑问，故借用七月黄老师的回复补充下：这个等式和复数乘法和向量乘积的联系有关
    
    考虑两个复数
    
    ![q = a + bi$q%20%3D%20a%20&plus;%20bi)
    
    ![k = c + di$k%20%3D%20c%20&plus;%20di)，![k$k)的共轭是![k^*= c - di$k%5E*%3D%20c%20-%20di)
    
    **一方面，对于等式的右边项而言**
    
    q和k\*的乘积是 ![q k^* = (a + bi)(c - di) = ac - adi + cbi + bd = (ac + bd) + (cb - ad)i$q%20k%5E*%20%3D%20%28a%20&plus;%20bi%29%28c%20-%20di%29%20%3D%20ac%20-%20adi%20&plus;%20cbi%20&plus;%20bd%20%3D%20%28ac%20&plus;%20bd%29%20&plus;%20%28cb%20-%20ad%29i)
    
    这个结果的实部是 ![ac + bd$ac%20&plus;%20bd)
    
    **二方面，对于等式的左边项而言**
    
    其对应于![q$q)对应的实数向量![[a, b]$%5Ba%2C%20b%5D)和![k$k)对应的实数向量![[c, d]$%5Bc%2C%20d%5D)的乘积
    
    ![[a, b] \cdot [c, d] = ac + bd$%5Ba%2C%20b%5D%20%5Ccdot%20%5Bc%2C%20d%5D%20%3D%20ac%20&plus;%20bd)
    
    综合以上两点，可知右边项所表示的“复数q和复数k的共轭k\*的乘积”，和左边项做表示的“q、k所对应向量的乘积”是一样的_
    
    总之，我们需要寻找一种![f(*)$f%28*%29)变换，使得
    
    ![Re[f(q,m)f^{*}(k,n)] = g(q,k,m-n)$Re%5Bf%28q%2Cm%29f%5E%7B*%7D%28k%2Cn%29%5D%20%3D%20g%28q%2Ck%2Cm-n%29)
    
2.  简单起见，我们假设存在复数![g(q,k,m-n)$g%28q%2Ck%2Cm-n%29)，使得![f(q,m)f^*(k,n)=g(q,k,m-n)$f%28q%2Cm%29f%5E*%28k%2Cn%29%3Dg%28q%2Ck%2Cm-n%29)，然后我们用复数的指数形式，设
    
    ![\begin{aligned} \boldsymbol{f}(\boldsymbol{q}, m) & =R_{f}(\boldsymbol{q}, m) e^{\mathrm{i} \Theta_{f}(\boldsymbol{q}, m)} \\ \boldsymbol{f}(\boldsymbol{k}, n) & =R_{f}(\boldsymbol{k}, n) e^{\mathrm{i} \Theta_{f}(\boldsymbol{k}, n)} \\ \boldsymbol{g}(\boldsymbol{q}, \boldsymbol{k}, m-n) & =R_{g}(\boldsymbol{q}, \boldsymbol{k}, m-n) e^{\mathrm{i} \Theta_{g}(\boldsymbol{q}, \boldsymbol{k}, m-n)} \end{aligned}$%5Cbegin%7Baligned%7D%20%5Cboldsymbol%7Bf%7D%28%5Cboldsymbol%7Bq%7D%2C%20m%29%20%26%20%3DR_%7Bf%7D%28%5Cboldsymbol%7Bq%7D%2C%20m%29%20e%5E%7B%5Cmathrm%7Bi%7D%20%5CTheta_%7Bf%7D%28%5Cboldsymbol%7Bq%7D%2C%20m%29%7D%20%5C%5C%20%5Cboldsymbol%7Bf%7D%28%5Cboldsymbol%7Bk%7D%2C%20n%29%20%26%20%3DR_%7Bf%7D%28%5Cboldsymbol%7Bk%7D%2C%20n%29%20e%5E%7B%5Cmathrm%7Bi%7D%20%5CTheta_%7Bf%7D%28%5Cboldsymbol%7Bk%7D%2C%20n%29%7D%20%5C%5C%20%5Cboldsymbol%7Bg%7D%28%5Cboldsymbol%7Bq%7D%2C%20%5Cboldsymbol%7Bk%7D%2C%20m-n%29%20%26%20%3DR_%7Bg%7D%28%5Cboldsymbol%7Bq%7D%2C%20%5Cboldsymbol%7Bk%7D%2C%20m-n%29%20e%5E%7B%5Cmathrm%7Bi%7D%20%5CTheta_%7Bg%7D%28%5Cboldsymbol%7Bq%7D%2C%20%5Cboldsymbol%7Bk%7D%2C%20m-n%29%7D%20%5Cend%7Baligned%7D)
3.  那么代入方程后就得到两个方程
    
    方程1：![Rf(q,m)Rf(k,n) = Rg(q,k,m-n)$Rf%28q%2Cm%29Rf%28k%2Cn%29%20%3D%20Rg%28q%2Ck%2Cm-n%29)
    
    方程2：**Θf(q,m)−Θf(k,n) = Θg(q,k,m−n)**
    
    ![\rightarrow$%5Crightarrow)  对于方程1，代入![m=n$m%3Dn)得到(接着，再把![m$m)和![n$n)都设为0)
    
    ![R_{f}(\boldsymbol{q}, m) R_{f}(\boldsymbol{k}, m)=R_{g}(\boldsymbol{q}, \boldsymbol{k}, 0)=R_{f}(\boldsymbol{q}, 0) R_{f}(\boldsymbol{k}, 0)=\|\boldsymbol{q}\|\|\boldsymbol{k}\|$R_%7Bf%7D%28%5Cboldsymbol%7Bq%7D%2C%20m%29%20R_%7Bf%7D%28%5Cboldsymbol%7Bk%7D%2C%20m%29%3DR_%7Bg%7D%28%5Cboldsymbol%7Bq%7D%2C%20%5Cboldsymbol%7Bk%7D%2C%200%29%3DR_%7Bf%7D%28%5Cboldsymbol%7Bq%7D%2C%200%29%20R_%7Bf%7D%28%5Cboldsymbol%7Bk%7D%2C%200%29%3D%5C%7C%5Cboldsymbol%7Bq%7D%5C%7C%5C%7C%5Cboldsymbol%7Bk%7D%5C%7C)
    
    最后一个等号源于初始条件![f(q,0) = q$f%28q%2C0%29%20%3D%20q)和![f(k,0) = k$f%28k%2C0%29%20%3D%20k)，所以现在我们可以很简单地设![Rf(q,m) = \left \| q \right \|$Rf%28q%2Cm%29%20%3D%20%5Cleft%20%5C%7C%20q%20%5Cright%20%5C%7C)，![Rf(k,m)= \left \| k \right \|$Rf%28k%2Cm%29%3D%20%5Cleft%20%5C%7C%20k%20%5Cright%20%5C%7C)，即它不依赖于![m$m)
    
    ![\rightarrow$%5Crightarrow)  至于方程2，同样代入![m=n$m%3Dn)得到
    
    Θf(q,m)−Θf(k,m) = Θg(q,k,0) = Θf(q,0)−Θf(k,0) \= Θ(q)−Θ(k)
    
    这里的![\Theta(q)$%5CTheta%28q%29)、![\Theta(k)$%5CTheta%28k%29)是![q$q)、![k$k)本身的幅角，而最后一个等号同样源于初始条件
    
    根据上式Θf(q,m)−Θf(k,m) = Θ(q)−Θ(k)，可得Θf(q,m)−Θ(q)=Θf(k,m)−Θ(k)，所以Θf(q,m)−Θ(q)的结果是一个只与m相关、跟q无关的函数，记为φ(m)，即Θf(q,m)=Θ(q)+φ(m)
    
4.  接着令n=m−1代入**Θf(q,m)−Θf(k,n) = Θg(q,k,m−n)**，可以得到 <u>Θf(q,m)−Θf(k,m-1) = Θg(q,k,1)</u>
    
    然后将 <u>Θf(q,m) 和 Θf(k,m-1)</u> 的等式代入Θf(q,m)=Θ(q)+φ(m)，我们可以得到 Θ(q) + φ(m) - (Θ(k) + φ(m-1)) = Θg(q,k,1)，整理一下就得到
    
    ![\varphi(m)-\varphi(m-1)=\Theta g(q, k, 1)+\Theta(k)-\Theta(q)$%5Cvarphi%28m%29-%5Cvarphi%28m-1%29%3D%5CTheta%20g%28q%2C%20k%2C%201%29&plus;%5CTheta%28k%29-%5CTheta%28q%29)
    
    即{φ(m)}是等差数列，设右端为θ，那么就解得**φ(m)=mθ**
    
    综上，我们得到二维情况下用复数表示的RoPE：
    
    **![\boldsymbol{f}(\boldsymbol{q}, m)=R_{f}(\boldsymbol{q}, m) e^{\mathrm{i} \Theta f(\boldsymbol{q}, m)}=\|q\| e^{\mathrm{i}(\Theta(\boldsymbol{q})+m \theta)}=\boldsymbol{q} e^{\mathrm{i} m \theta}$%5Cboldsymbol%7Bf%7D%28%5Cboldsymbol%7Bq%7D%2C%20m%29%3DR_%7Bf%7D%28%5Cboldsymbol%7Bq%7D%2C%20m%29%20e%5E%7B%5Cmathrm%7Bi%7D%20%5CTheta%20f%28%5Cboldsymbol%7Bq%7D%2C%20m%29%7D%3D%5C%7Cq%5C%7C%20e%5E%7B%5Cmathrm%7Bi%7D%28%5CTheta%28%5Cboldsymbol%7Bq%7D%29&plus;m%20%5Ctheta%29%7D%3D%5Cboldsymbol%7Bq%7D%20e%5E%7B%5Cmathrm%7Bi%7D%20m%20%5Ctheta%7D)**
    
5.  所以说，寻求的变换就是![q_me^{im\theta}$q_me%5E%7Bim%5Ctheta%7D)，也就是给![q_m$q_m)乘以![e^{im\theta}$e%5E%7Bim%5Ctheta%7D)，相应地，![k_n$k_n)乘以![e^{in\theta}$e%5E%7Bin%5Ctheta%7D)
    
    做了这样一个变换之后，根据复数的特性，有：![\langle q_m, k_n \rangle = Re[q_mk^*_n]$%5Clangle%20q_m%2C%20k_n%20%5Crangle%20%3D%20Re%5Bq_mk%5E*_n%5D) 也就是，如果把二维向量看做复数，那么它们的内积，等于一个复数乘以另一个复数的共轭，得到的结果再取实部，代入上面的变换，也就有：
    
    ![\langle q_me^{im\theta}, k_ne^{in\theta} \rangle = Re[(q_me^{im\theta}) (k_ne^{in\theta})^*] =Re[q_mk_n^*e^{i(m-n)\theta}]$%5Clangle%20q_me%5E%7Bim%5Ctheta%7D%2C%20k_ne%5E%7Bin%5Ctheta%7D%20%5Crangle%20%3D%20Re%5B%28q_me%5E%7Bim%5Ctheta%7D%29%20%28k_ne%5E%7Bin%5Ctheta%7D%29%5E*%5D%20%3DRe%5Bq_mk_n%5E*e%5E%7Bi%28m-n%29%5Ctheta%7D%5D)
    
    这样一来，内积的结果就只依赖于![(m-n)$%28m-n%29)，也就是相对位置了
    
    换言之，经过这样一番操作，通过给Embedding添加绝对位置信息，可以使得两个token的编码，经过内积变换（self-attn）之后，得到结果是受它们位置的差值，即相对位置影响的

于是，对于任意的位置为![m$m)的二维向量![[x, y]$%5Bx%2C%20y%5D)，把它看做复数，乘以![e^{im\theta}$e%5E%7Bim%5Ctheta%7D)，而根据欧拉公式，有：

![e^{im\theta}=\cos{m\theta}+i\sin{m\theta}$e%5E%7Bim%5Ctheta%7D%3D%5Ccos%7Bm%5Ctheta%7D&plus;i%5Csin%7Bm%5Ctheta%7D)

从而上述的相乘变换也就变成了(过程中注意：![i^2=-1$i%5E2%3D-1))：

![(x+iy)e^{im\theta} \\= (x+ i y) (\cos{m\theta}+i\sin{m\theta}) \\= x\cos{m\theta} + ix\sin{m\theta} + iy\cos{m\theta} - y\sin{m\theta} \\ = (x\cos{m\theta}-y\sin{m\theta})+i(x\sin{m\theta}+y\cos{m\theta})$%28x&plus;iy%29e%5E%7Bim%5Ctheta%7D%20%5C%5C%3D%20%28x&plus;%20i%20y%29%20%28%5Ccos%7Bm%5Ctheta%7D&plus;i%5Csin%7Bm%5Ctheta%7D%29%20%5C%5C%3D%20x%5Ccos%7Bm%5Ctheta%7D%20&plus;%20ix%5Csin%7Bm%5Ctheta%7D%20&plus;%20iy%5Ccos%7Bm%5Ctheta%7D%20-%20y%5Csin%7Bm%5Ctheta%7D%20%5C%5C%20%3D%20%28x%5Ccos%7Bm%5Ctheta%7D-y%5Csin%7Bm%5Ctheta%7D%29&plus;i%28x%5Csin%7Bm%5Ctheta%7D&plus;y%5Ccos%7Bm%5Ctheta%7D%29)

把上述式子写成矩阵形式：

![](https://i-blog.csdnimg.cn/blog_migrate/4793ba3f0c25f8c72e22726d7ddc7f7c.png)

而这个变换的几何意义，就是在二维坐标系下，对向量![(q_0, q_1)$%28q_0%2C%20q_1%29)进行了旋转，因而这种位置编码方法，被称为旋转位置编码

根据刚才的结论，结合内积的线性叠加性，可以将结论推广到高维的情形。可以理解为，每两个维度一组，进行了上述的“旋转”操作，然后再拼接在一起：

![](https://i-blog.csdnimg.cn/blog_migrate/676408e145beb378ada6312d723235e2.png)

由于矩阵的稀疏性，会造成计算上的浪费，所以在计算时采用逐位相乘再相加的方式进行：

![](https://i-blog.csdnimg.cn/blog_migrate/0caf002f246369b2641b1ca3bf99ef84.png)

其中![\otimes$%5Cotimes)为矩阵逐位相乘操作

#### 3.2 旋转位置编码的coding实现(分非LLaMA版和LLaMA版两种)

原理理解了，接下来可以代码实现旋转位置编码，考虑到LLaMA本身的实现不是特别好理解，所以我们先通过一份非LLaMA实现的版本，最后再看下LLaMA实现的版本

对于，非LLaMA版的实现，其核心就是实现下面这三个函数 (再次强调，本份关于RoPE的非LLaMA版的实现 与上面和之后的代码并非一体的，仅为方便理解RoPE的实现)

##### 3.2.1 非LLaMA版的实现

###### 3.2.1.1 sinusoidal\_position\_embedding的编码实现

sinusoidal\_position\_embedding：这个函数用来生成正弦形状的位置编码。这种编码用来在序列中的令牌中添加关于相对或绝对位置的信息

```python
def sinusoidal_position_embedding(batch_size, nums_head, max_len, output_dim, device):    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(-1)    ids = torch.arange(0, output_dim // 2, dtype=torch.float)      theta = torch.pow(10000, -2 * ids / output_dim)    embeddings = position * theta     embeddings = torch.stack([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)    embeddings = embeddings.repeat((batch_size, nums_head, *([1] * len(embeddings.shape))))      embeddings = torch.reshape(embeddings, (batch_size, nums_head, max_len, output_dim))    embeddings = embeddings.to(device)return embeddings
```

一般的文章可能解释道这个程度基本就over了，但为了让初学者一目了然计，我还是再通过一个完整的示例，来一步步说明上述各个步骤都是怎么逐一结算的，整个过程和之前此文里介绍过的transformer的位置编码本质上是一回事..

为方便和transformer的位置编码做对比，故这里也假定output\_dim = 512

1.  首先，我们有 ids 张量，当 output\_dim 为 512 时，则
    
    ![i = 0 // 2 = 0$i%20%3D%200%20//%202%20%3D%200)，![2i = 0$2i%20%3D%200)
    
    ![i = 1 //2 =0$i%20%3D%201%20//2%20%3D0)，![2i = 0,2i+1 = 1$2i%20%3D%200%2C2i&plus;1%20%3D%201)
    
    ![i = 2 // 2 = 1$i%20%3D%202%20//%202%20%3D%201)，![2i = 2$2i%20%3D%202)
    
    ![i = 3 // 2 = 1$i%20%3D%203%20//%202%20%3D%201)，![2i = 2,2i+1 = 3$2i%20%3D%202%2C2i&plus;1%20%3D%203)
    
    ![i = 4 // 2 = 2$i%20%3D%204%20//%202%20%3D%202)，![2i = 4$2i%20%3D%204)
    
    ![i = 5//2 = 2$i%20%3D%205//2%20%3D%202)，![2i = 4, 2i + 1 =5$2i%20%3D%204%2C%202i%20&plus;%201%20%3D5)
    
    ...
    
    ![i = 510 // 2 = 255$i%20%3D%20510%20//%202%20%3D%20255)，![2i = 510$2i%20%3D%20510)
    
    ![i = 511 // 2 = 255$i%20%3D%20511%20//%202%20%3D%20255)，![2i = 510,2i + 1 = 511$2i%20%3D%20510%2C2i%20&plus;%201%20%3D%20511)
    
    ids = \[0,0, 1,1, 2,2, ..., 254,254, 255,255\]
    
    然后我们有一个基数为10000的指数运算，使用了公式 torch.pow(10000, -2 \* ids / output\_dim)
    
    ![[\frac{1}{10000^{\frac{0}{512}}},\frac{1}{10000^{\frac{0}{512}}}, \frac{1}{10000^{\frac{2}{512}}},\frac{1}{10000^{\frac{2}{512}}}, \frac{1}{10000^{\frac{4}{512}}}, \frac{1}{10000^{\frac{4}{512}}},..., \frac{1}{10000^{\frac{510}{512}}},\frac{1}{10000^{\frac{510}{512}}}]$%5B%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B0%7D%7B512%7D%7D%7D%2C%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B0%7D%7B512%7D%7D%7D%2C%20%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B2%7D%7B512%7D%7D%7D%2C%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B2%7D%7B512%7D%7D%7D%2C%20%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B4%7D%7B512%7D%7D%7D%2C%20%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B4%7D%7B512%7D%7D%7D%2C...%2C%20%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B510%7D%7B512%7D%7D%7D%2C%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B510%7D%7B512%7D%7D%7D%5D)
    
2.  执行 embeddings = position \* theta 这行代码，它会将 position 的每个元素与 theta 的相应元素相乘，前三个元素为![[\frac{0}{10000^{\frac{0}{512}}},\frac{0}{10000^{\frac{0}{512}}}, \frac{0}{10000^{\frac{2}{512}}},\frac{0}{10000^{\frac{2}{512}}}, \frac{0}{10000^{\frac{4}{512}}}, \frac{0}{10000^{\frac{4}{512}}},..., \frac{0}{10000^{\frac{510}{512}}},\frac{0}{10000^{\frac{510}{512}}}]$%5B%5Cfrac%7B0%7D%7B10000%5E%7B%5Cfrac%7B0%7D%7B512%7D%7D%7D%2C%5Cfrac%7B0%7D%7B10000%5E%7B%5Cfrac%7B0%7D%7B512%7D%7D%7D%2C%20%5Cfrac%7B0%7D%7B10000%5E%7B%5Cfrac%7B2%7D%7B512%7D%7D%7D%2C%5Cfrac%7B0%7D%7B10000%5E%7B%5Cfrac%7B2%7D%7B512%7D%7D%7D%2C%20%5Cfrac%7B0%7D%7B10000%5E%7B%5Cfrac%7B4%7D%7B512%7D%7D%7D%2C%20%5Cfrac%7B0%7D%7B10000%5E%7B%5Cfrac%7B4%7D%7B512%7D%7D%7D%2C...%2C%20%5Cfrac%7B0%7D%7B10000%5E%7B%5Cfrac%7B510%7D%7B512%7D%7D%7D%2C%5Cfrac%7B0%7D%7B10000%5E%7B%5Cfrac%7B510%7D%7B512%7D%7D%7D%5D)
    
    ![[\frac{1}{10000^{\frac{0}{512}}},\frac{1}{10000^{\frac{0}{512}}}, \frac{1}{10000^{\frac{2}{512}}},\frac{1}{10000^{\frac{2}{512}}}, \frac{1}{10000^{\frac{4}{512}}}, \frac{1}{10000^{\frac{4}{512}}},..., \frac{1}{10000^{\frac{510}{512}}},\frac{1}{10000^{\frac{510}{512}}}]$%5B%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B0%7D%7B512%7D%7D%7D%2C%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B0%7D%7B512%7D%7D%7D%2C%20%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B2%7D%7B512%7D%7D%7D%2C%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B2%7D%7B512%7D%7D%7D%2C%20%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B4%7D%7B512%7D%7D%7D%2C%20%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B4%7D%7B512%7D%7D%7D%2C...%2C%20%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B510%7D%7B512%7D%7D%7D%2C%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B510%7D%7B512%7D%7D%7D%5D)
    
    ![[\frac{2}{10000^{\frac{0}{512}}},\frac{2}{10000^{\frac{0}{512}}}, \frac{2}{10000^{\frac{2}{512}}},\frac{2}{10000^{\frac{2}{512}}}, \frac{2}{10000^{\frac{4}{512}}}, \frac{2}{10000^{\frac{4}{512}}},..., \frac{2}{10000^{\frac{510}{512}}},\frac{2}{10000^{\frac{510}{512}}}]$%5B%5Cfrac%7B2%7D%7B10000%5E%7B%5Cfrac%7B0%7D%7B512%7D%7D%7D%2C%5Cfrac%7B2%7D%7B10000%5E%7B%5Cfrac%7B0%7D%7B512%7D%7D%7D%2C%20%5Cfrac%7B2%7D%7B10000%5E%7B%5Cfrac%7B2%7D%7B512%7D%7D%7D%2C%5Cfrac%7B2%7D%7B10000%5E%7B%5Cfrac%7B2%7D%7B512%7D%7D%7D%2C%20%5Cfrac%7B2%7D%7B10000%5E%7B%5Cfrac%7B4%7D%7B512%7D%7D%7D%2C%20%5Cfrac%7B2%7D%7B10000%5E%7B%5Cfrac%7B4%7D%7B512%7D%7D%7D%2C...%2C%20%5Cfrac%7B2%7D%7B10000%5E%7B%5Cfrac%7B510%7D%7B512%7D%7D%7D%2C%5Cfrac%7B2%7D%7B10000%5E%7B%5Cfrac%7B510%7D%7B512%7D%7D%7D%5D)
3.  接下来我们将对 embeddings 的每个元素应用 torch.sin 和 torch.cos 函数
    
    对于 torch.sin(embeddings)，我们将取 embeddings 中的每个元素的正弦值：
    
    ![[sin(\frac{0}{10000^{\frac{0}{512}}}), sin(\frac{0}{10000^{\frac{2}{512}}}), sin(\frac{0}{10000^{\frac{4}{512}}}),..., sin(\frac{0}{10000^{\frac{510}{512}}})]$%5Bsin%28%5Cfrac%7B0%7D%7B10000%5E%7B%5Cfrac%7B0%7D%7B512%7D%7D%7D%29%2C%20sin%28%5Cfrac%7B0%7D%7B10000%5E%7B%5Cfrac%7B2%7D%7B512%7D%7D%7D%29%2C%20sin%28%5Cfrac%7B0%7D%7B10000%5E%7B%5Cfrac%7B4%7D%7B512%7D%7D%7D%29%2C...%2C%20sin%28%5Cfrac%7B0%7D%7B10000%5E%7B%5Cfrac%7B510%7D%7B512%7D%7D%7D%29%5D)
    
    ![[sin(\frac{1}{10000^{\frac{0}{512}}}), sin(\frac{1}{10000^{\frac{2}{512}}}), sin(\frac{1}{10000^{\frac{4}{512}}}),..., sin(\frac{1}{10000^{\frac{510}{512}}})]$%5Bsin%28%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B0%7D%7B512%7D%7D%7D%29%2C%20sin%28%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B2%7D%7B512%7D%7D%7D%29%2C%20sin%28%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B4%7D%7B512%7D%7D%7D%29%2C...%2C%20sin%28%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B510%7D%7B512%7D%7D%7D%29%5D)
    
    ![[sin(\frac{2}{10000^{\frac{0}{512}}}), sin(\frac{2}{10000^{\frac{2}{512}}}), sin(\frac{2}{10000^{\frac{4}{512}}}),..., sin(\frac{2}{10000^{\frac{510}{512}}})]$%5Bsin%28%5Cfrac%7B2%7D%7B10000%5E%7B%5Cfrac%7B0%7D%7B512%7D%7D%7D%29%2C%20sin%28%5Cfrac%7B2%7D%7B10000%5E%7B%5Cfrac%7B2%7D%7B512%7D%7D%7D%29%2C%20sin%28%5Cfrac%7B2%7D%7B10000%5E%7B%5Cfrac%7B4%7D%7B512%7D%7D%7D%29%2C...%2C%20sin%28%5Cfrac%7B2%7D%7B10000%5E%7B%5Cfrac%7B510%7D%7B512%7D%7D%7D%29%5D)
    
    对于 torch.cos(embeddings)，我们将取 embeddings 中的每个元素的余弦值：
    
    ![[cos(\frac{0}{10000^{\frac{0}{512}}}),cos(\frac{0}{10000^{\frac{2}{512}}}), cos(\frac{0}{10000^{\frac{4}{512}}}),..., ,cos(\frac{0}{10000^{\frac{510}{512}}})]$%5Bcos%28%5Cfrac%7B0%7D%7B10000%5E%7B%5Cfrac%7B0%7D%7B512%7D%7D%7D%29%2Ccos%28%5Cfrac%7B0%7D%7B10000%5E%7B%5Cfrac%7B2%7D%7B512%7D%7D%7D%29%2C%20cos%28%5Cfrac%7B0%7D%7B10000%5E%7B%5Cfrac%7B4%7D%7B512%7D%7D%7D%29%2C...%2C%20%2Ccos%28%5Cfrac%7B0%7D%7B10000%5E%7B%5Cfrac%7B510%7D%7B512%7D%7D%7D%29%5D)
    
    ![[cos(\frac{1}{10000^{\frac{0}{512}}}),cos(\frac{1}{10000^{\frac{2}{512}}}), cos(\frac{1}{10000^{\frac{4}{512}}}),..., ,cos(\frac{1}{10000^{\frac{510}{512}}})]$%5Bcos%28%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B0%7D%7B512%7D%7D%7D%29%2Ccos%28%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B2%7D%7B512%7D%7D%7D%29%2C%20cos%28%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B4%7D%7B512%7D%7D%7D%29%2C...%2C%20%2Ccos%28%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B510%7D%7B512%7D%7D%7D%29%5D)
    
    ![[cos(\frac{2}{10000^{\frac{0}{512}}}),cos(\frac{2}{10000^{\frac{2}{512}}}), cos(\frac{2}{10000^{\frac{4}{512}}}),..., ,cos(\frac{2}{10000^{\frac{510}{512}}})]$%5Bcos%28%5Cfrac%7B2%7D%7B10000%5E%7B%5Cfrac%7B0%7D%7B512%7D%7D%7D%29%2Ccos%28%5Cfrac%7B2%7D%7B10000%5E%7B%5Cfrac%7B2%7D%7B512%7D%7D%7D%29%2C%20cos%28%5Cfrac%7B2%7D%7B10000%5E%7B%5Cfrac%7B4%7D%7B512%7D%7D%7D%29%2C...%2C%20%2Ccos%28%5Cfrac%7B2%7D%7B10000%5E%7B%5Cfrac%7B510%7D%7B512%7D%7D%7D%29%5D)
    
    最后，torch.stack(\[torch.sin(embeddings), torch.cos(embeddings)\], dim=-1) 将这两个新的张量沿着一个新的维度堆叠起来，得到的 embeddings如下
    
    ![PE_0 = [sin(\frac{0}{10000^{\frac{0}{512}}}),cos(\frac{0}{10000^{\frac{0}{512}}}), sin(\frac{0}{10000^{\frac{2}{512}}}),cos(\frac{0}{10000^{\frac{2}{512}}}), sin(\frac{0}{10000^{\frac{4}{512}}}), cos(\frac{0}{10000^{\frac{4}{512}}}),..., sin(\frac{0}{10000^{\frac{510}{512}}}),cos(\frac{0}{10000^{\frac{510}{512}}})]$PE_0%20%3D%20%5Bsin%28%5Cfrac%7B0%7D%7B10000%5E%7B%5Cfrac%7B0%7D%7B512%7D%7D%7D%29%2Ccos%28%5Cfrac%7B0%7D%7B10000%5E%7B%5Cfrac%7B0%7D%7B512%7D%7D%7D%29%2C%20sin%28%5Cfrac%7B0%7D%7B10000%5E%7B%5Cfrac%7B2%7D%7B512%7D%7D%7D%29%2Ccos%28%5Cfrac%7B0%7D%7B10000%5E%7B%5Cfrac%7B2%7D%7B512%7D%7D%7D%29%2C%20sin%28%5Cfrac%7B0%7D%7B10000%5E%7B%5Cfrac%7B4%7D%7B512%7D%7D%7D%29%2C%20cos%28%5Cfrac%7B0%7D%7B10000%5E%7B%5Cfrac%7B4%7D%7B512%7D%7D%7D%29%2C...%2C%20sin%28%5Cfrac%7B0%7D%7B10000%5E%7B%5Cfrac%7B510%7D%7B512%7D%7D%7D%29%2Ccos%28%5Cfrac%7B0%7D%7B10000%5E%7B%5Cfrac%7B510%7D%7B512%7D%7D%7D%29%5D)
    
    ![PE_1 = [sin(\frac{1}{10000^{\frac{0}{512}}}),cos(\frac{1}{10000^{\frac{0}{512}}}), sin(\frac{1}{10000^{\frac{2}{512}}}),cos(\frac{1}{10000^{\frac{2}{512}}}), sin(\frac{1}{10000^{\frac{4}{512}}}), cos(\frac{1}{10000^{\frac{4}{512}}}),..., sin(\frac{1}{10000^{\frac{510}{512}}}),cos(\frac{1}{10000^{\frac{510}{512}}})]$PE_1%20%3D%20%5Bsin%28%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B0%7D%7B512%7D%7D%7D%29%2Ccos%28%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B0%7D%7B512%7D%7D%7D%29%2C%20sin%28%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B2%7D%7B512%7D%7D%7D%29%2Ccos%28%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B2%7D%7B512%7D%7D%7D%29%2C%20sin%28%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B4%7D%7B512%7D%7D%7D%29%2C%20cos%28%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B4%7D%7B512%7D%7D%7D%29%2C...%2C%20sin%28%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B510%7D%7B512%7D%7D%7D%29%2Ccos%28%5Cfrac%7B1%7D%7B10000%5E%7B%5Cfrac%7B510%7D%7B512%7D%7D%7D%29%5D)
    
    ![PE_2 = [sin(\frac{2}{10000^{\frac{0}{512}}}),cos(\frac{2}{10000^{\frac{0}{512}}}), sin(\frac{2}{10000^{\frac{2}{512}}}),cos(\frac{2}{10000^{\frac{2}{512}}}), sin(\frac{2}{10000^{\frac{4}{512}}}), cos(\frac{2}{10000^{\frac{4}{512}}}),..., sin(\frac{2}{10000^{\frac{510}{512}}}),cos(\frac{2}{10000^{\frac{510}{512}}})]$PE_2%20%3D%20%5Bsin%28%5Cfrac%7B2%7D%7B10000%5E%7B%5Cfrac%7B0%7D%7B512%7D%7D%7D%29%2Ccos%28%5Cfrac%7B2%7D%7B10000%5E%7B%5Cfrac%7B0%7D%7B512%7D%7D%7D%29%2C%20sin%28%5Cfrac%7B2%7D%7B10000%5E%7B%5Cfrac%7B2%7D%7B512%7D%7D%7D%29%2Ccos%28%5Cfrac%7B2%7D%7B10000%5E%7B%5Cfrac%7B2%7D%7B512%7D%7D%7D%29%2C%20sin%28%5Cfrac%7B2%7D%7B10000%5E%7B%5Cfrac%7B4%7D%7B512%7D%7D%7D%29%2C%20cos%28%5Cfrac%7B2%7D%7B10000%5E%7B%5Cfrac%7B4%7D%7B512%7D%7D%7D%29%2C...%2C%20sin%28%5Cfrac%7B2%7D%7B10000%5E%7B%5Cfrac%7B510%7D%7B512%7D%7D%7D%29%2Ccos%28%5Cfrac%7B2%7D%7B10000%5E%7B%5Cfrac%7B510%7D%7B512%7D%7D%7D%29%5D)
    
4.  最终，得到如下结果
    
    ```cobol
    [  [    [      [sin(\frac{0}{10000^{\frac{0}{512}}}), cos(\frac{0}{10000^{\frac{0}{512}}}), sin(\frac{0}{10000^{\frac{2}{512}}}), cos(\frac{0}{10000^{\frac{2}{512}}}), ..., cos(\frac{0}{10000^{\frac{510}{512}}})],      [sin(\frac{1}{10000^{\frac{0}{512}}}), cos(\frac{1}{10000^{\frac{0}{512}}}), sin(\frac{1}{10000^{\frac{2}{512}}}), cos(\frac{1}{10000^{\frac{2}{512}}}), ..., cos(\frac{1}{10000^{\frac{510}{512}}})],      [sin(\frac{2}{10000^{\frac{0}{512}}}), cos(\frac{2}{10000^{\frac{0}{512}}}), sin(\frac{2}{10000^{\frac{2}{512}}}), cos(\frac{2}{10000^{\frac{2}{512}}}), ..., cos(\frac{2}{10000^{\frac{510}{512}}})]    ]  ]]
    ```
    

###### 3.2.1.2 RoPE的编码实现

RoPE：这个函数将相对位置编码（RoPE）应用到注意力机制中的查询和键上。这样，模型就可以根据相对位置关注不同的位置

```python
import torchimport torch.nn as nnimport torch.nn.functional as Fimport mathdef RoPE(q, k):    batch_size = q.shape[0]    nums_head = q.shape[1]    max_len = q.shape[2]    output_dim = q.shape[-1]    pos_emb = sinusoidal_position_embedding(batch_size, nums_head, max_len, output_dim, q.device)    cos_pos = pos_emb[...,  1::2].repeat_interleave(2, dim=-1)      sin_pos = pos_emb[..., ::2].repeat_interleave(2, dim=-1)      q2 = torch.stack([-q[..., 1::2], q[..., ::2]], dim=-1)    q2 = q2.reshape(q.shape)      q = q * cos_pos + q2 * sin_pos    k2 = torch.stack([-k[..., 1::2], k[..., ::2]], dim=-1)    k2 = k2.reshape(k.shape)    k = k * cos_pos + k2 * sin_posreturn q, k
```

老规矩，为一目了然起见，还是一步一步通过一个示例来加深理解

1.  sinusoidal\_position\_embedding函数生成位置嵌入。在output\_dim=512的情况下，每个位置的嵌入会有512个维度，但为了简单起见，我们只考虑前8个维度，前4个维度为sin编码，后4个维度为cos编码。所以，我们可能得到类似以下的位置嵌入
    
    ```cobol
    # 注意，这只是一个简化的例子，真实的位置嵌入的值会有所不同。pos_emb = torch.tensor([[[[0.0000, 0.8415, 0.9093, 0.1411, 1.0000, 0.5403, -0.4161, -0.9900],                          [0.8415, 0.5403, 0.1411, -0.7568, 0.5403, -0.8415, -0.9900, -0.6536],                          [0.9093, -0.4161, -0.8415, -0.9589, -0.4161, -0.9093, -0.6536, 0.2836]]]])
    ```
    
2.  然后，我们提取出所有的sin位置编码和cos位置编码，并在最后一个维度上每个位置编码进行复制
    
    ```cobol
    sin_pos = pos_emb[..., ::2].repeat_interleave(2, dim=-1)  # 提取出所有sin编码，并在最后一个维度上复制cos_pos = pos_emb[..., 1::2].repeat_interleave(2, dim=-1)  # 提取出所有cos编码，并在最后一个维度上复制
    ```
    
3.  更新query向量
    
    我们首先构建一个新的q2向量，这个向量是由原来向量的负的cos部分和sin部分交替拼接而成的
    
    我们用cos\_pos对q进行元素级乘法，用sin\_pos对q2进行元素级乘法，并将两者相加得到新的query向量
    
    ```cobol
    q2 = torch.stack([-q[..., 1::2], q[..., ::2]], dim=-1).flatten(start_dim=-2)# q2: tensor([[[[-0.2,  0.1, -0.4,  0.3, -0.6,  0.5, -0.8,  0.7],#               [-1.0,  0.9, -1.2,  1.1, -1.4,  1.3, -1.6,  1.5],#               [-1.8,  1.7, -2.0,  1.9, -2.2,  2.1, -2.4,  2.3]]]])q = q * cos_pos + q2 * sin_pos
    ```
    
    公式表示如下
    
    ![](https://i-blog.csdnimg.cn/blog_migrate/0caf002f246369b2641b1ca3bf99ef84.png)
    
4.  更新key向量
    
    对于key向量，我们的处理方法与query向量类似
    
    ```cobol
    k2 = torch.stack([-k[..., 1::2], k[..., ::2]], dim=-1).flatten(start_dim=-2)# k2: tensor([[[[-0.15,  0.05, -0.35,  0.25, -0.55,  0.45, -0.75,  0.65
    ```
    

###### 3.2.1.3 attention的编码实现

attention：这是注意力机制的主要功能

-   首先，如果use\_RoPE被设置为True，它会应用RoPE，通过取查询和键的点积（并进行缩放）
-   然后，进行softmax操作来计算注意力分数，以得到概率，输出是值的加权和，权重是计算出的概率
-   最后，旋转后的q和k计算点积注意力后，自然就具备了相对位置信息

```python
def attention(q, k, v, mask=None, dropout=None, use_RoPE=True):if use_RoPE:        q, k = RoPE(q, k)    d_k = k.size()[-1]    att_logits = torch.matmul(q, k.transpose(-2, -1))      att_logits /= math.sqrt(d_k)if mask is not None:        att_scores = att_logits.masked_fill(mask == 0, -1e-9)      att_scores = F.softmax(att_logits, dim=-1)  if dropout is not None:        att_scores = dropout(att_scores)return torch.matmul(att_scores, v), att_scoresif __name__ == '__main__':    q = torch.randn((8, 12, 10, 32))    k = torch.randn((8, 12, 10, 32))    v = torch.randn((8, 12, 10, 32))    res, att_scores = attention(q, k, v, mask=None, dropout=None, use_RoPE=True)print(res.shape, att_scores.shape)
```

##### 3.2.2 LLaMA版的实现

接下来，我们再来看下LLaMA里是怎么实现这个旋转位置编码的，具体而言，[LLaMA 的model.py](https://github.com/facebookresearch/llama/blob/main/llama/model.py "LLaMA 的model.py")文件里面实现了旋转位置编码(为方便大家理解，我给相关代码 加了下注释)

首先，逐一实现这三个函数

precompute\_freqs\_cis

reshape\_for\_broadcast

apply\_rotary\_emb

```cobol
# 预计算频率和复数的函数def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))    # 计算频率    t = torch.arange(end, device=freqs.device)    # 根据结束位置生成序列    freqs = torch.outer(t, freqs).float()    # 计算外积得到新的频率    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)    # 计算复数return freqs_cis    # 返回复数
```

```cobol
# 重塑的函数def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):    ndim = x.ndim    # 获取输入张量的维度    assert 0 <= 1 < ndim    # 检查维度的合理性    assert freqs_cis.shape == (x.shape[1], x.shape[-1])    # 检查复数的形状    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]    # 计算新的形状return freqs_cis.view(*shape)    # 重塑复数的形状并返回
```

```cobol
# 应用旋转嵌入的函数def apply_rotary_emb(    xq: torch.Tensor,    xk: torch.Tensor,    freqs_cis: torch.Tensor,) -> Tuple[torch.Tensor, torch.Tensor]:    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))    # 将xq视为复数    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))    # 将xk视为复数    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)    # 重塑复数的形状    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)    # 计算xq的输出    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)    # 计算xk的输出return xq_out.type_as(xq), xk_out.type_as(xk)    # 返回xq和xk的输出
```

之后，在注意力机制的前向传播函数中调用上面实现的第三个函数 apply\_rotary\_emb，赋上位置信息

```cobol
        # 对Query和Key应用旋转嵌入        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)
```

___

### 后记

最后，说明下为何像开头说的是「23年12.16日这天对本文做了大修」呢，原因在于

1.  我司《[论文审稿GPT第2版](https://blog.csdn.net/v_JULY_v/article/details/134183799 "论文审稿GPT第2版")》即将进入模型训练阶段，其涉及到三个候选模型：mistral-yarn、mistral、llama-longlora
    
    故准备解析下YaRN，顺带把外推、内插都全面介绍下，而过程中不可避免会提到RoPE，故也总算把RoPE彻底写清楚了
2.  这些东西，哪怕是近期最新的技术、模型等理解了后 会发现都不难，但我总想把理解的门槛无限降低，所以**想真正写清楚或讲清楚一个东西，必须得反复琢磨、反复修改，以让更多人因此看懂，更何况当我和我的团队每天看paper、做项目，更可以帮到大家不断进阶、深入**

如今博客的访问PV2000万，希望明年达到2000万UV以上，以上视为后记

### 

### 参考文献与推荐阅读

1.  马同学关于向量和欧拉公式的几篇科普文章
    
    [向量的加法](https://www.matongxue.com/parts/1991/ "向量的加法")
    
    [欧拉公式，复数域的成人礼](https://www.matongxue.com/madocs/2066 "欧拉公式，复数域的成人礼")
2.  关于欧拉公式的几篇文章
    
    [被众人膜拜的欧拉恒等式是个什么东东？](https://zhuanlan.zhihu.com/p/40302967 "被众人膜拜的欧拉恒等式是个什么东东？")
    
    [怎么向小学生解释欧拉公式 e^(πi)+1=0？](https://www.zhihu.com/question/41134540/answer/112430787 "怎么向小学生解释欧拉公式 e^(πi)+1=0？")
3.  [读懂旋转编码（RoPE）](https://zhuanlan.zhihu.com/p/647109286 "读懂旋转编码（RoPE）")
4.  [LLM学习记录（五）--超简单的RoPE理解方式](https://zhuanlan.zhihu.com/p/642289220 "LLM学习记录（五）--超简单的RoPE理解方式")，这篇文章很不错
5.  苏剑林：[Transformer升级之路：2、博采众长的旋转式位置编码](https://kexue.fm/archives/8265 "Transformer升级之路：2、博采众长的旋转式位置编码")
6.  [LLaMA的解读与其微调：Alpaca-LoRA/Vicuna/BELLE/中文LLaMA/姜子牙/LLaMA 2](https://blog.csdn.net/v_JULY_v/article/details/129709105 "LLaMA的解读与其微调：Alpaca-LoRA/Vicuna/BELLE/中文LLaMA/姜子牙/LLaMA 2")
7.  关于ALiBi的两篇文章
    
    [\[速读经典\]ALiBi - 给注意力加上线性偏置](https://zhuanlan.zhihu.com/p/632780188 "[速读经典]ALiBi - 给注意力加上线性偏置")
    
    [关于Transformer中的位置编码-ALiBi](https://zhuanlan.zhihu.com/p/642846676 "关于Transformer中的位置编码-ALiBi")
8.  [最强LLaMA突然来袭！只改一个超参数，实现上下文3.2万token，多个任务打败ChatGPT、Claude 2](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247697635&idx=1&sn=9cdb5237077dbc4500856ef1d02c9464&chksm=e8df7391dfa8fa87fa014ba09deffc81b7ccf64ce68bd3b6ef39b9eced9e157a066651cddef5&mpshare=1&scene=23&srcid=09301p6Q6O51wRLPlYryhkVJ&sharer_shareinfo=3c6f02f7dd69fe81e8114949c8cc0feb&sharer_shareinfo_first=dd23961fd33178f3f5fff3d6324d4593#rd "最强LLaMA突然来袭！只改一个超参数，实现上下文3.2万token，多个任务打败ChatGPT、Claude 2")
