---
created: 2025-09-26T15:07:39 (UTC +08:00)
tags: [DeepSeek-R1,GRPO,qwen3]
source: https://zhuanlan.zhihu.com/p/1888311680880080185
author: 关于作者灰一会AI算法研究员，专注AI领域知识研究、应用和分享回答9文章16关注者119关注他发私信
---

## 1. PPO

【后续几个是在其基础上进行改进】

**多采用[off policy](https://zhida.zhihu.com/search?content_id=255612206&content_type=Article&match_order=1&q=off+policy&zhida_source=entity) 训练。**

### 数据要求：

-   原始数据集：只需要准备prompt。
-   中间采样：需要利用old model进行response采样。

### **涉及模型：**

1.  reward model\[不训、及时奖励\]、
2.  reference model\[不训、计算KL散度、降低训偏风险\]、
3.  critic/value model\[训练、状态价值，是否生成人类偏好的评估，也是动作的期望估计\]，
4.  actor/policy model\[训练、目标模型，优化其按照人类偏好生成能力\]

### 采样过程：【单一采样】

-   利用old model对每个prompt采样单个response，没有特殊要求限制。

### **涉及计算：**

1.  **reward**: RM打分，通常用在last token 的打分\[主流\]。也有用所有token的平均值作为分值。\[模型打分 $R_t$\]
	1.  利用人类偏好数据，提起训练好的RM模型，通常为0-1分值。分值越大代表生成结果更符合人类偏好。
2.  **Value**: \[预期收益\]critic/value model 对response的每一个token进行打分，目的是评估每个token状态下的价值，同时估计每个token生成时的动作价值期望。【模型打分 $V_t$ 】
3.  **优势函数** $A_t$ ：表示当前动作产生的实际价值估计\[ $R_t+\gamma\ast V_{t+1}$ \] 与当前状态下执行所有可能动作的期望价值 $V_t$。
	
	1.  当大于0，表明当前动作产生的价值高于平均值，所以当前动作更有价值，更应该鼓励，当小于0时，说明当前动作产生的价值低于平均值，所有当前的动作价值不大，不应该鼓励。这样优化下可以在每步朝着最优的动作进行选择。 $$\begin{aligned}A_t &= (R_t+\gamma\ast V_{t+1}) - V_t\\\hat{A_t}&\overset{GAE}{=}(R_t+\gamma\ast V_{t+1} - V_t) + \gamma\ast \lambda\ast V_{t+1}  \end{aligned}$$
4.  **return**:\[实际收益\] 采用AGE的思想，利用后续N步的实际结果反推当前状态的实际收益，而非估计收益。结果更接近当前的收益。 这部分值是不会随着off policy过程中改变，主要是为了训练critic。$$return_t = \hat{A_t}+V_t$$
5.  **KL散度**: 防止模型训偏，分为两部分：
	1.  在采样数据时根据**old model和ref model之间构建KL**，结合"rewards"合并，并将其做为rewards的**token\_level\_rewards。** $$\begin{aligned}KL[Actor_{old}(X)||Ref(X)]&=E_{x\sim Actor_{old}(x)}[log\frac{Actor_{old}(x)}{Ref(x)}]\\&=log\_probs_{old}-ref\_log\_probs\end{aligned}$$
	2.  在off policy训练过程中计算**new model和ref model**之间的KL，随着训练进行计算。$$\begin{aligned}KL[Actor_{new}(X)||Ref(X)]&=E_{x\sim Actor_{new}(x)}[log\frac{Actor_{new}(x)}{Ref(x)}]\\&=log\_probs_{new}-ref\_log\_probs\end{aligned}$$

注： 可以利用参考比如”use\_kl\_loss“来控制是否使用KL。比如trl中利用直接使用第一种而不使用第二种。verl中利用”use\_kl\_loss“来控制是否使用\[同时使用\]。同时利用超参\[如kl\_loss\_coef\]控制KL的占比。

> PPO中需要计算value、return，常采用第一种方式将其作为rewards的一部分。

6. **重要性采样**：off policy训练时，训练时的输入不是当前模型的实时结果，是old模型的提前生成的结果，是用old的生成来训练新的模型，为了降低训练时new的偏差，需要使用重要性采样计算。$$r_t = \frac{p_{new}(a_t|s_t)}{p_{old}(a_t|s_t)}$$ 
7. **[Clip处理](https://zhida.zhihu.com/search?content_id=255612206&content_type=Article&match_order=1&q=Clip%E5%A4%84%E7%90%86&zhida_source=entity)**：重要性采样虽然能解决new的偏差问题，但是无法解决采样过程中导致的方差变大问题，为了降低方差，当两者分布差距比较大时，**采取丢弃数据不训练actor。**\[**token维度clip, 不区分token，固定对称clip范围**\]$$min(\frac{p_{new}(a_t|s_t)}{p_{old}(a_t|s_t)}\hat{A_t},clip(\frac{p_{new}(a_t|s_t)}{p_{old}(a_t|s_t)},1-\epsilon,1+\epsilon)\hat{A_t})$$

> off policy过程中，只更新对应的 $p_{new}(a_t|s_t)$ 和 $v_t$
> 
> 经典参数： $\epsilon=0.2$

### **涉及loss**

-   **[critic loss](https://zhida.zhihu.com/search?content_id=255612206&content_type=Article&match_order=1&q=critic+loss&zhida_source=entity): 优化critic model. 目的是为了使critic 对每个状态价值更准确的预测，即使实际价值与预测价值越接近越好，也就是优势函数A越小越好。所用MSE loss即可。同样为了对actor对齐，也会对value进行clip裁剪处理即。**
-   $$\overset{clip}{v_t} = clip(v_t, \overset{old}{v_t}-\delta, \overset{old}{v_t}+\delta)$$$$\begin{aligned}&critic\_loss_t = MSE(V_t - return_t)=\frac{1}{2}（V_t- return_t）^2\\ &critic\_loss_{clip}= MSE(\overset{clip}{V}_t - return_t)=\frac{1}{2}（\overset{clip}{V}_t- return_t）^2\\ &\boxed{\mathbf{critic\_loss} = max(critic\_loss_t , critic\_loss_{clip}) } \end{aligned}$$
-   **[actor loss](https://zhida.zhihu.com/search?content_id=255612206&content_type=Article&match_order=1&q=actor+loss&zhida_source=entity): 优化actor model。 目的是每次采取的动作能够产生更多的价值，即如果当前的动作价值高于平均期望值，环境话说就是当前产生的实际价值高于期望价值，即** $A_t>0$ 时，应该鼓励actor模型多产生这样的动作，相反，如果当前的动作价值低于评价期望值，即 $A_t<0$ ，应该尽量让actor模型少产生这样的动作。所有此时比较适用交叉熵作为损失函数。即
$$\begin{aligned}&actor\_loss \\&=\hat{ \mathbb{E}}\left[min(\frac{p_{new}(a_t|s_t)}{p_{old}(a_t|s_t)}\hat{A_t},clip(\frac{p_{new}(a_t|s_t)}{p_{old}(a_t|s_t)},1-\epsilon,1+\epsilon)\hat{A_t}\right]\\&=\frac{1}{G}\sum^G_{t=1}min(\frac{p_{new}(a_t|s_t)}{p_{old}(a_t|s_t)}\hat{A_t},clip(\frac{p_{new}(a_t|s_t)}{p_{old}(a_t|s_t)},1-\epsilon,1+\epsilon)\hat{A_t})\end{aligned}$$

> 注意，这里的Loss没有KL散度，是因为将KL散度放在了A中。

-   **最终backward时loss。**

-   **联合loss**：一种是讲critic loss 以一定权重\[如 $\delta=1$ \]将加入到 actor loss 上进行联合优化，这样actor loss 也会优化critic ，同样critic loss 也会优化actor。【如verl的实现】

$$loss_{backward} = actor\_loss + \delta\ast critic\_loss$$

-   **分别优化**： 即两个loss分别进行backward，将对方的值作为其中的常量处理。即critic loss 只优化critic model参数。 actor loss 只优化actor model参数。【如trl的实现】

> **在off policy 过程中，只更新对应new的 p和value，其中A、R、return、KL都不会变化。**

___

## 2.GRPO<sup data-text="灰一会：DeepSeek-V3关键点" data-url="https://zhuanlan.zhihu.com/p/1885375635846833438" data-numero="2" data-draft-node="inline" data-draft-type="reference" data-tooltip="灰一会：DeepSeek-V3关键点 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/1885375635846833438&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://zhuanlan.zhihu.com/p/1885375635846833438&lt;/a&gt;" data-tooltip-richtext="1" data-tooltip-preset="white" data-tooltip-classname="ztext-reference-tooltip"><a id="ref_2_0" href="https://zhuanlan.zhihu.com/p/1888311680880080185#ref_2" data-reference-link="true" aria-labelledby="ref_2">[2]</a></sup>

### 数据要求：

-   原始数据集：只需要准备prompt。【同PPO】
-   中间采样：需要利用old model进行response采样。 **每次每条prompt在PPO只需采样一个，但在GRPO每次每条prompt要采样更多的response，用来计算优势函数。**

### 采样过程：【多采样】

-   利用old model对每个prompt采样一组G>1个response，没有特殊要求限制。【PPO是单个response即可】

### **涉及模型：**

**相对PPO，无需critic/value model，只需要训练actor/policy model即可。**

1.  reward model\[不训、及时奖励\]、
2.  reference model\[不训、计算KL散度、降低训偏风险\]、
3.  actor/policy model\[训练、目标模型，优化其按照人类偏好生成能力\]

### **涉及计算**

1.  **reward: 多采取rule打分的reward**，同时也可以用PPO中RM的打分模型。因为涉及到优势函数的计算，所有对reward对要求更高。同时采用**多种打分规则加权**获得最终的reward结果。【如trl中实现】

```python3
# Apply weights to each reward function's output and sum
rewards = (rewards_per_func * self.reward_weights.to(device).unsqueeze(0)).sum(dim=1)
```

2\. Value【无】： GRPO作者认为，**value模型的效果好坏制约critic的训练效果，也限制模型潜力的发掘。所有在训练过程中丢弃了Value model的使用，所有GRPO中不存在Value值**。

3\. return【无】: 没有Value, **自然没有return值。**

**4\. 优势函数：** GRPO是利用多个采样结果进行投票仲裁的方式进行计算，当前response的奖励相对均值的大小进行估计，而不是PPO的value等反推逻辑进行计算。 **【只考虑当前step生成的response簇，当reward相同时，优势值为0, 无效更新模型】**

$$A_i=\frac{r_i-mean(\{r_1,r_2,\cdots,r_G\})}{\color{red}{\operatorname{std}(\{r_1,r_2,\cdots,r_G\})}}$$

> $r_i$ 表示每个prompt的采样G条response后对应第i条的rewards。

**5.KL散度： 相对PPO有部分存在差异。PPO中常用的K1方式，且DPO采用的K3.**<sup data-text="灰一会：PPO、GRPO等RL中的KL散度计算方法即实现" data-url="https://zhuanlan.zhihu.com/p/1888641323361342824" data-numero="3" data-draft-node="inline" data-draft-type="reference" data-tooltip="灰一会：PPO、GRPO等RL中的KL散度计算方法即实现 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/1888641323361342824&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://zhuanlan.zhihu.com/p/1888641323361342824&lt;/a&gt;" data-tooltip-richtext="1" data-tooltip-preset="white" data-tooltip-classname="ztext-reference-tooltip"><a id="ref_3_0" href="https://zhuanlan.zhihu.com/p/1888311680880080185#ref_3" data-reference-link="true" aria-labelledby="ref_3">[3]</a></sup>

$$\mathrm{\mathbb{D}}_{KL}\left(\pi_{old}||\pi_{ref}\right)=\frac{\pi_{ref}(o_i|q)}{\pi_{old}(o_i|q)}-\log\frac{\pi_{ref}(o_i|q)}{\pi_{old}(o_i|q)}-1$$

> GRPO 中的KL散度常加在new model 和 ref model上，在off policy 训练过程中进行计算和更新。

6\. 重要性采样： 同PPO

7\. clip处理: 同PPO。\[**token维度clip,不区分token，固定对称clip范围**\]

### **涉及loss：**

**GRPO中不涉及value model的训练，所有训练过程中只涉及到actor loss计算和更新。**

 $$\begin{aligned}\mathcal{T}_{GRPO}(\theta)&=\mathbb{E}[q\sim P({Q}),\{o_i\}_{i=1}^G\sim\pi_{\theta_{old}}(O|q)]\\&=\frac{1}{G}\sum_{i=1}^G\left(\min\left(\frac{\pi_\theta(o_i|q)}{\pi_{\theta_{old}}(o_i|q)}A_i,\operatorname{clip}\left(\frac{\pi_\theta(o_i|q)}{\pi_{\theta_{old}}(o_i|q)},1-\varepsilon,1+\varepsilon\right)A_i\right)-\beta\operatorname{\mathbb{D}}_{KL}\left(\pi_\theta||\pi_{ref}\right)\right)\\ &=\left[\color{red}{\frac{1}{G}\sum_{i=1}^G\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}}\min\left(r_{i,t}(\theta)\hat{A}_{i,t},\mathrm{~clip}\left(r_{i,t}(\theta),1-\color{red}{\varepsilon_{\mathrm{low}}},1+\color{red}{\varepsilon_{\mathrm{high}}}\right)\hat{A}_{i,t}\right)\right] \end{aligned}$$

> Verl代码实现中，其实是按照 $\color{red}{\frac{1}{\sum_{i=1}^G|o_i|}\sum_{i=1}^G\sum_{t=1}^{|o_i|}}$ 方式进行计算的loss，即所谓的”token-level loss“。

___

## 3\. Dr.GRPO <sup data-text="灰一会：DAPO前必看的Dr.GRPO创新" data-url="https://zhuanlan.zhihu.com/p/1895798683888513144" data-numero="4" data-draft-node="inline" data-draft-type="reference" data-tooltip="灰一会：DAPO前必看的Dr.GRPO创新 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/1895798683888513144&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://zhuanlan.zhihu.com/p/1895798683888513144&lt;/a&gt;" data-tooltip-richtext="1" data-tooltip-preset="white" data-tooltip-classname="ztext-reference-tooltip"><a id="ref_4_0" href="https://zhuanlan.zhihu.com/p/1888311680880080185#ref_4" data-reference-link="true" aria-labelledby="ref_4">[4]</a></sup>

> 进行了大量实验,在小模型上进行了多个实验，为后续DAPO做了实验基础。

### **涉及数据**：同GRPO

### **涉及模型**：同GRPO

### 采样过程：同GRPO

### 涉及计算：

1.  reward: Rule reward 同 GRPO $$R(y)=\begin{cases}1, \text{y is correct},\\ 0, other\end{cases}$$

2\. Value【无】： GRPO作者认为，**value模型的效果好坏制约critic的训练效果，也限制模型潜力的发掘。所有在训练过程中丢弃了Value model的使用，所有GRPO中不存在Value值**。

3\. return【无】: 没有Value, **自然没有return值。**

**4\. clip: 同PPO、GRPO。**\[**token维度clip,不区分token，固定对称clip范围**\]

**5\. 优势函数：** GRPO是利用多个采样结果进行投票仲裁的方式进行计算，当前response的奖励相对均值的大小进行估计，而不是PPO的value等反推逻辑进行计算。作者认为在当问题过难或过容易时， $\frac{1}{std}$ 会变大，导致相应数据权重被加大，所以取消了PPO中常用的std归一化操作。**【只考虑当前step生成的response簇，当reward相同时，优势值为0,无效更新模型】**

$$A_i=r_i-mean(\{r_1,r_2,\cdots,r_G\})$$

> $r_i$ 表示每个prompt的采样G条response后对应第i条的rewards。

### 涉及Loss:

相对GRPO，认为序列的token平均，会受到不同长度response的长度影响，导致出现每个token的作用在整体loss中会有不同的权重，进而弱化或强调token的影响，所有将 $\color{red}{\frac{1}{|o_i|}}$ 去掉，利用最大response长度M进行统一加权平均。
$$\begin{aligned}\mathcal{T}_{Dr.GRPO}(\theta)&=\color{red}{\frac{1}{G\times M}\sum_{i=1}^G\sum_{t=1}^{|\mathbf{o}_i|}}\left\{\min\left[\frac{\pi_\theta(o_{i,t}|\mathbf{q},\mathbf{o}_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|\mathbf{q},\mathbf{o}_{i,<t})}\hat{A}_{i,t},\mathrm{clip}\left(\frac{\pi_\theta(o_{i,t}|\mathbf{q},\mathbf{o}_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|\mathbf{q},\mathbf{o}_{i,<t})},1-\varepsilon,1+\varepsilon\right)\hat{A}_{i,t}\right]\right\},.\end{aligned}$$

___

## 4.DAPO<sup data-text="灰一会：DAPO改进点梳理" data-url="https://zhuanlan.zhihu.com/p/1888956040369898387" data-numero="5" data-draft-node="inline" data-draft-type="reference" data-tooltip="灰一会：DAPO改进点梳理 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/1888956040369898387&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://zhuanlan.zhihu.com/p/1888956040369898387&lt;/a&gt;" data-tooltip-richtext="1" data-tooltip-preset="white" data-tooltip-classname="ztext-reference-tooltip"><a id="ref_5_0" href="https://zhuanlan.zhihu.com/p/1888311680880080185#ref_5" data-reference-link="true" aria-labelledby="ref_5">[5]</a></sup>

结合Dr.GRPO, 在GPRO的基础上进行改进。

### **涉及数据**：同GRPO

### **涉及模型**：同GRPO

### 采样过程：【动态采样】

-   【动态采样】利用old model对每个prompt采样一组G>1\[DAPO=16\]个response，**同时要求G个response中必须同时包含正负样本且根据Reward方式进行丢弃或加权，否则对应prompt重新采样\[限定次数\]或被直接丢弃\[DAPO做法\]，候补其他prompt进行采样**。【PPO是单个response即可\\GRPO没有特殊要求】

$$\color{red}{\mathrm{S}}\color{red}{.\mathrm{t.}\quad0<\left|\{o_i\mid {is\_equivalent}(a,o_i)\}\right|<G}$$

> $o_i$ 表示G个采样中第i个的response。
> 
> a表示对应prompt的global label。
> 
> "is\_equivalent"表示是response结果和global label相同，即是否为正确的response。 $\left|\{o_i\mid{is\_equivalent}(a,o_i)\}\right|$ , 表示在G个采样response中，response为正确结果的个数。
> 
> 该公式表示，G个采样response 不能全部是True，也不能全部是False的response。否则需要进行“丢弃prompt替补新prompt”。

### 涉及计算：

1.  **reward**: 在GRPO的基础上，**增加对长度惩罚项或超长过滤。**

	1.  overlong filter: 超过最大长度的response直接丢弃，或在训练计算loss时利用mask方式将其设置为0\[常用的隐形丢弃方法，好处可以保持batch\_size稳定\]。
	2.  Soft Overlong Punishment \[软超长惩罚机制\]：对过长内容进行惩罚，作为reward的一部分。 详情看<sup data-text="DAPO改进点梳理 " data-url="https://zhuanlan.zhihu.com/p/1888956040369898387" data-numero="6" data-draft-node="inline" data-draft-type="reference" data-tooltip="DAPO改进点梳理  &lt;a href=&quot;https://zhuanlan.zhihu.com/p/1888956040369898387&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://zhuanlan.zhihu.com/p/1888956040369898387&lt;/a&gt;" data-tooltip-richtext="1" data-tooltip-preset="white" data-tooltip-classname="ztext-reference-tooltip"><a id="ref_6_0" href="https://zhuanlan.zhihu.com/p/1888311680880080185#ref_6" data-reference-link="true" aria-labelledby="ref_6">[6]</a></sup>$$\begin{aligned}R_{\mathrm{length}}(y)=\begin{cases}0,&|y|\leq L_{\mathrm{max}}-L_{\mathrm{cache}}\\\frac{(L_{\mathrm{max}}-L_{\mathrm{cache}})-|y|}{L_{\mathrm{cache}}},&L_{\mathrm{max}}-L_{\mathrm{cache}}<|y|\leq L_{\mathrm{max}}\\-1,&L_{\mathrm{max}}<|y|&\end{cases}\end{aligned}$$
-   $\left|y\right|$ : response 生成长度。
-   $L_{max}$ : 最大生成长度。比如DAPO中设置为2024\*20
-   $L_{cache}$ : 完全惩罚为-1的缓冲长度。比如DAPO中设置为2024\*4

2. **KL惩罚**： **删除KL惩罚**，也就是verl设置为use\_kl\_loss=False,且 kl\_coef\[ $\beta$ \]=0.0。

base模型的分布和期待具有思考能力的模型分布存在明显差异，KL不应该限制其模型的探索空间。

3\. **Clip处理**：\[Clip-high\]**扩大上限，增加低分区间上限多样性的探索空间。\[token维度clip,不区分token，固定非对称clip范围\]**

$$\mathrm{~clip}{\left(r_{i,t}(\theta),1-\color{red}{\varepsilon_{low}},1+\color{red}{\varepsilon_{high}}\right)}$$

PPO、GPRO中常将 $\mathrm{~clip}{\left(r_{i,t}(\theta),1-\color{red}{\varepsilon},1+\color{red}{\varepsilon}\right)}$ 上下限等宽，常设置 $\varepsilon=0.2$ ，也就是上下限分别为0.8和1.2。

-   **低分时**，假如 $\pi_{\theta_{old}} = 0.01$ 时，$\pi_{\theta}$ 对应不截取的下限为 0.008 和 上限为0.012。此时 $\pi_{\theta_{old}}$ 低分时，对模型分值区间有明显限制，**也就是模型在低分段探索空间上限被限制明显，即限制了低分段的“多样性”探索空间**。
-   **高分时**，假如 $\pi_{\theta_{old}} = 0.9$ 时， $\pi_{\theta}$ 对应不截取的上限为 0.072 和 上限为1.08。此时 $\pi_{\theta_{old}}$ 高分时，对分值大小没有明显约束，所有探索空间更大。

可以看出 $\pi_{\theta_{old}}$ 从0.01到增加到0.9时，**下限**一直在0.072下面徘徊，也就是对应 $\pi_{\theta}$ 低分值比较友好，但是**上限**却从0.012跨度到1.08。也就是当 $\pi_{\theta_{old}}$ 越低时，允许模型在低分值的探索空间上限过小。所以**有必要提升clip的上限，增加低分段的探索空间上限。**

**所以采用**分别设定限制的方式，即 $\mathrm{~clip}{\left(r_{i,t}(\theta),1-\color{red}{\varepsilon_{low}},1+\color{red}{\varepsilon_{high}}\right)}$ ，DAPO中

-   $\varepsilon_{low}=0.2,$ 与PPO、GRPO保持相同\[因为clip下限对探索空间影响不大\[值非常小\]，所有无需变大\]。
-   $\varepsilon_{high}=0.28,$ 提高clip的上限，提升低分空间的多样性探索空间。

3\. **其他同GRPO，如无value、无return、优势函数计算、重要性采样都相同。**

> **优于动态采样中已经将reward相同的response簇丢弃，所有采样后参与模型更新的advantage绝大多数非零（当reward定于复杂时，可能簇中部分response将归为0）**

### **涉及loss：** token-level loss

$$\begin{aligned}\mathcal{J}_{\mathrm{DAPO}}(\theta)&=\quad\mathbb{E}_{(q,a)\thicksim\mathcal{D},\{o_i\}_{i=1}^G\thicksim\pi_{\theta_{\mathrm{old}}}}(\cdot|q)\\ &=\left[\color{red}{\frac{1}{\sum_{i=1}^G|o_i|}\sum_{i=1}^G\sum_{t=1}^{|o_i|}}\min\left(r_{i,t}(\theta)\hat{A}_{i,t},\mathrm{~clip}\left(r_{i,t}(\theta),1-\color{red}{\varepsilon_{\mathrm{low}}},1+\color{red}{\varepsilon_{\mathrm{high}}}\right)\hat{A}_{i,t}\right)\right]\end{aligned}$$
$$\color{red}{\mathrm{S}}\color{red}{.\mathrm{t.}\quad0<\left|\{o_i\mid {is\_equivalent}(a,o_i)\}\right|<G}$$

**论文中提到DAPO是token-level loss计算，GRPO是sample-level loss计算，但在VERL代码中实际上GRPO、PPO也是token level计算。**

### **5\. GSPO**

> **在单一模型上进行验证，与PPO、GRPO、DAPO、Dr.GRPO主要区别在于clip从token维度转换为sequence维度，并针对MOE做了router.**

### **涉及数据**：同GRPO、Dr.GRPO、DAPO

### **涉及模型**：同GRPO、Dr.GRPO、DAPO

### 采样过程：同GRPO、Dr.GRPO, 区别于DAPO动态采样[全1/0]

### **涉及计算：**

1.  **reward，advantage 与GRPO、Dr.GRPO等相同，未像DAPO对长度进行惩罚，无value、无return。\[advantage 计算仍只考虑当前step的reward，会出现advantage为零的response簇。进而进行无效模型更新。\]**
2.  **clip:** \[sequence维度clip, 不区分response，固定非对称clip范围\],两个变型：

1\. 以sequence为单位，整体进行clip剪切$$\begin{align} s_i(x) &= exp\left(\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}log\frac{\pi_\theta(o_{i,t}|\mathbf{q},\mathbf{o}_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|\mathbf{q},\mathbf{o}_{i,<t})}\right)\end{align} \Rightarrow  1-\epsilon_{low} \le  s_i(x) \le 1+\epsilon_{high}$$对应的loss:
$$\begin{align} \mathcal{T}_{\mathrm{GSPO}}\left(\theta\right)=&\frac{1}{G}\sum_{i=1}^G\min\left( s_{i}\left(\theta\right)\hat{A}_{i}, \text{clip}(s_{i}\left(\theta\right),1-\color{red}{\varepsilon_{\mathrm{low}}},1+\color{red}{\varepsilon_{\mathrm{high}}})\hat{A}_{i} \right) \end{align}$$

2\. 为token为单位，就是将对应的 $s_i$ 剔除梯度，并将token自身对大小做归一。

$$\begin{align} s^i_t(x) &= exp\left[\pi_\theta(o_{i,t}|\mathbf{q},\mathbf{o}_{i,<t})-detach[\pi_\theta(o_{i,t}|\mathbf{q},\mathbf{o}_{i,<t})]+\color{red}{detach\left(\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}log\frac{\pi_\theta(o_{i,t}|\mathbf{q},\mathbf{o}_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|\mathbf{q},\mathbf{o}_{i,<t})}\right)}\right]\end{align} \Rightarrow  1-\epsilon_{low} \le  s^i_t(x) \le 1+\epsilon_{high}$$ 可以看出这里面只是将其sequence的重要权重等价分发到每个token上。对应loss也是采取sequence-level$$\begin{align} \mathcal{T}_{\mathrm{GSPO}}\left(\theta\right)=\left[\color{red}{\frac{1}{G}\sum_{i=1}^G\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}}\min\left(s_{i,t}(\theta)\hat{A}_{i,t},\mathrm{~clip}\left(s_{i,t}(\theta),1-\color{red}{\varepsilon_{\mathrm{low}}},1+\color{red}{\varepsilon_{\mathrm{high}}}\right)\hat{A}_{i,t}\right)\right]  \end{align}\tag{GSPO-token-loss}$$

> 对应 $\color{red}{\varepsilon_{\mathrm{low}}}=0.0003, \color{red}{\varepsilon_{\mathrm{high}}}=0.0004$ 相对token-level 切割会非常小，可以等价于token-level/response长度的平均。
> 
> **sequence-level clip从sequence维度进行切割，避免高熵sequence参与模型训练，提高训练的稳定性。笔者认为RL是为了充分发挥模型自身多样性探索的自身能力，而高熵sequence携带的信息有利于模型多样性的探索，更激进\[高熵\]的sequence更有力RL训练。所以GSPO在一定程度上能够提高稳定性，但限制模型训练的能力。**

**6\. DCPO**<sup data-text="DCPO" data-url="https://huggingface.co/papers/2509.02333" data-numero="7" data-draft-node="inline" data-draft-type="reference" data-tooltip="DCPO &lt;a href=&quot;https://huggingface.co/papers/2509.02333&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://huggingface.co/papers/2509.02333&lt;/a&gt;" data-tooltip-richtext="1" data-tooltip-preset="white" data-tooltip-classname="ztext-reference-tooltip"><a id="ref_7_0" href="https://zhuanlan.zhihu.com/p/1888311680880080185#ref_7" data-reference-link="true" aria-labelledby="ref_7">[7]</a></sup>

> **在qwen2.5系列上四个模型进行验证，并对比了GRPO\\DAPO\\GSPO,DCPO更加有效。**
> 
> **主要解决主流RL中普遍存在的问题：1）固定CLIP范围对低概率token不友好，导致模型在高熵”rare token“上探索能力不足；2）现有advantage计算只考虑当前生成的response簇，而生成过程中存在很大随机性，导致相同reward时梯度为零以及随机性生成的response归一化后的advantage会波动更大，对模型训练稳定性不友好问题。**

### **涉及数据**：同GRPO、Dr.GRPO、DAPO

### **涉及模型**：同GRPO、Dr.GRPO、DAPO

### 采样过程：同GRPO、Dr.GRPO, 区别于DAPO动态采样

### **涉及计算：**

1.  **无value、无return**。同RLVR系列\[GRPO\\DAPO\\Dr.GRPO\\GSPO\]
2.  **reward**: 区分引起答案错误原因：是格式错误还是答案错误。【多个实验表明其实这个影响不大】
$$R_j^i = \begin{cases}         1, & \text{答案和格式正确} \\         0, & \text{格式正确，答案不正确} \\         -1, & \text{格式错误}     \end{cases}$$ 3. **advantage 【**Smooth Advantage Standardization**】: 针对当前reward相同导致advantage为零以及随机采样对相同reward在不同step中抖动问题，考虑累计生成reward的归一化进行平滑**
$$\hat{A}_{new,j}^i=\frac{\left(R^i_j-\mu_{new}^i\right)}{\sigma_{new}^i}$$$$\hat{A}_{total,j}^i=\frac{\left(R^i_j-\mu_{total}^i\right)}{\sigma_{total}^i}$$ 对于同一个prompt，基于当前reward计算advantage和基于累计reward计算advantage。利用步数加权。
$$\begin{align} \hat{SA}^i_{new,j} = \frac{i-1}{i}\hat{A}_{new,j}^i + \frac{1}{i}\hat{A}_{total,j}^i\\ \hat{SA}^i_{total,j} = \frac{1}{i}\hat{A}_{new,j}^i + \frac{i-1}{i}\hat{A}_{total,j}^i \end{align} \tag{smooth-advantage}$$ 为稳定advantage波动性，参与模型训练的advantage取其绝对值最小值。

$$\hat{A}^i_j=\begin{cases} \hat{SA}^i_{new,j} , & \text{when} \ |\hat{SA}^i_{new,j}| < |\hat{SA}^i_{total,j}|\\         \hat{SA}^i_{total,j} , & \text{otherwise} \end{cases}         \tag{SAS}$$ **好处是**：当当然reward相同时，将以 $\frac{1}{i}\hat{A}_{total,j}^i$ 权重参与模型更新过程中，即保留了response参与模型更新，又限制了其以很小的权重参与模型更新。

4\. **CLIP(Dynamic-Adaptive Clipping):** **从动态采样方差的偏差出发，推到出对应不同概率的上下限范围，使clip剪切范围随着概率的变化成反比变化，在低旧概率时，可参与模型训练的新概率范围越大，这对模型在低概率高熵域有极大的探索空间。即有理论支持，同时更符合实际使用意义。具体推到可参考**[LLMs-RL中off-policy为什么要clip以及常见变体？(PPO\\GRPO\\DAPO\\DC...](https://zhuanlan.zhihu.com/p/1945898330975630395) 和[零梯度、零剪枝！DCPO带你玩转强化学习](https://zhuanlan.zhihu.com/p/1945134097014960877)。 【token维度，动态区间，更好学习rare token的信息。】

![](https://pic1.zhimg.com/v2-34512864d5a7b582d353351f12788bba_1440w.jpg)

固定clip和动态clip对比

$\begin{align} 0.5+\frac{1}{2}\sqrt{\max\left(1-\frac{4\epsilon_{low}}{q\left(x\right)},\ 0\right)}\le&r\left(x\right) \le 0.5+\frac{1}{2}\sqrt{1+\frac{4\epsilon_{high}}{q\left(x\right)}} \end{align}\tag{DAC}$

经典设置： $\epsilon_{low}=0.16,\epsilon_{high}=0.2, r(x)_{max}=10$

> 从图中可以看出，q(x)越小，可用的更新范围越大。比如：q(x)=0.9和q(x)=0.01时。
> 
> 固定的clip时， p(x)对应的上下限为0.72-1.08，和 0.008-0.012.
> 
> 动态的clip时，p(x)对应的上下限为0.69-1.06， 和 0.005, 0.05. 在概率小的位置，上限更大。

5\. loss: 在GRPO的sequence-level loss基础上删除batch之间的平均，因为在计算Advantage标准化时已经获得了标准化关系，所有response如果进行batch平局，相应的标准化结果将被削弱，在模型更新时未充分发挥。【维持advantage的关系，充分发挥其作用】

$\begin{aligned}\mathcal{T}_{DCPO}(\theta)&=\frac{1}{G}\sum_{i=1}^G\sum_{t=1}^{|\mathbf{o}_i|}\left\{\min\left[\frac{\pi_\theta(o_{i,t}|\mathbf{q},\mathbf{o}_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|\mathbf{q},\mathbf{o}_{i,<t})}\hat{A}_{i,t},\mathrm{clip}\left(\frac{\pi_\theta(o_{i,t}|\mathbf{q},\mathbf{o}_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|\mathbf{q},\mathbf{o}_{i,<t})},1-\varepsilon_{low},1+\varepsilon_{high}\right)\hat{A}_{i,t}\right]\right\},.\end{aligned}\tag{DCPO_loss}$

> DCPO从clip\\advantage\\loss三个方面对前面对算法进行改进，且具有理论支持和通用性。效果更加。

## 参考

1.  [^](https://zhuanlan.zhihu.com/p/1888311680880080185#ref_1_0)灰一会：AC-GAE-PPO重要性采样及CLIP疑问 [https://zhuanlan.zhihu.com/p/30980234155](https://zhuanlan.zhihu.com/p/30980234155)
2.  [^](https://zhuanlan.zhihu.com/p/1888311680880080185#ref_2_0)灰一会：DeepSeek-V3关键点 [https://zhuanlan.zhihu.com/p/1885375635846833438](https://zhuanlan.zhihu.com/p/1885375635846833438)
3.  [^](https://zhuanlan.zhihu.com/p/1888311680880080185#ref_3_0)灰一会：PPO、GRPO等RL中的KL散度计算方法即实现 [https://zhuanlan.zhihu.com/p/1888641323361342824](https://zhuanlan.zhihu.com/p/1888641323361342824)
4.  [^](https://zhuanlan.zhihu.com/p/1888311680880080185#ref_4_0)灰一会：DAPO前必看的Dr.GRPO创新 [https://zhuanlan.zhihu.com/p/1895798683888513144](https://zhuanlan.zhihu.com/p/1895798683888513144)
5.  [^](https://zhuanlan.zhihu.com/p/1888311680880080185#ref_5_0)灰一会：DAPO改进点梳理 [https://zhuanlan.zhihu.com/p/1888956040369898387](https://zhuanlan.zhihu.com/p/1888956040369898387)
6.  [^](https://zhuanlan.zhihu.com/p/1888311680880080185#ref_6_0)DAPO改进点梳理  [https://zhuanlan.zhihu.com/p/1888956040369898387](https://zhuanlan.zhihu.com/p/1888956040369898387)
7.  [^](https://zhuanlan.zhihu.com/p/1888311680880080185#ref_7_0)DCPO [https://huggingface.co/papers/2509.02333](https://huggingface.co/papers/2509.02333)



