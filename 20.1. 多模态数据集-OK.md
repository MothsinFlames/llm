---
source: "https://zhuanlan.zhihu.com/p/716631227"
---
### Infinity-MM

- 发布时间：2024-10-25
- 发布机构：北京智源人工智能研究院
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/4de8402ec6a3cc4e12bf8d4e0c9eb8cf)
- 数据集说明：2024年10月25日，智源研究院发布并开源了千万级多模态指令数据集 **Infinity-MM** ，包含数千万个样本，数据规模达 **4300万** 条，数据量达 **10TB** ，通过质量过滤和去重，确保了其数据的高质量和多样性。
- 此外，智源提出的基于开源模型和标签体系的合成数据生成方法，能够生成高质量的指令数据并有效地扩大指令数据集的规模。基于 Infinity-MM成功训练了一个 20 亿参数的多模态模型 Aquila-VL-2B，在同规模模型中取得了最先进的性能。

### PCLMM

- 发布时间：2024-09-08
- 发布机构：大连理工大学
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/692b194d2e2acebc606a854eb8d47f5a)
- 数据集说明：PCLMM数据集是由大连理工大学创建的，专门用于检测中文视频中的居高临下和轻蔑语言（PCL）。
- 该数据集包含715个从Bilibili平台收集并标注的视频，总时长超过21小时。数据集的创建过程包括定义PCL的语义、关键词列表的编译、视频的收集和手动标注。PCLMM数据集的应用领域主要集中在自动检测视频平台上的微攻击行为，旨在保护弱势群体免受歧视性语言的伤害。

### ARIO (All Robots In One)

- 发布时间：2024-08-20
- 发布机构：鹏城实验室、南方科技大学、中山大学
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/68e066b815bbb802c1f587885fd12a34)
- 数据集说明：ARIO（All Robots In One）具身大规模数据集，该数据集包含了 2D、3D、文本、触觉、声音 5 种模态的感知数据，涵盖操作和导航两大类任务，既有仿真数据，也有真实场景数据，并且包含多种机器人硬件，有很高的丰富度。
- 在数据规模达到三百万的同时，还保证了数据的统一格式，是目前具身智能领域同时达到高质量、多样化和大规模的开源数据集。

### SkyScript-100M

- 发布时间：2024-08-18
- 发布机构：Skywork AI、华中科技大学
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/7e726c71b0e94cf58d5ef30e94709716)
- 数据集说明：SkyScript-100M 是一个专注于短剧视频制作的大规模多模态数据集。该数据集汇集了来自互联网的6660部流行短剧，总计约80,000集，总时长超过2000小时，数据量高达10TB。通过关键帧提取与注释，SkyScript-100M提供了10亿对高质量的短剧本与拍摄脚本，为短剧视频生成领域提供了丰富的资源。

### MedTrinity-25M

- 发布时间：2024-08-06
- 发布机构：华中科技大学、加州大学、哈佛大学、斯坦福大学
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/86cac9ab58e2bdedcec5b32df3521e08)
- 数据集说明：MedTrinity-25M是一个大规模多模态医学数据集，包含超过2500万张图像，涉及10种模态和65种疾病。数据集通过自动化的数据构建流程生成，不依赖于配对的文本描述，而是通过专家模型和知识库增强的多模态大型语言模型生成多粒度视觉和文本注释。数据集的创建过程包括从90多个在线资源收集数据，应用专家模型识别感兴趣区域（ROIs），并构建知识库以生成详细的文本描述。MedTrinity-25M旨在支持广泛的医学多模态任务，如图像标注和报告生成，以及视觉中心的任务如分类和分割，推动医学领域基础模型的发展。

### MMEarth - 遥感多模态预训练数据集

- 发布时间：2024-07-29
- 发布机构：哥本哈根大学
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/e92969bbb9e8b862bc25850053ab56f6)
- 数据集说明：MMEarth 是哥本哈根大学构建的一个全球范围的多模态预训练数据集，覆盖了120万个地理位置，包含12种不同的模态数据。MMEarth利用地理位置和时间自动对齐不同传感器的数据，几乎无需人工干预。MMEarth的设计目标是用于全球范围内的环境监测和遥感任务。

### Medical-CXR-VQA

- 发布时间：2024-07-15
- 发布机构：得克萨斯大学、理化学研究所、国立卫生研究院、日本癌症研究中心、东京大学
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/0d097c5b83b4f806eb89328a385aaf9f)
- 数据集说明：Medical-CXR-VQA是一个大规模医学多模态数据集。其基于MIMIC-CXR数据集构建，该数据集包括在215,547张胸部X光片上的780,014个问题答案对，问题含盖异常，存在，位置，级别，拍摄角度和类型，共7种类型的问题。

### MMSci

- 发布时间：2024-07-06
- 发布机构：加利福尼亚大学等
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/dbc1b9ccfe6668a3a9947ec829b32ee1)
- 数据集说明：MMSci数据集是一个多模态、多学科的高质量学术文章和图表集合，涵盖72个科学领域。数据集包含131,393篇文章和742,273个图表，主要来源于Nature Communications期刊。创建过程中，数据集通过爬取开放获取的文章和图表，确保了数据的真实性和高质量。该数据集主要用于评估和提升大型多模态模型在科学领域的理解和应用能力，特别是在理解和生成科学图表方面。

### OBIMD

- 发布时间：2024-07-04
- 发布机构：安阳师范学院、腾讯等
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/9de01e0f6fe1edda1af326d07f66a014)
- 数据集说明：甲骨文多模态数据集（ Oracle Bone Inscriptions Multi-modal Dataset - OBIMD）共包含一万片甲骨的拓片、摹本，甲骨单字对应位置、对应字头、对应释文以及辞例分组、释读顺序等数据。据介绍，所有研究者都能基于该数据集研发甲骨文检测、识别、摹本生成、字形匹配和释读等算法，加速甲骨文研究智能化进程。

### OpenVid-1M

- 发布时间：2024-07-02
- 发布机构：南京大学、字节跳动、南开大学
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/a850da6cc7ad394a696787f606e181dd)
- 数据集说明：OpenVid-1M是由南京大学、字节跳动和南开大学联合创建的一个大规模高质量文本到视频生成数据集。该数据集包含超过100万个视频片段，每个视频具有至少512×512的高分辨率，并配有详细的字幕。数据集的创建过程严格筛选了美学、时间一致性、运动差异和清晰度等方面，确保了视频的高质量。OpenVid-1M主要应用于文本到视频生成领域，旨在解决现有数据集质量不高或过于庞大的问题，推动高清晰度视频生成技术的发展。

### MINT-1T

- 发布时间：2024-06-17
- 发布机构：华盛顿大学、Salesforce Research、斯坦福大学
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/7e5508a5346b7c581926a1afe92bc1f3)
- 数据集说明：MINT-1T是由华盛顿大学和Salesforce Research合作创建的开放源代码多模态交错数据集，包含一万亿文本令牌和三十亿图像，是目前最大且最多样化的开放源代码多模态交错数据集。数据集内容丰富，涵盖HTML、PDF和ArXiv等多种来源，旨在通过提供大规模、多样化的训练数据，推动前沿大型多模态模型（LMMs）的发展，解决现有开放源代码多模态数据集规模和多样性不足的问题。

### VEGA

- 发布时间：2024-06-14
- 发布机构：厦门大学
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/4c4b09d826442dfee1eacf59822b6813)
- 数据集说明：VEGA 是一个专注于科学论文理解的多模态数据集，它由厦门大学纪荣嵘团队于 2024 年提出，并被设计用于评估和提升模型在处理包含复杂图文交错信息的输入时的表现，相关论文为「VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language Large Models」。该数据集包含超过 50,000 篇科学论文的图文数据，并且特别为交错图文阅读理解（Interleaved Image-Text Comprehension, IITC）任务而构建。 VEGA 数据集的构建过程包括问题筛选、上下文构建和答案修改三个步骤，旨在提供更长、更复杂的图文交错内容作为输入，并要求模型在回答时指明参考的图片。

### OmniCorpus

- 发布时间：2024-06-12
- 发布机构：上海人工智能实验室、哈尔滨工业大学、南京大学、复旦大学等
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/9731b275203407ee4315c98224f24df3)
- 数据集说明：OmniCorpus由上海人工智能实验室联合多所知名高校及研究机构共同构建，是迄今为止最大的多模态数据集。该数据集包含了86亿张图像和1696亿个文本Token，支持中英双语。与现有的数据集相比，其在以下方面具有显著优势：1）更大的数据规模：与之前最大的多模态数据集LAION-5B相比，OmniCorpus的数据集在图像方面大了1.7倍，在文本方面大了12.5倍，同时保持了出色的数据质量。2）更丰富的数据多样性：从更广泛的数据源中提取数据，OmniCorpus数据集比其他图像-文本交错数据集更具多样性。它包括中英文双语多模态数据，并包括从常见网站和视频平台提取的以文本为中心和以视觉为中心的文档。3）更灵活的格式：OmniCorpus的流式数据格式提供了非凡的灵活性，允许适应各种数据结构，包括纯文本语料库、图像-文本对和交错数据格式。

### ShareGPT4Video

- 发布时间：2024-06-06
- 发布机构：ShareGPT4V
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/281f8c9e82a472f0153c45421dbbc934)
- 数据集说明：ShareGPT4Video数据集是一个大型“视频-文本描述”数据集，其中包括4万条（共291小时）由GPT-4V标注的视频数据。这些数据涵盖了广泛的类别，生成的描述包含丰富的世界知识、对象属性、摄像机运动，以及详细和精确的事件时间描述。描述文本的字数主要在200-400之间，提供了丰富的时间信息，可以很好地完成视频理解和生成任务。为了进一步扩大数据集规模，以及便于开源社区在自有数据上的使用，在ShareGPT4Video数据集的基础上，研究者们进一步设计开发了ShareCaptioner-Video，一个能够有效地为任意视频生成高质量描述的多功能多模态大模型。在得到了优异的视频描述模型后，研究者们用它进一步标注了480万条，总时长3000小时的丰富的视频数据ShareGPT4Video 4.8M。

### Touch100k

- 发布时间：2024-06-06
- 发布机构：北京交通大学、腾讯微信AI团队、北京邮电大学
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/1799010657012625409)
- 数据集说明：Touch100k数据集是北京交通大学联合腾讯微信AI团队及北京邮电大学构建的一个大规模触觉-语言-视觉多模态数据集。该数据集包含了10万个与触觉、视觉和语言描述相关联的样本，这些样本描述了不同粒度的触觉感受，比如句子级别的自然表达和短语级别的关键特征描述。研究人员首先从公开的触觉数据集中收集和整理了视觉-触觉观察结果，然后使用GPT-4V生成了多粒度的文本描述，并通过多步骤的质量增强过程确保了数据的准确性和实用性。Touch100k数据集以其丰富的触觉感知描述，为机器人学和人工智能领域提供了宝贵的资源。

### Video-MME

- 发布时间：2024-06-03
- 发布机构：北京大学、香港大学等
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/488cb10044aec2e9714e8a4eccc3b3b0)
- 数据集说明：Video-MME是北京大学、香港大学等6所高校联手，发布的首个专为视频分析设计的多模态大模型评估基准。该数据集包含900个视频，总时长达256小时，研究人员通过反复观看视频内容，手动选择和注释共设计了2,700个高质量的多选题。数据集涵盖6大视觉领域，包括知识、电影与电视、体育竞赛、艺术表演、生活记录和多语言，并进一步细分为天文学、科技、纪录片等30个类别，视频长度从11秒到1小时不等。此外，Video-MME还整合字幕和音频轨道，增强了对视频理解的多模态输入分析。更难能可贵的是，Video-MME中所有数据，包括问答、视频、字幕和音频，都是手工收集和整理的，确保了该基准的高质量。该数据集的创建不仅为研究人员提供了一个富有挑战性的测试基准，也为研究外部信息对视频理解性能的影响提供了宝贵的资源。

### MultiOOD

- 发布时间：2024-05-27
- 发布机构：苏黎世联邦理工学院、南加州大学、洛桑联邦理工学院
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/6965000668c1a3f1399176a6f99f4033)
- 数据集说明：MultiOOD基准是由苏黎世联邦理工学院、南加州大学和洛桑联邦理工学院的研究人员联合创建的多模态异常检测数据集。该数据集数据源于五个公开的动作识别数据集（HMDB51、UCF101、EPIC-Kitchens、HAC 和 Kinetics-600），共计超过85,000个视频片段，这些数据集在类别数量和大小上各不相同，类别数从7到229不等，数据集大小从3,000到57,000不等。该数据集使用了视频、光流和音频作为不同的模态类型。MultiOOD是一个创新的基准数据集，它通过结合多种类型的数据（视频、光流和音频），为研究人员提供了一个更为全面的数据资源来开发和测试异常检测类算法。

### MagicBathyNet - 多模态遥感基准数据集

- 发布时间：2024-05-24
- 发布机构：柏林工业大学等
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/5c9bd2b05c672245d396b823d4ca86e2)
- 数据集说明：该数据集整合了Sentinel-2、SPOT-6卫星图像及航空影像的图像块，同时涵盖栅格格式的水深信息与海底类型注释，专为浅水区域的水深预测与像素级分类研究设计。MagicBathyNet数据集覆盖了两个截然不同的沿海区域：塞浦路斯的Agia Napa地区和波兰的Puck Lagoon地区，这些区域在水体特征和海底类型上有很大的差异。数据集包含3355组Sentinel-2、SPOT-6及航空影像的RGB共注册三元组，辅以1244组双影像对，3354幅航空影像对应的DSM（数字表面模型）栅格图块，以及3396幅Sentinel-2与SPOT-6对应的DSM图像块。此外，还包括533幅用于监督式像素分类的海底生境标注图块，每块图像对应180x180米实地范围，分别以18x18、30x30、720x720像素分辨率呈现于不同来源影像中。MagicBathyNet数据集旨在促进浅水区环境的精细解析，为海洋地质学、生态监测及水下地形建模提供了宝贵的资源。

### RLAIF-V-Dataset

- 发布时间：2024-05-19
- 发布机构：OpenBMB
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/ca46fc1930471cb025231f286ad6dd52)
- 数据集说明：RLAIF-V-Dataset是OpenBMB构建的一个大规模多模态偏好数据集。该数据集是由AI生成的偏好数据集，涵盖各种任务和领域，包含44,757组高质量对比对。RLAIF-V-数据集通过一个新颖的方法，采用开源大模型来对模型响应进行去混杂处理，并提供高质量的反馈。该数据集应用在了MiniCPM-Llama3-V 2.5模型的训练中，MiniCPM-Llama3-V 2.5 是第一个具有 GPT-4V 性能的端侧多模态大模型。RLAIF-V-Dataset数据集可以有效减少多模态大模型的幻觉。

### AnyWord-3M

- 发布时间：2024-04-18
- 发布机构：阿里巴巴
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/035bf10bda3f3d3cd96a9178fa083566)
- 数据集说明：目前，针对文字生成任务的公开数据集尤其是涉及非拉丁语系语言的，还相对缺乏。因此，我们提出了一个大规模多语言数据集AnyWord-3M。数据集中的图片的来源包括Noah-Wukong、LAION-400M以及用于OCR识别任务的数据集，如ArT、COCO-Text、RCTW、LSVT、MLT、MTWI、ReCTS等。这些图片涵盖了包含文本的多种场景，包括街景、书籍封面、广告、海报、电影帧等。除了OCR数据集直接使用标注的信息外，所有其他图片都通过使用PP-OCR的检测和识别模型进行处理。然后，使用BLIP-2生成文本描述。通过严格的过滤规则和细致的后处理，我们共获得了3,034,486张图片，包含超过900万行文本和超过2000万个字符或拉丁文字。

### RELI11D

- 发布时间：2024-03-28
- 发布机构：厦门大学、上海科技大学
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/b3b15f0b2c3df244742d727d761c08d5)
- 数据集说明：针对复杂且快速的全局人体动作捕捉问题，厦门大学联合上海科技大学基于激光雷达、IMU、RGB相机和事件相机构建了多模态人体运动数据集-RELI11D。该数据集包含10名采集者在7个不同的真实体育场景中进行的5项体育运动（乒乓球、跆拳道、拳击、击剑和羽毛球）的3.32小时同步LiDAR点云、IMU测量数据、RGB视频和事件流。总计包含199.26分钟的视频数据，涵盖了239k帧的人体点云数据。RELI11D数据集是一个高质量的多模态人体运动数据集，为人体运动估计任务提供了丰富的基准测试数据，它通过多模态数据的融合，为全面理解人体运动提供了新的视角。

### NineRec

- 发布时间：2024-03-17
- 发布机构：西湖大学
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/71e7c28525a727f8495f8a810d0c6422)
- 数据集说明：NineRec是西湖大学提出的一个大规模、多样性的推荐系统评估基准数据集，旨在解决推荐系统领域迁移学习模型发展的瓶颈问题，尤其是缺乏大规模、高质量的迁移学习推荐数据集和基准测试套件。NineRec包含一个大规模源域数据集和九个多样化的目标域数据集，涵盖短视频、新闻、图像等多种类型的原始内容。每条数据均配有描述性文本和高分辨率封面图像，使得模型能够通过学习原始多模态特征而非仅依赖预提取的特征来进行训练。NineRec的丰富视觉与语义多样性，为推荐模型的可迁移性研究提供了宝贵的预训练资源，同时揭示了TransRec模型在跨界推荐任务中的潜力与挑战。

### AlgoPuzzleVQA

- 发布时间：2024-03-13
- 发布机构：北新加坡科技设计大学
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/40e35c4affeaffcea7e12ac38351696b)
- 数据集说明：AlgoPuzzleVQA是由新加坡科技设计大学构建的一个多模态推理数据集，旨在挑战和评估多模态语言模型在解决需要视觉理解、语言理解和复杂算法推理的算法谜题方面的能力。数据集包含18种不同的谜题，涵盖了诸如布尔逻辑、组合学、图论、优化、搜索等多样化的数学和算法主题。该数据集通过自动化的方式从人类编写的代码生成谜题，确保了数据集可以任意扩展推理复杂性和数据集大小。这些谜题都是有确切解决方案的，可以通过算法找到，无需繁琐的人工计算。AlgoPuzzleVQA可以作为多模态推理能力的基准测试，用于评估和推动多模态语言模型在解决结合视觉、语言理解和算法推理的复杂问题方面的能力。

### MMRS - 多模态遥感指令跟随数据集

- 发布时间：2024-03-08
- 发布机构：北京理工大学
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/07b498df1cb6aa0d5f01dda3b657f899)
- 数据集说明：MMRS数据集，包含了100万多个图像-文本对，涵盖了分类、检测、图像描述、VQA、视觉定位等多个任务，并包括光学、红外和SAR三种视觉模态。该数据集旨在促进遥感领域中MLLMs的持续发展。

### MM-AU

- 发布时间：2024-03-01
- 发布机构：西安交通大学等
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/1828630866779267073)
- 数据集说明：MM-AU是一个多模态事故视频理解数据集，该数据集包含11,727个野外视角的事故视频，每个视频都配有时间上对齐的文本描述。该数据集标注了超过223万个目标框和58,650对基于视频的事故原因，涵盖了58种事故类别。MM-AU支持各种事故理解任务，特别是多模态视频扩散以理解事故的因果链，从而实现安全驾驶。

### MIntRec2.0

- 发布时间：2024-01-16
- 发布机构：清华大学等
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/8ac99e12b2ba237e544bfaedd082de8e)
- 数据集说明：MIntRec2.0是清华大学等提出的一个大规模多模态多方基准数据集，专门用于识别对话中的意图和检测非意图内容。相较于先前的MIntRec，MIntRec2.0的数据量增至15K，涵盖30种意图类别，并包含约9.3K个意图内及5.7K个意图外的标注语句，涉及文本、视频和音频等多种模态。该数据集由1,245个对话组成，每个对话平均12个语句，每个语句均配有意图标签，且每个对话至少涉及两位发言者，所有语句均标记发言者身份。此外，针对开放世界场景的需求，MIntRec2.0引入OOS标签，用于识别不属于已知意图类别的语句，以增强系统的鲁棒性。该数据集旨在促进多模态意图理解相关研究，为实现更自然的人机交互并通往AGI之路奠定坚实基础。

### CapsFusion-120M

- 发布时间：2024-01-08
- 发布机构：清华大学、北京智源人工智能研究院
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/6d39a9e737d0d45ffb8b344dd35ea9c1)
- 数据集说明：该数据集是清华大学和北京智源人工智能研究院于 2024 年推出的多模态图文数据集。该数据集可用于大规模多模态预训练的高质量资源。此版本包含来自 LAION-2B 和 LAION-COCO 数据集的相应字幕，便于进行比较分析和进一步深入研究图像文本数据的质量。每个数据条目有四个字段：图片网址、LAION-2B 标题（来自网络的原始替代文本）、LAION-COCO 字幕（由 BLIP 合成）、CapsFusion 标题（研究团队的）。

### Multimodal C4 (mmc4)

- 发布时间：2023-04-14
- 发布机构：加州大学、华盛顿大学、艾伦人工智能研究所
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/f3de660ac33a2c201d92822bf8001fa0)
- 数据集说明：Multimodal C4 的数据集，语料库包含 103M 文档，其中包含了 585M 张图片和 43B 个英文单词，这些图片和文字相互交织。通过该数据集进行训练，可以更好地实现多模态的上下文学习，这对于未来更加丰富的多模态语言技术的发展非常重要。此外，还对数据集进行了详细的分析和筛选，确保了其中的图片和文字具有高度相关性。

### ShareGPT4V

- 发布时间：2023-11-21
- 发布机构：中国科学技术大学、上海人工智能实验室
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/f96fe883c702ef7265b02c0ced9c7cfc)
- 数据集说明：ShareGPT4V 数据集是一个由大量图像-文本对组成的高质量数据集，它被用于训练视觉-语言模型 (VLM），以提高模型在图像理解和文本生成方面的能力。该数据集包含 120 万对图像-文本配对，这些数据有效地对齐了视觉和语言特征，增强了模型遵循指令的能力，并纳入了更多学术任务，例如 ScienceQA 、 TextVQA 、 SBU 等。通过引入这个数据集，模型在图像-文本对齐能力方面得到了显著提升，这对于多模态表示学习是一个关键方面。

### LAION-5B

- 发布时间：2022-03-31
- 发布机构：LAION
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/e7369c9c513952cdba90e48e27ae7a69)
- 数据集说明：LAION 5B 是一个用于研究目的的大规模图文数据集。由58.5亿个CLIP过滤的图像-文本对组成，其中包含23.2亿的英语，22.6亿的样本来自100多种其他语言，及12.7亿的未知样本。此外，发布方提供了几个最近邻索引、用于探索和子集创建的改进Web界面以及水印和NSFW的检测分数。

### WuDaoMM

- 发布时间：2022-03-22
- 发布机构：北京智源人工智能研究院
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/5d2b4ade53602c3c95bf37123264afe2)
- 数据集说明：WuDaoMM属于北京智源人工智能研究院WuDaoCorpora开源数据集的一部分。WuDaoMM是图文多模态预训练数据，全量数据集包含6.5亿图文对，为Wenlan、Cogview等大规模中文多模态预训练模型提供了数据支撑，数据集包含强相关数据5千万对和弱相关数据6亿对。目前智源开放了基础版WuDaoMM-base，该数据集是由强相关数据按照类别均衡抽取组成的，包含19个大类，分别为:能源、表情、工业、医疗、风景、动物、新闻、花卉、教育、艺术、人物、科学、大海、树木、汽车、社交、科技、运动等，单类别数据约7万~40万左右。

### TaiSu（太素）

- 发布时间：2022-02-10
- 发布机构：中国科学院、西安交通大学、北京师范大学
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/adc2ba9270396481e80f492c2d48dcb4)
- 数据集说明：TaiSu（太素）一个亿级大规模中文视觉语言预训练数据，该多模态数据集包含了1.66亿张图片和2.19亿条中文描述，是当时规模最大的开源中文多模态数据集，也是唯一一个为图像提供多个中文文本描述的数据集。

### Wukong

- 发布时间：2022-01-30
- 发布机构：华为诺亚方舟实验室
- 地址： [数据集地址](https://link.zhihu.com/?target=https%3A//www.selectdataset.com/dataset/4908425c4aaca5b4a47665bfee583846)
- 数据集说明：Wukong是由华为诺亚方舟实验室创建的大规模中文跨模态预训练数据集，包含1亿对中文图像-文本对，用于推动视觉-语言预训练研究。数据集通过高频中文词汇列表收集，覆盖广泛视觉和文本概念，适用于多种下游任务，如零样本图像分类和图像-文本检索，旨在解决中文环境下跨模态学习的挑战。