---
created: 2025-08-25T09:10:37 (UTC +08:00)
tags: [人工智能算法,大模型对齐]
source: https://zhuanlan.zhihu.com/p/952982823
author: 关于作者想飞的石头带着团队在agi大潮里做挖掘机的邱锡鹏、迷途小书僮、桂花糖也关注了他回答150文章110关注者20,848关注他发私信
---
## 直接偏好优化

受在大规模问题如微调语言模型时应用强化学习算法所面临挑战的启发，我们的目标是提出一种直接使用偏好进行策略优化的简单方法。与先前的 RLHF 方法不同，这些方法先学习一个奖励函数，然后通过 RL 进行优化，我们的方法利用了特定奖励函数参数化方式的选择，使我们能够以闭式形式提取其最优策略，而无需进行 RL 训练循环。正如我们将详细描述的那样，我们的关键洞察是利用奖励函数到最优策略的分析映射，从而将一个基于奖励函数的损失函数转换为基于策略的损失函数。这种变量变换的方法避免了显式拟合独立奖励模型的同时，仍然可以在现有的人类偏好模型（如 [Bradley-Terry 模型](https://zhida.zhihu.com/search?content_id=249139516&content_type=Article&match_order=1&q=Bradley-Terry+%E6%A8%A1%E5%9E%8B&zhida_source=entity)）下进行优化。本质上而言，策略网络同时代表了语言模型和（隐含）奖励。

我们从先前工作的相同 RL 目标出发（公式 3），在一般奖励函数 r 的情况下是显而易见的。遵循先前的工作的方法可以很容易地证明，在公式 3 中带有 KL 约束的最大化奖励目标的最佳解形式如下：

$\pi_r(y|x) = \frac{1}{Z(x)} \pi_{ref}(y|x) \exp\left(\frac{1}{\beta} r(x, y)\right),$

其中 $Z(x) = \sum_{y} \pi_{\text{ref}}(y | x) \exp\left(-\frac{1}{\beta} r(x, y)\right)$是分区函数。。即使我们使用地面真值奖励函数 $r^*$ 的 MLE 估计 $r^\phi$ ，估计分区函数 $Z(x)$ 仍然是昂贵的操作，这使得这种表示难以在实践中使用。然而，我们可以重新排列公式 4 来以最优策略 $\pi_r$ ，参考策略 $\pi_{ref}$ ，以及未知分区函数 $Z(\cdot)$ 表达奖励函数。具体来说，我们首先对方程 4 取对数然后通过一些代数运算得到：

$r(x, y) = \beta \log\frac{\pi_r(y|x)}{\pi_{ref}(y|x)} + \beta \log Z(x)$

我们可以将这个重新参数化应用于地面真值奖励$r^*$和对应的最优模型 $\pi^*$。幸运的是，Bradley-Terry 模型仅依赖于两个完成之间的奖励差异，即 $p^*(y_1\succ y_2|x) = \sigma(r^*(x, y_1) - r^*(x, y_2))$。将公式 5 中对 $r^*(x, y)$ 的重新参数化代入偏好模型公式 1 中，则分区函数会相互抵消，并且我们可以仅用最优策略 $\pi^*$和参考策略 $\pi_{ref}$ 表达人类偏好概率：

$p^*(y_1\succ y_2|x) =\frac{1}{1 + exp\left(\beta\log\frac{\pi^*(y_2|x)}{\pi_{ref}(y_2|x)} -\beta\log\frac{\pi^*(y_1|x)}{\pi_{ref}(y_1|x)}\right)}$

虽然公式 6 使用了 Bradley-Terry 模型，但我们可以类似地推导出更一般的 [Plackett-Luce 模型](https://zhida.zhihu.com/search?content_id=249139516&content_type=Article&match_order=1&q=Plackett-Luce+%E6%A8%A1%E5%9E%8B&zhida_source=entity)下的表达式。现在我们已经将人类偏好数据的概率用最优策略表示出来而不是用奖励模型表示出来，我们可以为参数化的策略 πθ 形成最大似然目标。 类似于奖励建模的方法（即公式 2），我们的策略目标变为：

$LDPO(\pi_\theta; \pi_{ref}) = -E((x,y_w,y_l)\sim D)\left[\log\sigma\left(\beta\log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} -\beta\log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\right)\right].$

这样我们就用另一种参数化拟合了一个隐含的奖励，并且其最优策略就是简单的 $\pi_\theta$ 。此外由于我们的过程等同于拟合重新参数化的 Bradley-Terry 模型。

**DPO更新的作用是什么？** 为了对DPO有一个机械的理解，分析损失函数LDPO的梯度是有帮助的。梯度相对于参数θ可以写成：

$\nabla_{\theta} \mathcal{L}(\hat{y}_{i}, \pi_{\text{ref}}) = -\beta \mathbb{E}_{(x_{i}, y_{i}) \sim \mathcal{D}} \left[ \sigma(\hat{y}(x_{i}, y_{i}), \hat{y}(x_{i}, y_{u})) \left[ \nabla_{\theta} \log \pi(y_{u} | x_{i}) \right] \right]$

其中，$\hat{r}_\theta(x, y) = \beta \frac{\pi_\theta(y | x)}{\pi_{\text{ref}}(y | x)}$ 是由语言模型$\pi_\theta$和参考模型$\pi_{\text{ref}}$隐式定义的奖励（更多内容见第5节）。直观上，损失函数LDPO的梯度增加了优选完成$y_w$的似然性，同时减少了不优选完成$y_l$的似然性。重要的是，例子被赋予了权重，即隐式奖励模型$\hat{r}_\theta$对不优选完成给出的奖励估计值高出的部分，再乘以β（即隐式奖励模型错误排序的程度），并考虑到KL约束强度。我们的实验表明这种加权的重要性，因为没有加权系数的原始方法可能导致语言模型退化。

**DPO概述**: DPO的一般流程如下：

1.  对于每个提示x，从参考模型$π_{ref}$中采样完成$y1$, $y2∼π_{ref}(·|x)$，并根据人类偏好进行标记以构建偏好数据集$D={x(i), y(i)_w, y_l)(i)}^N_{i=1}$；
2.  优化语言模型π\_θ以最小化给定$π_{ref}$和D以及所需β值的LDPO损失函数。在实践中，人们希望能够重用公开可用的偏好数据集而不是生成样本并收集人类偏好。由于偏好数据集是使用$π^{SFT}$采样的，因此只要可用就初始化$π_{ref}=π^{SFT}$。然而，在没有$π^{SFT}$的情况下，我们通过最大化优选完成$(x, y_w)$的概率来初始化$π_{ref}$，即$π_{ref}= arg max_π Ex,y_w∼D[log π(y_w | x)]$。这一过程有助于缓解由于真实参考分布不可用而产生的分布偏移与DPO使用的$π_{ref}$之间的偏移。。

## DPO的理论分析

在本节中，我们将进一步解释DPO方法，并提供理论支持，还将讨论DPO的优势与用于RLHF（例如PPO）中的演员-评论家算法的问题进行对比。

## 你的语言模型实际上是隐含的一个奖励模型

DPO能够绕过显式地拟合奖励和使用单一的最大似然目标进行RL学习政策的方法。注意优化目标式(5)等价于一个Bradley-Terry模型，并且奖励参数化为$r^*(x, y) = \beta \frac{\pi^*_\theta(y | x)}{\pi_{\text{ref}}(y | x)}$。我们优化参数化模型$\pi_\theta$等同于在变量变化下优化式(2)中的奖励模型。在本节中我们将建立这种重参数化的理论基础，并展示它不会限制学习到的奖励模型类别，并允许精确恢复最优策略。我们首先定义两个奖励函数之间的等价关系。

-   **定义1**. 我们说两个奖励函数$r(x, y)$和$r'(x, y)$是等价的，如果存在某个函数$f$使得 $r(x, y) - r'(x, y) = f(x).$ 很容易看出这是一个等价关系，并将所有奖励函数划分为不同的类。 我们可以陈述以下两个引理：
    
-   **引理1**. 在Plackett-Luce框架下，特别是Bradley-Terry框架下，来自同一类别的两个奖励函数诱导相同的偏好分布。
    
-   **引理2**. 来自同一等价类别的两个奖励函数在受限RL问题下诱导相同的最优策略。
    

第一个引理是Plackett-Luce家族模型的一个众所周知的问题。由于这个欠定性问题，通常我们需要施加额外的可识别性约束，以确保从式2得到的最大似然估计(MLE)具有任何保证。

第二个引理指出，相同类别的所有奖励函数都会导致相同的最优策略，因此对于我们的最终目标，我们仅对从最优类中恢复任意一个奖励函数感兴趣。

**定理1**. 在一些温和的假设下，与Plackett-Luce模型（特别是Bradley-Terry模型）一致的所有奖励类别都可以通过以下重参数化表示： $r(x, y) = \beta \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)}$ 其中， $\pi(y|x)$ 是某个模型，$\pi_{\text{ref}}(y|x)$ 是给定的参考模型。

证明概要。考虑任意一个奖励函数 $r(x, y)$，它诱导出一个相应的最优模型$\pi_r(y|x)$，由式4给出。我们将展示如何使用上述重参数化形式表示来自 $r$ 的等价类中的任意一个奖励函数。我们定义投影 $f$ 为：

$f(r; \pi_{\text{ref}}, \beta)(x, y) = r(x, y) - \beta \log \left( \sum_y \pi_{\text{ref}}(y|x) \exp\left(\frac{1}{\beta} r(x, y)\right) \right)$

操作符 $f$ 通过将奖励函数与模型 $r$ 的分区函数的对数进行归一化来简化。由于添加的归一化项仅依赖于前缀 $x$，$f(r; \pi_{\text{ref}}, \beta)(x, y)$ 是与$r(x, y)$ 等价类中的一个奖励函数。最后，用式5右边替换$r$（对于任何奖励函数都成立），我们得到：

$f(r; \pi_{\text{ref}}, \beta)(x, y) = \beta \log \frac{\pi_r(y|x)}{\pi_{\text{ref}}(y|x)}.$

也就是说，投影操作 (f) 产生了一个等价类中的成员，并且这种重参数化不会在我们的奖励模型中丢失任何一般性。

我们还可以将定理1视为指定每个等价类中DPO重参数化选择的具体哪个奖励函数，即满足：

$X_y \pi_{\text{ref}}(y|x) \exp\left(\frac{1}{\beta} r(x, y)\right) = 1,$

即，$\pi(y|x)$ 是一个有效的分布（概率为正且相加为1）。然而，根据式4，式9是诱导出奖励函数 $r(x, y)$ 的最优策略的分区函数。DPO算法的关键洞察是：我们可以对Plackett-Luce（特别是Bradley-Terry）偏好模型家族施加某些约束条件，这样可以保持可表示的奖励模型类别不变，并且明确地使式4中的最优策略在所有提示 $x$下变得解析上可处理。

## 标准[Actor-Critic算法](https://zhida.zhihu.com/search?content_id=249139516&content_type=Article&match_order=1&q=Actor-Critic%E7%AE%97%E6%B3%95&zhida_source=entity)的不稳定性

我们还可以使用我们的框架来诊断用于RLHF的标准演员-评论家算法（如PPO）中的不稳定性问题。我们遵循RLHF管道，并聚焦于第3节中概述的RL微调步骤。我们可以将控制作为推理框架应用于第3节中概述的约束RL问题。假设一个参数化的模型$\pi_\theta(y|x)$，并最小化 $D_{KL} [\pi_\theta(y|x)||\pi^*(y|x)]$ 其中，$\pi^*$ 是由奖励函数 $r_\phi(y, x)$ 引导出的最佳策略。通过一些代数变换，这会导致优化目标：

$\max_{\pi(y|x)} \mathbb{E}_{\pi(y|x)} \left[ r(x, y) - \beta \log \sum_{y} \pi_{\text{ref}}(y | x) \exp\left(-\frac{1}{\beta} r(x, y)\right) \right] - \beta \log \frac{\pi(y | x)}{\pi_{\text{ref}}(y | x)}$

即，在该设置下，我们可以将$f(r_\phi, π_{\text{ref}}, β)$ 中的归一化项解释为参考策略$π_{\text{ref}}$ 的软值函数。虽然这项不会影响最优解，但如果没有它，则目标的策略梯度可能会有高方差，从而导致学习不稳定。我们可以使用学习到的价值函数来容纳归一化项，但这也可能难以优化。或者，在先前的工作中已经使用了人类完成基线来规范化奖励——实际上是一个单样本蒙特卡洛估计归一化项。相比之下，DPO重参数化产生了一个不需要任何基线的奖励函数。
