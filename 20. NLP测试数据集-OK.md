---
source: "https://zhuanlan.zhihu.com/p/658495680"
---
| 数据集                                                                                                                                    |                                                                                                                                  |              |                  |
| -------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- | ------------ | ---------------- |
| 1-MMLU                                                                                                                                 | 7- [C-Eval](https://zhida.zhihu.com/search?content_id=234461093&content_type=Article&match_order=1&q=C-Eval&zhida_source=entity) | 13-USMLE     | 19-BoolQ         |
| 2- [CMMLU](https://zhida.zhihu.com/search?content_id=234461093&content_type=Article&match_order=1&q=CMMLU&zhida_source=entity)         | 8- [GaoKao](https://zhida.zhihu.com/search?content_id=234461093&content_type=Article&match_order=1&q=GaoKao&zhida_source=entity) | 14-MCMLE     | 20-SIQA          |
| 3- [GSM8K](https://zhida.zhihu.com/search?content_id=234461093&content_type=Article&match_order=1&q=GSM8K&zhida_source=entity)         | 9-AGIEval                                                                                                                        | 15-MedMCQA   | 21-WinoGrande    |
| 4- [MATH](https://zhida.zhihu.com/search?content_id=234461093&content_type=Article&match_order=1&q=MATH&zhida_source=entity)           | 10-BBH                                                                                                                           | 16-CSQA      | 22-ARC easy      |
| 5- [HumanEval](https://zhida.zhihu.com/search?content_id=234461093&content_type=Article&match_order=1&q=HumanEval&zhida_source=entity) | 11-JEC-QA                                                                                                                        | 17-HelloSwag | 23-ARC challenge |
| 6- [MBPP](https://zhida.zhihu.com/search?content_id=234461093&content_type=Article&match_order=1&q=MBPP&zhida_source=entity)           | 12-CMC                                                                                                                           | 18-PIQA      | 24-OpenBookQA    |

1、MMLU
- MMLU（大规模多任务语言理解）通过在zero-shot和few-shot设置中专门评估模型来衡量在预训练期间获得的知识。
- 涵盖 STEM、人文科学、社会科学等 57个多选问答任务的英文评测。
- 它的难度从初级到高级专业水平不等，它考验着世界知识和解决问题的能力。
- 科目范围从传统领域，如数学和历史，到更专业的领域，如法律和道德。
- 主题的粒度和广度使基准测试成为识别模型盲点的理想选择。

![|575](https://pic2.zhimg.com/v2-0cc26ff33759437a003ca3cb65d7e847_1440w.jpg)

2、CMMLU

- CMMLU，一个全面的 ***中文大模型基准。它涵盖了 67 个主题*** 
- 涉及自然科学、社会科学、工程、人文、以及常识等，可以全面地评估大模型在中文知识储备和语言理解上的能力。数据集包含 11,528 个问题，涵盖了 67 个学科。每个学科至少有 105 个问题，我们将其分为包含 5 个问题的训练样本集（few-shot development set），以及包含超过 100 个问题的测试集（test set）。

选择题。

![|675](https://pic3.zhimg.com/v2-39b1a8366d304da1790987e40b57165a_1440w.jpg)

数据样例

3、GSM8K

一个由8.5K高质量的语言多样化的 **小学数学** 单词问题组成的数据集。这些问题都是由人类写手创造的。我们将这些问题分为 **7.5K 训练问题和 1K 测试问题** 
[数据样例](https://link.zhihu.com/?target=https%3A//huggingface.co/datasets/gsm8k/viewer/main/train%3Frow%3D0)

![|725](https://pic4.zhimg.com/v2-918f506eb7d2f0e4ea767fb8e434b36f_1440w.jpg)

4、 MATH

**MATH数据集** 则包含初等代数，代数，数论，计数和概率，几何，中等代数和微积分等领域的多种数学问题，是用LaTeX写的，也就是说并不全是自然语言，而有点像代码阅读了。

5\. HumanEval

数据集包含了164个人工编写的python编程问题，这些问题旨在评估模型在解决实际编程任务方面的表现。

6\. MBPP

测试大语言模型在从自然语言描述中生成Python代码的能力，尤其是在处理基础编程任务时的表现。这个数据集不仅为研究者提供了一个平台来开发和测试AI模型，而且对于开发能够辅助编程工作的工具具有重要意义。

7\. C-Eval

相当于中文版本的MMLU ：一个覆盖人文，社科，理工，其他专业四个大方向，52 个学科（微积分，线代 …），从中学到大学研究生以及职业考试，一共 13948 道题目的中文知识和推理型测试集，我们管它叫 C-Eval，来帮助中文社区研发大模型。

8\. GaoKao

是以中国高考题作为评测大语言模型能力的数据集，用以评估模型的语言能力和逻辑推理能力；

9\. AGIEval

发布了一个新的基准测试AGIEval，用于评估基础模型在人类认知任务中的表现，包括高考、公务员考试、法学院入学考试、数学竞赛和律师资格考试等。
