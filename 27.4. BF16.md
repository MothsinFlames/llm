---
created: 2025-08-20T14:17:09 (UTC +08:00)
tags: [大模型,自然语言处理,ChatGPT]
source: https://zhuanlan.zhihu.com/p/692410068
author: 北京大学 计算机科学与技术硕士
---
如今大模型的训练、推理和权重存储都常使用BF16（BFloat16，全称为Brain Floating Point），它和之前更常见的float16都属于每个数值占16 bits的半精度格式，为什么BF16在大模型时代得到了更多青睐呢？

float32有**8个指数位和23个小数位**
BF16有**8个指数位和7个小数位**，和float32能表达的范围大小相同
float16有**5个指数位、10个小数位**

BF16能表示的范围更大，不易发生数值上溢或下溢，更适合大模型的训练和推理。

下面具体来看。计算机中的浮点数都是以**符号域-指数域-尾数域**二进制的形式存储和运算的，符号位占1位（0表示正，1表示负），设指数域占E位，指数域的值为e，尾数域占F位，尾数域的值为f，则该浮点数的值为：$(-1)^s \cdot 2^{e-2^{E-1}+1} \cdot 1.f \tag{1}$

从上式可以看出，**指数域大小主要决定能表示的大小范围（大到2的多少次方，小到2的多少次分之一），尾数域大小则决定精度（相同数量级的相邻数字之间间隔有多细）。**

在大模型时代之前，由于模型参数量一般较小，大家（尤其是做研究时）显存压力没那么大，一般使用**1-8-23的float32单精度格式**：

![|1300](https://pic3.zhimg.com/v2-8ea3ec9072f25c3d8059c0fed03a2320_1440w.jpg)


![|800](https://pic3.zhimg.com/v2-76ff5a4226fdcf9b8a0ef46a2f74c5c8_1440w.jpg)

可以看出：**BF16的指数域位数（8位）和float32一样多，能表示的大小范围类似，只是精度降低了（也就是相邻数之间的间隔略微变大，大多数情况下对神经网络的表现影响不显著），而float16的指数域位数只有5，可以表达的大数上限降低，接近0的小数下限升高，比BF16更容易发生上溢和下溢等数值问题，因此大模型的训练和推理更常用BF16**。

当然，天下没有十全十美的选择，**BF16精度较低的问题有时候也会带来严重影响，比如导致相邻位置的位置编码无法区分，这时候应该对相应模块采用float32编码**，如[Baichuan-2](https://zhida.zhihu.com/search?content_id=241999474&content_type=Article&match_order=1&q=Baichuan-2&zhida_source=entity)技术报告的讨论：

![](https://pic3.zhimg.com/v2-6637d914ef0b8809f88d5da29300e6cc_1440w.jpg)

