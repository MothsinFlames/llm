Infini-attention是一种创新的注意力机制，通过结合标准的多头点积注意力和压缩内存技术，优化长期和局部信息的处理效率，具体实现步骤如下： 
1. **多头点积注意力计算**：多头点积注意力（MHA）是大型语言模型的核心组件。对于输入序列$X \in \mathbb{R}^{N × d_{model}}$，每个头会计算注意力查询（$Q$）、键（$K$）和值（$V$）状态，计算公式为$K = XW_{K}$，$V = XW_{V}$，$Q = XW_{Q}$ ，其中$W_{K} \in \mathbb{R}^{d_{model } × d_{key}}$、$W_{V} \in \mathbb{R}^{d_{model } × d_{value}}$和$W_{Q} \in \mathbb{R}^{d_{model } × d_{key}}$是可训练的投影矩阵。之后，注意力上下文通过公式$A_{dot }=softmax\left(\frac{QK^{T}}{\sqrt{d_{model }}}\right)V$计算得出。在多头注意力机制中，会为每个序列元素并行计算$H$个注意力上下文向量，然后将这些向量沿第二维度连接，并最终投影回模型空间，得到注意力输出。 
2. **压缩内存机制** - **内存重用**：Infini-attention在计算点积注意力后，不会创建新的内存条目，而是重用$Q$、$K$、$V$状态，以此提升插拔式长上下文适应的效率，加速训练和推理过程。 
	- **内存检索**：从内存$M_{s - 1} \in \mathbb{R}^{d_{key} × d_{value}}$中检索新内容$A_{mem } \in \mathbb{R}^{N × d_{value}}$时，使用公式$A_{mem }=\frac{\sigma(Q)M_{s - 1}}{\sigma(Q)z_{s - 1}}$ ，其中$\sigma$是非线性激活函数，$z_{s - 1} \in \mathbb{R}^{d_{key}}$是归一化项。为保证训练稳定性，通常将所有键的总和记录为归一化项$z_{s - 1}$，并使用元素级ELU + 1作为激活函数。
	- **内存更新**：检索完成后，使用公式$M_{s} \leftarrow M_{s - 1}+\sigma(K)^{T}V$和$z_{s} \leftarrow z_{s - 1}+\sum_{t = 1}^{N} \sigma\left(K_{t}\right)$来更新内存和归一化项，新的内存状态$M_{s}$和$z_{s}$会被传递到下一个片段。此外，还引入了delta rule，改进后的更新公式为$M_{s} \leftarrow M_{s - 1}+\sigma(K)^{T}\left(V-\frac{\sigma(K)M_{s - 1}}{\sigma(K)z_{s - 1}}\right)$，该公式在保证数值稳定的同时，若$KV$绑定已存在于内存中，会保持关联矩阵不变 。 
3. **长期上下文注入**：通过学习得到的门控标量$\beta$，Infini-attention聚合局部注意力状态$A_{dot }$和内存检索内容$A_{mem }$，计算公式为$A = sigmoid(\beta) \odot A_{mem }+(1 - sigmoid(\beta)) \odot A_{dot }$。这样，每个头仅增加一个标量值作为训练参数，就能实现模型中长期和局部信息流之间的可学习权衡。最后，如同标准的多头注意力，多头Infini-attention会并行计算$H$个上下文状态，然后连接并投影这些状态，得到最终的注意力输出$O \in \mathbb{R}^{N × d_{model}}$，投影公式为$O=[A^{1};... A^{H}]W_{O}$，其中$W_{O} \in \mathbb{R}^{H × d_{value } × d_{model}}$是可训练的权重。