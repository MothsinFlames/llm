---
created: 2025-09-17T10:55:51 (UTC +08:00)
tags: [大模型,人工智能,强化学习 (Reinforcement Learning)]
source: https://zhuanlan.zhihu.com/p/1924996315324093973
author: 关于作者大模型最新论文全网同名回答27文章65关注者367关注发私信
---
**论文标题**

MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent

**论文地址**

[https://arxiv.org/pdf/2507.02259](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2507.02259)

代码地址

[https://github.com/BytedTsinghua-SIA/MemAgent](https://link.zhihu.com/?target=https%3A//github.com/BytedTsinghua-SIA/MemAgent)

**作者背景**

字节seed，清华大学

## **动机**

大模型的长文本处理能力是解决复杂问题的基础，当前一般通过可以通过以下三类方法来提高此项能力：

-   **[长文本微调](https://zhida.zhihu.com/search?content_id=259964008&content_type=Article&match_order=1&q=%E9%95%BF%E6%96%87%E6%9C%AC%E5%BE%AE%E8%B0%83&zhida_source=entity)：**通过位置编码的内插、外推让模型感知更长的文本长度，然后使用长文本语料对模型进行微调，这是最常见的做法
-   **[注意力优化](https://zhida.zhihu.com/search?content_id=259964008&content_type=Article&match_order=1&q=%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%BC%98%E5%8C%96&zhida_source=entity)：**通过更高效的注意力计算方式（如线性attention、滑动窗口等），或在推理过程中只保留重要信息的kv缓存（如H2O等），优化注意力的计算与显存开销，从而能原生支持长文本
-   **[外部压缩](https://zhida.zhihu.com/search?content_id=259964008&content_type=Article&match_order=1&q=%E5%A4%96%E9%83%A8%E5%8E%8B%E7%BC%A9&zhida_source=entity)：**将长文本分段压缩，模型每次只处理固定长度的关键信息

上述方案各有不足，要么需要引入复杂的模块设计，要么需要重头开始训练，并且应对极长上下文时，效果往往没有所宣称的那么好

上面第三种方案比较接近人类处理过长内容时的行为：对过往信息进行总结，只记忆其中的关键信息，而不必全盘掌握每一处细节。但当前此类方法对冗长信息的压缩与关键内容的提取比较初级，往往都是简单的串联合并，依然无法满足对超长上下文进行准确处理的需求

于是作者希望在训练长文本模型的过程中，专门优化模型压缩信息的能力，从而真正实现超长，甚至无限上下文的准确处理

## **相关文章**

-   **高效注意力**

[Apple新作：结合滑动窗口与线性注意力](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzk3NTU0Mjg5MQ%3D%3D%26mid%3D2247484530%26idx%3D1%26sn%3Db3dc0425eaee873914e05c729c47583e%26scene%3D21%23wechat_redirect)

[PowerAttention: 更准确高效的静态稀疏注意力](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzk3NTU0Mjg5MQ%3D%3D%26mid%3D2247483706%26idx%3D1%26sn%3D47c7e2183961e0b47c75f76b6b13b641%26scene%3D21%23wechat_redirect)

-   **缓存优化**

[使用频域变换轻松压缩kv-cache](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzk3NTU0Mjg5MQ%3D%3D%26mid%3D2247484245%26idx%3D1%26sn%3Da585bc88e6d108db7b66e9abbdb5a84c%26scene%3D21%23wechat_redirect)

[大模型推理加速：通过蒸馏kv-cache降低显存开销](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzk3NTU0Mjg5MQ%3D%3D%26mid%3D2247483806%26idx%3D1%26sn%3D1d60ba765592017777a048918e1aa7ff%26scene%3D21%23wechat_redirect)

-   **上下文压缩**

[EdgeInfinite: 用3B模型处理无限长的上下文](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzk3NTU0Mjg5MQ%3D%3D%26mid%3D2247484028%26idx%3D1%26sn%3D58fd875a72547aaa7b8c6c9afe2d2a28%26scene%3D21%23wechat_redirect)

[提示词压缩方法总结与开源工具包](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzk3NTU0Mjg5MQ%3D%3D%26mid%3D2247484226%26idx%3D1%26sn%3D20da0e5e760467346ec5d097355e8706%26scene%3D21%23wechat_redirect)

## **本文方法**

本文提出 MemAgent，围绕一个固定长度的辅助记忆模块，结合多轮对话式的推理流程和强化学习策略，训练模型像人类阅读长文章一样，逐段阅读，逐步摘要，最后整合回答

![](https://pic3.zhimg.com/v2-4b536b2611a8758de34c5781d30630f8_1440w.jpg)

如上图所示，MemAgent 将超长文本切分成多块，然后分多次输入给大模型进行总结，抽取与问题相关的关键信息，最终输出准确的回答

这一过程简单直观，但如果直接用一个未经优化的通用模型执行上述流程，往往会出现错误累积的问题。MemAgent 的核心优势在于将其视作反馈延迟的 Agent 流程，并利用强化学习进行**端到端优化**，让模型学会分辨什么信息应该舍弃、什么信息应该保留

![](https://pic2.zhimg.com/v2-4462c0bd1bb23a2d6bf3ce523e229b99_1440w.jpg)

MemAgent 基于 verl 框架实现了多轮对话的 [DAPO 算法](https://zhida.zhihu.com/search?content_id=259964008&content_type=Article&match_order=1&q=DAPO+%E7%AE%97%E6%B3%95&zhida_source=entity)训练，对于每条样本，策略模型需要多次采样生成多条阅读理解轨迹，然后对比输出结果与标准答案之间的匹配情况来计算奖励分数

## **实验结果**

从 [HotpotQA](https://zhida.zhihu.com/search?content_id=259964008&content_type=Article&match_order=1&q=HotpotQA&zhida_source=entity) 与数据集中整理了三万多条样本（长文档、问题、答案），然后在 RL 环境中拆分为多轮交互：模型每轮接收“当前片段 + 问题 + 上一步记忆”，输出“更新的记忆”（最后一轮输出最终答案）；测试样本格式相同，但被划分成了多种上下文长度，测试结果如下：

![](https://pic4.zhimg.com/v2-37802ad3d1d4852b946eee2650905301_1440w.jpg)

实验结果表明，MemAgent 在超长上下文任务中取得了压倒性优势，并且随着测试文档长度不断提高，回答准确率一直保持在较高水准，相比之下，即便是更大尺寸的长上下文模型，准确率下降明显，最后束手无策

![](https://pic1.zhimg.com/v2-523076676b923c2e087c22bf5a929886_1440w.jpg)

如果将强化学习训练替换为直接微调，模型几乎无法学会有效的记忆更新策略，导致长文本表现急剧下降，这说明端到端RL优化是MemAgent成功的关键

![](https://pic4.zhimg.com/v2-7f629bbb2660692cf99ecad0483c136b_1440w.jpg)

此外作者还在 [RULER 基准](https://zhida.zhihu.com/search?content_id=259964008&content_type=Article&match_order=1&q=RULER+%E5%9F%BA%E5%87%86&zhida_source=entity)上对域外任务进行了一系列测试，结果表明 MemAgent 具有强大的泛化能力

![](https://pic4.zhimg.com/v2-d9664d792e9fd38f144f639fc0d2d41d_1440w.jpg)
