---
source: https:/zhuanlan.zhihu.com/p/639228219
---
## 1\. 前言

GPT3、LLaMA、ChatGLM、BLOOM等大语言模型输入输出的最大序列长度只有2048或4096，扩展到更长序列的难度在哪里呢？

本质原因是，transformer模型的计算复杂度和空间复杂度都是 $O \left(\right. N^{2} \left.\right)$ 的，其中 $N$ 为序列长度。具体地，当输入批次大小为 $b$ ，序列长度为 $N$ 时， $l$ 层transformer模型的计算量为 $l * \left(\right. 24 b N h^{2} + 4 b N^{2} h \left.\right)$ ，中间激活的显存大小为 $l * \left(\right. 34 b N h + 5 b N^{2} a \left.\right)$，其中 $a$ 为注意力头数。可以看到，**transformer模型的计算量和储存复杂度随着序列长度** $N$ **呈二次方增长**。这限制了大语言模型的最大序列长度 $N$ 的大小。计算量和中间激活见[《分析transformer模型的参数量、计算量、中间激活、KV cache》](https:/zhuanlan.zhihu.com/p/624740065)。

最近，GPT4将最大序列长度 $N$ 扩大到了32K，Claude更是将最大序列长度 $N$ 扩大到了100K，这些工作一定采用了一些优化方法来降低原生transformer的复杂度。我们知道，每个transformer层分为两部分：[self-attention块](https:/zhida.zhihu.com/search?content_id=230181648&content_type=Article&match_order=1&q=self-attention%E5%9D%97&zhida_source=entity)和[MLP块](https:/zhida.zhihu.com/search?content_id=230181648&content_type=Article&match_order=1&q=MLP%E5%9D%97&zhida_source=entity)。上面计算量中的 $4 b N^{2} h$ 项和中间激活中的 $5 b N^{2} a$ 项都是self-attention块产生的，与MLP块无关。[FlashAttention](https:/zhida.zhihu.com/search?content_id=230181648&content_type=Article&match_order=1&q=FlashAttention&zhida_source=entity)[1]提出了一种**加速计算，节省显存，IO感知的精确注意力**，可以有效地缓解上述问题。Meta推出的开源大模型LLaMA[2]，阿联酋推出的开源大模型Falcon[2]都使用了Flash Attention来加速计算和节省显存。目前，Flash Attention已经集成到了pytorch2.0中，另外triton、xformer等开源框架也进行了整合实现。

从论文题目《Flashattention: Fast and memory-efficient exact attention with io-awareness》入手，简要总结Flash Attention的优点。
1\. **加快了计算（Fast）**。Flash Attention并没有减少计算量FLOPs，而是从IO感知出发，减少了[HBM](https:/zhida.zhihu.com/search?content_id=230181648&content_type=Article&match_order=1&q=HBM&zhida_source=entity)访问次数，从而减少了计算时间。论文中用到了"wall-clock time"这个词，时钟时间不仅包含了GPU运行耗时，还包含了IO读写的阻塞时间。减少HBM访问次数，是通过tiling技术分块和算子融合来实现的。
2\. **节省了显存（Memory-efficient）**。Flash Attention通过引入统计量，改变注意力机制的计算顺序，避免了实例化注意力矩阵 $P , S \in R^{N \times N}$ ，将显存复杂度从 $O \left(\right. N^{2} \left.\right)$降低到了 $O \left(\right. N \left.\right)$ 。这个技巧不是首创的，在\[4\]\[5\]中也有应用。
3\. **精确注意力（Exact Attention）**。不同于稀疏注意力，Flash Attention只是分块计算，而不是近似计算，Flash Attention与原生注意力的结果是完全等价的。

## 2\. 背景知识

### 2.1 计算受限与内存受限

transformer的核心组件**self-attention块的计算复杂度和空间复杂度是序列长度 N 的二次方**，已经有许多近似注意力的方法尝试减少attention的计算和内存要求。例如，稀疏近似和低秩近似的方法，将计算复杂度降低到了序列长度的线性或亚线性，但这些方法并没有得到广泛应用。因为这些方法过于关注FLOPs(浮点数计算次数)的减少，而忽略了IO读写的内存访问开销。在现代GPU中，计算速度已经远超过了显存访问速度，**transformer中的大部分计算操作的瓶颈是显存访问**。对于显存受限的操作，IO感知是非常重要的，因为显存读写占用了大部分的运行时间。

在计算科学中，给定硬件，找出一个操作的性能瓶颈是非常重要的。一个操作的性能瓶颈有两类：计算受限（math-bound）和内存受限（memory-bound）。接下来从一些基本概念入手，来讨论计算受限和内存受限。

**计算带宽（math bandwidth）**指的是处理器每秒钟可以执行的数学计算次数，单位通常是OPS(即operations/second)。如果用浮点数进行计算，单位是FLOPS。例如，A100-40GB SXM的计算带宽为312TFLOPS。

> **注意区分FLOPS和FLOPs。**  
> FLOPS：全大写，floating point operations per second。每秒钟执行的浮点数操作次数，理解为运算速度，是衡量硬件性能的指标。  
> FLOPs：s为小写，floating point operation。表示浮点数运算次数，理解为计算量，衡量模型或算法的复杂度。

**内存带宽 （memory bandwidth）**指的是处理器每秒钟从内存中读取的数据量，单位是bytes/second。例如，A100-40GB SXM的内存带宽为1555GB/s。

从数学上分析，浮点数计算次数为 $N_{op}$ ，内存访问量为 $N_{byte}$ ，计算带宽为 $BW_{math}$ ，内存带宽为 $BW_{mem}$ 。访问内存花费的时间为 $T_{mem}$ ，计算花费的时间为 $T_{math}$ 。由于可以“边计算上一个，边读/写下一个”，访问内存和计算的时间可以重叠，此时总的运行时间为 $max(T_{mem},T_{math})$ $$T_{mem}=\frac{N_{byte}}{BW_{mem}} $$$$T_{math}=\frac{N_{op}}{BW_{math}}$$
当 $T_{math}>T_{mem}$ 时，硬件性能受限于计算带宽，为**计算受限（math-bound）**。反之，当 $T_{math}< T_{mem}$ 时，硬件性能受限于内存带宽，为**内存受限（memory-bound）**。

在数学上，对于计算受限，下面的不等式成立：
$$T_{math}>T_{mem} \rightarrow \frac{N_{op}}{N_{byte}}>\frac{BW_{math}}{BW_{mem}}$$
对于内存受限，下面的不等式成立：

$$T_{math}<T_{mem} \rightarrow \frac{N_{op}}{N_{byte}}<\frac{BW_{math}}{BW_{mem}}$$

通常将 $ops/bytes= \frac{N_{op}}{N_{byte}}$ 定义为**算术强度（arithmetic intensity**）。给定硬件的情况下，对于一个模型或算法，若算术强度 $>\frac{BW_{math}}{BW_{mem}}$ ，则为计算受限；反之为内存受限。
 
假如用float16进行计算，A100-40GB SXM的 $\frac{BW_{math}}{BW_{mem}}=\frac{312 \times 10^{12}}{1555\times 10^9}\approx201 flops/bytes$ 。若一个操作的算术强度 $ops/types>201$ ，此时的性能受限于计算带宽；反之，性能受限于内存带宽。

> **如何计算矩阵乘法的计算量？**  
> 对于 $A\in R^{1\times n},B\in R^{n\times 1}$ ，计算 AB 需要 n 次乘法操作和 n 次加法操作，计算量为 2n flops。 对于 $A\in R^{m\times n},B\in R^{n\times p}$，计算 AB 需要的计算量为 2mnp flops。  
> **如何计算一个矩阵的内存大小？**  
> 对于 $A\in R^{m\times n}$ ，矩阵共有 mn 个元素。若用float16或bfloat16进行存储，每个元素需要2个bytes。则矩阵A需要的内存大小为 2mn bytes。

对于注意力中的矩阵乘法， $Q,K\in R^{N\times d}$ ，计算 $P=QK^{\top}\in R^{N\times N}$ 。其中， d 为每个注意力头的维度。 
$$ops/bytes = \frac{2N^2d}{2Nd + 2Nd+2N^2} = \frac{N^2d}{2Nd+N^2}$$

从下表中可以看到，即使对于计算量较大的矩阵乘法来说，当 d=64 时，性能主要是受限于内存带宽的。对于LLaMA和BLOOM模型，注意力头维度 d=128 ，最大序列长度 N=2048 ，矩阵乘法是计算受限的。

| N | d | ops/bytes | 受限类型 |
| --- | --- | --- | --- |
| 256 | 64 | 43 | <201, memory-bound |
| 2048 | 64 | 60 | <201, memory-bound |
| 4096 | 64 | 62 | <201, memory-bound |
| 256 | 128 | 64 | <201, memory-bound |
| 2048 | 128 | 114 | <201, memory-bound |
| 4096 | 128 | 120 | <201, memory-bound |
| 256 | 256 | 85 | <201, memory-bound |
| 2048 | 256 | 205 | \>201, math-bound |
| 4096 | 256 | 228 | \>201, math-bound |

- **math-bound**：性能受限于计算带宽。比如：**大矩阵乘法、通道数很大的卷积运算**。
- **memory-bound**：性能受限于内存带宽。**逐点运算的操作**大多是内存受限的，比如：**激活函数、dropout、mask**；另外**reduction操作**也是内存受限的，比如：**softmax，batch normalization和layer normalization**。

对于self-attention块，除了大矩阵乘法是计算受限的，其他操作（计算softmax，dropout，mask）都是内存受限的。尽管近似注意力方法将计算复杂度降低为序列长度的线性或亚线性，**由于忽略了内存访问开销，这些近似注意力方法并没有有效减少运行时间（wall-clock time）**。 Flash Attention则是IO感知的，通过减少内存访问，来计算精确注意力，从而减少运行时间，实现计算加速。

### 2.2 GPU内存分级

如下图左所示，GPU的内存由多个不同大小和不同读写速度的内存组成。内存越小，读写速度越快。对于A100-40GB来说，内存分级图如下所示。**[SRAM](https:/zhida.zhihu.com/search?content_id=230181648&content_type=Article&match_order=1&q=SRAM&zhida_source=entity)内存**分布在108个流式多处理器上，每个处理器的大小为192K。合计为 $192 * 108 KB=20,736KM=20MB$ 。**高带宽内存HBM（High Bandwidth Memory）**，也就是我们常说的显存，大小为40GB。SRAM的读写速度为19TB/s，而HBM的读写速度只有1.5TB/s，不到SRAM的1/10。上面讲到计算注意力的主要瓶颈是显存访问，因此减少对HBM的读写次数，有效利用更高速的SRAM来进行计算是非常重要的。

![](https:/pic2.zhimg.com/v2-ca27af05a418949c0036a488a6682195_1440w.jpg)

### 2.3 运行模式

GPU有大量的线程来执行某个操作，称为**kernel**。GPU执行操作的典型方式分为三步：（1）每个kernel将输入数据从低速的HBM中加载到高速的SRAM中；（2）在SRAM中，进行计算；（3）计算完毕后，将计算结果从SRAM中写入到HBM中。

### 2.4 [kernel融合](https:/zhida.zhihu.com/search?content_id=230181648&content_type=Article&match_order=1&q=kernel%E8%9E%8D%E5%90%88&zhida_source=entity)

对于性能受限于内存带宽的操作，进行加速的常用方式就是**kernel融合**。**kernel融合**的基本思想是：避免反复执行“从HBM中读取输入数据，执行计算，将计算结果写入到HBM中”，将多个操作融合成一个操作，**减少读写HBM的次数**。

需要注意的是，模型训练通常会影响到算子融合的效果，因为为了后向传递计算梯度，通常需要将某些中间结果写入到HBM中。

### 2.5 safe softmax

对于向量 \[x_1, x_2,\cdots,x_d\] ，原生softmax的计算过程如下：

$$softmax(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{d}e^{x_j}}$$

在实际硬件中，浮点数表示的范围是有限的。**对于float32和bfloat16来说，当 $x\ge89$ 时，$e^x$ 就会变成inf，发生数据上溢的问题**。**为了避免发生数值溢出的问题，保证数值稳定性，计算时通常会“减去最大值”，称为“safe softmax”**。现在所有的深度学习框架中都采用了“safe softmax”这种计算方式。 
$$m=\underset{i}{max}(x_i); \space\space\space\space softmax(x_i)=\frac{e^{x_i-m}}{\sum_{j=1}^{d}e^{x_j-m}}$$

在训练语言模型时，通常会采用交叉熵损失函数。**交叉熵损失函数等价于先执行log_softmax函数，再计算负对数似然函数**。在计算log_softmax时，同样会执行“减去最大值”，这不仅**可以避免数值溢出，提高数值稳定性；还可以加快计算速度**。 

$$log(softmax(x_i)) = log(\frac{e^{x_i-m}}{\sum_{j=1}^de^{x_j-m}}) = x_i -m - log(\sum_{j=1}^de^{x_j-m})$$

## 3\. 前向传递

### 3.1 Standard Attention

transformer中注意力机制的计算过程为：

$$Attention(Q,K,V)=softmax(\frac{QK^{\top}}{\sqrt{d}})V$$

其中， $Q,K,V\in R^{N\times d}$ ，其中 N 是序列长度， d 是每个注意力头的维度。输出可以记为 $O\in R^{N\times d}$ 。上面的式子可以拆解为：

$$S=QK^{\top}\in R^{N\times N}, P=softmax(S)\in R^{N\times N}, O=PV\in R^{N\times d}$$

在标准注意力实现中， $S,P\in R^{N\times N}$ 都要写回到HBM中，占用了 $O(N^2)$ 的内存。通常 $N\gg d$ ，例如，对于GPT2， $N=1024,d=64$ ；对于GPT3， $N=2048,d=128$ 。注意力矩阵 P,S 需要的内存 $O(N^2)$ 远大于 Q,K,V,O 所需要的内存 $O(Nd)$ 。**self-attention中，大部分操作都是内存受限的逐点运算**，例如，对 S 的mask操作， S 的softmax操作，对 P 的dropout操作。这些逐点操作的性能是受限于内存带宽的，会减慢运行时间。

下图展示了标准注意力的实现过程。标准注意力实现存在两个问题：（1）**显存占用多**。实例化了完整的注意力矩阵 $P,S\in R^{N\times N}$ ，导致了 $O(N^2)$ 的内存要求。（2）**HBM读写次数多**，减慢了运行时间（wall- clock time）。

![](https:/picx.zhimg.com/v2-f7736d0eabc38db64c849254f34dffaf_1440w.jpg)

### 3.2 Memory-efficient Attention

在注意力计算过程中，节省显存的主要挑战是softmax与 K,V 的列是耦合的。我们的方法是单独计算softmax的归一化因子，来实现解耦。这种方法在《Online normalizer calculation for softmax》\[4\]，《Self-attention Does Not Need O (n^ 2) Memory》\[5\]中已经使用过。这种方法**避免了实例化完整的注意力矩阵 S,P ，不再需要 O(N^2) 的显存占用。然而HBM访问次数仍然是 O({N^2}) 的**，因此运行时间并没有减少。

为了简化分析，忽略计算softmax时“减去最大值”的步骤。记 Q 的第 i 列为 $q_i\in R^{d}$ ， K 的第 j 列为 $K_j\in R^d$ ，有 $S_{ij} = q_i^{\top}k_j \in R$ 。定义softmax的归一化因子为:

$$L_i = \sum_{j} e^{q_i^{\top}k_j} \in R$$

记 $v_j \in R^d$ 为 $V$ 的第 $j$ 个列向量，则输出 O 的第 i 个列向量 $o_i$ 为： $$o_i = P_{i:}V = \sum_{j} P_{ij}v_j=\sum_{j} \frac{e^{q_i^{\top}k_j}}{L_i}v_j$$
在计算得到归一化因子 L_i 后，就可以通过反复累加 $\frac{e^{q_i^{\top}k_j}}{L_i}v_j$ 来得到 $o_i$ 。节省内存（memory-efficient）的注意力机制，改变了计算顺序，从而避免了实例化完整的注意力矩阵 S,P ，达到了节省显存的效果。相比于标准注意力机制，**节省显存的注意力机制将显存复杂度从 $O(N^2)$ 降低到了 $O(N)$** 。《Self-attention Does Not Need $O (n^ 2)$ Memory》\[4\]将这种方法称为“lazy softmax”。然而这种方法HBM访问次数仍然是 $O({N^2})$ 的，因此运行时间并没有减少。

### 3.3 Flash Attention

在标准注意力实现中，注意力的性能主要受限于内存带宽，是内存受限的。频繁地从HBM中读写$N\times N$ 的矩阵是影响性能的主要瓶颈。稀疏近似和低秩近似等近似注意力方法虽然减少了计算量FLOPs，但对于内存受限的操作，运行时间的瓶颈是从HBM中读写数据的耗时，减少计算量并不能有效地减少运行时间（wall-clock time）。针对内存受限的标准注意力，Flash Attention是IO感知的，目标是**避免频繁地从HBM中读写数据**。

### 3.3.1 tiling，分块计算

从GPU显存分级来看，SRAM的读写速度比HBM高一个数量级，但内存大小要小很多。通过kernel融合的方式，将多个操作融合为一个操作，利用高速的SRAM进行计算，可以减少读写HBM的次数，从而有效减少内存受限操作的运行时间。但SRAM的内存大小有限，不可能一次性计算完整的注意力，因此必须进行分块计算，使得分块计算需要的内存不超过SRAM的大小。

**为什么要进行分块计算呢？**内存受限 --> 减少HBM读写次数 --> kernel融合 --> 满足SRAM的内存大小 --> 分块计算。因此分块大小block_size不能太大，否则会导致OOM。

**分块计算的难点是什么呢？**注意力机制的计算过程是“矩阵乘法 --> scale --> mask --> softmax --> dropout --> 矩阵乘法”，矩阵乘法和逐点操作（scale，mask，dropout）的分块计算是容易实现的，难点在于softmax的分块计算。由于计算softmax的归一化因子（分母）时，需要获取到完整的输入数据，进行分块计算的难度比较大。论文中也是重点对softmax的分块计算进行了阐述。

tiling的主要思想是分块计算注意力。**分块计算的难点在于softmax的分块计算**，softmax与 K 的列是耦合的，通过引入了两个额外的统计量 m(x), l(x) 来进行解耦，实现了分块计算。为了保证数值稳定性，对于 $x\in R^B$ ，执行“减去最大值”的safe softmax的计算过程如下：

![](https:/pic3.zhimg.com/v2-1c6e908b8857c7c358b1db808eaa4f48_1440w.png)

对于两个向量 $x^{(1)},x^{(2)}\in R^{B}$ ，解耦拼接向量 $x=[x^{(1)},x^{(2)}]\in R^{2B}$ 的softmax计算：

![](https:/pic2.zhimg.com/v2-b62cf9dd4f72b18212da6875e4d49a83_1440w.png)

通过保持两个额外的统计量 m(x), l(x) ，可以实现softmax的分块计算。需要注意的是，可以利用GPU多线程同时并行计算多个block的softmax。为了充分利用硬件性能，**多个block的计算不是串行（sequential）的， 而是并行的**。

下面通过例子说明如何分块计算softmax。对向量 \[1,2,3,4\] 计算softmax，分成两块 \[1,2\] 和 \[3,4\] 进行计算。 计算block 1：

$$m_1 = max([1,2]) = 2$$
$$f_1=[e^{1-2},e^{2-2}]=[e^{-1},e^0]$$$$l_1=\sum f_1 = e^{-1}+e^0$$$$o_1=\frac{f_1}{l_1}=\frac{[e^{-1},e^0]}{e^{-1}+e^0}$$

计算block 2:

$$m_2 = max([3,4])=4$$

$$f_2=[e^{3-4},e^{4-4}]=[e^{-1},e^0]$$

$$l_2=\sum f_2 = e^{-1}+e^0$$

$$o_2 = \frac{f_2}{l_2} = \frac{[e^{-1},e^0]}{e^{-1}+e^0}$$

合并得到完整的softmax结果：

$$m=max(m_1,m_2) = 4$$

$$f=[e^{m_1-m}f_1,e^{m_2-m}f_2]=[e^{-3},e^{-2},e^{-1},e^{0}]$$

$$l=e^{m_1-m}l_1+e^{m_2-m}l_2 = e^{-3}+e^{-2}+e^{-1}+e^{0}$$

$$o=\frac{f}{l}=\frac{[e^{-3},e^{-2},e^{-1},e^{0}]}{e^{-3}+e^{-2}+e^{-1}+e^{0}}$$

在忽略mask和dropout的情况下，简化分析，Flash Attention算法的前向计算过程如下所示。从下图可以看到，该算法在 K,V 的维度上做外循环，在 Q 的维度上做内循环。而在[triton](https:/link.zhihu.com/?target=https%3A/github.com/openai/triton/blob/main/python/tutorials/06-fused-attention.py)的代码实现中，则采用了在 Q 的维度上做外循环，在 K,V 的维度上做内循环。

![](https:/picx.zhimg.com/v2-63d633b88fceba063ce8964b2f28b967_1440w.jpg)

### 3.3.2 重计算

上文讲到，模型训练会影响kernel融合的效果，为了后向传递计算梯度，前向计算时通常需要将某些中间结果写回到HBM中，这会产生额外的HBM读写次数，减慢运行时间。因此，Flash Attention没有为后向传递保存很大的中间结果矩阵。

在标准注意力实现中，后向传递计算 Q,K,V 的梯度时，需要用到 N\times N 的中间矩阵 S,P ，但这两个矩阵并没有保存下来。这里的技巧是重计算，保存了两个统计量 m(x)，l(x) ，后向传递时在高速的SRAM上快速地重新计算Attention，通过分块的方式重新计算注意力矩阵 S,P 。相比于标准注意力中，从HBM中读取很大的中间注意力矩阵的方法，重计算的方法要快得多。

总的来说，Flash Attention通过调整注意力的计算顺序，引入两个额外的统计量进行分块计算，避免了实例化完整的 N\times N 的注意力矩阵 P,S ，将显存复杂度从 O(N^2) 降低到了 O(N) 。另外，对于内存受限的标准注意力，**Flash Attention通过kernel融合和分块计算，大量减少了HBM访问次数，尽管由于后向传递中的重计算增加了额外的计算量FLOPs，减少了运行时间，计算速度更快**（GPT2的7.6倍）。

### 3.3.3 kernel融合

为了简化分析，上文介绍注意力时忽略了mask和dropout操作。下面详细介绍Flash Attention前向传递的细节。给定输入 Q,K,V\in R^{N\times d} ，计算得到注意力输出 O^{N\times d} 。 S=\tau QK^{\top}\in R^{N\times N}\\S^{masked} = MASK(S)\in R^{N\times N}\\P=softmax(S^{masked})\in R^{N\times N}\\P^{dropped}=dropout(P,p_{drop})\in R^{N\times N}\\O=P^{dropped}V\in R^{N\times d}\\ 其中， \tau 是softmax的缩放因子，典型的比如 \frac{1}{\sqrt{d_k}} 。MASK操作将输入中的某些元素置为 \-\infty ，计算softmax后就变成了0，其他元素保持不变；causal-lm结构和prefix-lm结构的主要差别就是MASK矩阵不同。 dropout(x,p) 逐点作用在 x 的每个元素上，以 p 的概率将该元素置为0，以 1-p 的概率将元素置为 \frac{x}{1-p} 。

tiling分块计算使得我们可以用**一个CUDA kernel**来执行注意力的所有操作。**从HBM中加载输入数据，在SRAM中执行所有的计算操作（矩阵乘法，mask，softmax，dropout，矩阵乘法），再将计算结果写回到HBM中**。通过kernel融合将多个操作融合为一个操作，避免了反复地从HBM中读写数据。 kernel融合如下图所示，图片来源于[bilibili.com/video/BV1Z](https:/link.zhihu.com/?target=https%3A/www.bilibili.com/video/BV1Zz4y1q7FX/)。

![](https:/pic1.zhimg.com/v2-6d4e18e32908b4a9f2ddf4cf74774e14_1440w.jpg)

考虑mask和dropout操作，完整Flash Attention算法的前向计算过程如下所示：

![](https:/pica.zhimg.com/v2-5d354580d161b11134246e04d456ed70_1440w.jpg)

## 4\. 后向传递

在标准注意力实现中，后向传递计算Q,K,V的梯度时，需要用到 $N\times N$ 的中间矩阵 S,P 。Flash Attention没有保存这两个矩阵，而是保存了两个统计量 $m(x)$，$l(x)$ ，在后向传递时进行重计算。

在反向传递过程中，需要计算损失函数 $\phi$ 对 $O,Q,K,V$ 的梯度。在给定 $dO\in R^{N\times d}$ 的情况下，计算梯度 $dQ,dK,dV\in R^{N\times d}$ 。其中， $dO,dQ,dK,dV$ 分别表示 $\frac{\partial \phi}{\partial O},\frac{\partial \phi}{\partial Q},\frac{\partial \phi}{\partial K},\frac{\partial \phi}{\partial V}$。

梯度 $dV$ 是容易计算的。由 $O=PV$ ，基于矩阵求导算法和链式法则，得到矩阵形式的梯度 $dV =P^{\top}dO$ 。在元素形式上，有： $$dv_j = \sum_{i}P_{ij}do_i=\sum_{i}\frac{e^{q_i^{\top}k_j}}{L_i}do_i$$之前已经计算好 $L_i$ ，就可以通过反复累加的方式计算得到 $dv_j$ 。 梯度 $dQ,dK$ 的计算是略微复杂的。首先要计算 $dP，dS$ 。由 $O=PV$ ，得到矩阵形式的梯度 $dP=dOV^{\top}$ 。在元素形式上，有： $$dP_{ij} = do_i^{\top}v_j$$ 有 $P_{i:} =softmax(S_{i:})$ 。基于 $y=softmax(x)$ 的雅各比矩阵为 $diag(y)-yy^{\top}$ 。可以得到： $$dS_{i:}=(diag(P_{i:})-P_{i:}P_{i:}^{\top})dP_{i:}=P_{i:} \circ dP_{i:} - (P_{i:}^{\top}dP_{i:})P_{i:}$$其中 $\circ$ 表示逐点相乘。

可以定义： $$D_i=P_{i:}^{\top}dP_{i:} = \sum_{j} \frac{e^{q_i^{\top}k_j}}{L_i}do_i^{\top}v_j =do_i^{\top}\sum_{j} \frac{e^{q_i^{\top}k_j}}{L_i}v_j = do_i^{\top}o_i$$将该定义代回到上式中，可以得到： $$dS_{i:} = P_{i:} \circ dP_{i:} - D_iP_{i:}$$ 因此，梯度 $dS$ 可以表示为以下形式： $$dS_{ij} = P_{ij}dP_{ij}-D_iP_{ij}=P_{ij}(dP_{ij}-D_i)$$在计算得到 $dP_{ij},dS_{ij}$ 后，可以计算 $dQ,dK$ 。有前向计算公式 $S_{ij}=q_i^{\top}k_j$ ，可以得到： $$dq_i = \sum_{j}dS_{ij}k_j = \sum_{j}P_{ij} (dP_{ij} -D_i)k_j =\sum_{j}\frac{e^{q_i^{\top}k_j}}{L_i}(do_i^{\top}v_j-D_i)k_j\\dk_j=\sum_{i}dS_{ij}q_i =\sum_{i}P_{ij}(dP_{ij}-D_i)q_i =\sum_{i}\frac{e^{q_i^{\top}k_j}}{L_i}(do_{i}^{\top}v_j-D_i)q_i$$与前向计算类似，在计算得到$L_i$后，就可以通过反复累加的方式计算得到 $dq_i,dk_j,dv_j$ 。避免了实例化矩阵 P,S ，节省了显存，后向传递的显存复杂度为 $O(N)$ 。

## 5\. 性能分析

### 5.1 计算量与显存占用

Flash Attention的计算量FLOPs主要来源于矩阵乘法。

从算法1来看，在内循环中，第9行，计算 $Q_iK_i^{\top} \in R^{B_r\times B_c}$ ，其中， $Q_i\in R^{B_r\times d},K_i\in R^{B_c \times d}$ 。计算量为 $2B_rB_cd$ ，即 $O(B_rB_cd)$ FLOPs。

在第12行，计算 $\tilde{P_{ij}}V_j\in R^{B_r\times d}$ ，其中 $\tilde{P_{ij}} \in R^{B_r\times B_c},V_j\in R^{B_c\times d}$ ，计算量为 $2B_rB_cd$ ，即 $O(B_rB_cd)$ FLOPs。共执行了 $T_cT_r=[\frac{N}{B_c}][\frac{N}{B_r}]$ 次内循环。**总的计算量**为：

$$O(\frac{N^2}{B_rB_c}B_rB_cd) = O(N^2d)$$ 关于额外的显存占用，需要 $O(N)$的显存来保存统计量 $(l,m)$ 。

### 5.2 IO复杂度

**先分析标准注意力实现的HBM访问次数**：

第一步，从HBM中读取 $Q,K \in R^{N\times d}$ ，计算 $S=QK^{\top} \in R^{N\times N}$ ，将计算结果 S 写入到HBM中。HBM的访问次数为 $O(Nd+N^2)$ 。

第二步：从HBM中读取 $S\in R^{N \times N}$ ，计算 $P=softmax(S)$ ，将计算结果 $P\in R^{N\times N}$ 写入到HBM中。HBM的访问次数为 $O(N^2)$ 。

第三步：从HBM中读取 $P\in R^{N\times N}$ 以及 $V\in R^{N\times d}$ ，计算 $O=PV$ ，将计算结果 $O\in R^{N\times d}$ 写入到HBM中。HBM的访问次数为 $O(N^2+Nd)$ 。

总的来说，**标准注意力实现的HBM访问次数为** $O(Nd+N^2)$ 。

基于算法1，**分析flash attention的HBM访问次数**：

从第6行来看， $K,V\in R^{N\times d}$ 中的每个block都只加载了一次，HBM的访问次数为 $O(Nd)$ 。

从第8行来看，对于每个 K,V 的block，完整的遍历了 $Q,O \in R^{N\times d}$ ，从HBM中读取了 $Q$ ，向HBM中写入了 $O$ 。外循环共有 $T_c$ 次。因此，HBM访问次数为 $O(T_cNd)$ 。

总的来说，**flash attention的HBM访问次数为** $O(T_cNd+Nd) = O(T_cNd)$ 。

其中， $T_c = \frac{N}{B_c}=\frac{4Nd}{M}$ 。因此flash attention的HBM访问次数为 $O(\frac{N^2d^2}{M})$ 。

注意力头维度 d 通常取值为64或128，而 M 通常为100K左右，有 $\frac{d^2}{M} \ll 1$ 。因此，f**lash attention的HBM访问次数要远远小于标准注意力实现**。这样就实现了更快的运行速度，更低的显存占用。

### 5.3 实验验证

![](https:/pic3.zhimg.com/v2-b2a7adcfa9d1a3ca0c70bd92bb32bb20_1440w.jpg)

从上图左可以看到，有 $\frac{ops}{bytes} = \frac{66.6}{40.3} \approx 1.65 < 200$ ，GPT2的训练是显存受限的。尽管由于重计算，flash attention的计算量更多，但得益于更少的HBM访问，flash attention的运行速度远快于标准注意力实现。

**block size** B_c **的大小对运行时间的影响**。从上图右可以看到，随着 $B_c$ 增大，HBM的访问次数减少，运行时间也随之减少。当 $B_c$ 超过256时，尽管HBM访问次数在减少，但运行时间并没有减少。这时性能受到了其他因素的限制，例如，计算受限。另外需要注意的是，更大的 $B_c$ 可能会导致执行一次kernel需要的显存超出SRAM的大小。

### 5.4 与位置编码的关联

**Flash Attention与位置编码的关联**。对于旋转位置编码RoPE，是作用在每个transformer层self-attention块的 Q,K 上，再计算attention scores，RoPE是作用在Flash Attention算子之外。而对于ALiBi位置编码，是作用在attention scores上的，在Flash Attention算子之内。因此，如果要使用ALiBi位置编码，在进行kernel融合时要考虑到ALiBi。目前，[flash-attention](https:/link.zhihu.com/?target=https%3A/github.com/HazyResearch/flash-attention/)原作者用CUDA实现的 flash attention还不支持ALiBi位置编码，但[triton](https:/link.zhihu.com/?target=https%3A/github.com/openai/triton/blob/main/python/tutorials/06-fused-attention.py)实现版本已经支持了ALiBi位置编码。

## 6\. 总结

多数大语言模型输入输出的最大序列长度只有2K或4K，本质原因是transformer的核心组件self-attention块的计算复杂度和空间复杂度是 O(N^2) 的。FlashAttention是一种加速计算，节省显存，IO感知的精确注意力。self- attention的性能是受限于内存带宽的，FlashAttention通过分块计算、算子融合和重计算技术，有效减少了内存访问次数，从而实现了计算加速。

## 7\. 参考链接

1. Dao T, Fu D, Ermon S, et al. Flashattention: Fast and memory-efficient exact attention with io-awareness\[J\]. Advances in Neural Information Processing Systems, 2022, 35: 16344-16359.
2. Touvron H, Lavril T, Izacard G, et al. Llama: Open and efficient foundation language models\[J\]. arXiv preprint arXiv:2302.13971, 2023.
3. Penedo G, Malartic Q, Hesslow D, et al. The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only\[J\]. arXiv preprint arXiv:2306.01116, 2023.
4. Rabe M N, Staats C. Self-attention Does Not Need O (n^ 2) Memory\[J\]. arXiv preprint arXiv:2112.05682, 2021.
5. Milakov M, Gimelshein N. Online normalizer calculation for softmax\[J\]. arXiv preprint arXiv:1805.02867, 2018.
6. [bilibili.com/video/BV1S](https:/link.zhihu.com/?target=https%3A/www.bilibili.com/video/BV1SW4y1X7kh/)