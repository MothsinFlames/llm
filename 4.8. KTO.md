---
created: 2025-09-01T14:58:07 (UTC +08:00)
tags: [LLM]
source: https://zhuanlan.zhihu.com/p/717258811
author: 关于作者张Mc回答26文章39关注者959关注他发私信
---
[PPO](https://zhida.zhihu.com/search?content_id=247520706&content_type=Article&match_order=1&q=PPO&zhida_source=entity)是做标准的强化学习，DPO跳过了复杂的RL，一步到位直接优化[reward loss](https://zhida.zhihu.com/search?content_id=247520706&content_type=Article&match_order=1&q=reward+loss&zhida_source=entity)。这两者所用的[fine tune数据](https://zhida.zhihu.com/search?content_id=247520706&content_type=Article&match_order=1&q=fine+tune%E6%95%B0%E6%8D%AE&zhida_source=entity)一样，都是同一问题的两个回答的偏好（大于/小于），且都是在最大化 reward。

而KTO则是另一种思路，它**关注的是答案偏离平均水准的程度**——**比平均好还是坏**。所以它的训练数据集是对单个问答的“好/差”标注，而不再是成对数据间谁好谁差（所以用户对LLM结果的点赞或踩就可以当做反馈使用了）。

（下文乃《KTO》学习笔记，提到“作者”都指《KTO》一文作者）


**KTO 的 loss**

loss 如下：

$$L_{KTO}(\pi_\theta, \pi_{ref})  = \text{E}_{x, y\sim D}[\lambda (1-v_{KTO}(x, y, \beta))]$$其中：

（1） x 是问，y 是x的回答，且要有该回答好还是坏的标注 desirable / undesirable。好与坏时 $\lambda$ 分别取不同的值 $\lambda_D 与 \lambda_U$ 。 $\pi_{ref}$ 为[SFT](https://zhida.zhihu.com/search?content_id=247520706&content_type=Article&match_order=1&q=SFT&zhida_source=entity)阶段后的待KTO的模型。

（2） $v_{KTO}(x, y, \beta) :=  \begin{cases}  \sigma(r_{KTO}(x, y) - z_{ref}) & \text{if } x, y \sim desirable \\ \sigma(z_{ref} - r_{KTO}(x, y)) & \text{if } x, y \sim undesirable \end{cases}$

（3） $r_{KTO}(x, y) := \beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}$ ，此为 DPO 一文所推导出的策略 $\pi$ 所蕴含的 reward

（4）$z_{ref} := \text{E}_{x'\sim D}[\beta \cdot \text{KL}(\pi_\theta(y'|x') || \pi_{ref}(y'|x'))]$，它正好是平均 reward的计算公式。乃比对的参照原点。

解释：

（1）直观上看： $z_{ref}$ 乃平均 reward，代表不好不坏的居中的结果。$v_{KTO}$ 代表 DPO-reward 偏离该均值的程度，偏离越大则取sigmoid后越趋近于1， 无偏离则为0。所以KTO本意是要让标注为正或负的样本的 $v_{KTO}$ 能取得最大值1。而 loss 函数是最小化操作，所以要变成 $1-v_{KTO}$ 形式。另外为了平衡 desirable / undesirable两种情况，而用相应的 loss weight $\lambda_D 或 \lambda_U$ 。

（2）关于 $r_{KTO}(x, y) := \beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}$ ：

![](https://picx.zhimg.com/v2-554ef8fdb7e8cecd2ac157de92b67373_1440w.jpg)

此乃[《DPO》](https://zhida.zhihu.com/search?content_id=247520706&content_type=Article&match_order=1&q=%E3%80%8ADPO%E3%80%8B&zhida_source=entity)一文的结果。简单说：对 RL 的目标函数 $\text{E}_{\space x,y}r_\theta - \beta\cdot\text{KL}$ ，居然可以直接解出待优化的参数，然后反解出reward函数。鉴于目标函数是RL目标函数，因此可以说DPO没做RL，但还是RL相关，而 KTO 继承自 DPO，所以说 KTO 也还是 RL 相关的。且按DPO一文，这样的reward，如果彼此只差一个与 y 无关的 h(x) 函数的，都可以算为一个等价类。同一类内不同元素，可以当同一个看待，所以这里就把 $\beta \log Z(x)$ 省了。

注意：既然 $r_{KTO}(x, y)$ 取了那样形式，那么经 KTO 优化后上图(4)式成立【毕竟少了个 $\beta \log Z(x)$ ，而按DPO的lemma 2，同一等价类也就是差一个h(x)项的，会导出同一个（4）式】，从而 RL 的目标 $\text{E}_{\space x,y}r_\theta - \beta\cdot\text{KL}$ 也得到了最大化。

（3）关于 $z_{ref} := \text{E}_{x'\sim D}[\beta \cdot \text{KL}(\pi_\theta(y'|x') || \pi_{ref}(y'|x'))]$ ：按定义 $z_{ref}$ = avg(reward), 即直接推导有：

$\begin{align} z_{ref} &= \text{E}_{x∼D,\space y∼π(x)} [r_{KTO}(x, y)] \\ &= \text{E}_{x∼D,\space y∼π(x)} \beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)} \\  &= \beta \text{E}_{x\sim D} \sum_y  \pi_\theta(y|x)  \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}  \\ &= \beta \text{E}_{x \sim D} \text{KL}(\pi_\theta(y|x), \pi_{ref}(y|x))  \end{align}$


**实操**

从loss角度，还是很好理解的。实操上，

（1）平均reward 的 [KL 项](https://zhida.zhihu.com/search?content_id=247520706&content_type=Article&match_order=1&q=KL+%E9%A1%B9&zhida_source=entity)估计，kto作者用一个batch内数据来估计之：

$KL=\max(0, \frac{1}{m}\sum_{x \in batch, y \in batch} \log \frac {\pi_\theta(y|x)}{\pi_{ref}(y|x)})$ , 且对 $z_{ref}$ 项不梯度回传。

（2）beta 超参和DPO中的beta一样，作者取0.1。loss weight $\lambda_D / \lambda_U$ 取值要反映正负两类的样本数量： $\frac{\lambda_正 \cdot num_{正}}{ \lambda_负  \cdot num_{负} }\in [1, 4/3]$ 区间，即 $\frac{\lambda_D }{ \lambda_U }\in \frac{num_负}{num_正}\cdot[1, 4/3]$ 。这样，则更偏重正样本。

（3）无论标注数据什么来源，都应转化成好坏二值标注。对于DPO格式数据，y1>y2, kto作者用y1为正，y2为负。和DPO 一样，都是在提前准备好的数据上训练，过程中不作采样。而 PPO 开训后的训练数据是动态采样生成的。


**啥为 KT，上面 loss 咋来的**

从 loss 倒推，其逻辑也说得过去。但据《KTO》一文，它是取法于K某与T某二人发明的前景理论。

![](https://pic1.zhimg.com/v2-ce720d3098a9a06d266d4aad53aa4520_1440w.jpg)

如图，KT 理论讲的是，对于某一可客观外在衡量的东西，比如金钱，当你收到了一定量的该物时，你内心的感受是怎样的？你的感受并不会线性于其客观度量值(有如经济学中的效用理论，吃5个馒头的满足感，并不是5倍的单个馒头）。

按KT理论，如果你一次获得的回报z是服从一定概率分布的，则感知的回报u(z)取决于w与v两者的sum(w\*v)，如果z是定值，则只取决于 $v(z； \lambda, \alpha, $z_0$)$ 。而对于 $v(z; ..)$，则是人心中自有它的基准点 $z_0$，如果收获的z值超过$z_0$, 则认为赚到了，不足$z_0$, 则是赔了 ——假如是工作的金钱回报，可以认为 $z_0$ 代表你认为你的工作付出值多少钱，老板给你的超过了该预期，则感觉赚到了，否则感觉没得到应有的回报。如图中曲线的凹凸，偏离$z_0$越远（即$|z-$z_0$|$越大），则v越不敏感。

KTO 在借鉴以上理论时，把 $z$ 当做定值，从而 $u(z)=v(z; ..)$ 直接即为human 感知的 utility。然后从 v(z; ..)开始，其作者构建出了KTO方法。

(1）KT 中的 $z$（比如money），在 KTO 中就是 reward： $r_{KTO}(x, y) := \beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}$

(2）KT 中的 $z_0$，在 KTO 中就是平均reward： $z_{ref} = \text{E} [r_{KTO}(x, y)]  = \beta \text{E}_{x \sim D} \text{KL}(\pi_\theta||\pi_{ref})$

(3）KT 中的 $\lambda$ ，用于调节gain与loss两者时的不同感受程度的，在 KTO 中就是 $λ_D与λ_U$ 。但是 KT 更侧重 loss 一侧，而 KTO 中更侧重于 gain/desirable一侧。

(4）KT中的 $(z-z_0)^a$, 在 KTO 中则是$sigmoid(z-z_0)$：即

$$v(x, y) =  \begin{cases}  \lambda_D \cdot \sigma(r(x, y) - z_{ref}) & \text{if } gain \\ -\lambda_U\cdot\sigma(z_{ref} - r(x,y) & \text{if } loss \end{cases}$$

KT 与 KTO 中，两者图像在$z_0$左右的凹凸性是一致的（结合上图与下图可见，KTO在凹凸性上比PPO以及DPO更一致）。不同在于：KT中是连续分段函数，KTO 中不连续。

![](https://picx.zhimg.com/v2-44cfa12cd2e4314acf56cc09ce4f557f_1440w.jpg)

————

**关于 [HALO](https://zhida.zhihu.com/search?content_id=247520706&content_type=Article&match_order=1&q=HALO&zhida_source=entity)**

KTO 一文引入了 HALO 的概念，其目的大概在于说明：PPO-clip / DPO 是 HALO的，而某某另两者不是 HALO的，而实验证实HALO 更有效。于是说明 PPO-clip / DPO 之成功，大概 HALO 起着重要作用。而 KTO 也是 HALO 的，所以 KTO当然不会差。

所谓HALO，描述的是loss函数。简单说指的是：如果从KT角度有某一种 $v(z-z_0)=v(r_\theta - \text{E}(r_\theta))$ , 满足整体单调上升且在$z_0$右侧是concave(凹，上凸)的，然后如果某一loss函数的形式是该v的线性形式，则该loss称为HALO的。

![](https://pic1.zhimg.com/v2-6652878e21666e4db3283bfac991240e_1440w.jpg)

见上图：对PPO_clip / DPO / KTO，按作者构造的v, 确实满足 HALO 的要求：$z_0$右侧凹。但PPO_clip / DPO左侧凹凸性和KT不符，而KTO是左右都和KT一致。所以作者对HALO的独特要求，大概是看着 PPO_clip/DPO 图搞出来的。

**其效果**

一般是先SFT再作对齐。按kto作者的实验，无论pre-train后是否作SFT，KTO都比DPO效果好。特别地，即使不做SFT直接KTO，好像效果比SFT+KTO也差不太多——也就是说，用 KTO 甚至可以跳过 SFT。

如果你有DPO的pair标注数据，但是标注噪声太大，矛盾标注太多，那么用KTO可能比DPO更好。

**为啥KTO效果好**

按 Theoretical Analysis 一节，有以下：

（1）不容易把reward学得偏离正常太远：因为 $loss = \text{E} [\lambda (1-\sigma(r-r_0)]$ （这里只看这一分支），如果学出的r太大，因sigmoid有很强约束作用，取值只是稍为更趋于1而已。这时候，r的大幅变动不会导致loss的大的变动，从而它导致的参数更新会很小。

（2）按 DPO 理论，同一reward等价类的不同reward函数，对应着同一个最优策略与BT preference distribution。但是这些reward，其实对应着不同的 KT-value函数。

作模型最终是给人看的，最终在于人觉得好才是好。而KTO正是直接优化人的感知。DPO等优化搞出的reward 的 KT-value函数，未必对应着最佳value函数，终究是隔着一层皮。

![](https://picx.zhimg.com/v2-1bbb782f3bb821e37daf834d4437a92d_1440w.jpg)

（3）对于DPO的一条pair标注数据 $x\sim{y_a, y_b}$，如果有矛盾标注(不同人标注的不同)，按正常思路应该是少数服从多数。在某些情况下，DPO居然会采信于少数派，而 KTO 并不会如此（若 $\lambda_D = \lambda_U$ ）。

![](https://pic1.zhimg.com/v2-00c786cd90cabf623a2d87e10bff83b2_1440w.jpg)

————

附注：其文有v1, v2两版本，v1更容易看懂。

___

other:

1.  其实KTO 可以只用负样本来训。负样本是相对于基准点的。
2.  KTO 没法融入通用样本，所以经它之后效果可能退化，一种方式是 KTO 之后再加一轮 SFT。




KTO算法（KTO: Model Alignment as Prospect Theoretic Optimization）作为DPO算法系列的延伸

很好的解决了DPO算法必须使用偏好对数据的问题，因为很多时候收集偏好对确实是麻烦且昂贵的。

而KTO只需要针对每一个（prompt, response）有一个好或坏的二值信号即可。


https://zhuanlan.zhihu.com/p/699635576
## 1\. 背景及介绍

现有的对齐算法如PPO和DPO在对齐人类偏好方面取得了很大的进步，我们可以将他们归类描述为人类感知的损失函数（_human-aware loss functions_, HALOs）。本文基于1979年Kahneman和Tversky提出的前景理论（ prospect theory），在1B到30B大小的模型中大幅超越了PPO和DPO的性能表现。

简单来说，本文主要说明了以下几点：

1.  KTO在1B到30B的模型中的性能表现超越了PPO和DPO，具体可见图1。
2.  KTO可以在减少90%的preferred样本的情况下，达到与DPO同样的性能。
3.  如果预训练模型足够好，相比于PPO和DPO必须在经过一次SFT，KTO可以直接跳过sft阶段也能得到很好的性能。
4.  **当数据噪声多、正负分布不均的情况下，KTO是一个很好的选择。但如果偏好对数据足够且噪声小，DPO或许表现更佳。**

![|700](https://pic4.zhimg.com/v2-d8fb7d2d8bd5b16773669657ea69ce6d_1440w.jpg)

图1：各算法 win rate

## 2.1. 前景理论

KTO算法基于前景理论而提出。前景理论（Prospect Theory）由心理学家丹尼尔·卡尼曼（Daniel Kahneman）和阿莫斯·特沃斯基（Amos Tversky）在1979年提出。

该理论旨在解释人们在面对不确定性和风险时的决策行为，特别是为什么人们的决策往往偏离传统经济学中的预期效用理论，如：为什么人们会购买保险但同时也参与赌博；为什么投资者在市场下跌时往往表现出恐慌性抛售行为。

### 2.1.1. 核心概念

前景理论的**核心概念**包括以下几个方面：

1.  **[参考点](https://zhida.zhihu.com/search?content_id=243604931&content_type=Article&match_order=1&q=%E5%8F%82%E8%80%83%E7%82%B9&zhida_source=entity)（Reference Point）**：人们通常不是基于绝对结果做决策，而是基于相对于某个参考点的变化。参考点可以是当前的财富水平、预期的结果或其他某个基准。
2.  **[价值函数](https://zhida.zhihu.com/search?content_id=243604931&content_type=Article&match_order=1&q=%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0&zhida_source=entity)（Value Function）**：前景理论中的价值函数解释了人们对收益和损失的感知和评价，具有以下特点：
	-   **参考点（Reference Point）**：价值是基于参考点进行计算的。
	-   **[损失厌恶](https://zhida.zhihu.com/search?content_id=243604931&content_type=Article&match_order=1&q=%E6%8D%9F%E5%A4%B1%E5%8E%8C%E6%81%B6&zhida_source=entity)（Loss Aversion）**：价值函数对损失的敏感度大于对同等数额收益的敏感度。
		-   换句话说，损失带来的痛苦大于等量收益带来的快乐。这意味着面对相同的数额，人们更愿意避免损失而不是获取收益。
		-   数学上表现为损失区域的斜率比收益区域的斜率陡。
		-   **[边际效用递减](https://zhida.zhihu.com/search?content_id=243604931&content_type=Article&match_order=1&q=%E8%BE%B9%E9%99%85%E6%95%88%E7%94%A8%E9%80%92%E5%87%8F&zhida_source=entity)（Diminishing Sensitivity）**：随着收益或损失的增加，边际效用（或边际痛苦）递减。也就是说，100美元和200美元的差别带来的效用变化大于1100美元和1200美元的差别

3.  **概率加权（Probability Weighting）**：人们对概率的感知不是线性的，倾向于高估小概率事件和低估大概率事件。这意味着在决策过程中，人们往往会给予小概率事件更多的权重，而对高概率事件的权重则相对不足。

### 2.1.2. 数学角度

从数学角度来看，由以下几个部分构成：

1.  **价值函数（Value Function)**

价值函数 $v(x)$ 描述了相对于参考点的收益或损失的主观价值。其数学形式通常是分段函数：
$$v(z)= \begin{cases}(z-z_{\text{ref}})^\alpha & \text { if } x \geq 0 \\ -\lambda(z_{\text{ref}}-z)^\beta & \text { o/w } \end{cases}$$

其中:
-   $x$ 是相对于参考点的收益或损失。
-   $\alpha$ 和 $\beta$ 通常在 $0<\alpha, \beta \leq 1$, 表示递减敏感性，即价值函数的变化速度。


-   当 $\alpha$ 和 $\beta$ 较小时（接近 0), 函数的增长速度会减缓, 表现出对小变化更加敏感, 即对收益和损失的敏感性较高。
![|500](https://pic3.zhimg.com/v2-437a15723a9b90ffe80b37fb2d06d75a_1440w.jpg)


- 当 $\alpha$ 和 $\beta$ 较大时（接近 1), 函数接近线性, 表示对变化的敏感性降低, 即对收益和损失的敏感性较低。
![|500](https://pic1.zhimg.com/v2-ac47db3c7f669c1426901f20bcd639f6_1440w.jpg)

-   $\lambda$ 是损失规避参数, 通常 $\lambda>1$, 表示损失比收益更痛苦。

1.  **[决策权重函数](https://zhida.zhihu.com/search?content_id=243604931&content_type=Article&match_order=1&q=%E5%86%B3%E7%AD%96%E6%9D%83%E9%87%8D%E5%87%BD%E6%95%B0&zhida_source=entity) (Probability Weighting Function)**

决策权重函数 $\pi(p)$ 描述了人们对概率的主观感知。常用的形式是:

$$\pi(p)=\frac{p^\gamma}{\left(p^\gamma+(1-p)^\gamma\right)^{1 / \gamma}}$$

其中:

-   $p$ 是客观概率。
-   $\gamma$ 通常在 $0<\gamma<1$, 表示人们对小概率事件的高估和对大概率事件的低估。

下图蓝色曲线和红色曲线分别表示$\gamma=0.5 \text{和} 2$时的函数曲线。当$\gamma=0.5$时，输入概率$p<0.23$左右时实际计算出的概率$\pi(p) \gt p$。

![|550](https://pic4.zhimg.com/v2-6a2ece82c231361075e7bf401daee82d_1440w.jpg)

1.  **总体前景值 (Overall Prospect Value)**

前景值 $V$ 是所有可能结果的加权价值之和。对于一个有 $n$ 个可能结果的决策问题, 其数学表示为:

$$V=\sum_{i=1}^n \pi\left(p_i\right) v\left(z_i\right)$$

其中:

-   $p_i$ 是第 $i$ 个结果的客观概率。
-   $x_i$ 是第 $i$ 个结果的收益或损失。
-   $\pi\left(p_i\right)$ 是对应的决策权重。
-   $v\left(z_i\right)$ 是对应的价值。

## 2.2. KTO算法推导

备注：本文desirable来表示preferred样本，undesirable表示un-preferred。

从[DPO的推导](https://zhuanlan.zhihu.com/p/681559204)中，我们知道其最优策略的表示为：

$$\pi^{*}(y \mid x)=\frac{1}{Z(x)} \pi_{\text {ref }}(y \mid x) \exp \left(\frac{1}{\beta} r^{*}(x, y)\right)$$

其中，$Z(x)$为配分函数。我们转换形式，可以得到奖励函数：

$$r^{*}(x, y)=\beta \log \frac{\pi^{*}(y \mid x)}{\pi_{\text {ref }}(y \mid x)}+\beta \log Z(x)$$

与DPO将该奖励函数的表示代入Bradley-Terry model以获得损失函数式子不同，这里将其代入修改版的Kahneman-Tversky model的价值函数中。我们得到该理论模型的各部分具体如下：

1.  参考点$z_\text{ref}$。本文假设人类是根据看过的所有$(x,y)$数据后来判断某个$(x,y)$的相对质量的，而不是其绝对质量。因此本文将参考点作为最优策略下的期望报酬。假设所有输入$x$的配分函数期望值为零，参考点$z_\text{ref}$可以简化为$\pi^\star$和$\pi_\text{ref}$之间的KL散度：

$$z_{\text {ref }}=\mathbb{E}_{x^{\prime} \sim D}\left[\beta \mathrm{KL}\left(\pi_{\theta}\left(y^{\prime} \mid x^{\prime}\right) \| \pi_{\mathrm{ref}}\left(y^{\prime} \mid x^{\prime}\right)\right)\right]$$

1.  价值函数$v_{\text{KTO}}$。原始价值函数中包含指数项，导致计算量大难以优化，这里用logistic函数$\sigma$:$\frac{1}{1+e^{-x}}$代替，同样起到对小变化敏感的效果。

$$v_{\mathrm{KTO}}(x, y ; \beta)=\left\{\begin{array}{l}\sigma\left(r_{\mathrm{KTO}}(x, y)-z_{\text {ref }}\right) \text { if } y \sim y_{\text {desirable }} \mid x \\ \sigma\left(z_{\text {ref }}-r_{\mathrm{KTO}}(x, y)\right) \text { if } y \sim y_{\text {undesirable }} \mid x\end{array}\right.$$

$$r_{\mathrm{KTO}}(x, y)=\beta \log \frac{\pi_{\theta}(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}$$

![|500](https://pic4.zhimg.com/v2-c9324a43b36f774049230f15ed250eb3_1440w.jpg)

1.  决策权重函数$w$。简单起见，本文用两个超参来分别加权对喜欢和厌恶样本的损失：

$$w(y)=\left\{\begin{array}{ll}\lambda_{D} & \text { if } y \sim y_{\text {desirable }} \mid x \\ \lambda_{U} & \text { if } y \sim y_{\text {undesirable }} \mid x\end{array}\right.$$

1.  损失函数。综合上面三个式子以及前面我们讲到的_总体前景值公式_。我们可以推导出其损失函数如下：

$$L_{\mathrm{KTO}}\left(\pi_{\theta}, \pi_{\text {ref }}\right)=\mathbb{E}_{x, y \sim D}\left[w(y)\left(1-v_{\mathrm{KTO}}(x, y ; \beta)\right)\right]$$

## 3\. 实验配置

## 3.1. 超参数

1.  $\beta$ 。与DPO算法中一致，根据经验，一般情况下$\beta=0.1$时效果最佳
2.  $\lambda_D \text{与} \lambda_U$。令$n_D$与$n_U$分别为desirable和undesirable样本的数量。从实验经验来看，这几项之间最好保持在以下范围：$\frac{\lambda_{D} n_{D}}{\lambda_{U} n_{U}} \in\left[1, \frac{4}{3}\right]$

## 3.2. 实验结果

从实验结果可以看出，在输出长度控制以及胜率两个方面，KTO都要优于DPO的表现。

1.  输出长度对比

![|575](https://picx.zhimg.com/v2-9e52e1b86e07a71a0bee0dc0fa354599_1440w.jpg)

1.  胜率对比

![|700](https://pic3.zhimg.com/v2-6924647d4eaf91ff1bf3cd7c2d84c6d6_1440w.jpg)

## 3.3. 代码实现

参考trl的实现：[https://github.com/huggingface/trl/blob/main/trl/trainer/kto\_trainer.py](https://link.zhihu.com/?target=https%3A//github.com/huggingface/trl/blob/main/trl/trainer/kto_trainer.py)

## 4\. Reference

1.  [《前景理论》](https://link.zhihu.com/?target=https%3A//baijiahao.baidu.com/s%3Fid%3D1757762346347953823%26wfr%3Dspider%26for%3Dpc)
2.  [《KTO: Model Alignment as Prospect Theoretic Optimization》](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2402.01306)
3.  [《Direct Preference Optimization: Your Language Model is Secretly a Reward Model》](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2305.18290.pdf)
4.  [https://zhuanlan.zhihu.com/p/681559204](https://zhuanlan.zhihu.com/p/681559204)
5.  [https://github.com/huggingface/trl/blob/main/trl/trainer/kto\_trainer.py](https://link.zhihu.com/?target=https%3A//github.com/huggingface/trl/blob/main/trl/trainer/kto_trainer.py)
