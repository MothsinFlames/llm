---
source: "https://zhuanlan.zhihu.com/p/17434772442"
---
## 2\. KV Cache的产生
为了缓解上述缺陷， **引入KV Cache** ，将历史每个head的key和value缓存起来，避免不必要的重复计算，减少计算量，本质是通过**空间换时间的方式提升推理速度**。
**KV Cache存在的问题：** KV Cache在缓存KV时占用显存，且KV Cache的大小与序列长度是线性相关的。如果输入序列长度越来越大，可能会导致模型KV Cache大小超过单卡的显存量，基于"卡内通信带宽 > 卡间通信带宽 > 机间通信带宽" 原理，使得模型在长文本推理时速度变慢。
**缓解上述问题方案：** 在 **尽可能** 保证效果的前提下(减少KV意味着减少特征信息)，减少KV Cache的大小，使得模型在更少的设备上推理更长的文本。
因此MQA和GQA和MLA都是为了在尽可能保证效果的同事，减少KV Cache的大小，对MHA进行改进。
### 5.2 MLA with RoPE
上述都是在讲不到旋转位置编码的，现在考虑在QK中加入位置编码RoPE，直接加入是不兼容的，具体原因如下：
**RoPE无法直接合并原因：**
假设在MLA上QK直接加入RoPE，QK矩阵运算就变成如下，相比4.1中 **权重矩阵** $Wq(s)Wk(s)\boldsymbol{W}_q^{(s)} \boldsymbol{W}_k^{(s)}\boldsymbol{W}_q^{(s)} \boldsymbol{W}_k^{(s)}$ **中间加了一个矩阵R(与输入有关系)** ，由于矩阵相乘不满足乘法交换律，导致 **中间的矩阵在推理前无法吸收合并** ，MLA无法直接与RoPE进行结合。  
$qi(s)=xiWq(s)Ri,ki(s)=ciWk(s)Riqt(s)ki(s)⊤=(xtWq(s)Rt)(ciWk(s)Ri)⊤=xt(Wq(s)Rt→iWk(s)⊤)ci⊤\begin{aligned} \boldsymbol{q}_i^{(s)} &= \boldsymbol{x}_i \boldsymbol{W}_q^{(s)} \boldsymbol{\mathcal{R}}_i, \quad \boldsymbol{k}_i^{(s)} = \boldsymbol{c}_i \boldsymbol{W}_k^{(s)} \boldsymbol{\mathcal{R}}_i \\ \boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)\top} &= \left( \boldsymbol{x}_t \boldsymbol{W}_q^{(s)} \boldsymbol{\mathcal{R}}_t \right) \left( \boldsymbol{c}_i \boldsymbol{W}_k^{(s)} \boldsymbol{\mathcal{R}}_i \right)^\top = \boldsymbol{x}_t \left( \boldsymbol{W}_q^{(s)} \boldsymbol{\mathcal{R}}_{t \rightarrow i} \boldsymbol{W}_k^{(s)\top} \right) \boldsymbol{c}_i^\top \end{aligned}$ **解耦QK加入RoPE：**
为了将旋转位置编码加入，MLA将QK进行解耦成两部分。  
$ci′=xiWc′∈Rdc′,Wc′∈Rd×dc′ci=xiWc∈Rdc,Wc∈Rd×dc\begin{aligned} \boldsymbol{c}'_i &= \boldsymbol{x}_i \boldsymbol{W}'_c \in \mathbb{R}^{d'_c}, & \boldsymbol{W}'_c \in \mathbb{R}^{d \times d'_c} \\ \boldsymbol{c}_i &= \boldsymbol{x}_i \boldsymbol{W}_c \in \mathbb{R}^{d_c}, & \boldsymbol{W}_c \in \mathbb{R}^{d \times d_c}  \end{aligned} \\\begin{aligned} \boldsymbol{c}'_i &= \boldsymbol{x}_i \boldsymbol{W}'_c \in \mathbb{R}^{d'_c}, & \boldsymbol{W}'_c \in \mathbb{R}^{d \times d'_c} \\ \boldsymbol{c}_i &= \boldsymbol{x}_i \boldsymbol{W}_c \in \mathbb{R}^{d_c}, & \boldsymbol{W}_c \in \mathbb{R}^{d \times d_c}  \end{aligned}$ $qi(s)=[ci′Wqc(s),ci′Wqr(s)Ri]∈Rdk+dr,Wqc(s)∈Rdc′×dk,Wqr(s)∈Rdc′×drki(s)=[ciWkc(s),xiWkrRi]∈Rdk+dr,Wkc(s)∈Rdc×dk,Wkr∈Rd×drvi(s)=ciWv(s)∈Rdv,Wv(s)∈Rdc×dv\begin{aligned} \boldsymbol{q}_i^{(s)} &= \left[ \boldsymbol{c}'_i \boldsymbol{W}_{qc}^{(s)}, \boldsymbol{c}'_i \boldsymbol{W}_{qr}^{(s)} \boldsymbol{\mathcal{R}}_i \right] \in \mathbb{R}^{d_k + d_r}, & \boldsymbol{W}_{qc}^{(s)} \in \mathbb{R}^{d'_c \times d_k}, & \boldsymbol{W}_{qr}^{(s)} \in \mathbb{R}^{d'_c \times d_r} \\ \boldsymbol{k}_i^{(s)} &= \left[ \boldsymbol{c}_i \boldsymbol{W}_{kc}^{(s)}, \boldsymbol{x}_i \boldsymbol{W}_{kr}^{} \boldsymbol{\mathcal{R}}_i \right] \in \mathbb{R}^{d_k + d_r}, & \boldsymbol{W}_{kc}^{(s)} \in \mathbb{R}^{d_c \times d_k}, & \boldsymbol{W}_{kr}^{} \in \mathbb{R}^{d \times d_r} \\ \boldsymbol{v}_i^{(s)} &= \boldsymbol{c}_i \boldsymbol{W}_v^{(s)} \in \mathbb{R}^{d_v}, & \boldsymbol{W}_v^{(s)} \in \mathbb{R}^{d_c \times d_v} \\  \end{aligned}$ Q包含没有RoPE多头的 $ci′Wqc(s)\boldsymbol{c}'_i \boldsymbol{W}_{qc}^{(s)}$ 和具有RoPE多头的 $ci′Wqr(s)Ri\boldsymbol{c}'_i \boldsymbol{W}_{qr}^{(s)} \boldsymbol{\mathcal{R}}_i$ ，K包含没有RoPE的多头 $ciWkc(s)\boldsymbol{c}_i \boldsymbol{W}_{kc}^{(s)}$ 和有RoPE共享的 $xiWkrRi\boldsymbol{x}_i \boldsymbol{W}_{kr}^{} \boldsymbol{\mathcal{R}}_i$ 。QK矩阵计算就变成如下：  
$qt(s)ki(s)⊤=[ct′Wqc(s),ct′Wqr(s)Rt][ciWkc(s),xiWkrRi]⊤=ct′Wqc(s)Wkc(s)⊤ci⊤+ct′Wqr(s)RtRi⊤Wkr⊤xi⊤\begin{aligned}  \boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)\top} = \left[ \boldsymbol{c}'_t \boldsymbol{W}_{qc}^{(s)}, \boldsymbol{c}'_t \boldsymbol{W}_{qr}^{(s)} \boldsymbol{\mathcal{R}}_t \right]   \left[ \boldsymbol{c}_i \boldsymbol{W}_{kc}^{(s)}, \boldsymbol{x}_i \boldsymbol{W}_{kr}^{} \boldsymbol{\mathcal{R}}_i \right]^\top   \\ =  \boldsymbol{c}'_t \boldsymbol{W}_{qc}^{(s)} {\boldsymbol{W}_{kc}^{(s)}}^\top {\boldsymbol{c}_i}^\top + \boldsymbol{c}'_t \boldsymbol{W}_{qr}^{(s)} \boldsymbol{\mathcal{R}}_t {\boldsymbol{\mathcal{R}}_i}^\top {\boldsymbol{W}_{kr}}^\top {\boldsymbol{x}_i}^\top  \end{aligned}$ **MLA推理过程：**
通过矩阵吸收合并，以上可以简化为：
$ci′=xiWc′∈Rdc′,Wc′∈Rd×dc′ci=xiWc∈Rdc,Wc∈Rd×dc\begin{aligned} \boldsymbol{c}'_i &= \boldsymbol{x}_i \boldsymbol{W}'_c \in \mathbb{R}^{d'_c}, & \boldsymbol{W}'_c \in \mathbb{R}^{d \times d'_c} \\ \boldsymbol{c}_i &= \boldsymbol{x}_i \boldsymbol{W}_c \in \mathbb{R}^{d_c}, & \boldsymbol{W}_c \in \mathbb{R}^{d \times d_c}  \end{aligned} \\\begin{aligned} \boldsymbol{c}'_i &= \boldsymbol{x}_i \boldsymbol{W}'_c \in \mathbb{R}^{d'_c}, & \boldsymbol{W}'_c \in \mathbb{R}^{d \times d'_c} \\ \boldsymbol{c}_i &= \boldsymbol{x}_i \boldsymbol{W}_c \in \mathbb{R}^{d_c}, & \boldsymbol{W}_c \in \mathbb{R}^{d \times d_c}  \end{aligned}$ $qi(s)=[ci′Wqc(s)Wkc(s)⊤,ci′Wqr(s)Ri]∈Rdc+dr,Wqc(s)∈Rdc′×dk,Wkc(s)⊤∈Rdk×dc,Wqr(s)∈Rdc′×drki(s)=[ci,xiWkrRi]∈Rdc+dr,Wkr∈Rd×dr\begin{aligned} \boldsymbol{q}_i^{(s)} &= \left[ \boldsymbol{c}'_i \boldsymbol{W}_{qc}^{(s)} {\boldsymbol{W}_{kc}^{(s)}}^\top, \boldsymbol{c}'_i \boldsymbol{W}_{qr}^{(s)} \boldsymbol{\mathcal{R}}_i \right] \in \mathbb{R}^{d_c + d_r}, & \boldsymbol{W}_{qc}^{(s)} \in \mathbb{R}^{d'_c \times d_k}, & {\boldsymbol{W}_{kc}^{(s)}}^\top \in \mathbb{R}^{d_k \times d_c}, & \boldsymbol{W}_{qr}^{(s)} \in \mathbb{R}^{d'_c \times d_r} \\  \boldsymbol{k}_i^{(s)} &= \left[ \boldsymbol{c}_i , \boldsymbol{x}_i \boldsymbol{W}_{kr}^{} \boldsymbol{\mathcal{R}}_i \right] \in \mathbb{R}^{d_c + d_r}, &  \boldsymbol{W}_{kr}^{} \in \mathbb{R}^{d \times d_r} \\   \end{aligned}$ $ot(s)=Attention(qt(s),k≤t(s),c≤t)≜∑i≤texp⁡(qt(s)ki(s)⊤)ci∑i≤texp⁡(qt(s)ki(s)⊤)\boldsymbol{o}_t^{(s)} = \text{Attention} \left( \boldsymbol{q}_t^{(s)}, \boldsymbol{k}_{\leq t}^{(s)}, \boldsymbol{c}_{\leq t} \right) \triangleq \frac{\sum_{i \leq t} \exp \left( \boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)\top} \right) \boldsymbol{c}_i}{\sum_{i \leq t} \exp \left( \boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)\top} \right)}$ $ot=[ot(1)Wv(1),ot(2)Wv(2),⋯,ot(h)Wv(h)]\boldsymbol{o}_t = \left[ \boldsymbol{o}_t^{(1)} \boldsymbol{W}_v^{(1)}, \boldsymbol{o}_t^{(2)} \boldsymbol{W}_v^{(2)}, \cdots, \boldsymbol{o}_t^{(h)} \boldsymbol{W}_v^{(h)} \right]$ **此时，KV Cache只缓存** $ci,xiWkrRi\boldsymbol{c}_i , \boldsymbol{x}_i \boldsymbol{W}_{kr}^{} \boldsymbol{\mathcal{R}}_i$ **这两个就可以。缓存大小变为** $(dc+dr)l(d_c+d_r)l$ ，注意可以发现V没有了，V被分为两部分了， $ci\boldsymbol{c}_i$ 被放在Attention中的分子部分了， $Wv(s)\boldsymbol{W}_v^{(s)}$ 被吸收到 $ot(s)\boldsymbol{o}_t^{(s)}$ 上了。  
**总结：** MLA即达到了降低KV Cache大小的作用，还实现添加了RoPE，虽然带了点计算量(降维和升维)。
> 以上是学习苏神对MLA的解读记录，越写越感叹大佬强大扎实的数学功底。
### 5.3 DeepSeek中MLA表达
上面苏神的公式表达和论文中的公式符号只是存在一点点区别，下面我们看看论文中的表达方法。如果理解了前面的讲解，论文中非常好理解。
**Latent Vector计算：**  
$ctQ=WDQht\begin{aligned} \mathbf{c}_t^Q &= \mathbf{W}^{DQ} \mathbf{h}_t  \end{aligned} \\\begin{aligned} \mathbf{c}_t^Q &= \mathbf{W}^{DQ} \mathbf{h}_t  \end{aligned}$ $ctKV=WDKVht\begin{aligned} \mathbf{c}_t^{KV} &= \mathbf{W}^{DKV} \mathbf{h}_t \end{aligned}$ **Query计算:**  
$[qt,1C;qt,2C;…;qt,nhC]=qtC=WUQctQ,[qt,1R;qt,2R;…;qt,nhR]=qtR=RoPE(WQRctQ),qt,i=[qt,iC;qt,iR]\begin{aligned}  \left [\mathbf{q}_{t,1}^C; \mathbf{q}_{t,2}^C; \ldots; \mathbf{q}_{t,n_h}^C \right] &= \mathbf{q}_t^C = \mathbf{W}^{UQ} \mathbf{c}_t^Q, \\  \left [\mathbf{q}_{t,1}^R; \mathbf{q}_{t,2}^R; \ldots; \mathbf{q}_{t,n_h}^R \right] &= \mathbf{q}_t^R = \text{RoPE}(\mathbf{W}^{QR} \mathbf{c}_t^Q), \\ \mathbf{q}_{t,i} &= [\mathbf{q}_{t,i}^C; \mathbf{q}_{t,i}^R] \end{aligned}$ **Keys计算:**  
$[kt,1C;kt,2C;…;kt,nhC]=ktC=WUKctKV,ktR=RoPE(WKRht),kt,i=[kt,iC;ktR],\begin{aligned}  \left [\mathbf{k}_{t,1}^C; \mathbf{k}_{t,2}^C; \ldots; \mathbf{k}_{t,n_h}^C \right ] &= \mathbf{k}_t^C = \mathbf{W}^{UK} \mathbf{c}_t^{KV}, \\ \mathbf{k}_t^R &= \text{RoPE}(\mathbf{W}^{KR} \mathbf{h}_t), \\ \mathbf{k}_{t,i} &= [\mathbf{k}_{t,i}^C; \mathbf{k}_t^R], \\  \end{aligned}$  
**Values计算:**  
$vt,1C;vt,2C;…;vt,nhC]=vtC=WUVctKV\begin{aligned} \mathbf{v}_{t,1}^C; \mathbf{v}_{t,2}^C; \ldots; \mathbf{v}_{t,n_h}^C] &= \mathbf{v}_t^C = \mathbf{W}^{UV} \mathbf{c}_t^{KV} \end{aligned}$ **Attention及输出：**  
$ot,i=∑j=1tSoftmaxj(qt,i⊤kj,idh+dhR)vj,iC,ut=WO[ot,1;ot,2;…;ot,nh],\begin{aligned} \mathbf{o}_{t,i} &= \sum_{j=1}^{t} \text{Softmax}_j \left( \frac{\mathbf{q}_{t,i}^\top \mathbf{k}_{j,i}}{\sqrt{d_h + d_h^R}} \right) \mathbf{v}_{j,i}^C, \\ \mathbf{u}_t &= \mathbf{W}^O [\mathbf{o}_{t,1}; \mathbf{o}_{t,2}; \ldots; \mathbf{o}_{t,n_h}], \end{aligned}$ **顺序与4.2章节"解耦QK加入RoPE"部分的表达式对应，** 只是公式符号发生了点变化 **。**  
其中输入序列变为 $ht∈Rd\mathbf{h}_t \in \mathbb{R}^{d}$ ；  
$WDQ,WDKV\mathbf{W}^{DQ},\mathbf{W}^{DKV}$ 分别为Q和KV的低秩投影矩阵(降维);  
$WUQ∈Rdh∗nh∗dc′\mathbf{W}^{UQ} \in \mathbb{R}^{d_h*n_h*d'_c}$ (多Head)负责对query进行升维，  
$WUK,WUV∈Rdh∗nh∗dc\mathbf{W}^{UK}, \mathbf{W}^{UV}\in \mathbb{R}^{d_h*n_h*d_c}$ (多头)负责生成keys和values中不带RoPE部分，  
$WQR∈RdhR∗nh∗dc′\mathbf{W}^{QR} \in \mathbb{R}^{d^{R}_{h} * n_h*d'_c}$ (多头)和 $WKR∈RdhR∗d\mathbf{W}^{KR} \in \mathbb{R}^{d^{R}_{h} * d}$ `(多头共享)` 负责生成keys和values中带有RoPE的部分。  
**推理过程中，只需要缓存** $ctKV∈Rdc,ktR∈RdhR\mathbf{c}_t^{KV} \in \mathbb{R}^{d_c},\mathbf{k}_t^R \in \mathbb{R}^{d^{R}_h}$ **即可** ，缓存大小为 $(dc+dhR)l(d_c + d^{R}_h)l$ 。同时可以将 $WUK\mathbf{W}^{UK}$ 吸收到 $WUQ\mathbf{W}^{UQ}$ 上， $WUV\mathbf{W}^{UV}$ 吸收到 $WO\mathbf{W}^{O}$ 上，对于每个query不需要在计算keys和values了。
**优势：**
- 长文本推理：kV Cache减小，可以对超长文本进行推理；
- 性能优势：通过低秩投影和矩阵吸收的方式，性能比MHA更好。
## 6\. 效果比较
在DeepSeek-V2中，设置 $dc=4dh,dhR=dh/2d_c=4d_h, d^{R}_h=d_h /2d_c=4d_h, d^{R}_h=d_h /2$ ，因此MLA的KV Cache大小约等于2.25组的GQA，但效果比MHA还好。4种方法的KV Cache比较如下：
![](https://pica.zhimg.com/v2-d430566c2808a14252515aff17852d14_1440w.jpg)